=== PHD PROMPT LOG ===

Interaction 1:
\n~~~~~~~~~~\nHistory: \n~~~~~~~~~~\nCurrent Step #0, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: \nNotes: \nYour previous command was: . Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 2:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\n~~~~~~~~~~\nCurrent Step #1, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n\nNotes: \nYour previous command was: ```SUMMARY\nreal-time edge image classification optimization\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 3:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\n~~~~~~~~~~\nCurrent Step #2, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2409.08943v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 4:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #2, Phase: literature review, Feedback: 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co, Your response: ```SUMMARY\nedge device image classification optimization\n```\n~~~~~~~~~~\nCurrent Step #3, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification optimization\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 5:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #2, Phase: literature review, Feedback: 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co, Your response: ```SUMMARY\nedge device image classification optimization\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\n~~~~~~~~~~\nCurrent Step #4, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance.\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2307.04443v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 6:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #2, Phase: literature review, Feedback: 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co, Your response: ```SUMMARY\nedge device image classification optimization\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #4, Phase: literature review, Feedback: 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance., Your response: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, accuracy, and denoising performance. Key experimental results show their NAS-designed models outperform manually designed alternatives, achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency. The integrated approach demonstrates superior denoising quality compared to sequential models while maintaining competitive classification performance, making it particularly suitable for applications requiring human verification like surveillance and medical imaging.\n```\n~~~~~~~~~~\nCurrent Step #5, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2409.08943v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, accuracy, and denoising performance. Key experimental results show their NAS-designed models outperform manually designed alternatives, achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency. The integrated approach demonstrates superior denoising quality compared to sequential models while maintaining competitive classification performance, making it particularly suitable for applications requiring human verification like surveillance and medical imaging.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 7:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #4, Phase: literature review, Feedback: 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance., Your response: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, accuracy, and denoising performance. Key experimental results show their NAS-designed models outperform manually designed alternatives, achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency. The integrated approach demonstrates superior denoising quality compared to sequential models while maintaining competitive classification performance, making it particularly suitable for applications requiring human verification like surveillance and medical imaging.\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\n~~~~~~~~~~\nCurrent Step #6, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2307.04443v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 8:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #4, Phase: literature review, Feedback: 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance., Your response: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, accuracy, and denoising performance. Key experimental results show their NAS-designed models outperform manually designed alternatives, achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency. The integrated approach demonstrates superior denoising quality compared to sequential models while maintaining competitive classification performance, making it particularly suitable for applications requiring human verification like surveillance and medical imaging.\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\n~~~~~~~~~~\nCurrent Step #7, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification latency optimization\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 9:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\n~~~~~~~~~~\nCurrent Step #8, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Latency-aware Spatial-wise Dynamic NetworksYizeng Han1\u2217Zhihang Yuan2\u2217 Yifan Pu1\u2217 Chenhao Xue2Shiji Song1 Guangyu Sun2 Gao Huang1\u20201 Department of Automation, BNRist, Tsinghua University, Beijing, China2 School of Electronics Engineering and Computer Science, Peking University, Beijing, China{hanyz18, pyf20}@mails.tsinghua.edu.cn, {shijis, gaohuang}@tsinghua.edu.cn{yuanzhihang, xch927027, gsun}@pku.edu.cnAbstractSpatial-wise dynamic convolution has become a promising approach to improvingthe inference ef\ufb01ciency of deep networks. By allocating more computation to themost informative pixels, such an adaptive inference paradigm reduces the spatialredundancy in image features and saves a considerable amount of unnecessarycomputation. However, the theoretical ef\ufb01ciency achieved by previous methodscan hardly translate into a realistic speedup, especially on the multi-core processors(e.g. GPUs). The key challenge is that the existing literature has only focusedon designing algorithms with minimal computation, ignoring the fact that thepractical latency can also be in\ufb02uenced by scheduling strategies and hardwareproperties. To bridge the gap between theoretical computation and practical ef-\ufb01ciency, we propose a latency-aware spatial-wise dynamic network (LASNet),which performs coarse-grained spatially adaptive inference under the guidanceof a novel latency prediction model. The latency prediction model can ef\ufb01cientlyestimate the inference latency of dynamic networks by simultaneously consideringalgorithms, scheduling strategies, and hardware properties. We use the latencypredictor to guide both the algorithm design and the scheduling optimization onvarious hardware platforms. Experiments on image classi\ufb01cation, object detectionand instance segmentation demonstrate that the proposed framework signi\ufb01cantlyimproves the practical inference ef\ufb01ciency of deep networks. For example, theaverage latency of a ResNet-101 on the ImageNet validation set could be reducedby 36% and 46% on a server GPU (Nvidia Tesla-V100) and an edge device (NvidiaJetson TX2 GPU) respectively without sacri\ufb01cing the accuracy. Code is availableat https://github.com/LeapLabTHU/LASNet.1 IntroductionDynamic neural networks [7] have attracted great research interests in recent years. Compared tostatic models [11, 17, 13, 23] which treat different inputs equally during inference, dynamic networkscan allocate the computation in a data-dependent manner. For example, they can conditionally skipthe computation of network layers [ 15, 9, 32, 30] or convolutional channels [ 19, 1], or performspatially adaptive inference on the most informative image regions ( e.g. the foreground areas)[6, 5, 31, 35, 33, 8]. Spatial-wise dynamic networks, which typically decide whether to computeeach feature pixel with masker modules [5, 31, 35, 8] (Figure 1 (a)), have shown promising results inimproving the inference ef\ufb01ciency of convolution neural networks (CNNs).Despite the remarkable theoretical ef\ufb01ciency achieved by spatial-wise dynamic networks [5, 31, 35],researchers have found it challenging to translate the theoretical results into realistic speedup,\u2217Equal contribution.\u2020Corresponding author.36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.06223v1  [cs.CV]  12 Oct 2022--- Page 2 ---Maskergather ...convolution(s) scatter...Dynamic Convolution(s) AlgorithmSchedulingHardwareLatency(a) Algorithm: Spatially adaptive inference with a masker.(b) Scheduling: the practical inference pipeline of dynamic convolutions.(c) The general idea of our work.in\ufb02uence guidanceselected pixel locationsFigure 1: An overview of our method. (a) illustrates the spatially adaptive inference algorithm; (b) isthe scheduling strategy; and (c) presents the three key factors to the practical latency. For a givenhardware, the latency is used to guide our algorithm design and scheduling optimization.especially on some multi-core processors, e.g., GPUs [35, 3, 8]. The challenges are two-fold: 1) mostprevious approaches [5, 31, 35] perform spatially adaptive inference at the \ufb01nest granularity: everypixel is \ufb02exibly decided whether to be computed or not. Such \ufb02exibility induces non-contiguousmemory access [35] and requires specialized scheduling strategies (Figure 1 (b)); 2) the existingliterature has only adopted the hardware-agnostic FLOPs (\ufb02oating-point operations) as an inaccurateproxy for the ef\ufb01ciency, lacking latency-aware guidance on the algorithm design. For dynamicnetworks, the adaptive computation with sub-optimal scheduling strategies further enlarges thediscrepancy between the theoretical FLOPs and the practical latency. Note that it has been validatedby previous works that the latency on CPUs has a strong correlation with FLOPs [8, 35]. Therefore,we mainly focus on the GPU platform in this paper, which is more challenging and less explored.We address the above challenges by proposing a latency-aware spatial-wise dynamic network(LASNet). Three key factors to the inference latency are considered: the algorithm, the schedulingstrategy, and the hardware properties. Given a target hardware device, we directly use the latency,rather than the FLOPs, to guide our algorithm design and scheduling optimization (Figure 1 (c)).Because the memory access pattern and the scheduling strategies in our dynamic operators differfrom those in static networks, the libraries developed for static models (e.g. cuDNN) are sub-optimalfor the acceleration of dynamic models. Without the support of libraries, each dynamic operatorrequires scheduling optimization, code optimization, compiling, and deployment for each device.Therefore, it is laborious to evaluate the network latency on different hardware platforms. To this end,we propose a novel latency prediction model to ef\ufb01ciently estimate the realistic latency of a networkby simultaneously considering the aforementioned three factors. Compared to the hardware-agnosticFLOPs, our predicted latency can better re\ufb02ect the practical ef\ufb01ciency of dynamic models.Guided by this latency prediction model, we establish our latency-aware spatial-wise dynamic network(LASNet), which adaptively decides whether to allocate computation on feature patches insteadof pixels [5, 31, 35] (Figure 2 top). We name this paradigm as spatially adaptive inference at acoarse granularity. While less \ufb02exible than the pixel-level adaptive computation in previous works[5, 31, 35], it facilitates more contiguous memory access, bene\ufb01ting the realistic speedup on hardware.The scheduling strategy and the implementation are further ameliorated for faster inference.It is worth noting that LASNet is designed as a general framework in two aspects: 1) the coarse-grained spatially adaptive inference paradigm can be conveniently implemented in various CNNbackbones, e.g., ResNets [11], DenseNets [17] and RegNets [25]; and 2) the latency predictor is anoff-the-shell tool which can be directly used for various computing platforms (e.g. server GPUs andedge devices).We evaluate the performance of LASNet on multiple CNN architectures on image classi\ufb01cation,object detection, and instance segmentation tasks. Experiment results show that our LASNet improvesthe ef\ufb01ciency of deep CNNs both theoretically and practically. For example, the inference latencyof ResNet-101 on ImageNet [ 4] is reduced by 36% and 46% on an Nvidia Tesla V100 GPU andan Nvidia Jetson TX2 GPU, respectively, without sacri\ufb01cing the accuracy. Moreover, the proposedmethod outperforms various lightweight networks in a low-FLOPs regime.Our main contributions are summarized as follows:2--- Page 3 ---1. We propose LASNet, which performs coarse-grained spatially adaptive inference guidedby the practical latency instead of the theoretical FLOPs. To the best of our knowledge,LASNet is the \ufb01rst framework that directly considers the real latency in the design phase ofdynamic neural networks;2. We propose a latency prediction model, which can ef\ufb01ciently and accurately estimate thelatency of dynamic operators by simultaneously considering the algorithm, the schedulingstrategy, and the hardware properties;3. Experiments on image classi\ufb01cation and downstream tasks verify that our proposed LAS-Net can effectively improve the practical ef\ufb01ciency of different CNN architectures.2 Related worksSpatial-wise dynamic network is a common type of dynamic neural networks [ 7]. Compared tostatic models which treat different feature locations evenly during inference, these networks performspatially adaptive inference on the most informative regions ( e.g., foregrounds), and reduce theunnecessary computation on less important areas (e.g., backgrounds). Existing works mainly includethree levels of dynamic computation: resolution level [ 36, 37], region level [ 33] and pixel level[5, 31, 35]. The former two generally manipulate the network inputs [ 33, 37] or require specialarchitecture design [36]. In contrast, pixel-level dynamic networks can \ufb02exibly skip the convolutionson certain feature pixels in arbitrary CNN backbones [5, 31, 35]. Despite its remarkable theoreticalef\ufb01ciency, pixel-wise dynamic computation brings considerable dif\ufb01culty to achieving realisticspeedup on multi-core processors, e.g., GPUs. Compared to the previous approaches [ 5, 31, 35]which only focus on reducing the theoretical computation, we propose to directly use the latency toguide our algorithm design and scheduling optimization.Hardware-aware network design. To bridge the gap between theoretical and practical ef\ufb01ciencyof deep models, researchers have started to consider the real latency in the network design phase.There are two lines of works in this direction. One directly performs speed tests on targeteddevices, and summarizes some guidelines to facilitate hand-designing lightweight models [23]. Theother line of work searches for fast models using the neural architecture search (NAS) technique[29, 34]. However, all existing works try to build static models, which have intrinsic computationalredundancy by treating different inputs in the same way. However, speed tests for dynamic operatorson different hardware devices can be very laborious and impractical. In contrast, our proposed latencyprediction model can ef\ufb01ciently estimate the inference latency on any given computing platforms bysimultaneously considering algorithm design, scheduling strategies and hardware properties.3 MethodologyIn this section, we \ufb01rst introduce the preliminaries of spatially adaptive inference, and then demon-strate the architecture design of our LASNet. The latency prediction model is then explained, whichguides the granularity settings and the scheduling optimization for LASNet. We further present theimplementation improvements for faster inference, followed by the training strategies.3.1 PreliminariesSpatially adaptive inference. The existing spatial-wise dynamic networks are usually establishedby attaching a masker Min each convolutional block of a CNN backbone (Figure 1 (a)). Speci\ufb01cally,let x\u2208RH\xd7W\xd7C denote the input of a block, where H and W are the feature height and width, andC is the channel number. The masker Mtakes x as input, and generates a binary-valued spatialmask M = M(x) \u2208{0,1}H\xd7W . Each element of M determines whether to perform convolutionoperations on the corresponding location of the output feature. The unselected regions will be \ufb01lledwith the values from the input [5, 31] or obtained via interpolation [35]. We de\ufb01ne the activation rateof a block as r=\u2211i,j Mi,jH\xd7W , representing the ratio of the calculated pixels.Scheduling strategy. During inference, the current scheduling strategy for spatial-wise dynamicconvolutions generally involve three steps [26] (Figure 1 (b)): 1) gathering, which re-organizes theselected pixels (if the convolution kernel size is greater than 1 \xd71, the neighbors are also required)along the batch dimension; 2) computation, which performs convolution on the gathered input; and3) scattering, which \ufb01lls the computed pixels on their corresponding locations of the output feature.3--- Page 4 ---MaskerMasker-Conv1x1 Conv1x1 ...Scatter-Add Conv1x1 Conv3x3 Gumbel NoiseT raining Inference Input OutputUpsample Element-wise  multiplication Selected pixelGather-Conv3x3 Conv1x1 Regular convolution Operator fusionInput OutputFigure 2: Our proposed LASNet block. Top: we \ufb01rst generate a low-resolution spatial mask Mcoarse,which is then upsampled to obtain the mask M with the same size as the output feature. GumbelSoftmax [18, 24] is used for end-to-end training (Sec. 3.5). Bottom: the scheduling optimization isperformed to decrease the memory access for faster inference (Sec. 3.4).Limitations. Compared to performing convolutions on the entire feature map, the aforementionedscheduling strategy reduces the computation while bringing considerable overhead to memory accessdue to the mask generation and the non-contiguous memory access. Such overhead would increasethe overall latency, especially when the granularity of dynamic convolution is at the \ufb01nest pixel level.3.2 Architecture designSpatial granularity. As mentioned above, pixel-level dynamic convolutions [5, 31, 35] raise sub-stantial challenges to achieving realistic speedup on multi-core processors due to the non-contiguousmemory access. To this end, we propose to optimize the granularity of spatially adaptive inference.Speci\ufb01cally, take the commonly used bottleneck structure in [11] as an example, our coarse-grainedspatial-wise dynamic convolutional block is illustrated in Figure 2. Instead of directly producinga mask with the shape of H\xd7W, we \ufb01rst generate a low-resolution mask Mcoarse \u2208{0,1}HS \xd7WS ,where Sis named as the spatial granularity. Each element in Mcoarse determines the computationof an S\xd7S-sized feature patch. For instance, the feature size in the \ufb01rst ResNet stage3 is 56 \xd756.Then the possible choices for Sare {1,2,4,7,8,14,28,56}.The mask Mcoarse is then upsampledto the size of H\xd7W. Notably, S = 1means that the granularity is still at the pixel level as previousmethods [5, 31, 35]. Note that the other extreme situation (S = 56) is not considered in this paper,when the masker directly determines whether to skip the whole block (i.e. layer skipping [30, 32]).Such an overly aggressive approach will lead to considerable drop of accuracy, as we presented inAppendix C.2. The masker is composed of a pooling layer followed by a 1 \xd71 convolution.Differences to existing works. Without using the interpolation operation [ 35] or the carefullydesigned two-branch structure [8], the proposed block architecture is simple and suf\ufb01ciently generalto be plugged into most backbones with minimal modi\ufb01cation. Our formulation is mostly similar tothat in [31], which could be viewed as a variant of our method with the spatial granularity S=1 for allblocks. Instead of performing spatially adaptive inference at the \ufb01nest pixel level, our granularity Sisoptimized under the guidance of our latency prediction model (details are presented in the followingSec. 4.2) to achieve realistic speedup on target computing platforms.3.3 Latency prediction modelAs stated before, it is laborious to evaluate the latency of dynamic operators on different hardwareplatforms. To ef\ufb01ciently seek preferable granularity settings on arbitrary hardware devices, wepropose a latency prediction model G, which can directly predict the delay of executing dynamicoperators on any target devices. For a spatial-wise dynamic convolutional block, the latency predictorGtakes the hardware propertiesH, the layer parametersP, the spatial granularityS, and the activationrate ras input and predicts the latency \u2113of a dynamic convolutional block: \u2113= G(H,P,S,r ).Hardware modeling. We model a hardware device as multiple processing engines (PEs), and parallelcomputation can be executed on these PEs. As shown in Figure 3, we model the memory system as a3Here we refer to a stage as the cascading of multiple blocks which process features with the same resolution.4--- Page 5 ---Off-chip Memory(GDDR/DDR/HBM)On-chip Memory(LLC/Global Buffer)PE1PEMemoryComputation UnitsPE2PEMemoryComputation UnitsPEnPEMemoryComputation Units\u2026Figure 3: Our hardware model.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)101520253035Latency ( s)PredictedReal Figure 4: Latency prediction results.three-level structure [12]: 1) off-chip memory, 2) on-chip global memory, and 3) memory in PE. Sucha hardware model enables us to accurately predict the cost on both data movement and computation.Latency prediction. When simulating the data movement procedure, the ef\ufb01ciency of non-contiguous memory accesses under different granularity Ssettings is considered. As for the com-putation latency, it is important to adopt a proper scheduling strategy to increase the parallelism ofcomputation. Therefore, we search for the optimal scheduling (the con\ufb01guration of tiling and in-PEparallel computing) of dynamic operations to maximize the utilization of hardware resources. A moredetailed description of our latency prediction model is presented in Appendix A.Empirical validation. We take the \ufb01rst block in ResNet-101 as an example and vary the activationrate rto evaluate the performance of our prediction model. The comparison between our predictionsand the real testing latency on the Nvidia V100 GPU is illustrated in Figure 4, from which we canobserve that our predictor can accurately estimate the real latency in a wide range of activation rates.3.4 Implementation detailsWe use general optimization methods like fusing activation functions and batch normalizationlayers into convolution layers. We also optimize the speci\ufb01c operators in our spatial-wise dynamicconvolutional blocks as follows (see also Figure 2 for an overview).Fusing the masker and the \ufb01rst convolution. As mentioned in Sec. 3.1, the masker in each blockconsumes very little computation, but it takes the whole feature map as input. Therefore, it is amemory-bounded operation (the inference time is mainly spent on memory access). Since the maskerand the \ufb01rst convolution in the block share the same input, there is an opportunity to fuse these twooperations to avoid the repeated access of the input data. Note that a spatial-wise dynamic convolutionrequires the output of the masker. If we fuse the two layers, the \ufb01rst convolution will be changed to astatic operation, which may increase the inference latency. There exists a threshold of activation raterth, when r>r th, the overall latency can be reduced. We decide whether to fuse them according tothe average activation rate. See more details in Appendix B.1.Fusing the gather operation and the dynamic convolution. Traditional approaches \ufb01rst gather theinput pixels of the \ufb01rst dynamic convolution in a block (Figure 1 (b)). The gather operation is alsoa memory-bounded operation. Furthermore, when the size of the convolution kernel exceeds 1\xd71,the area of input patches may overlap, resulting in repeated memory load/store. We fuse the gatheroperation into the dynamic convolution to reduce the memory access.Fusing the scatter operation and the add operation. Traditional approaches scatter the outputpixels of the last dynamic convolution, and then execute the element-wise addition (Figure 1 (b)). Wefuse these two operators to reduce the memory access. The ablation study in Sec. 4.4 validates theeffectiveness of the proposed fusing methods.3.5 TrainingOptimization of non-differentiable maskers. The masker modules are required to produce binary-valued spatial masks for making discrete decisions, and cannot be directly optimized with backpropagation. Following [35, 31, 8], we adopt straight-through Gumbel Softmax [18, 24] to train thenetwork in an end-to-end fashion. Speci\ufb01cally, let \u02dcM \u2208RH\xd7W\xd72 denote the output of the maskgenerator. The decisions are obtained with the argmax function during inference. In the trainingphase, a differentiable approximation is de\ufb01ned by replacing the argmax operation with a Softmax:\u02c6M =exp{(log(\u02dcM:,:,0)+ G:,:,0)/\u03c4}\u22111k=0 exp{(log(\u02dcM:,:,k)+ G:,:,k)/\u03c4} \u2208 [0,1]H\xd7W , (1)5--- Page 6 ---0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1(a) Relationship between r\u2113 and rfor LAS-ResNet blocks on the Nvidia Tesla V100 GPU.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.75Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.752.00Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.4Latency ratio (r )S = 1(b) Relationship between r\u2113 and rfor LAS-RegNetY-800MF blocks on the Nvidia Jetson TX2 GPU.Figure 5: Latency prediction results of LAS-ResNet blocks on V100 (a) and LAS-RegNet blocks onTX2 (b). For both networks, we plot the relationship between the latency ratio r\u2113 and the activationrate r for the blocks in 4 stages with the convolutional stride 1. The practical ef\ufb01ciency is onlyimproved when r\u2113 <1. Note that S = 1can harm the practical latency even for a small r(reducedcomputation), while a larger Swill alleviate this problem. See detailed analysis in Sec. 4.2.where \u03c4 is the Softmax temperature. Following the common practice [ 31, 8], we let \u03c4 decreaseexponentially from 5.0 to 0.1 in training to facilitate the optimization of maskers.Training objective. The FLOPs of each spatial-wise dynamic convolutional block can be calculatedbased on our de\ufb01ned activation rate r[31]. Then we can obtain the FLOPs of the overall dynamicnetwork Fdyn. Let Fstat denotes the FLOPs of its static counterpart. We optimize their ratio toapproximate a target 0 <t< 1:LFLOPs = (FdynFstat\u2212t)2.In addition, we de\ufb01ne loss item Lbounds asin [31] to constrain the upper bound and the lower bound of activation rates in early training epochs.We further propose to leverage the static counterparts of our dynamic networks as \u201cteachers\u201d to guidethe optimization procedure. Let y and y\u2032denote the output logits of a dynamic model (\u201cstudent\u201d)and its \u201cteacher\u201d, respectively. Our \ufb01nal loss can be written asL= Ltask + \u03b1(LFLOPs + Lbounds) +\u03b2T2 \xb7KL(\u03c3(y/T)||\u03c3(y\u2032/T)), (2)where Ltask represents the task-related loss, e.g., cross-entropy loss in image classi\ufb01cation. KL(\xb7||\xb7)denotes the Kullback\u2013Leibler divergence, and \u03b1,\u03b2 are the coef\ufb01cients balancing these items. We use\u03c3to denote the log-Softmax function, and T is the temperature for computing KL-divergence.4 ExperimentsIn this section, we \ufb01rst introduce the experiment settings in Sec. 4.1. Then the latency of differentgranularity settings are analyzed in Sec. 4.2. The performance of our LASNet on ImageNet is furtherevaluated in Sec. 4.3, followed by the ablation studies in Sec. 4.4. Visualization results are illustratedin Sec. 4.5, and we \ufb01nally validate our method on the object detection task (Sec. 4.6). The results onthe instance segmentation task are presented in . For simplicity, we add \u201cLAS-\u201d as a pre\ufb01x beforemodel names to denote our LASNet, e.g., LAS-ResNet-50.4.1 Experiment settingsLatency prediction. Various types of hardware platforms are tested, including a server GPU (TeslaV100), a desktop GPU (GTX1080) and edge devices ( e.g., Nvidia Nano and Jetson TX2). Themajor properties considered by our latency prediction model include the number of processingengines (#PE), the \ufb02oating-point computation in a processing engine (#FP32), the frequency and thebandwidth. It can be observed that the server GPUs generally have a larger #PE than the IoT devices.The batch size is set as 1 for all dynamic models and computing platforms.Image classi\ufb01cation. The image classi\ufb01cation experiments are conducted on the ImageNet [ 4]dataset. Following [31], we initialize the backbone parameter from a pre-trained checkpoint4, and4We use the torchvision pre-trained models at https://pytorch.org/vision/stable/models.html.6--- Page 7 ---1 2 4 7 8 14 28Spatial granularity (S)12345Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.51.01.52.02.53.03.54.0Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.51.01.52.02.53.0Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(a) Relationship between r\u2113 and Sfor LAS-ResNet blocks on V100.1 2 4 7 8 14 28Spatial granularity (S)0.250.500.751.001.251.501.75Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.250.500.751.001.251.501.752.00Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.20.40.60.81.01.21.41.6Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(b) Relationship between r\u2113 and Sfor LAS-RegNetY-800MF blocks on Nvidia Jetson TX2 GPU.Figure 6: The relationship between the latency ratio r\u2113 and the spatial granularity S.\ufb01netune the whole network for 100 epochs with the loss function in Eq. (2). We \ufb01x \u03b1= 10,\u03b2 = 0.5and T = 4.0 for all dynamic models. More details are provided in Appendix B.4.2 Latency prediction resultsIn this subsection, we present the latency prediction results of the spatial-wise dynamic convolutionalblocks in two different models: LAS-ResNet-101 [11] (on V100) and LAS-RegNetY-800MF [25] (onTX2). All the blocks have the bottleneck structure with different channel numbers and convolutiongroups, and the RegNetY is equipped with Squeeze-and-Excitation (SE) [14] modules.We \ufb01rst de\ufb01ne \u2113dyn as the latency of a spatial-wise dynamic convolutional block, and \u2113stat as that ofa static block without a masker. Their ratio is denoted as r\u2113 = \u2113dyn\u2113stat. We investigate the relationshipbetween r\u2113 and the activation rate r(cf. Sec. 3.5) for different granularity settings. The results inFigure 5 demonstrate that: 1) even equipped with our special optimization on the scheduling strategies,pixel-level spatially adaptive inference (S=1) cannot always improve the practical ef\ufb01ciency. Such\ufb01ne-grained adaptive inference is adopted by most previous works [31, 35], and our result can explainthe reason why they can only achieve realistic speedup on less powerful CPUs [35] or specializeddevices [3]; 2) a proper granularity S >1 effectively alleviates this problem on both hardwaredevices. By setting S >1, realistic speedup could be achieved with larger activation rates.The latency prediction results are further used to seek for a preferable granularity setting for the\ufb01rst 3 stages (we \ufb01x S = 1 for the last stage, where the feature resolution is 7 \xd77). Therefore,we plot the relationship between r\u2113 and S in Figure 10. It can be observed that: 1) r\u2113 generallydecreases with Sincreasing for a given r; 2) an overly largeS(less \ufb02exible adaptive inference) bringsinsigni\ufb01cant improvement on both devices. Especially, enlarging S from 8 to 28 in the \ufb01rst stageof a LAS-ResNet brings very little improvement on V100. Based on the results in Figure 10, wecan trade off between \ufb02exibility and ef\ufb01ciency by selecting appropriate Sfor different models andhardware devices. For example, we can simply set Snet=8-4-7-15 in a LAS-ResNet-101 to achiverealistic speedup. The accuracy-latency plots in Figure 7 also validate this observation. More resultsof our latency prediction model on the desktop-level GPU, GTX1080, are presented in Appendix C.1.4.3 ImageNet classi\ufb01cation resultsWe now empirically evaluate our proposed LASNet on the ImageNet dataset. The network perfor-mance is measured in terms of the trade-off between classi\ufb01cation accuracy and inference ef\ufb01ciency.Both theoretical (i.e. FLOPs) and practical ef\ufb01ciency (i.e. latency) are tested in our experiments.5We use this form to represnet the Ssettings for the 4 stages of a network.7--- Page 8 ---2 3 4 5 6 7 8FLOPs (x1e9)73747576777879Accuracy (%)static ResNetsConvAIG-ResNet-101SkipNet-ResNet-101BAS-ResNet-50DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)15 20 25 30 35 40Latency on Nvidia Jetson TX2 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)1.5 2.0 2.5 3.0 3.5Latency on Nvidia Tesla V100 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)(a) LAS-ResNet results.0.2 0.3 0.4 0.5 0.6 0.7 0.8FLOPs (x1e9)717273747576Accuracy (%)RegNets-XRegNets-YMobileNets-v2ShuffleNets-v2CondenseNetsLAS-RegNetY-400MF (Snet=1-1-1-1)LAS-RegNetY-400MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)3 4 5 6 7 8Latency on Nvidia Jetson TX2 GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-XRegNets-YMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)8 10 12 14 16 18 20Latency on Nvidia nano GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-YRegNets-XMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)(b) LAS-RegNetY results.Figure 7: Experimental results on ImageNet. The proposed coarse-grained spatially adaptive inferenceis tested on standard ResNets (a) and lightweight RegNets (b).4.3.1 Standard baseline comparison: ResNetsWe \ufb01rst establish our LASNet based on the standard ResNets [ 11]. Speci\ufb01cally, we build LAS-ResNet-50 and LAS-ResNet-101 by plugging our maskers in the two common ResNet structures.The baselines include various types of dynamic inference approaches: 1) layer skipping (SkipNet[32] and Conv-AIG [30]); 2) channel skipping (BAS [1]); and 3) pixel-level spatial-wise dynamicnetwork (DynConv [31]). For our LASNet, we compare various settings of the spatial granularitySnet. We set training targets (cf. Sec. 3.5) t\u2208{0,4,0.5,0.6,0.7}for our dynamic models to evaluatetheir performance in different sparsity regimes. We apply the same operator fusion (Sec. 3.4) for bothour models and the compared baselines [30, 31] for fair comparison.Results are presented in Figure 7 (a). On the left we plot the relationship of accuracy v.s. FLOPs. Itcan be observed that our LAS-ResNets with different granularity settings signi\ufb01cantly outperformthe competing dynamic neural networks. Surprisingly, coarse-grained spatially adaptive inference(Snet=4-4-2-1 and Snet=8-4-7-1 for the 4 stages) can achieve even higher accuracy when consumingsimilar FLOPs on ResNets, despite the sacri\ufb01ced \ufb02exibility compared toSnet=1-1-1-1. We conjecturethat a larger Sis also bene\ufb01cial to the optimization of maskers.We compare the practical latency of three granularity settings in Figure 7 (a) predicted by our latencyprediction model (middle on TX2 and right on V100). We can witness that although they achievecomparable theoretical ef\ufb01ciency (Figure 7 (a) left), larger Sis more hardware-friendly comparedto the \ufb01nest granularity. For example, the inference latency of LAS-ResNet-101 ( Snet=1-1-1-1)is signi\ufb01cantly higher than the ResNet-101 baseline on V100 (Figure 7 (a) right), even though itstheoretical computation is much smaller than that of the static model. However, larger granularities(Snet=4-4-2-1 and Snet=8-4-7-1) can effectively improve the inference latency due to its lower burdenon the memory access. Remarkably, the latency of ResNet-101 could be reduced by 36% and 46% onV100 and TX2 respectively without sacri\ufb01cing the accuracy when t=0.4. The classi\ufb01cation accuracyis increased by 1.9% with similar inference ef\ufb01ciency. It can be observed that the realistic speedupratio r\u2113 is more close to the theoretical FLOPs ratio target ton the less powerful TX2, because thelatency is computation-bounded (i.e. the latency is mainly spent on computation) on such IoT devices.In contrast, there is a larger gap between practical and theoretical ef\ufb01ciency on the more powerfulV100, as the latency is bounded by the memory access cost.4.3.2 Lightweight baseline comparison: RegNetsWe further evaluate our LASNet in lightweight CNN architectures,i.e. RegNets-Y [25]. Two differentsized models are tested: RegNetY-400MF and RegNetY-800MF. Compared baselines include othertypes of ef\ufb01cient models, e.g., MobileNets-v2 [28], Shuf\ufb02etNets-v2 [23] and CondenseNets [16].The results are presented in Figure 7 (b). The x-axis for the three sub-\ufb01gures are the FLOPs, thelatency on TX2, and the latency on the Nvidia nano GPU, respectively. We can observe that our8--- Page 9 ---1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2Latency on Nvidia Tesla V100 GPU (ms)76.577.077.578.078.579.0Accuracy (%) static ResNetsLAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=4-4-7-1)LAS-ResNet-101 (Snet=8-4-7-1)Figure 8: Ablation studies on S. Figure 9: Visualization results.method outperforms various types of static models in terms of the trade-off between accuracy andef\ufb01ciency. More results on image classi\ufb01cation are provided in Appendix C.2.4.4 Ablation studiesWe conduct ablation studies to validate the effectiveness of our coarse-grained spatially adaptiveinference (Sec. 3.2) and operator fusion operations (Sec. 3.4).More granularities settings. We test various granularity settings on LAS-ResNet-101 to examinethe effects of Sin different stages. The results on the Tesla-V100 GPU are presented in Figure 8. Itcan be found that the \ufb01nest granularity (Snet=1-1-1-1) leads to substantial inef\ufb01ciency despite thereduced FLOPs (cf. Figure 7 (a) left). Coarse-grained spatially adaptive inference in the \ufb01rst twostages (Snet=4-4-1-1) effectively reduces the inference latency. We further increase Sin the thirdstage to 2 and 7, and this procedure consistently improves the realistic ef\ufb01ciency on the V100 GPU.This trend also holds on the GTX 1080 GPU (see the results in Appendix C.2.Table 1: Ablation studies on operator fusion.Masker- Gather- Scatter- LatencyConv1x1 Conv3x3 Add (\xb5s)   163.2   90.1   86.7   71.4Operator fusion. We investigate the effect of ouroperator fusion introduced in Sec. 3.4. One convolu-tional block in the \ufb01rst stage of a LAS-ResNet-101(S=4, r=0.6) is tested. The results in Table 1 vali-date that every step of operator fusion bene\ufb01ts thepractical latency of a block, as the overhead onmemory access is effectively reduced. Especially,the fusion of the masker operation and the \ufb01rst con-volution is crucial to reducing the latency.4.5 VisualizationWe visualize the masks generated by our masker in the third block of a LAS-ResNet-101 (Snet=4-4-2-1) in Figure 12. The brilliant areas correspond to the locations of 1 elements in a mask, andthe computation on the dimmed regions is skipped by our dynamic model. We can witness that themasker accurately locate the most task-related regions (even the tiny aircraft at the corner), whichhelps reduce the unnecessary computation on background areas. These resultsalso suggest that forthe \ufb01rst stage, the granularity S=4 is suf\ufb01ciently \ufb02exible to recognize the important regions, anda win-win can be achieved between accuracy and ef\ufb01ciency. Interestingly, the masker could selectsome objects that are not labeled for the sample, e.g., the \ufb02ower beside the hummingbird and thehuman holding the camera. This indicates that our spatial-wise dynamic networks can automaticallyrecognize the regions with semantics, and their capability is not limited by the classi\ufb01cation labels.This property is helpful in some downstream tasks, such as object detection (Sec. 4.6) and instancesegmentation (Sec. 4.7), which require detecting multiple classes and objects in an image. Morevisualization results could be found in Appendix C.3.4.6 COCO Object detectionWe further evaluate our LASNet on the COCO [22] object detection task. The mean average precision(mAP), the average backbone FLOPs, and the average backbone latency on the validation set areused to measure the network performance. We test two commonly used detection frameworks: FasterR-CNN [27] with Feature Pyramid Network [20] and RetinaNet [21]. Thanks to the generality ofour method, we can conveniently replace the backbones with ours pre-trained on ImageNet, andthe whole models are \ufb01netuned on COCO with the standard setting for 12 epochs (see detailed9--- Page 10 ---Table 2: Object detection results on the COCO dataset.Detection Backbone Backbone Backbone Latency (ms)mAP (%)Framework FLOPs (G) V100 GTX1080 TX2Faster R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 39.4LAS-ResNet-101 (Snet=4-4-2-1,t=0.6) 90.7 33.8 90.6 524.9 40.3LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 79.3 30.7 82.5 477.1 39.8LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.5 29.0 79.9 464.8 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 67.9 25.3 69.1 401.3 39.5RetinaNetResNet-101 (Baseline) 141.2 39.5 119.3 729.4 38.5LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 77.8 30.4 81.8 472.6 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.4 28.9 79.9 464.8 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 66.4 25.3 69.1 401.3 38.9Table 3: Instance Segmentation results on the COCO dataset.Segmentation Backbone Backbone Backbone Latency (ms)APmask(%) APbox(%)Framework FLOPs (G) V100 GTX1080 TX2Mask R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 36.1 40.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 80.5 31.1 83.3 481.9 37.0 41.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.4) 69.2 27.9 74.8 431.6 36.1 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 68.8 25.8 70.9 411.8 36.2 40.0setup in Appendix B.3). The input images are resized to a short side of 800 and a long side notexceeding 1333. The results of our LAS-ResNet-101 with different Snet settings are presented inTable 2. We can observe from the results that when setting the training target as 0.4, the latency ofour LAS-ResNet-101 with Snet=4-4-7-1 is signi\ufb01cantly lower than the static baseline on all deviceswithout sacri\ufb01cing mAP in both detection frameworks. With a larger training targets, our LASNetcan increase the mAP by 0.9% and 0.8% in Faster R-CNN and RetinaNet respectively, while stillbeing faster than the baseline method.4.7 COCO instance segmentationWe also present the results of instance segmentation on COCO, which demonstrate the effectivenessof our LASNet on the dense prediction task. From the results in Table 3, we can observe that whensetting the training target as 0.4, the Mask R-CNN [ 10] models (Snet=4-4-2-1 and Snet=4-4-7-1)runs faster on all tested hardware devices without sacri\ufb01cing the performance. With a training targetof 0.5, the APmask and APbox of the Mask R-CNN model could be increased by 0.9% and 1.0%respectively while still running faster than the baseline method.5 ConclusionIn this paper, we propose to build latency-aware spatial-wise dynamic networks (LASNet) under theguidance of a latency prediction model. By simultaneously considering the algorithm, the schedulingstrategy and the hardware properties, we can ef\ufb01ciently estimate the practical latency of spatial-wise dynamic operators on arbitrary computing platforms. Based on the empirical analysis on therelationship between the latency and the granularity of spatially adaptive inference, we optimizeboth the algorithm and the scheduling strategies to achieve realistic speedup on many multi-coreprocessors, e.g., the Tesla V100 GPU and the Jetson TX2 GPU. Experiments on image classi\ufb01cation,object detection and instance segmentation tasks validate that the proposed method signi\ufb01cantlyimproves the practical ef\ufb01ciency of deep CNNs, and outperforms various competing approaches.AcknowledgementThis work is supported in part by the National Key R&D Program of China under Grant2020AAA0105200, the National Natural Science Foundation of China under Grants 62022048and the Tsinghua University-China Mobile Communications Group Co.,Ltd. Joint Institute..10--- Page 11 ---References[1] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping for learningconditional channel gated networks. In ICLR, 2020.[2] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, MeghanCowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. Tvm: An automated end-to-end optimizingcompiler for deep learning. In OSDI, 2018.[3] Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian Verhelst. Proces-sor architecture optimization for spatially dynamic neural networks. In VLSI-SoC, 2021.[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In CVPR, 2009.[5] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicatednetwork with less inference complexity. In CVPR, 2017.[6] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov,and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In CVPR,2017.[7] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamicneural networks: A survey. TPAMI, 2021.[8] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, and Haojun Jiang. Spatiallyadaptive feature re\ufb01nement for ef\ufb01cient inference. TIP, 2021.[9] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfen Cao, Wenhui Huang,Chao Deng, and Gao Huang. Learning to weight samples for dynamic early-exiting networks.In ECCV, 2022.[10] Kaiming He, Georgia Gkioxari, Piotr Doll\xe1r, and Ross Girshick. Mask r-cnn. In Proceedings ofthe IEEE international conference on computer vision, pages 2961\u20132969, 2017.[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In CVPR, 2016.[12] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach .Elsevier, 2011.[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, TobiasWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ef\ufb01cient convolutional neuralnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.[15] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Wein-berger. Multi-scale dense networks for resource ef\ufb01cient image classi\ufb01cation. In ICLR, 2018.[16] Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: Anef\ufb01cient densenet using learned group convolutions. In CVPR, 2018.[17] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Weinberger.Densely connected convolutional networks. TPAMI, 2019.[18] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.In ICLR, 2017.[19] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In NeurIPS, 2017.[20] Tsung-Yi Lin, Piotr Doll\xe1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.Feature pyramid networks for object detection. In CVPR, 2017.[21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Focal loss for denseobject detection. In ICCV, 2017.11--- Page 12 ---[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,Piotr Doll\xe1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,2014.[23] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shuf\ufb02enet v2: Practical guidelinesfor ef\ufb01cient cnn architecture design. In ECCV, 2018.[24] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuousrelaxation of discrete random variables. In ICLR, 2017.[25] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Design-ing network design spaces. In CVPR, 2020.[26] Mengye Ren, Andrei Pokrovsky, Bin Yang, and Raquel Urtasun. SBNet: Sparse Blocks Networkfor Fast Inference. CVPR, 2018.[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-timeobject detection with region proposal networks. In NeurIPS, 2015.[28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.[29] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, andQuoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820\u20132828,2019.[30] Andreas Veit and Serge Belongie. Convolutional networks with adaptive inference graphs. InECCV, 2018.[31] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: Exploiting Spatial Sparsity forFaster Inference. In CVPR, 2020.[32] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learningdynamic routing in convolutional networks. In ECCV, 2018.[33] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus:a dynamic approach to reducing spatial redundancy in image classi\ufb01cation. In NeurIPS, 2020.[34] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, YuandongTian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware ef\ufb01cient convnetdesign via differentiable neural architecture search. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 10734\u201310742, 2019.[35] Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptiveinference with stochastic feature sampling and interpolation. In ECCV, 2020.[36] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptivenetworks for ef\ufb01cient inference. In CVPR, 2020.[37] Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan, and Yunhe Wang.Dynamic resolution network. NeurIPS, 2021.12--- Page 13 ---AppendixA Latency prediction model.As the dynamic operators in our method have not been supported by current deep learning libraries,we propose a latency prediction model to ef\ufb01ciently estimate the real latency of these operators onhardware device. The inputs of the latency prediction model include: 1) the structural con\ufb01gurationof a convolutional block, 2) its activation rate rwhich decides the computation amount, 3) the spatialgranularity S, and 4) the hardware properties mentioned in Table 4. The latency of a dynamic blockis predicted as follows.Input/output shape de\ufb01nition. The \ufb01rst step of predicting the latency of an operation is to calculatethe shape of input and output. Taking the gather-conv2 operation as an example, the input of thisoperation is the activation with the shape of Cin \xd7H \xd7W, where Cin is the number of inputchannels, and H and W are the resolution of the feature map. The shape of the output tensor isP \xd7Cout \xd7S\xd7S, where P is the number of output patches, Cout is the number of output channelsand Sis the spatial granularity. Note that P is obtained based on the output of our maskers.Operation-to-hardware mapping. Next, we map the operations to hardware. As is mentioned in thepaper, we model a hardware device as multiple processing engines (PEs). We assign the computationof each element in the output feature map to a PE. Speci\ufb01cally, we consecutively split the outputfeature map into multiple tiles. The shape of each tile is TP \xd7TC \xd7TS1 \xd7TS2. These split tilesare assigned to multiple PEs. The computation of the elements in each tile is executed in a PE. Wecan con\ufb01gure different shapes of tiles. In order to determine the optimal shape of the tile, we makea search space of different tile shapes. The tile shape has 4 dimensions. The candidates of eachdimension are power-of-2 and do not exceed the corresponding dimension of the feature map.Latency estimation. Then, we evaluate the latency of each tile shape in the search space and selectthe optimal tile shape with the lowest latency. The latency includes the data movement latency andthe computation latency: \u2113= \u2113data + \u2113computation.1) Data movement latency \u2113data. The estimation of the latency for data movement requires us tomodel the memory system of a hardware device. We model the memory system of hardware as athree-level architecture [12]: off-chip memory, on-chip global memory, and local memory in PE.The input data and weight data are \ufb01rst transferred from the off-chip memory to the on-chip globalmemory. We assume the hardware can make full use of the off-chip memory bandwidth to simplifythe latency prediction model.After that, the data used to compute the output tiles is transferred from on-chip global memory tothe local m\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2210.06223v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 10:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #8, Phase: literature review, Feedback: 3\n--- Page 1 ---Latency-aware Spatial-wise Dynamic NetworksYizeng Han1\u2217Zhihang Yuan2\u2217 Yifan Pu1\u2217 Chenhao Xue2Shiji Song1 Guangyu Sun2 Gao Huang1\u20201 Department of Automation, BNRist, Tsinghua University, Beijing, China2 School of Electronics Engineering and Computer Science, Peking University, Beijing, China{hanyz18, pyf20}@mails.tsinghua.edu.cn, {shijis, gaohuang}@tsinghua.edu.cn{yuanzhihang, xch927027, gsun}@pku.edu.cnAbstractSpatial-wise dynamic convolution has become a promising approach to improvingthe inference ef\ufb01ciency of deep networks. By allocating more computation to themost informative pixels, such an adaptive inference paradigm reduces the spatialredundancy in image features and saves a considerable amount of unnecessarycomputation. However, the theoretical ef\ufb01ciency achieved by previous methodscan hardly translate into a realistic speedup, especially on the multi-core processors(e.g. GPUs). The key challenge is that the existing literature has only focusedon designing algorithms with minimal computation, ignoring the fact that thepractical latency can also be in\ufb02uenced by scheduling strategies and hardwareproperties. To bridge the gap between theoretical computation and practical ef-\ufb01ciency, we propose a latency-aware spatial-wise dynamic network (LASNet),which performs coarse-grained spatially adaptive inference under the guidanceof a novel latency prediction model. The latency prediction model can ef\ufb01cientlyestimate the inference latency of dynamic networks by simultaneously consideringalgorithms, scheduling strategies, and hardware properties. We use the latencypredictor to guide both the algorithm design and the scheduling optimization onvarious hardware platforms. Experiments on image classi\ufb01cation, object detectionand instance segmentation demonstrate that the proposed framework signi\ufb01cantlyimproves the practical inference ef\ufb01ciency of deep networks. For example, theaverage latency of a ResNet-101 on the ImageNet validation set could be reducedby 36% and 46% on a server GPU (Nvidia Tesla-V100) and an edge device (NvidiaJetson TX2 GPU) respectively without sacri\ufb01cing the accuracy. Code is availableat https://github.com/LeapLabTHU/LASNet.1 IntroductionDynamic neural networks [7] have attracted great research interests in recent years. Compared tostatic models [11, 17, 13, 23] which treat different inputs equally during inference, dynamic networkscan allocate the computation in a data-dependent manner. For example, they can conditionally skipthe computation of network layers [ 15, 9, 32, 30] or convolutional channels [ 19, 1], or performspatially adaptive inference on the most informative image regions ( e.g. the foreground areas)[6, 5, 31, 35, 33, 8]. Spatial-wise dynamic networks, which typically decide whether to computeeach feature pixel with masker modules [5, 31, 35, 8] (Figure 1 (a)), have shown promising results inimproving the inference ef\ufb01ciency of convolution neural networks (CNNs).Despite the remarkable theoretical ef\ufb01ciency achieved by spatial-wise dynamic networks [5, 31, 35],researchers have found it challenging to translate the theoretical results into realistic speedup,\u2217Equal contribution.\u2020Corresponding author.36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.06223v1  [cs.CV]  12 Oct 2022--- Page 2 ---Maskergather ...convolution(s) scatter...Dynamic Convolution(s) AlgorithmSchedulingHardwareLatency(a) Algorithm: Spatially adaptive inference with a masker.(b) Scheduling: the practical inference pipeline of dynamic convolutions.(c) The general idea of our work.in\ufb02uence guidanceselected pixel locationsFigure 1: An overview of our method. (a) illustrates the spatially adaptive inference algorithm; (b) isthe scheduling strategy; and (c) presents the three key factors to the practical latency. For a givenhardware, the latency is used to guide our algorithm design and scheduling optimization.especially on some multi-core processors, e.g., GPUs [35, 3, 8]. The challenges are two-fold: 1) mostprevious approaches [5, 31, 35] perform spatially adaptive inference at the \ufb01nest granularity: everypixel is \ufb02exibly decided whether to be computed or not. Such \ufb02exibility induces non-contiguousmemory access [35] and requires specialized scheduling strategies (Figure 1 (b)); 2) the existingliterature has only adopted the hardware-agnostic FLOPs (\ufb02oating-point operations) as an inaccurateproxy for the ef\ufb01ciency, lacking latency-aware guidance on the algorithm design. For dynamicnetworks, the adaptive computation with sub-optimal scheduling strategies further enlarges thediscrepancy between the theoretical FLOPs and the practical latency. Note that it has been validatedby previous works that the latency on CPUs has a strong correlation with FLOPs [8, 35]. Therefore,we mainly focus on the GPU platform in this paper, which is more challenging and less explored.We address the above challenges by proposing a latency-aware spatial-wise dynamic network(LASNet). Three key factors to the inference latency are considered: the algorithm, the schedulingstrategy, and the hardware properties. Given a target hardware device, we directly use the latency,rather than the FLOPs, to guide our algorithm design and scheduling optimization (Figure 1 (c)).Because the memory access pattern and the scheduling strategies in our dynamic operators differfrom those in static networks, the libraries developed for static models (e.g. cuDNN) are sub-optimalfor the acceleration of dynamic models. Without the support of libraries, each dynamic operatorrequires scheduling optimization, code optimization, compiling, and deployment for each device.Therefore, it is laborious to evaluate the network latency on different hardware platforms. To this end,we propose a novel latency prediction model to ef\ufb01ciently estimate the realistic latency of a networkby simultaneously considering the aforementioned three factors. Compared to the hardware-agnosticFLOPs, our predicted latency can better re\ufb02ect the practical ef\ufb01ciency of dynamic models.Guided by this latency prediction model, we establish our latency-aware spatial-wise dynamic network(LASNet), which adaptively decides whether to allocate computation on feature patches insteadof pixels [5, 31, 35] (Figure 2 top). We name this paradigm as spatially adaptive inference at acoarse granularity. While less \ufb02exible than the pixel-level adaptive computation in previous works[5, 31, 35], it facilitates more contiguous memory access, bene\ufb01ting the realistic speedup on hardware.The scheduling strategy and the implementation are further ameliorated for faster inference.It is worth noting that LASNet is designed as a general framework in two aspects: 1) the coarse-grained spatially adaptive inference paradigm can be conveniently implemented in various CNNbackbones, e.g., ResNets [11], DenseNets [17] and RegNets [25]; and 2) the latency predictor is anoff-the-shell tool which can be directly used for various computing platforms (e.g. server GPUs andedge devices).We evaluate the performance of LASNet on multiple CNN architectures on image classi\ufb01cation,object detection, and instance segmentation tasks. Experiment results show that our LASNet improvesthe ef\ufb01ciency of deep CNNs both theoretically and practically. For example, the inference latencyof ResNet-101 on ImageNet [ 4] is reduced by 36% and 46% on an Nvidia Tesla V100 GPU andan Nvidia Jetson TX2 GPU, respectively, without sacri\ufb01cing the accuracy. Moreover, the proposedmethod outperforms various lightweight networks in a low-FLOPs regime.Our main contributions are summarized as follows:2--- Page 3 ---1. We propose LASNet, which performs coarse-grained spatially adaptive inference guidedby the practical latency instead of the theoretical FLOPs. To the best of our knowledge,LASNet is the \ufb01rst framework that directly considers the real latency in the design phase ofdynamic neural networks;2. We propose a latency prediction model, which can ef\ufb01ciently and accurately estimate thelatency of dynamic operators by simultaneously considering the algorithm, the schedulingstrategy, and the hardware properties;3. Experiments on image classi\ufb01cation and downstream tasks verify that our proposed LAS-Net can effectively improve the practical ef\ufb01ciency of different CNN architectures.2 Related worksSpatial-wise dynamic network is a common type of dynamic neural networks [ 7]. Compared tostatic models which treat different feature locations evenly during inference, these networks performspatially adaptive inference on the most informative regions ( e.g., foregrounds), and reduce theunnecessary computation on less important areas (e.g., backgrounds). Existing works mainly includethree levels of dynamic computation: resolution level [ 36, 37], region level [ 33] and pixel level[5, 31, 35]. The former two generally manipulate the network inputs [ 33, 37] or require specialarchitecture design [36]. In contrast, pixel-level dynamic networks can \ufb02exibly skip the convolutionson certain feature pixels in arbitrary CNN backbones [5, 31, 35]. Despite its remarkable theoreticalef\ufb01ciency, pixel-wise dynamic computation brings considerable dif\ufb01culty to achieving realisticspeedup on multi-core processors, e.g., GPUs. Compared to the previous approaches [ 5, 31, 35]which only focus on reducing the theoretical computation, we propose to directly use the latency toguide our algorithm design and scheduling optimization.Hardware-aware network design. To bridge the gap between theoretical and practical ef\ufb01ciencyof deep models, researchers have started to consider the real latency in the network design phase.There are two lines of works in this direction. One directly performs speed tests on targeteddevices, and summarizes some guidelines to facilitate hand-designing lightweight models [23]. Theother line of work searches for fast models using the neural architecture search (NAS) technique[29, 34]. However, all existing works try to build static models, which have intrinsic computationalredundancy by treating different inputs in the same way. However, speed tests for dynamic operatorson different hardware devices can be very laborious and impractical. In contrast, our proposed latencyprediction model can ef\ufb01ciently estimate the inference latency on any given computing platforms bysimultaneously considering algorithm design, scheduling strategies and hardware properties.3 MethodologyIn this section, we \ufb01rst introduce the preliminaries of spatially adaptive inference, and then demon-strate the architecture design of our LASNet. The latency prediction model is then explained, whichguides the granularity settings and the scheduling optimization for LASNet. We further present theimplementation improvements for faster inference, followed by the training strategies.3.1 PreliminariesSpatially adaptive inference. The existing spatial-wise dynamic networks are usually establishedby attaching a masker Min each convolutional block of a CNN backbone (Figure 1 (a)). Speci\ufb01cally,let x\u2208RH\xd7W\xd7C denote the input of a block, where H and W are the feature height and width, andC is the channel number. The masker Mtakes x as input, and generates a binary-valued spatialmask M = M(x) \u2208{0,1}H\xd7W . Each element of M determines whether to perform convolutionoperations on the corresponding location of the output feature. The unselected regions will be \ufb01lledwith the values from the input [5, 31] or obtained via interpolation [35]. We de\ufb01ne the activation rateof a block as r=\u2211i,j Mi,jH\xd7W , representing the ratio of the calculated pixels.Scheduling strategy. During inference, the current scheduling strategy for spatial-wise dynamicconvolutions generally involve three steps [26] (Figure 1 (b)): 1) gathering, which re-organizes theselected pixels (if the convolution kernel size is greater than 1 \xd71, the neighbors are also required)along the batch dimension; 2) computation, which performs convolution on the gathered input; and3) scattering, which \ufb01lls the computed pixels on their corresponding locations of the output feature.3--- Page 4 ---MaskerMasker-Conv1x1 Conv1x1 ...Scatter-Add Conv1x1 Conv3x3 Gumbel NoiseT raining Inference Input OutputUpsample Element-wise  multiplication Selected pixelGather-Conv3x3 Conv1x1 Regular convolution Operator fusionInput OutputFigure 2: Our proposed LASNet block. Top: we \ufb01rst generate a low-resolution spatial mask Mcoarse,which is then upsampled to obtain the mask M with the same size as the output feature. GumbelSoftmax [18, 24] is used for end-to-end training (Sec. 3.5). Bottom: the scheduling optimization isperformed to decrease the memory access for faster inference (Sec. 3.4).Limitations. Compared to performing convolutions on the entire feature map, the aforementionedscheduling strategy reduces the computation while bringing considerable overhead to memory accessdue to the mask generation and the non-contiguous memory access. Such overhead would increasethe overall latency, especially when the granularity of dynamic convolution is at the \ufb01nest pixel level.3.2 Architecture designSpatial granularity. As mentioned above, pixel-level dynamic convolutions [5, 31, 35] raise sub-stantial challenges to achieving realistic speedup on multi-core processors due to the non-contiguousmemory access. To this end, we propose to optimize the granularity of spatially adaptive inference.Speci\ufb01cally, take the commonly used bottleneck structure in [11] as an example, our coarse-grainedspatial-wise dynamic convolutional block is illustrated in Figure 2. Instead of directly producinga mask with the shape of H\xd7W, we \ufb01rst generate a low-resolution mask Mcoarse \u2208{0,1}HS \xd7WS ,where Sis named as the spatial granularity. Each element in Mcoarse determines the computationof an S\xd7S-sized feature patch. For instance, the feature size in the \ufb01rst ResNet stage3 is 56 \xd756.Then the possible choices for Sare {1,2,4,7,8,14,28,56}.The mask Mcoarse is then upsampledto the size of H\xd7W. Notably, S = 1means that the granularity is still at the pixel level as previousmethods [5, 31, 35]. Note that the other extreme situation (S = 56) is not considered in this paper,when the masker directly determines whether to skip the whole block (i.e. layer skipping [30, 32]).Such an overly aggressive approach will lead to considerable drop of accuracy, as we presented inAppendix C.2. The masker is composed of a pooling layer followed by a 1 \xd71 convolution.Differences to existing works. Without using the interpolation operation [ 35] or the carefullydesigned two-branch structure [8], the proposed block architecture is simple and suf\ufb01ciently generalto be plugged into most backbones with minimal modi\ufb01cation. Our formulation is mostly similar tothat in [31], which could be viewed as a variant of our method with the spatial granularity S=1 for allblocks. Instead of performing spatially adaptive inference at the \ufb01nest pixel level, our granularity Sisoptimized under the guidance of our latency prediction model (details are presented in the followingSec. 4.2) to achieve realistic speedup on target computing platforms.3.3 Latency prediction modelAs stated before, it is laborious to evaluate the latency of dynamic operators on different hardwareplatforms. To ef\ufb01ciently seek preferable granularity settings on arbitrary hardware devices, wepropose a latency prediction model G, which can directly predict the delay of executing dynamicoperators on any target devices. For a spatial-wise dynamic convolutional block, the latency predictorGtakes the hardware propertiesH, the layer parametersP, the spatial granularityS, and the activationrate ras input and predicts the latency \u2113of a dynamic convolutional block: \u2113= G(H,P,S,r ).Hardware modeling. We model a hardware device as multiple processing engines (PEs), and parallelcomputation can be executed on these PEs. As shown in Figure 3, we model the memory system as a3Here we refer to a stage as the cascading of multiple blocks which process features with the same resolution.4--- Page 5 ---Off-chip Memory(GDDR/DDR/HBM)On-chip Memory(LLC/Global Buffer)PE1PEMemoryComputation UnitsPE2PEMemoryComputation UnitsPEnPEMemoryComputation Units\u2026Figure 3: Our hardware model.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)101520253035Latency ( s)PredictedReal Figure 4: Latency prediction results.three-level structure [12]: 1) off-chip memory, 2) on-chip global memory, and 3) memory in PE. Sucha hardware model enables us to accurately predict the cost on both data movement and computation.Latency prediction. When simulating the data movement procedure, the ef\ufb01ciency of non-contiguous memory accesses under different granularity Ssettings is considered. As for the com-putation latency, it is important to adopt a proper scheduling strategy to increase the parallelism ofcomputation. Therefore, we search for the optimal scheduling (the con\ufb01guration of tiling and in-PEparallel computing) of dynamic operations to maximize the utilization of hardware resources. A moredetailed description of our latency prediction model is presented in Appendix A.Empirical validation. We take the \ufb01rst block in ResNet-101 as an example and vary the activationrate rto evaluate the performance of our prediction model. The comparison between our predictionsand the real testing latency on the Nvidia V100 GPU is illustrated in Figure 4, from which we canobserve that our predictor can accurately estimate the real latency in a wide range of activation rates.3.4 Implementation detailsWe use general optimization methods like fusing activation functions and batch normalizationlayers into convolution layers. We also optimize the speci\ufb01c operators in our spatial-wise dynamicconvolutional blocks as follows (see also Figure 2 for an overview).Fusing the masker and the \ufb01rst convolution. As mentioned in Sec. 3.1, the masker in each blockconsumes very little computation, but it takes the whole feature map as input. Therefore, it is amemory-bounded operation (the inference time is mainly spent on memory access). Since the maskerand the \ufb01rst convolution in the block share the same input, there is an opportunity to fuse these twooperations to avoid the repeated access of the input data. Note that a spatial-wise dynamic convolutionrequires the output of the masker. If we fuse the two layers, the \ufb01rst convolution will be changed to astatic operation, which may increase the inference latency. There exists a threshold of activation raterth, when r>r th, the overall latency can be reduced. We decide whether to fuse them according tothe average activation rate. See more details in Appendix B.1.Fusing the gather operation and the dynamic convolution. Traditional approaches \ufb01rst gather theinput pixels of the \ufb01rst dynamic convolution in a block (Figure 1 (b)). The gather operation is alsoa memory-bounded operation. Furthermore, when the size of the convolution kernel exceeds 1\xd71,the area of input patches may overlap, resulting in repeated memory load/store. We fuse the gatheroperation into the dynamic convolution to reduce the memory access.Fusing the scatter operation and the add operation. Traditional approaches scatter the outputpixels of the last dynamic convolution, and then execute the element-wise addition (Figure 1 (b)). Wefuse these two operators to reduce the memory access. The ablation study in Sec. 4.4 validates theeffectiveness of the proposed fusing methods.3.5 TrainingOptimization of non-differentiable maskers. The masker modules are required to produce binary-valued spatial masks for making discrete decisions, and cannot be directly optimized with backpropagation. Following [35, 31, 8], we adopt straight-through Gumbel Softmax [18, 24] to train thenetwork in an end-to-end fashion. Speci\ufb01cally, let \u02dcM \u2208RH\xd7W\xd72 denote the output of the maskgenerator. The decisions are obtained with the argmax function during inference. In the trainingphase, a differentiable approximation is de\ufb01ned by replacing the argmax operation with a Softmax:\u02c6M =exp{(log(\u02dcM:,:,0)+ G:,:,0)/\u03c4}\u22111k=0 exp{(log(\u02dcM:,:,k)+ G:,:,k)/\u03c4} \u2208 [0,1]H\xd7W , (1)5--- Page 6 ---0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1(a) Relationship between r\u2113 and rfor LAS-ResNet blocks on the Nvidia Tesla V100 GPU.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.75Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.752.00Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.4Latency ratio (r )S = 1(b) Relationship between r\u2113 and rfor LAS-RegNetY-800MF blocks on the Nvidia Jetson TX2 GPU.Figure 5: Latency prediction results of LAS-ResNet blocks on V100 (a) and LAS-RegNet blocks onTX2 (b). For both networks, we plot the relationship between the latency ratio r\u2113 and the activationrate r for the blocks in 4 stages with the convolutional stride 1. The practical ef\ufb01ciency is onlyimproved when r\u2113 <1. Note that S = 1can harm the practical latency even for a small r(reducedcomputation), while a larger Swill alleviate this problem. See detailed analysis in Sec. 4.2.where \u03c4 is the Softmax temperature. Following the common practice [ 31, 8], we let \u03c4 decreaseexponentially from 5.0 to 0.1 in training to facilitate the optimization of maskers.Training objective. The FLOPs of each spatial-wise dynamic convolutional block can be calculatedbased on our de\ufb01ned activation rate r[31]. Then we can obtain the FLOPs of the overall dynamicnetwork Fdyn. Let Fstat denotes the FLOPs of its static counterpart. We optimize their ratio toapproximate a target 0 <t< 1:LFLOPs = (FdynFstat\u2212t)2.In addition, we de\ufb01ne loss item Lbounds asin [31] to constrain the upper bound and the lower bound of activation rates in early training epochs.We further propose to leverage the static counterparts of our dynamic networks as \u201cteachers\u201d to guidethe optimization procedure. Let y and y\u2032denote the output logits of a dynamic model (\u201cstudent\u201d)and its \u201cteacher\u201d, respectively. Our \ufb01nal loss can be written asL= Ltask + \u03b1(LFLOPs + Lbounds) +\u03b2T2 \xb7KL(\u03c3(y/T)||\u03c3(y\u2032/T)), (2)where Ltask represents the task-related loss, e.g., cross-entropy loss in image classi\ufb01cation. KL(\xb7||\xb7)denotes the Kullback\u2013Leibler divergence, and \u03b1,\u03b2 are the coef\ufb01cients balancing these items. We use\u03c3to denote the log-Softmax function, and T is the temperature for computing KL-divergence.4 ExperimentsIn this section, we \ufb01rst introduce the experiment settings in Sec. 4.1. Then the latency of differentgranularity settings are analyzed in Sec. 4.2. The performance of our LASNet on ImageNet is furtherevaluated in Sec. 4.3, followed by the ablation studies in Sec. 4.4. Visualization results are illustratedin Sec. 4.5, and we \ufb01nally validate our method on the object detection task (Sec. 4.6). The results onthe instance segmentation task are presented in . For simplicity, we add \u201cLAS-\u201d as a pre\ufb01x beforemodel names to denote our LASNet, e.g., LAS-ResNet-50.4.1 Experiment settingsLatency prediction. Various types of hardware platforms are tested, including a server GPU (TeslaV100), a desktop GPU (GTX1080) and edge devices ( e.g., Nvidia Nano and Jetson TX2). Themajor properties considered by our latency prediction model include the number of processingengines (#PE), the \ufb02oating-point computation in a processing engine (#FP32), the frequency and thebandwidth. It can be observed that the server GPUs generally have a larger #PE than the IoT devices.The batch size is set as 1 for all dynamic models and computing platforms.Image classi\ufb01cation. The image classi\ufb01cation experiments are conducted on the ImageNet [ 4]dataset. Following [31], we initialize the backbone parameter from a pre-trained checkpoint4, and4We use the torchvision pre-trained models at https://pytorch.org/vision/stable/models.html.6--- Page 7 ---1 2 4 7 8 14 28Spatial granularity (S)12345Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.51.01.52.02.53.03.54.0Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.51.01.52.02.53.0Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(a) Relationship between r\u2113 and Sfor LAS-ResNet blocks on V100.1 2 4 7 8 14 28Spatial granularity (S)0.250.500.751.001.251.501.75Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.250.500.751.001.251.501.752.00Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.20.40.60.81.01.21.41.6Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(b) Relationship between r\u2113 and Sfor LAS-RegNetY-800MF blocks on Nvidia Jetson TX2 GPU.Figure 6: The relationship between the latency ratio r\u2113 and the spatial granularity S.\ufb01netune the whole network for 100 epochs with the loss function in Eq. (2). We \ufb01x \u03b1= 10,\u03b2 = 0.5and T = 4.0 for all dynamic models. More details are provided in Appendix B.4.2 Latency prediction resultsIn this subsection, we present the latency prediction results of the spatial-wise dynamic convolutionalblocks in two different models: LAS-ResNet-101 [11] (on V100) and LAS-RegNetY-800MF [25] (onTX2). All the blocks have the bottleneck structure with different channel numbers and convolutiongroups, and the RegNetY is equipped with Squeeze-and-Excitation (SE) [14] modules.We \ufb01rst de\ufb01ne \u2113dyn as the latency of a spatial-wise dynamic convolutional block, and \u2113stat as that ofa static block without a masker. Their ratio is denoted as r\u2113 = \u2113dyn\u2113stat. We investigate the relationshipbetween r\u2113 and the activation rate r(cf. Sec. 3.5) for different granularity settings. The results inFigure 5 demonstrate that: 1) even equipped with our special optimization on the scheduling strategies,pixel-level spatially adaptive inference (S=1) cannot always improve the practical ef\ufb01ciency. Such\ufb01ne-grained adaptive inference is adopted by most previous works [31, 35], and our result can explainthe reason why they can only achieve realistic speedup on less powerful CPUs [35] or specializeddevices [3]; 2) a proper granularity S >1 effectively alleviates this problem on both hardwaredevices. By setting S >1, realistic speedup could be achieved with larger activation rates.The latency prediction results are further used to seek for a preferable granularity setting for the\ufb01rst 3 stages (we \ufb01x S = 1 for the last stage, where the feature resolution is 7 \xd77). Therefore,we plot the relationship between r\u2113 and S in Figure 10. It can be observed that: 1) r\u2113 generallydecreases with Sincreasing for a given r; 2) an overly largeS(less \ufb02exible adaptive inference) bringsinsigni\ufb01cant improvement on both devices. Especially, enlarging S from 8 to 28 in the \ufb01rst stageof a LAS-ResNet brings very little improvement on V100. Based on the results in Figure 10, wecan trade off between \ufb02exibility and ef\ufb01ciency by selecting appropriate Sfor different models andhardware devices. For example, we can simply set Snet=8-4-7-15 in a LAS-ResNet-101 to achiverealistic speedup. The accuracy-latency plots in Figure 7 also validate this observation. More resultsof our latency prediction model on the desktop-level GPU, GTX1080, are presented in Appendix C.1.4.3 ImageNet classi\ufb01cation resultsWe now empirically evaluate our proposed LASNet on the ImageNet dataset. The network perfor-mance is measured in terms of the trade-off between classi\ufb01cation accuracy and inference ef\ufb01ciency.Both theoretical (i.e. FLOPs) and practical ef\ufb01ciency (i.e. latency) are tested in our experiments.5We use this form to represnet the Ssettings for the 4 stages of a network.7--- Page 8 ---2 3 4 5 6 7 8FLOPs (x1e9)73747576777879Accuracy (%)static ResNetsConvAIG-ResNet-101SkipNet-ResNet-101BAS-ResNet-50DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)15 20 25 30 35 40Latency on Nvidia Jetson TX2 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)1.5 2.0 2.5 3.0 3.5Latency on Nvidia Tesla V100 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)(a) LAS-ResNet results.0.2 0.3 0.4 0.5 0.6 0.7 0.8FLOPs (x1e9)717273747576Accuracy (%)RegNets-XRegNets-YMobileNets-v2ShuffleNets-v2CondenseNetsLAS-RegNetY-400MF (Snet=1-1-1-1)LAS-RegNetY-400MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)3 4 5 6 7 8Latency on Nvidia Jetson TX2 GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-XRegNets-YMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)8 10 12 14 16 18 20Latency on Nvidia nano GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-YRegNets-XMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)(b) LAS-RegNetY results.Figure 7: Experimental results on ImageNet. The proposed coarse-grained spatially adaptive inferenceis tested on standard ResNets (a) and lightweight RegNets (b).4.3.1 Standard baseline comparison: ResNetsWe \ufb01rst establish our LASNet based on the standard ResNets [ 11]. Speci\ufb01cally, we build LAS-ResNet-50 and LAS-ResNet-101 by plugging our maskers in the two common ResNet structures.The baselines include various types of dynamic inference approaches: 1) layer skipping (SkipNet[32] and Conv-AIG [30]); 2) channel skipping (BAS [1]); and 3) pixel-level spatial-wise dynamicnetwork (DynConv [31]). For our LASNet, we compare various settings of the spatial granularitySnet. We set training targets (cf. Sec. 3.5) t\u2208{0,4,0.5,0.6,0.7}for our dynamic models to evaluatetheir performance in different sparsity regimes. We apply the same operator fusion (Sec. 3.4) for bothour models and the compared baselines [30, 31] for fair comparison.Results are presented in Figure 7 (a). On the left we plot the relationship of accuracy v.s. FLOPs. Itcan be observed that our LAS-ResNets with different granularity settings signi\ufb01cantly outperformthe competing dynamic neural networks. Surprisingly, coarse-grained spatially adaptive inference(Snet=4-4-2-1 and Snet=8-4-7-1 for the 4 stages) can achieve even higher accuracy when consumingsimilar FLOPs on ResNets, despite the sacri\ufb01ced \ufb02exibility compared toSnet=1-1-1-1. We conjecturethat a larger Sis also bene\ufb01cial to the optimization of maskers.We compare the practical latency of three granularity settings in Figure 7 (a) predicted by our latencyprediction model (middle on TX2 and right on V100). We can witness that although they achievecomparable theoretical ef\ufb01ciency (Figure 7 (a) left), larger Sis more hardware-friendly comparedto the \ufb01nest granularity. For example, the inference latency of LAS-ResNet-101 ( Snet=1-1-1-1)is signi\ufb01cantly higher than the ResNet-101 baseline on V100 (Figure 7 (a) right), even though itstheoretical computation is much smaller than that of the static model. However, larger granularities(Snet=4-4-2-1 and Snet=8-4-7-1) can effectively improve the inference latency due to its lower burdenon the memory access. Remarkably, the latency of ResNet-101 could be reduced by 36% and 46% onV100 and TX2 respectively without sacri\ufb01cing the accuracy when t=0.4. The classi\ufb01cation accuracyis increased by 1.9% with similar inference ef\ufb01ciency. It can be observed that the realistic speedupratio r\u2113 is more close to the theoretical FLOPs ratio target ton the less powerful TX2, because thelatency is computation-bounded (i.e. the latency is mainly spent on computation) on such IoT devices.In contrast, there is a larger gap between practical and theoretical ef\ufb01ciency on the more powerfulV100, as the latency is bounded by the memory access cost.4.3.2 Lightweight baseline comparison: RegNetsWe further evaluate our LASNet in lightweight CNN architectures,i.e. RegNets-Y [25]. Two differentsized models are tested: RegNetY-400MF and RegNetY-800MF. Compared baselines include othertypes of ef\ufb01cient models, e.g., MobileNets-v2 [28], Shuf\ufb02etNets-v2 [23] and CondenseNets [16].The results are presented in Figure 7 (b). The x-axis for the three sub-\ufb01gures are the FLOPs, thelatency on TX2, and the latency on the Nvidia nano GPU, respectively. We can observe that our8--- Page 9 ---1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2Latency on Nvidia Tesla V100 GPU (ms)76.577.077.578.078.579.0Accuracy (%) static ResNetsLAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=4-4-7-1)LAS-ResNet-101 (Snet=8-4-7-1)Figure 8: Ablation studies on S. Figure 9: Visualization results.method outperforms various types of static models in terms of the trade-off between accuracy andef\ufb01ciency. More results on image classi\ufb01cation are provided in Appendix C.2.4.4 Ablation studiesWe conduct ablation studies to validate the effectiveness of our coarse-grained spatially adaptiveinference (Sec. 3.2) and operator fusion operations (Sec. 3.4).More granularities settings. We test various granularity settings on LAS-ResNet-101 to examinethe effects of Sin different stages. The results on the Tesla-V100 GPU are presented in Figure 8. Itcan be found that the \ufb01nest granularity (Snet=1-1-1-1) leads to substantial inef\ufb01ciency despite thereduced FLOPs (cf. Figure 7 (a) left). Coarse-grained spatially adaptive inference in the \ufb01rst twostages (Snet=4-4-1-1) effectively reduces the inference latency. We further increase Sin the thirdstage to 2 and 7, and this procedure consistently improves the realistic ef\ufb01ciency on the V100 GPU.This trend also holds on the GTX 1080 GPU (see the results in Appendix C.2.Table 1: Ablation studies on operator fusion.Masker- Gather- Scatter- LatencyConv1x1 Conv3x3 Add (\xb5s)   163.2   90.1   86.7   71.4Operator fusion. We investigate the effect of ouroperator fusion introduced in Sec. 3.4. One convolu-tional block in the \ufb01rst stage of a LAS-ResNet-101(S=4, r=0.6) is tested. The results in Table 1 vali-date that every step of operator fusion bene\ufb01ts thepractical latency of a block, as the overhead onmemory access is effectively reduced. Especially,the fusion of the masker operation and the \ufb01rst con-volution is crucial to reducing the latency.4.5 VisualizationWe visualize the masks generated by our masker in the third block of a LAS-ResNet-101 (Snet=4-4-2-1) in Figure 12. The brilliant areas correspond to the locations of 1 elements in a mask, andthe computation on the dimmed regions is skipped by our dynamic model. We can witness that themasker accurately locate the most task-related regions (even the tiny aircraft at the corner), whichhelps reduce the unnecessary computation on background areas. These resultsalso suggest that forthe \ufb01rst stage, the granularity S=4 is suf\ufb01ciently \ufb02exible to recognize the important regions, anda win-win can be achieved between accuracy and ef\ufb01ciency. Interestingly, the masker could selectsome objects that are not labeled for the sample, e.g., the \ufb02ower beside the hummingbird and thehuman holding the camera. This indicates that our spatial-wise dynamic networks can automaticallyrecognize the regions with semantics, and their capability is not limited by the classi\ufb01cation labels.This property is helpful in some downstream tasks, such as object detection (Sec. 4.6) and instancesegmentation (Sec. 4.7), which require detecting multiple classes and objects in an image. Morevisualization results could be found in Appendix C.3.4.6 COCO Object detectionWe further evaluate our LASNet on the COCO [22] object detection task. The mean average precision(mAP), the average backbone FLOPs, and the average backbone latency on the validation set areused to measure the network performance. We test two commonly used detection frameworks: FasterR-CNN [27] with Feature Pyramid Network [20] and RetinaNet [21]. Thanks to the generality ofour method, we can conveniently replace the backbones with ours pre-trained on ImageNet, andthe whole models are \ufb01netuned on COCO with the standard setting for 12 epochs (see detailed9--- Page 10 ---Table 2: Object detection results on the COCO dataset.Detection Backbone Backbone Backbone Latency (ms)mAP (%)Framework FLOPs (G) V100 GTX1080 TX2Faster R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 39.4LAS-ResNet-101 (Snet=4-4-2-1,t=0.6) 90.7 33.8 90.6 524.9 40.3LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 79.3 30.7 82.5 477.1 39.8LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.5 29.0 79.9 464.8 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 67.9 25.3 69.1 401.3 39.5RetinaNetResNet-101 (Baseline) 141.2 39.5 119.3 729.4 38.5LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 77.8 30.4 81.8 472.6 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.4 28.9 79.9 464.8 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 66.4 25.3 69.1 401.3 38.9Table 3: Instance Segmentation results on the COCO dataset.Segmentation Backbone Backbone Backbone Latency (ms)APmask(%) APbox(%)Framework FLOPs (G) V100 GTX1080 TX2Mask R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 36.1 40.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 80.5 31.1 83.3 481.9 37.0 41.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.4) 69.2 27.9 74.8 431.6 36.1 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 68.8 25.8 70.9 411.8 36.2 40.0setup in Appendix B.3). The input images are resized to a short side of 800 and a long side notexceeding 1333. The results of our LAS-ResNet-101 with different Snet settings are presented inTable 2. We can observe from the results that when setting the training target as 0.4, the latency ofour LAS-ResNet-101 with Snet=4-4-7-1 is signi\ufb01cantly lower than the static baseline on all deviceswithout sacri\ufb01cing mAP in both detection frameworks. With a larger training targets, our LASNetcan increase the mAP by 0.9% and 0.8% in Faster R-CNN and RetinaNet respectively, while stillbeing faster than the baseline method.4.7 COCO instance segmentationWe also present the results of instance segmentation on COCO, which demonstrate the effectivenessof our LASNet on the dense prediction task. From the results in Table 3, we can observe that whensetting the training target as 0.4, the Mask R-CNN [ 10] models (Snet=4-4-2-1 and Snet=4-4-7-1)runs faster on all tested hardware devices without sacri\ufb01cing the performance. With a training targetof 0.5, the APmask and APbox of the Mask R-CNN model could be increased by 0.9% and 1.0%respectively while still running faster than the baseline method.5 ConclusionIn this paper, we propose to build latency-aware spatial-wise dynamic networks (LASNet) under theguidance of a latency prediction model. By simultaneously considering the algorithm, the schedulingstrategy and the hardware properties, we can ef\ufb01ciently estimate the practical latency of spatial-wise dynamic operators on arbitrary computing platforms. Based on the empirical analysis on therelationship between the latency and the granularity of spatially adaptive inference, we optimizeboth the algorithm and the scheduling strategies to achieve realistic speedup on many multi-coreprocessors, e.g., the Tesla V100 GPU and the Jetson TX2 GPU. Experiments on image classi\ufb01cation,object detection and instance segmentation tasks validate that the proposed method signi\ufb01cantlyimproves the practical ef\ufb01ciency of deep CNNs, and outperforms various competing approaches.AcknowledgementThis work is supported in part by the National Key R&D Program of China under Grant2020AAA0105200, the National Natural Science Foundation of China under Grants 62022048and the Tsinghua University-China Mobile Communications Group Co.,Ltd. Joint Institute..10--- Page 11 ---References[1] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping for learningconditional channel gated networks. In ICLR, 2020.[2] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, MeghanCowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. Tvm: An automated end-to-end optimizingcompiler for deep learning. In OSDI, 2018.[3] Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian Verhelst. Proces-sor architecture optimization for spatially dynamic neural networks. In VLSI-SoC, 2021.[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In CVPR, 2009.[5] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicatednetwork with less inference complexity. In CVPR, 2017.[6] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov,and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In CVPR,2017.[7] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamicneural networks: A survey. TPAMI, 2021.[8] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, and Haojun Jiang. Spatiallyadaptive feature re\ufb01nement for ef\ufb01cient inference. TIP, 2021.[9] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfen Cao, Wenhui Huang,Chao Deng, and Gao Huang. Learning to weight samples for dynamic early-exiting networks.In ECCV, 2022.[10] Kaiming He, Georgia Gkioxari, Piotr Doll\xe1r, and Ross Girshick. Mask r-cnn. In Proceedings ofthe IEEE international conference on computer vision, pages 2961\u20132969, 2017.[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In CVPR, 2016.[12] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach .Elsevier, 2011.[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, TobiasWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ef\ufb01cient convolutional neuralnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.[15] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Wein-berger. Multi-scale dense networks for resource ef\ufb01cient image classi\ufb01cation. In ICLR, 2018.[16] Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: Anef\ufb01cient densenet using learned group convolutions. In CVPR, 2018.[17] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Weinberger.Densely connected convolutional networks. TPAMI, 2019.[18] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.In ICLR, 2017.[19] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In NeurIPS, 2017.[20] Tsung-Yi Lin, Piotr Doll\xe1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.Feature pyramid networks for object detection. In CVPR, 2017.[21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Focal loss for denseobject detection. In ICCV, 2017.11--- Page 12 ---[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,Piotr Doll\xe1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,2014.[23] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shuf\ufb02enet v2: Practical guidelinesfor ef\ufb01cient cnn architecture design. In ECCV, 2018.[24] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuousrelaxation of discrete random variables. In ICLR, 2017.[25] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Design-ing network design spaces. In CVPR, 2020.[26] Mengye Ren, Andrei Pokrovsky, Bin Yang, and Raquel Urtasun. SBNet: Sparse Blocks Networkfor Fast Inference. CVPR, 2018.[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-timeobject detection with region proposal networks. In NeurIPS, 2015.[28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.[29] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, andQuoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820\u20132828,2019.[30] Andreas Veit and Serge Belongie. Convolutional networks with adaptive inference graphs. InECCV, 2018.[31] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: Exploiting Spatial Sparsity forFaster Inference. In CVPR, 2020.[32] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learningdynamic routing in convolutional networks. In ECCV, 2018.[33] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus:a dynamic approach to reducing spatial redundancy in image classi\ufb01cation. In NeurIPS, 2020.[34] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, YuandongTian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware ef\ufb01cient convnetdesign via differentiable neural architecture search. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 10734\u201310742, 2019.[35] Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptiveinference with stochastic feature sampling and interpolation. In ECCV, 2020.[36] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptivenetworks for ef\ufb01cient inference. In CVPR, 2020.[37] Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan, and Yunhe Wang.Dynamic resolution network. NeurIPS, 2021.12--- Page 13 ---AppendixA Latency prediction model.As the dynamic operators in our method have not been supported by current deep learning libraries,we propose a latency prediction model to ef\ufb01ciently estimate the real latency of these operators onhardware device. The inputs of the latency prediction model include: 1) the structural con\ufb01gurationof a convolutional block, 2) its activation rate rwhich decides the computation amount, 3) the spatialgranularity S, and 4) the hardware properties mentioned in Table 4. The latency of a dynamic blockis predicted as follows.Input/output shape de\ufb01nition. The \ufb01rst step of predicting the latency of an operation is to calculatethe shape of input and output. Taking the gather-conv2 operation as an example, the input of thisoperation is the activation with the shape of Cin \xd7H \xd7W, where Cin is the number of inputchannels, and H and W are the resolution of the feature map. The shape of the output tensor isP \xd7Cout \xd7S\xd7S, where P is the number of output patches, Cout is the number of output channelsand Sis the spatial granularity. Note that P is obtained based on the output of our maskers.Operation-to-hardware mapping. Next, we map the operations to hardware. As is mentioned in thepaper, we model a hardware device as multiple processing engines (PEs). We assign the computationof each element in the output feature map to a PE. Speci\ufb01cally, we consecutively split the outputfeature map into multiple tiles. The shape of each tile is TP \xd7TC \xd7TS1 \xd7TS2. These split tilesare assigned to multiple PEs. The computation of the elements in each tile is executed in a PE. Wecan con\ufb01gure different shapes of tiles. In order to determine the optimal shape of the tile, we makea search space of different tile shapes. The tile shape has 4 dimensions. The candidates of eachdimension are power-of-2 and do not exceed the corresponding dimension of the feature map.Latency estimation. Then, we evaluate the latency of each tile shape in the search space and selectthe optimal tile shape with the lowest latency. The latency includes the data movement latency andthe computation latency: \u2113= \u2113data + \u2113computation.1) Data movement latency \u2113data. The estimation of the latency for data movement requires us tomodel the memory system of a hardware device. We model the memory system of hardware as athree-level architecture [12]: off-chip memory, on-chip global memory, and local memory in PE.The input data and weight data are \ufb01rst transferred from the off-chip memory to the on-chip globalmemory. We assume the hardware can make full use of the off-chip memory bandwidth to simplifythe latency prediction model.After that, the data used to compute the output tiles is transferred from on-chip global memory tothe local m, Your response: ```ADD_PAPER\n2210.06223v1\nThis paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach guided by a novel latency prediction model that accounts for algorithm design, scheduling strategies, and hardware properties. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method achieves these gains through optimized spatial granularity selection and operator fusion techniques, demonstrating significant improvements over pixel-level dynamic networks in practical deployment scenarios.\n```\n~~~~~~~~~~\nCurrent Step #9, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2210.06223v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2210.06223v1\nThis paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach guided by a novel latency prediction model that accounts for algorithm design, scheduling strategies, and hardware properties. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method achieves these gains through optimized spatial granularity selection and operator fusion techniques, demonstrating significant improvements over pixel-level dynamic networks in practical deployment scenarios.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 11:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #8, Phase: literature review, Feedback: 3\n--- Page 1 ---Latency-aware Spatial-wise Dynamic NetworksYizeng Han1\u2217Zhihang Yuan2\u2217 Yifan Pu1\u2217 Chenhao Xue2Shiji Song1 Guangyu Sun2 Gao Huang1\u20201 Department of Automation, BNRist, Tsinghua University, Beijing, China2 School of Electronics Engineering and Computer Science, Peking University, Beijing, China{hanyz18, pyf20}@mails.tsinghua.edu.cn, {shijis, gaohuang}@tsinghua.edu.cn{yuanzhihang, xch927027, gsun}@pku.edu.cnAbstractSpatial-wise dynamic convolution has become a promising approach to improvingthe inference ef\ufb01ciency of deep networks. By allocating more computation to themost informative pixels, such an adaptive inference paradigm reduces the spatialredundancy in image features and saves a considerable amount of unnecessarycomputation. However, the theoretical ef\ufb01ciency achieved by previous methodscan hardly translate into a realistic speedup, especially on the multi-core processors(e.g. GPUs). The key challenge is that the existing literature has only focusedon designing algorithms with minimal computation, ignoring the fact that thepractical latency can also be in\ufb02uenced by scheduling strategies and hardwareproperties. To bridge the gap between theoretical computation and practical ef-\ufb01ciency, we propose a latency-aware spatial-wise dynamic network (LASNet),which performs coarse-grained spatially adaptive inference under the guidanceof a novel latency prediction model. The latency prediction model can ef\ufb01cientlyestimate the inference latency of dynamic networks by simultaneously consideringalgorithms, scheduling strategies, and hardware properties. We use the latencypredictor to guide both the algorithm design and the scheduling optimization onvarious hardware platforms. Experiments on image classi\ufb01cation, object detectionand instance segmentation demonstrate that the proposed framework signi\ufb01cantlyimproves the practical inference ef\ufb01ciency of deep networks. For example, theaverage latency of a ResNet-101 on the ImageNet validation set could be reducedby 36% and 46% on a server GPU (Nvidia Tesla-V100) and an edge device (NvidiaJetson TX2 GPU) respectively without sacri\ufb01cing the accuracy. Code is availableat https://github.com/LeapLabTHU/LASNet.1 IntroductionDynamic neural networks [7] have attracted great research interests in recent years. Compared tostatic models [11, 17, 13, 23] which treat different inputs equally during inference, dynamic networkscan allocate the computation in a data-dependent manner. For example, they can conditionally skipthe computation of network layers [ 15, 9, 32, 30] or convolutional channels [ 19, 1], or performspatially adaptive inference on the most informative image regions ( e.g. the foreground areas)[6, 5, 31, 35, 33, 8]. Spatial-wise dynamic networks, which typically decide whether to computeeach feature pixel with masker modules [5, 31, 35, 8] (Figure 1 (a)), have shown promising results inimproving the inference ef\ufb01ciency of convolution neural networks (CNNs).Despite the remarkable theoretical ef\ufb01ciency achieved by spatial-wise dynamic networks [5, 31, 35],researchers have found it challenging to translate the theoretical results into realistic speedup,\u2217Equal contribution.\u2020Corresponding author.36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.06223v1  [cs.CV]  12 Oct 2022--- Page 2 ---Maskergather ...convolution(s) scatter...Dynamic Convolution(s) AlgorithmSchedulingHardwareLatency(a) Algorithm: Spatially adaptive inference with a masker.(b) Scheduling: the practical inference pipeline of dynamic convolutions.(c) The general idea of our work.in\ufb02uence guidanceselected pixel locationsFigure 1: An overview of our method. (a) illustrates the spatially adaptive inference algorithm; (b) isthe scheduling strategy; and (c) presents the three key factors to the practical latency. For a givenhardware, the latency is used to guide our algorithm design and scheduling optimization.especially on some multi-core processors, e.g., GPUs [35, 3, 8]. The challenges are two-fold: 1) mostprevious approaches [5, 31, 35] perform spatially adaptive inference at the \ufb01nest granularity: everypixel is \ufb02exibly decided whether to be computed or not. Such \ufb02exibility induces non-contiguousmemory access [35] and requires specialized scheduling strategies (Figure 1 (b)); 2) the existingliterature has only adopted the hardware-agnostic FLOPs (\ufb02oating-point operations) as an inaccurateproxy for the ef\ufb01ciency, lacking latency-aware guidance on the algorithm design. For dynamicnetworks, the adaptive computation with sub-optimal scheduling strategies further enlarges thediscrepancy between the theoretical FLOPs and the practical latency. Note that it has been validatedby previous works that the latency on CPUs has a strong correlation with FLOPs [8, 35]. Therefore,we mainly focus on the GPU platform in this paper, which is more challenging and less explored.We address the above challenges by proposing a latency-aware spatial-wise dynamic network(LASNet). Three key factors to the inference latency are considered: the algorithm, the schedulingstrategy, and the hardware properties. Given a target hardware device, we directly use the latency,rather than the FLOPs, to guide our algorithm design and scheduling optimization (Figure 1 (c)).Because the memory access pattern and the scheduling strategies in our dynamic operators differfrom those in static networks, the libraries developed for static models (e.g. cuDNN) are sub-optimalfor the acceleration of dynamic models. Without the support of libraries, each dynamic operatorrequires scheduling optimization, code optimization, compiling, and deployment for each device.Therefore, it is laborious to evaluate the network latency on different hardware platforms. To this end,we propose a novel latency prediction model to ef\ufb01ciently estimate the realistic latency of a networkby simultaneously considering the aforementioned three factors. Compared to the hardware-agnosticFLOPs, our predicted latency can better re\ufb02ect the practical ef\ufb01ciency of dynamic models.Guided by this latency prediction model, we establish our latency-aware spatial-wise dynamic network(LASNet), which adaptively decides whether to allocate computation on feature patches insteadof pixels [5, 31, 35] (Figure 2 top). We name this paradigm as spatially adaptive inference at acoarse granularity. While less \ufb02exible than the pixel-level adaptive computation in previous works[5, 31, 35], it facilitates more contiguous memory access, bene\ufb01ting the realistic speedup on hardware.The scheduling strategy and the implementation are further ameliorated for faster inference.It is worth noting that LASNet is designed as a general framework in two aspects: 1) the coarse-grained spatially adaptive inference paradigm can be conveniently implemented in various CNNbackbones, e.g., ResNets [11], DenseNets [17] and RegNets [25]; and 2) the latency predictor is anoff-the-shell tool which can be directly used for various computing platforms (e.g. server GPUs andedge devices).We evaluate the performance of LASNet on multiple CNN architectures on image classi\ufb01cation,object detection, and instance segmentation tasks. Experiment results show that our LASNet improvesthe ef\ufb01ciency of deep CNNs both theoretically and practically. For example, the inference latencyof ResNet-101 on ImageNet [ 4] is reduced by 36% and 46% on an Nvidia Tesla V100 GPU andan Nvidia Jetson TX2 GPU, respectively, without sacri\ufb01cing the accuracy. Moreover, the proposedmethod outperforms various lightweight networks in a low-FLOPs regime.Our main contributions are summarized as follows:2--- Page 3 ---1. We propose LASNet, which performs coarse-grained spatially adaptive inference guidedby the practical latency instead of the theoretical FLOPs. To the best of our knowledge,LASNet is the \ufb01rst framework that directly considers the real latency in the design phase ofdynamic neural networks;2. We propose a latency prediction model, which can ef\ufb01ciently and accurately estimate thelatency of dynamic operators by simultaneously considering the algorithm, the schedulingstrategy, and the hardware properties;3. Experiments on image classi\ufb01cation and downstream tasks verify that our proposed LAS-Net can effectively improve the practical ef\ufb01ciency of different CNN architectures.2 Related worksSpatial-wise dynamic network is a common type of dynamic neural networks [ 7]. Compared tostatic models which treat different feature locations evenly during inference, these networks performspatially adaptive inference on the most informative regions ( e.g., foregrounds), and reduce theunnecessary computation on less important areas (e.g., backgrounds). Existing works mainly includethree levels of dynamic computation: resolution level [ 36, 37], region level [ 33] and pixel level[5, 31, 35]. The former two generally manipulate the network inputs [ 33, 37] or require specialarchitecture design [36]. In contrast, pixel-level dynamic networks can \ufb02exibly skip the convolutionson certain feature pixels in arbitrary CNN backbones [5, 31, 35]. Despite its remarkable theoreticalef\ufb01ciency, pixel-wise dynamic computation brings considerable dif\ufb01culty to achieving realisticspeedup on multi-core processors, e.g., GPUs. Compared to the previous approaches [ 5, 31, 35]which only focus on reducing the theoretical computation, we propose to directly use the latency toguide our algorithm design and scheduling optimization.Hardware-aware network design. To bridge the gap between theoretical and practical ef\ufb01ciencyof deep models, researchers have started to consider the real latency in the network design phase.There are two lines of works in this direction. One directly performs speed tests on targeteddevices, and summarizes some guidelines to facilitate hand-designing lightweight models [23]. Theother line of work searches for fast models using the neural architecture search (NAS) technique[29, 34]. However, all existing works try to build static models, which have intrinsic computationalredundancy by treating different inputs in the same way. However, speed tests for dynamic operatorson different hardware devices can be very laborious and impractical. In contrast, our proposed latencyprediction model can ef\ufb01ciently estimate the inference latency on any given computing platforms bysimultaneously considering algorithm design, scheduling strategies and hardware properties.3 MethodologyIn this section, we \ufb01rst introduce the preliminaries of spatially adaptive inference, and then demon-strate the architecture design of our LASNet. The latency prediction model is then explained, whichguides the granularity settings and the scheduling optimization for LASNet. We further present theimplementation improvements for faster inference, followed by the training strategies.3.1 PreliminariesSpatially adaptive inference. The existing spatial-wise dynamic networks are usually establishedby attaching a masker Min each convolutional block of a CNN backbone (Figure 1 (a)). Speci\ufb01cally,let x\u2208RH\xd7W\xd7C denote the input of a block, where H and W are the feature height and width, andC is the channel number. The masker Mtakes x as input, and generates a binary-valued spatialmask M = M(x) \u2208{0,1}H\xd7W . Each element of M determines whether to perform convolutionoperations on the corresponding location of the output feature. The unselected regions will be \ufb01lledwith the values from the input [5, 31] or obtained via interpolation [35]. We de\ufb01ne the activation rateof a block as r=\u2211i,j Mi,jH\xd7W , representing the ratio of the calculated pixels.Scheduling strategy. During inference, the current scheduling strategy for spatial-wise dynamicconvolutions generally involve three steps [26] (Figure 1 (b)): 1) gathering, which re-organizes theselected pixels (if the convolution kernel size is greater than 1 \xd71, the neighbors are also required)along the batch dimension; 2) computation, which performs convolution on the gathered input; and3) scattering, which \ufb01lls the computed pixels on their corresponding locations of the output feature.3--- Page 4 ---MaskerMasker-Conv1x1 Conv1x1 ...Scatter-Add Conv1x1 Conv3x3 Gumbel NoiseT raining Inference Input OutputUpsample Element-wise  multiplication Selected pixelGather-Conv3x3 Conv1x1 Regular convolution Operator fusionInput OutputFigure 2: Our proposed LASNet block. Top: we \ufb01rst generate a low-resolution spatial mask Mcoarse,which is then upsampled to obtain the mask M with the same size as the output feature. GumbelSoftmax [18, 24] is used for end-to-end training (Sec. 3.5). Bottom: the scheduling optimization isperformed to decrease the memory access for faster inference (Sec. 3.4).Limitations. Compared to performing convolutions on the entire feature map, the aforementionedscheduling strategy reduces the computation while bringing considerable overhead to memory accessdue to the mask generation and the non-contiguous memory access. Such overhead would increasethe overall latency, especially when the granularity of dynamic convolution is at the \ufb01nest pixel level.3.2 Architecture designSpatial granularity. As mentioned above, pixel-level dynamic convolutions [5, 31, 35] raise sub-stantial challenges to achieving realistic speedup on multi-core processors due to the non-contiguousmemory access. To this end, we propose to optimize the granularity of spatially adaptive inference.Speci\ufb01cally, take the commonly used bottleneck structure in [11] as an example, our coarse-grainedspatial-wise dynamic convolutional block is illustrated in Figure 2. Instead of directly producinga mask with the shape of H\xd7W, we \ufb01rst generate a low-resolution mask Mcoarse \u2208{0,1}HS \xd7WS ,where Sis named as the spatial granularity. Each element in Mcoarse determines the computationof an S\xd7S-sized feature patch. For instance, the feature size in the \ufb01rst ResNet stage3 is 56 \xd756.Then the possible choices for Sare {1,2,4,7,8,14,28,56}.The mask Mcoarse is then upsampledto the size of H\xd7W. Notably, S = 1means that the granularity is still at the pixel level as previousmethods [5, 31, 35]. Note that the other extreme situation (S = 56) is not considered in this paper,when the masker directly determines whether to skip the whole block (i.e. layer skipping [30, 32]).Such an overly aggressive approach will lead to considerable drop of accuracy, as we presented inAppendix C.2. The masker is composed of a pooling layer followed by a 1 \xd71 convolution.Differences to existing works. Without using the interpolation operation [ 35] or the carefullydesigned two-branch structure [8], the proposed block architecture is simple and suf\ufb01ciently generalto be plugged into most backbones with minimal modi\ufb01cation. Our formulation is mostly similar tothat in [31], which could be viewed as a variant of our method with the spatial granularity S=1 for allblocks. Instead of performing spatially adaptive inference at the \ufb01nest pixel level, our granularity Sisoptimized under the guidance of our latency prediction model (details are presented in the followingSec. 4.2) to achieve realistic speedup on target computing platforms.3.3 Latency prediction modelAs stated before, it is laborious to evaluate the latency of dynamic operators on different hardwareplatforms. To ef\ufb01ciently seek preferable granularity settings on arbitrary hardware devices, wepropose a latency prediction model G, which can directly predict the delay of executing dynamicoperators on any target devices. For a spatial-wise dynamic convolutional block, the latency predictorGtakes the hardware propertiesH, the layer parametersP, the spatial granularityS, and the activationrate ras input and predicts the latency \u2113of a dynamic convolutional block: \u2113= G(H,P,S,r ).Hardware modeling. We model a hardware device as multiple processing engines (PEs), and parallelcomputation can be executed on these PEs. As shown in Figure 3, we model the memory system as a3Here we refer to a stage as the cascading of multiple blocks which process features with the same resolution.4--- Page 5 ---Off-chip Memory(GDDR/DDR/HBM)On-chip Memory(LLC/Global Buffer)PE1PEMemoryComputation UnitsPE2PEMemoryComputation UnitsPEnPEMemoryComputation Units\u2026Figure 3: Our hardware model.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)101520253035Latency ( s)PredictedReal Figure 4: Latency prediction results.three-level structure [12]: 1) off-chip memory, 2) on-chip global memory, and 3) memory in PE. Sucha hardware model enables us to accurately predict the cost on both data movement and computation.Latency prediction. When simulating the data movement procedure, the ef\ufb01ciency of non-contiguous memory accesses under different granularity Ssettings is considered. As for the com-putation latency, it is important to adopt a proper scheduling strategy to increase the parallelism ofcomputation. Therefore, we search for the optimal scheduling (the con\ufb01guration of tiling and in-PEparallel computing) of dynamic operations to maximize the utilization of hardware resources. A moredetailed description of our latency prediction model is presented in Appendix A.Empirical validation. We take the \ufb01rst block in ResNet-101 as an example and vary the activationrate rto evaluate the performance of our prediction model. The comparison between our predictionsand the real testing latency on the Nvidia V100 GPU is illustrated in Figure 4, from which we canobserve that our predictor can accurately estimate the real latency in a wide range of activation rates.3.4 Implementation detailsWe use general optimization methods like fusing activation functions and batch normalizationlayers into convolution layers. We also optimize the speci\ufb01c operators in our spatial-wise dynamicconvolutional blocks as follows (see also Figure 2 for an overview).Fusing the masker and the \ufb01rst convolution. As mentioned in Sec. 3.1, the masker in each blockconsumes very little computation, but it takes the whole feature map as input. Therefore, it is amemory-bounded operation (the inference time is mainly spent on memory access). Since the maskerand the \ufb01rst convolution in the block share the same input, there is an opportunity to fuse these twooperations to avoid the repeated access of the input data. Note that a spatial-wise dynamic convolutionrequires the output of the masker. If we fuse the two layers, the \ufb01rst convolution will be changed to astatic operation, which may increase the inference latency. There exists a threshold of activation raterth, when r>r th, the overall latency can be reduced. We decide whether to fuse them according tothe average activation rate. See more details in Appendix B.1.Fusing the gather operation and the dynamic convolution. Traditional approaches \ufb01rst gather theinput pixels of the \ufb01rst dynamic convolution in a block (Figure 1 (b)). The gather operation is alsoa memory-bounded operation. Furthermore, when the size of the convolution kernel exceeds 1\xd71,the area of input patches may overlap, resulting in repeated memory load/store. We fuse the gatheroperation into the dynamic convolution to reduce the memory access.Fusing the scatter operation and the add operation. Traditional approaches scatter the outputpixels of the last dynamic convolution, and then execute the element-wise addition (Figure 1 (b)). Wefuse these two operators to reduce the memory access. The ablation study in Sec. 4.4 validates theeffectiveness of the proposed fusing methods.3.5 TrainingOptimization of non-differentiable maskers. The masker modules are required to produce binary-valued spatial masks for making discrete decisions, and cannot be directly optimized with backpropagation. Following [35, 31, 8], we adopt straight-through Gumbel Softmax [18, 24] to train thenetwork in an end-to-end fashion. Speci\ufb01cally, let \u02dcM \u2208RH\xd7W\xd72 denote the output of the maskgenerator. The decisions are obtained with the argmax function during inference. In the trainingphase, a differentiable approximation is de\ufb01ned by replacing the argmax operation with a Softmax:\u02c6M =exp{(log(\u02dcM:,:,0)+ G:,:,0)/\u03c4}\u22111k=0 exp{(log(\u02dcM:,:,k)+ G:,:,k)/\u03c4} \u2208 [0,1]H\xd7W , (1)5--- Page 6 ---0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1(a) Relationship between r\u2113 and rfor LAS-ResNet blocks on the Nvidia Tesla V100 GPU.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.75Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.752.00Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.4Latency ratio (r )S = 1(b) Relationship between r\u2113 and rfor LAS-RegNetY-800MF blocks on the Nvidia Jetson TX2 GPU.Figure 5: Latency prediction results of LAS-ResNet blocks on V100 (a) and LAS-RegNet blocks onTX2 (b). For both networks, we plot the relationship between the latency ratio r\u2113 and the activationrate r for the blocks in 4 stages with the convolutional stride 1. The practical ef\ufb01ciency is onlyimproved when r\u2113 <1. Note that S = 1can harm the practical latency even for a small r(reducedcomputation), while a larger Swill alleviate this problem. See detailed analysis in Sec. 4.2.where \u03c4 is the Softmax temperature. Following the common practice [ 31, 8], we let \u03c4 decreaseexponentially from 5.0 to 0.1 in training to facilitate the optimization of maskers.Training objective. The FLOPs of each spatial-wise dynamic convolutional block can be calculatedbased on our de\ufb01ned activation rate r[31]. Then we can obtain the FLOPs of the overall dynamicnetwork Fdyn. Let Fstat denotes the FLOPs of its static counterpart. We optimize their ratio toapproximate a target 0 <t< 1:LFLOPs = (FdynFstat\u2212t)2.In addition, we de\ufb01ne loss item Lbounds asin [31] to constrain the upper bound and the lower bound of activation rates in early training epochs.We further propose to leverage the static counterparts of our dynamic networks as \u201cteachers\u201d to guidethe optimization procedure. Let y and y\u2032denote the output logits of a dynamic model (\u201cstudent\u201d)and its \u201cteacher\u201d, respectively. Our \ufb01nal loss can be written asL= Ltask + \u03b1(LFLOPs + Lbounds) +\u03b2T2 \xb7KL(\u03c3(y/T)||\u03c3(y\u2032/T)), (2)where Ltask represents the task-related loss, e.g., cross-entropy loss in image classi\ufb01cation. KL(\xb7||\xb7)denotes the Kullback\u2013Leibler divergence, and \u03b1,\u03b2 are the coef\ufb01cients balancing these items. We use\u03c3to denote the log-Softmax function, and T is the temperature for computing KL-divergence.4 ExperimentsIn this section, we \ufb01rst introduce the experiment settings in Sec. 4.1. Then the latency of differentgranularity settings are analyzed in Sec. 4.2. The performance of our LASNet on ImageNet is furtherevaluated in Sec. 4.3, followed by the ablation studies in Sec. 4.4. Visualization results are illustratedin Sec. 4.5, and we \ufb01nally validate our method on the object detection task (Sec. 4.6). The results onthe instance segmentation task are presented in . For simplicity, we add \u201cLAS-\u201d as a pre\ufb01x beforemodel names to denote our LASNet, e.g., LAS-ResNet-50.4.1 Experiment settingsLatency prediction. Various types of hardware platforms are tested, including a server GPU (TeslaV100), a desktop GPU (GTX1080) and edge devices ( e.g., Nvidia Nano and Jetson TX2). Themajor properties considered by our latency prediction model include the number of processingengines (#PE), the \ufb02oating-point computation in a processing engine (#FP32), the frequency and thebandwidth. It can be observed that the server GPUs generally have a larger #PE than the IoT devices.The batch size is set as 1 for all dynamic models and computing platforms.Image classi\ufb01cation. The image classi\ufb01cation experiments are conducted on the ImageNet [ 4]dataset. Following [31], we initialize the backbone parameter from a pre-trained checkpoint4, and4We use the torchvision pre-trained models at https://pytorch.org/vision/stable/models.html.6--- Page 7 ---1 2 4 7 8 14 28Spatial granularity (S)12345Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.51.01.52.02.53.03.54.0Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.51.01.52.02.53.0Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(a) Relationship between r\u2113 and Sfor LAS-ResNet blocks on V100.1 2 4 7 8 14 28Spatial granularity (S)0.250.500.751.001.251.501.75Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.250.500.751.001.251.501.752.00Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.20.40.60.81.01.21.41.6Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(b) Relationship between r\u2113 and Sfor LAS-RegNetY-800MF blocks on Nvidia Jetson TX2 GPU.Figure 6: The relationship between the latency ratio r\u2113 and the spatial granularity S.\ufb01netune the whole network for 100 epochs with the loss function in Eq. (2). We \ufb01x \u03b1= 10,\u03b2 = 0.5and T = 4.0 for all dynamic models. More details are provided in Appendix B.4.2 Latency prediction resultsIn this subsection, we present the latency prediction results of the spatial-wise dynamic convolutionalblocks in two different models: LAS-ResNet-101 [11] (on V100) and LAS-RegNetY-800MF [25] (onTX2). All the blocks have the bottleneck structure with different channel numbers and convolutiongroups, and the RegNetY is equipped with Squeeze-and-Excitation (SE) [14] modules.We \ufb01rst de\ufb01ne \u2113dyn as the latency of a spatial-wise dynamic convolutional block, and \u2113stat as that ofa static block without a masker. Their ratio is denoted as r\u2113 = \u2113dyn\u2113stat. We investigate the relationshipbetween r\u2113 and the activation rate r(cf. Sec. 3.5) for different granularity settings. The results inFigure 5 demonstrate that: 1) even equipped with our special optimization on the scheduling strategies,pixel-level spatially adaptive inference (S=1) cannot always improve the practical ef\ufb01ciency. Such\ufb01ne-grained adaptive inference is adopted by most previous works [31, 35], and our result can explainthe reason why they can only achieve realistic speedup on less powerful CPUs [35] or specializeddevices [3]; 2) a proper granularity S >1 effectively alleviates this problem on both hardwaredevices. By setting S >1, realistic speedup could be achieved with larger activation rates.The latency prediction results are further used to seek for a preferable granularity setting for the\ufb01rst 3 stages (we \ufb01x S = 1 for the last stage, where the feature resolution is 7 \xd77). Therefore,we plot the relationship between r\u2113 and S in Figure 10. It can be observed that: 1) r\u2113 generallydecreases with Sincreasing for a given r; 2) an overly largeS(less \ufb02exible adaptive inference) bringsinsigni\ufb01cant improvement on both devices. Especially, enlarging S from 8 to 28 in the \ufb01rst stageof a LAS-ResNet brings very little improvement on V100. Based on the results in Figure 10, wecan trade off between \ufb02exibility and ef\ufb01ciency by selecting appropriate Sfor different models andhardware devices. For example, we can simply set Snet=8-4-7-15 in a LAS-ResNet-101 to achiverealistic speedup. The accuracy-latency plots in Figure 7 also validate this observation. More resultsof our latency prediction model on the desktop-level GPU, GTX1080, are presented in Appendix C.1.4.3 ImageNet classi\ufb01cation resultsWe now empirically evaluate our proposed LASNet on the ImageNet dataset. The network perfor-mance is measured in terms of the trade-off between classi\ufb01cation accuracy and inference ef\ufb01ciency.Both theoretical (i.e. FLOPs) and practical ef\ufb01ciency (i.e. latency) are tested in our experiments.5We use this form to represnet the Ssettings for the 4 stages of a network.7--- Page 8 ---2 3 4 5 6 7 8FLOPs (x1e9)73747576777879Accuracy (%)static ResNetsConvAIG-ResNet-101SkipNet-ResNet-101BAS-ResNet-50DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)15 20 25 30 35 40Latency on Nvidia Jetson TX2 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)1.5 2.0 2.5 3.0 3.5Latency on Nvidia Tesla V100 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)(a) LAS-ResNet results.0.2 0.3 0.4 0.5 0.6 0.7 0.8FLOPs (x1e9)717273747576Accuracy (%)RegNets-XRegNets-YMobileNets-v2ShuffleNets-v2CondenseNetsLAS-RegNetY-400MF (Snet=1-1-1-1)LAS-RegNetY-400MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)3 4 5 6 7 8Latency on Nvidia Jetson TX2 GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-XRegNets-YMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)8 10 12 14 16 18 20Latency on Nvidia nano GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-YRegNets-XMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)(b) LAS-RegNetY results.Figure 7: Experimental results on ImageNet. The proposed coarse-grained spatially adaptive inferenceis tested on standard ResNets (a) and lightweight RegNets (b).4.3.1 Standard baseline comparison: ResNetsWe \ufb01rst establish our LASNet based on the standard ResNets [ 11]. Speci\ufb01cally, we build LAS-ResNet-50 and LAS-ResNet-101 by plugging our maskers in the two common ResNet structures.The baselines include various types of dynamic inference approaches: 1) layer skipping (SkipNet[32] and Conv-AIG [30]); 2) channel skipping (BAS [1]); and 3) pixel-level spatial-wise dynamicnetwork (DynConv [31]). For our LASNet, we compare various settings of the spatial granularitySnet. We set training targets (cf. Sec. 3.5) t\u2208{0,4,0.5,0.6,0.7}for our dynamic models to evaluatetheir performance in different sparsity regimes. We apply the same operator fusion (Sec. 3.4) for bothour models and the compared baselines [30, 31] for fair comparison.Results are presented in Figure 7 (a). On the left we plot the relationship of accuracy v.s. FLOPs. Itcan be observed that our LAS-ResNets with different granularity settings signi\ufb01cantly outperformthe competing dynamic neural networks. Surprisingly, coarse-grained spatially adaptive inference(Snet=4-4-2-1 and Snet=8-4-7-1 for the 4 stages) can achieve even higher accuracy when consumingsimilar FLOPs on ResNets, despite the sacri\ufb01ced \ufb02exibility compared toSnet=1-1-1-1. We conjecturethat a larger Sis also bene\ufb01cial to the optimization of maskers.We compare the practical latency of three granularity settings in Figure 7 (a) predicted by our latencyprediction model (middle on TX2 and right on V100). We can witness that although they achievecomparable theoretical ef\ufb01ciency (Figure 7 (a) left), larger Sis more hardware-friendly comparedto the \ufb01nest granularity. For example, the inference latency of LAS-ResNet-101 ( Snet=1-1-1-1)is signi\ufb01cantly higher than the ResNet-101 baseline on V100 (Figure 7 (a) right), even though itstheoretical computation is much smaller than that of the static model. However, larger granularities(Snet=4-4-2-1 and Snet=8-4-7-1) can effectively improve the inference latency due to its lower burdenon the memory access. Remarkably, the latency of ResNet-101 could be reduced by 36% and 46% onV100 and TX2 respectively without sacri\ufb01cing the accuracy when t=0.4. The classi\ufb01cation accuracyis increased by 1.9% with similar inference ef\ufb01ciency. It can be observed that the realistic speedupratio r\u2113 is more close to the theoretical FLOPs ratio target ton the less powerful TX2, because thelatency is computation-bounded (i.e. the latency is mainly spent on computation) on such IoT devices.In contrast, there is a larger gap between practical and theoretical ef\ufb01ciency on the more powerfulV100, as the latency is bounded by the memory access cost.4.3.2 Lightweight baseline comparison: RegNetsWe further evaluate our LASNet in lightweight CNN architectures,i.e. RegNets-Y [25]. Two differentsized models are tested: RegNetY-400MF and RegNetY-800MF. Compared baselines include othertypes of ef\ufb01cient models, e.g., MobileNets-v2 [28], Shuf\ufb02etNets-v2 [23] and CondenseNets [16].The results are presented in Figure 7 (b). The x-axis for the three sub-\ufb01gures are the FLOPs, thelatency on TX2, and the latency on the Nvidia nano GPU, respectively. We can observe that our8--- Page 9 ---1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2Latency on Nvidia Tesla V100 GPU (ms)76.577.077.578.078.579.0Accuracy (%) static ResNetsLAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=4-4-7-1)LAS-ResNet-101 (Snet=8-4-7-1)Figure 8: Ablation studies on S. Figure 9: Visualization results.method outperforms various types of static models in terms of the trade-off between accuracy andef\ufb01ciency. More results on image classi\ufb01cation are provided in Appendix C.2.4.4 Ablation studiesWe conduct ablation studies to validate the effectiveness of our coarse-grained spatially adaptiveinference (Sec. 3.2) and operator fusion operations (Sec. 3.4).More granularities settings. We test various granularity settings on LAS-ResNet-101 to examinethe effects of Sin different stages. The results on the Tesla-V100 GPU are presented in Figure 8. Itcan be found that the \ufb01nest granularity (Snet=1-1-1-1) leads to substantial inef\ufb01ciency despite thereduced FLOPs (cf. Figure 7 (a) left). Coarse-grained spatially adaptive inference in the \ufb01rst twostages (Snet=4-4-1-1) effectively reduces the inference latency. We further increase Sin the thirdstage to 2 and 7, and this procedure consistently improves the realistic ef\ufb01ciency on the V100 GPU.This trend also holds on the GTX 1080 GPU (see the results in Appendix C.2.Table 1: Ablation studies on operator fusion.Masker- Gather- Scatter- LatencyConv1x1 Conv3x3 Add (\xb5s)   163.2   90.1   86.7   71.4Operator fusion. We investigate the effect of ouroperator fusion introduced in Sec. 3.4. One convolu-tional block in the \ufb01rst stage of a LAS-ResNet-101(S=4, r=0.6) is tested. The results in Table 1 vali-date that every step of operator fusion bene\ufb01ts thepractical latency of a block, as the overhead onmemory access is effectively reduced. Especially,the fusion of the masker operation and the \ufb01rst con-volution is crucial to reducing the latency.4.5 VisualizationWe visualize the masks generated by our masker in the third block of a LAS-ResNet-101 (Snet=4-4-2-1) in Figure 12. The brilliant areas correspond to the locations of 1 elements in a mask, andthe computation on the dimmed regions is skipped by our dynamic model. We can witness that themasker accurately locate the most task-related regions (even the tiny aircraft at the corner), whichhelps reduce the unnecessary computation on background areas. These resultsalso suggest that forthe \ufb01rst stage, the granularity S=4 is suf\ufb01ciently \ufb02exible to recognize the important regions, anda win-win can be achieved between accuracy and ef\ufb01ciency. Interestingly, the masker could selectsome objects that are not labeled for the sample, e.g., the \ufb02ower beside the hummingbird and thehuman holding the camera. This indicates that our spatial-wise dynamic networks can automaticallyrecognize the regions with semantics, and their capability is not limited by the classi\ufb01cation labels.This property is helpful in some downstream tasks, such as object detection (Sec. 4.6) and instancesegmentation (Sec. 4.7), which require detecting multiple classes and objects in an image. Morevisualization results could be found in Appendix C.3.4.6 COCO Object detectionWe further evaluate our LASNet on the COCO [22] object detection task. The mean average precision(mAP), the average backbone FLOPs, and the average backbone latency on the validation set areused to measure the network performance. We test two commonly used detection frameworks: FasterR-CNN [27] with Feature Pyramid Network [20] and RetinaNet [21]. Thanks to the generality ofour method, we can conveniently replace the backbones with ours pre-trained on ImageNet, andthe whole models are \ufb01netuned on COCO with the standard setting for 12 epochs (see detailed9--- Page 10 ---Table 2: Object detection results on the COCO dataset.Detection Backbone Backbone Backbone Latency (ms)mAP (%)Framework FLOPs (G) V100 GTX1080 TX2Faster R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 39.4LAS-ResNet-101 (Snet=4-4-2-1,t=0.6) 90.7 33.8 90.6 524.9 40.3LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 79.3 30.7 82.5 477.1 39.8LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.5 29.0 79.9 464.8 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 67.9 25.3 69.1 401.3 39.5RetinaNetResNet-101 (Baseline) 141.2 39.5 119.3 729.4 38.5LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 77.8 30.4 81.8 472.6 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.4 28.9 79.9 464.8 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 66.4 25.3 69.1 401.3 38.9Table 3: Instance Segmentation results on the COCO dataset.Segmentation Backbone Backbone Backbone Latency (ms)APmask(%) APbox(%)Framework FLOPs (G) V100 GTX1080 TX2Mask R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 36.1 40.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 80.5 31.1 83.3 481.9 37.0 41.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.4) 69.2 27.9 74.8 431.6 36.1 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 68.8 25.8 70.9 411.8 36.2 40.0setup in Appendix B.3). The input images are resized to a short side of 800 and a long side notexceeding 1333. The results of our LAS-ResNet-101 with different Snet settings are presented inTable 2. We can observe from the results that when setting the training target as 0.4, the latency ofour LAS-ResNet-101 with Snet=4-4-7-1 is signi\ufb01cantly lower than the static baseline on all deviceswithout sacri\ufb01cing mAP in both detection frameworks. With a larger training targets, our LASNetcan increase the mAP by 0.9% and 0.8% in Faster R-CNN and RetinaNet respectively, while stillbeing faster than the baseline method.4.7 COCO instance segmentationWe also present the results of instance segmentation on COCO, which demonstrate the effectivenessof our LASNet on the dense prediction task. From the results in Table 3, we can observe that whensetting the training target as 0.4, the Mask R-CNN [ 10] models (Snet=4-4-2-1 and Snet=4-4-7-1)runs faster on all tested hardware devices without sacri\ufb01cing the performance. With a training targetof 0.5, the APmask and APbox of the Mask R-CNN model could be increased by 0.9% and 1.0%respectively while still running faster than the baseline method.5 ConclusionIn this paper, we propose to build latency-aware spatial-wise dynamic networks (LASNet) under theguidance of a latency prediction model. By simultaneously considering the algorithm, the schedulingstrategy and the hardware properties, we can ef\ufb01ciently estimate the practical latency of spatial-wise dynamic operators on arbitrary computing platforms. Based on the empirical analysis on therelationship between the latency and the granularity of spatially adaptive inference, we optimizeboth the algorithm and the scheduling strategies to achieve realistic speedup on many multi-coreprocessors, e.g., the Tesla V100 GPU and the Jetson TX2 GPU. Experiments on image classi\ufb01cation,object detection and instance segmentation tasks validate that the proposed method signi\ufb01cantlyimproves the practical ef\ufb01ciency of deep CNNs, and outperforms various competing approaches.AcknowledgementThis work is supported in part by the National Key R&D Program of China under Grant2020AAA0105200, the National Natural Science Foundation of China under Grants 62022048and the Tsinghua University-China Mobile Communications Group Co.,Ltd. Joint Institute..10--- Page 11 ---References[1] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping for learningconditional channel gated networks. In ICLR, 2020.[2] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, MeghanCowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. Tvm: An automated end-to-end optimizingcompiler for deep learning. In OSDI, 2018.[3] Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian Verhelst. Proces-sor architecture optimization for spatially dynamic neural networks. In VLSI-SoC, 2021.[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In CVPR, 2009.[5] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicatednetwork with less inference complexity. In CVPR, 2017.[6] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov,and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In CVPR,2017.[7] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamicneural networks: A survey. TPAMI, 2021.[8] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, and Haojun Jiang. Spatiallyadaptive feature re\ufb01nement for ef\ufb01cient inference. TIP, 2021.[9] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfen Cao, Wenhui Huang,Chao Deng, and Gao Huang. Learning to weight samples for dynamic early-exiting networks.In ECCV, 2022.[10] Kaiming He, Georgia Gkioxari, Piotr Doll\xe1r, and Ross Girshick. Mask r-cnn. In Proceedings ofthe IEEE international conference on computer vision, pages 2961\u20132969, 2017.[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In CVPR, 2016.[12] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach .Elsevier, 2011.[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, TobiasWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ef\ufb01cient convolutional neuralnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.[15] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Wein-berger. Multi-scale dense networks for resource ef\ufb01cient image classi\ufb01cation. In ICLR, 2018.[16] Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: Anef\ufb01cient densenet using learned group convolutions. In CVPR, 2018.[17] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Weinberger.Densely connected convolutional networks. TPAMI, 2019.[18] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.In ICLR, 2017.[19] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In NeurIPS, 2017.[20] Tsung-Yi Lin, Piotr Doll\xe1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.Feature pyramid networks for object detection. In CVPR, 2017.[21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Focal loss for denseobject detection. In ICCV, 2017.11--- Page 12 ---[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,Piotr Doll\xe1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,2014.[23] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shuf\ufb02enet v2: Practical guidelinesfor ef\ufb01cient cnn architecture design. In ECCV, 2018.[24] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuousrelaxation of discrete random variables. In ICLR, 2017.[25] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Design-ing network design spaces. In CVPR, 2020.[26] Mengye Ren, Andrei Pokrovsky, Bin Yang, and Raquel Urtasun. SBNet: Sparse Blocks Networkfor Fast Inference. CVPR, 2018.[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-timeobject detection with region proposal networks. In NeurIPS, 2015.[28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.[29] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, andQuoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820\u20132828,2019.[30] Andreas Veit and Serge Belongie. Convolutional networks with adaptive inference graphs. InECCV, 2018.[31] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: Exploiting Spatial Sparsity forFaster Inference. In CVPR, 2020.[32] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learningdynamic routing in convolutional networks. In ECCV, 2018.[33] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus:a dynamic approach to reducing spatial redundancy in image classi\ufb01cation. In NeurIPS, 2020.[34] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, YuandongTian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware ef\ufb01cient convnetdesign via differentiable neural architecture search. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 10734\u201310742, 2019.[35] Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptiveinference with stochastic feature sampling and interpolation. In ECCV, 2020.[36] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptivenetworks for ef\ufb01cient inference. In CVPR, 2020.[37] Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan, and Yunhe Wang.Dynamic resolution network. NeurIPS, 2021.12--- Page 13 ---AppendixA Latency prediction model.As the dynamic operators in our method have not been supported by current deep learning libraries,we propose a latency prediction model to ef\ufb01ciently estimate the real latency of these operators onhardware device. The inputs of the latency prediction model include: 1) the structural con\ufb01gurationof a convolutional block, 2) its activation rate rwhich decides the computation amount, 3) the spatialgranularity S, and 4) the hardware properties mentioned in Table 4. The latency of a dynamic blockis predicted as follows.Input/output shape de\ufb01nition. The \ufb01rst step of predicting the latency of an operation is to calculatethe shape of input and output. Taking the gather-conv2 operation as an example, the input of thisoperation is the activation with the shape of Cin \xd7H \xd7W, where Cin is the number of inputchannels, and H and W are the resolution of the feature map. The shape of the output tensor isP \xd7Cout \xd7S\xd7S, where P is the number of output patches, Cout is the number of output channelsand Sis the spatial granularity. Note that P is obtained based on the output of our maskers.Operation-to-hardware mapping. Next, we map the operations to hardware. As is mentioned in thepaper, we model a hardware device as multiple processing engines (PEs). We assign the computationof each element in the output feature map to a PE. Speci\ufb01cally, we consecutively split the outputfeature map into multiple tiles. The shape of each tile is TP \xd7TC \xd7TS1 \xd7TS2. These split tilesare assigned to multiple PEs. The computation of the elements in each tile is executed in a PE. Wecan con\ufb01gure different shapes of tiles. In order to determine the optimal shape of the tile, we makea search space of different tile shapes. The tile shape has 4 dimensions. The candidates of eachdimension are power-of-2 and do not exceed the corresponding dimension of the feature map.Latency estimation. Then, we evaluate the latency of each tile shape in the search space and selectthe optimal tile shape with the lowest latency. The latency includes the data movement latency andthe computation latency: \u2113= \u2113data + \u2113computation.1) Data movement latency \u2113data. The estimation of the latency for data movement requires us tomodel the memory system of a hardware device. We model the memory system of hardware as athree-level architecture [12]: off-chip memory, on-chip global memory, and local memory in PE.The input data and weight data are \ufb01rst transferred from the off-chip memory to the on-chip globalmemory. We assume the hardware can make full use of the off-chip memory bandwidth to simplifythe latency prediction model.After that, the data used to compute the output tiles is transferred from on-chip global memory tothe local m, Your response: ```ADD_PAPER\n2210.06223v1\nThis paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach guided by a novel latency prediction model that accounts for algorithm design, scheduling strategies, and hardware properties. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method achieves these gains through optimized spatial granularity selection and operator fusion techniques, demonstrating significant improvements over pixel-level dynamic networks in practical deployment scenarios.\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\n~~~~~~~~~~\nCurrent Step #10, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification energy efficiency\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 12:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #8, Phase: literature review, Feedback: 3\n--- Page 1 ---Latency-aware Spatial-wise Dynamic NetworksYizeng Han1\u2217Zhihang Yuan2\u2217 Yifan Pu1\u2217 Chenhao Xue2Shiji Song1 Guangyu Sun2 Gao Huang1\u20201 Department of Automation, BNRist, Tsinghua University, Beijing, China2 School of Electronics Engineering and Computer Science, Peking University, Beijing, China{hanyz18, pyf20}@mails.tsinghua.edu.cn, {shijis, gaohuang}@tsinghua.edu.cn{yuanzhihang, xch927027, gsun}@pku.edu.cnAbstractSpatial-wise dynamic convolution has become a promising approach to improvingthe inference ef\ufb01ciency of deep networks. By allocating more computation to themost informative pixels, such an adaptive inference paradigm reduces the spatialredundancy in image features and saves a considerable amount of unnecessarycomputation. However, the theoretical ef\ufb01ciency achieved by previous methodscan hardly translate into a realistic speedup, especially on the multi-core processors(e.g. GPUs). The key challenge is that the existing literature has only focusedon designing algorithms with minimal computation, ignoring the fact that thepractical latency can also be in\ufb02uenced by scheduling strategies and hardwareproperties. To bridge the gap between theoretical computation and practical ef-\ufb01ciency, we propose a latency-aware spatial-wise dynamic network (LASNet),which performs coarse-grained spatially adaptive inference under the guidanceof a novel latency prediction model. The latency prediction model can ef\ufb01cientlyestimate the inference latency of dynamic networks by simultaneously consideringalgorithms, scheduling strategies, and hardware properties. We use the latencypredictor to guide both the algorithm design and the scheduling optimization onvarious hardware platforms. Experiments on image classi\ufb01cation, object detectionand instance segmentation demonstrate that the proposed framework signi\ufb01cantlyimproves the practical inference ef\ufb01ciency of deep networks. For example, theaverage latency of a ResNet-101 on the ImageNet validation set could be reducedby 36% and 46% on a server GPU (Nvidia Tesla-V100) and an edge device (NvidiaJetson TX2 GPU) respectively without sacri\ufb01cing the accuracy. Code is availableat https://github.com/LeapLabTHU/LASNet.1 IntroductionDynamic neural networks [7] have attracted great research interests in recent years. Compared tostatic models [11, 17, 13, 23] which treat different inputs equally during inference, dynamic networkscan allocate the computation in a data-dependent manner. For example, they can conditionally skipthe computation of network layers [ 15, 9, 32, 30] or convolutional channels [ 19, 1], or performspatially adaptive inference on the most informative image regions ( e.g. the foreground areas)[6, 5, 31, 35, 33, 8]. Spatial-wise dynamic networks, which typically decide whether to computeeach feature pixel with masker modules [5, 31, 35, 8] (Figure 1 (a)), have shown promising results inimproving the inference ef\ufb01ciency of convolution neural networks (CNNs).Despite the remarkable theoretical ef\ufb01ciency achieved by spatial-wise dynamic networks [5, 31, 35],researchers have found it challenging to translate the theoretical results into realistic speedup,\u2217Equal contribution.\u2020Corresponding author.36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.06223v1  [cs.CV]  12 Oct 2022--- Page 2 ---Maskergather ...convolution(s) scatter...Dynamic Convolution(s) AlgorithmSchedulingHardwareLatency(a) Algorithm: Spatially adaptive inference with a masker.(b) Scheduling: the practical inference pipeline of dynamic convolutions.(c) The general idea of our work.in\ufb02uence guidanceselected pixel locationsFigure 1: An overview of our method. (a) illustrates the spatially adaptive inference algorithm; (b) isthe scheduling strategy; and (c) presents the three key factors to the practical latency. For a givenhardware, the latency is used to guide our algorithm design and scheduling optimization.especially on some multi-core processors, e.g., GPUs [35, 3, 8]. The challenges are two-fold: 1) mostprevious approaches [5, 31, 35] perform spatially adaptive inference at the \ufb01nest granularity: everypixel is \ufb02exibly decided whether to be computed or not. Such \ufb02exibility induces non-contiguousmemory access [35] and requires specialized scheduling strategies (Figure 1 (b)); 2) the existingliterature has only adopted the hardware-agnostic FLOPs (\ufb02oating-point operations) as an inaccurateproxy for the ef\ufb01ciency, lacking latency-aware guidance on the algorithm design. For dynamicnetworks, the adaptive computation with sub-optimal scheduling strategies further enlarges thediscrepancy between the theoretical FLOPs and the practical latency. Note that it has been validatedby previous works that the latency on CPUs has a strong correlation with FLOPs [8, 35]. Therefore,we mainly focus on the GPU platform in this paper, which is more challenging and less explored.We address the above challenges by proposing a latency-aware spatial-wise dynamic network(LASNet). Three key factors to the inference latency are considered: the algorithm, the schedulingstrategy, and the hardware properties. Given a target hardware device, we directly use the latency,rather than the FLOPs, to guide our algorithm design and scheduling optimization (Figure 1 (c)).Because the memory access pattern and the scheduling strategies in our dynamic operators differfrom those in static networks, the libraries developed for static models (e.g. cuDNN) are sub-optimalfor the acceleration of dynamic models. Without the support of libraries, each dynamic operatorrequires scheduling optimization, code optimization, compiling, and deployment for each device.Therefore, it is laborious to evaluate the network latency on different hardware platforms. To this end,we propose a novel latency prediction model to ef\ufb01ciently estimate the realistic latency of a networkby simultaneously considering the aforementioned three factors. Compared to the hardware-agnosticFLOPs, our predicted latency can better re\ufb02ect the practical ef\ufb01ciency of dynamic models.Guided by this latency prediction model, we establish our latency-aware spatial-wise dynamic network(LASNet), which adaptively decides whether to allocate computation on feature patches insteadof pixels [5, 31, 35] (Figure 2 top). We name this paradigm as spatially adaptive inference at acoarse granularity. While less \ufb02exible than the pixel-level adaptive computation in previous works[5, 31, 35], it facilitates more contiguous memory access, bene\ufb01ting the realistic speedup on hardware.The scheduling strategy and the implementation are further ameliorated for faster inference.It is worth noting that LASNet is designed as a general framework in two aspects: 1) the coarse-grained spatially adaptive inference paradigm can be conveniently implemented in various CNNbackbones, e.g., ResNets [11], DenseNets [17] and RegNets [25]; and 2) the latency predictor is anoff-the-shell tool which can be directly used for various computing platforms (e.g. server GPUs andedge devices).We evaluate the performance of LASNet on multiple CNN architectures on image classi\ufb01cation,object detection, and instance segmentation tasks. Experiment results show that our LASNet improvesthe ef\ufb01ciency of deep CNNs both theoretically and practically. For example, the inference latencyof ResNet-101 on ImageNet [ 4] is reduced by 36% and 46% on an Nvidia Tesla V100 GPU andan Nvidia Jetson TX2 GPU, respectively, without sacri\ufb01cing the accuracy. Moreover, the proposedmethod outperforms various lightweight networks in a low-FLOPs regime.Our main contributions are summarized as follows:2--- Page 3 ---1. We propose LASNet, which performs coarse-grained spatially adaptive inference guidedby the practical latency instead of the theoretical FLOPs. To the best of our knowledge,LASNet is the \ufb01rst framework that directly considers the real latency in the design phase ofdynamic neural networks;2. We propose a latency prediction model, which can ef\ufb01ciently and accurately estimate thelatency of dynamic operators by simultaneously considering the algorithm, the schedulingstrategy, and the hardware properties;3. Experiments on image classi\ufb01cation and downstream tasks verify that our proposed LAS-Net can effectively improve the practical ef\ufb01ciency of different CNN architectures.2 Related worksSpatial-wise dynamic network is a common type of dynamic neural networks [ 7]. Compared tostatic models which treat different feature locations evenly during inference, these networks performspatially adaptive inference on the most informative regions ( e.g., foregrounds), and reduce theunnecessary computation on less important areas (e.g., backgrounds). Existing works mainly includethree levels of dynamic computation: resolution level [ 36, 37], region level [ 33] and pixel level[5, 31, 35]. The former two generally manipulate the network inputs [ 33, 37] or require specialarchitecture design [36]. In contrast, pixel-level dynamic networks can \ufb02exibly skip the convolutionson certain feature pixels in arbitrary CNN backbones [5, 31, 35]. Despite its remarkable theoreticalef\ufb01ciency, pixel-wise dynamic computation brings considerable dif\ufb01culty to achieving realisticspeedup on multi-core processors, e.g., GPUs. Compared to the previous approaches [ 5, 31, 35]which only focus on reducing the theoretical computation, we propose to directly use the latency toguide our algorithm design and scheduling optimization.Hardware-aware network design. To bridge the gap between theoretical and practical ef\ufb01ciencyof deep models, researchers have started to consider the real latency in the network design phase.There are two lines of works in this direction. One directly performs speed tests on targeteddevices, and summarizes some guidelines to facilitate hand-designing lightweight models [23]. Theother line of work searches for fast models using the neural architecture search (NAS) technique[29, 34]. However, all existing works try to build static models, which have intrinsic computationalredundancy by treating different inputs in the same way. However, speed tests for dynamic operatorson different hardware devices can be very laborious and impractical. In contrast, our proposed latencyprediction model can ef\ufb01ciently estimate the inference latency on any given computing platforms bysimultaneously considering algorithm design, scheduling strategies and hardware properties.3 MethodologyIn this section, we \ufb01rst introduce the preliminaries of spatially adaptive inference, and then demon-strate the architecture design of our LASNet. The latency prediction model is then explained, whichguides the granularity settings and the scheduling optimization for LASNet. We further present theimplementation improvements for faster inference, followed by the training strategies.3.1 PreliminariesSpatially adaptive inference. The existing spatial-wise dynamic networks are usually establishedby attaching a masker Min each convolutional block of a CNN backbone (Figure 1 (a)). Speci\ufb01cally,let x\u2208RH\xd7W\xd7C denote the input of a block, where H and W are the feature height and width, andC is the channel number. The masker Mtakes x as input, and generates a binary-valued spatialmask M = M(x) \u2208{0,1}H\xd7W . Each element of M determines whether to perform convolutionoperations on the corresponding location of the output feature. The unselected regions will be \ufb01lledwith the values from the input [5, 31] or obtained via interpolation [35]. We de\ufb01ne the activation rateof a block as r=\u2211i,j Mi,jH\xd7W , representing the ratio of the calculated pixels.Scheduling strategy. During inference, the current scheduling strategy for spatial-wise dynamicconvolutions generally involve three steps [26] (Figure 1 (b)): 1) gathering, which re-organizes theselected pixels (if the convolution kernel size is greater than 1 \xd71, the neighbors are also required)along the batch dimension; 2) computation, which performs convolution on the gathered input; and3) scattering, which \ufb01lls the computed pixels on their corresponding locations of the output feature.3--- Page 4 ---MaskerMasker-Conv1x1 Conv1x1 ...Scatter-Add Conv1x1 Conv3x3 Gumbel NoiseT raining Inference Input OutputUpsample Element-wise  multiplication Selected pixelGather-Conv3x3 Conv1x1 Regular convolution Operator fusionInput OutputFigure 2: Our proposed LASNet block. Top: we \ufb01rst generate a low-resolution spatial mask Mcoarse,which is then upsampled to obtain the mask M with the same size as the output feature. GumbelSoftmax [18, 24] is used for end-to-end training (Sec. 3.5). Bottom: the scheduling optimization isperformed to decrease the memory access for faster inference (Sec. 3.4).Limitations. Compared to performing convolutions on the entire feature map, the aforementionedscheduling strategy reduces the computation while bringing considerable overhead to memory accessdue to the mask generation and the non-contiguous memory access. Such overhead would increasethe overall latency, especially when the granularity of dynamic convolution is at the \ufb01nest pixel level.3.2 Architecture designSpatial granularity. As mentioned above, pixel-level dynamic convolutions [5, 31, 35] raise sub-stantial challenges to achieving realistic speedup on multi-core processors due to the non-contiguousmemory access. To this end, we propose to optimize the granularity of spatially adaptive inference.Speci\ufb01cally, take the commonly used bottleneck structure in [11] as an example, our coarse-grainedspatial-wise dynamic convolutional block is illustrated in Figure 2. Instead of directly producinga mask with the shape of H\xd7W, we \ufb01rst generate a low-resolution mask Mcoarse \u2208{0,1}HS \xd7WS ,where Sis named as the spatial granularity. Each element in Mcoarse determines the computationof an S\xd7S-sized feature patch. For instance, the feature size in the \ufb01rst ResNet stage3 is 56 \xd756.Then the possible choices for Sare {1,2,4,7,8,14,28,56}.The mask Mcoarse is then upsampledto the size of H\xd7W. Notably, S = 1means that the granularity is still at the pixel level as previousmethods [5, 31, 35]. Note that the other extreme situation (S = 56) is not considered in this paper,when the masker directly determines whether to skip the whole block (i.e. layer skipping [30, 32]).Such an overly aggressive approach will lead to considerable drop of accuracy, as we presented inAppendix C.2. The masker is composed of a pooling layer followed by a 1 \xd71 convolution.Differences to existing works. Without using the interpolation operation [ 35] or the carefullydesigned two-branch structure [8], the proposed block architecture is simple and suf\ufb01ciently generalto be plugged into most backbones with minimal modi\ufb01cation. Our formulation is mostly similar tothat in [31], which could be viewed as a variant of our method with the spatial granularity S=1 for allblocks. Instead of performing spatially adaptive inference at the \ufb01nest pixel level, our granularity Sisoptimized under the guidance of our latency prediction model (details are presented in the followingSec. 4.2) to achieve realistic speedup on target computing platforms.3.3 Latency prediction modelAs stated before, it is laborious to evaluate the latency of dynamic operators on different hardwareplatforms. To ef\ufb01ciently seek preferable granularity settings on arbitrary hardware devices, wepropose a latency prediction model G, which can directly predict the delay of executing dynamicoperators on any target devices. For a spatial-wise dynamic convolutional block, the latency predictorGtakes the hardware propertiesH, the layer parametersP, the spatial granularityS, and the activationrate ras input and predicts the latency \u2113of a dynamic convolutional block: \u2113= G(H,P,S,r ).Hardware modeling. We model a hardware device as multiple processing engines (PEs), and parallelcomputation can be executed on these PEs. As shown in Figure 3, we model the memory system as a3Here we refer to a stage as the cascading of multiple blocks which process features with the same resolution.4--- Page 5 ---Off-chip Memory(GDDR/DDR/HBM)On-chip Memory(LLC/Global Buffer)PE1PEMemoryComputation UnitsPE2PEMemoryComputation UnitsPEnPEMemoryComputation Units\u2026Figure 3: Our hardware model.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)101520253035Latency ( s)PredictedReal Figure 4: Latency prediction results.three-level structure [12]: 1) off-chip memory, 2) on-chip global memory, and 3) memory in PE. Sucha hardware model enables us to accurately predict the cost on both data movement and computation.Latency prediction. When simulating the data movement procedure, the ef\ufb01ciency of non-contiguous memory accesses under different granularity Ssettings is considered. As for the com-putation latency, it is important to adopt a proper scheduling strategy to increase the parallelism ofcomputation. Therefore, we search for the optimal scheduling (the con\ufb01guration of tiling and in-PEparallel computing) of dynamic operations to maximize the utilization of hardware resources. A moredetailed description of our latency prediction model is presented in Appendix A.Empirical validation. We take the \ufb01rst block in ResNet-101 as an example and vary the activationrate rto evaluate the performance of our prediction model. The comparison between our predictionsand the real testing latency on the Nvidia V100 GPU is illustrated in Figure 4, from which we canobserve that our predictor can accurately estimate the real latency in a wide range of activation rates.3.4 Implementation detailsWe use general optimization methods like fusing activation functions and batch normalizationlayers into convolution layers. We also optimize the speci\ufb01c operators in our spatial-wise dynamicconvolutional blocks as follows (see also Figure 2 for an overview).Fusing the masker and the \ufb01rst convolution. As mentioned in Sec. 3.1, the masker in each blockconsumes very little computation, but it takes the whole feature map as input. Therefore, it is amemory-bounded operation (the inference time is mainly spent on memory access). Since the maskerand the \ufb01rst convolution in the block share the same input, there is an opportunity to fuse these twooperations to avoid the repeated access of the input data. Note that a spatial-wise dynamic convolutionrequires the output of the masker. If we fuse the two layers, the \ufb01rst convolution will be changed to astatic operation, which may increase the inference latency. There exists a threshold of activation raterth, when r>r th, the overall latency can be reduced. We decide whether to fuse them according tothe average activation rate. See more details in Appendix B.1.Fusing the gather operation and the dynamic convolution. Traditional approaches \ufb01rst gather theinput pixels of the \ufb01rst dynamic convolution in a block (Figure 1 (b)). The gather operation is alsoa memory-bounded operation. Furthermore, when the size of the convolution kernel exceeds 1\xd71,the area of input patches may overlap, resulting in repeated memory load/store. We fuse the gatheroperation into the dynamic convolution to reduce the memory access.Fusing the scatter operation and the add operation. Traditional approaches scatter the outputpixels of the last dynamic convolution, and then execute the element-wise addition (Figure 1 (b)). Wefuse these two operators to reduce the memory access. The ablation study in Sec. 4.4 validates theeffectiveness of the proposed fusing methods.3.5 TrainingOptimization of non-differentiable maskers. The masker modules are required to produce binary-valued spatial masks for making discrete decisions, and cannot be directly optimized with backpropagation. Following [35, 31, 8], we adopt straight-through Gumbel Softmax [18, 24] to train thenetwork in an end-to-end fashion. Speci\ufb01cally, let \u02dcM \u2208RH\xd7W\xd72 denote the output of the maskgenerator. The decisions are obtained with the argmax function during inference. In the trainingphase, a differentiable approximation is de\ufb01ned by replacing the argmax operation with a Softmax:\u02c6M =exp{(log(\u02dcM:,:,0)+ G:,:,0)/\u03c4}\u22111k=0 exp{(log(\u02dcM:,:,k)+ G:,:,k)/\u03c4} \u2208 [0,1]H\xd7W , (1)5--- Page 6 ---0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1(a) Relationship between r\u2113 and rfor LAS-ResNet blocks on the Nvidia Tesla V100 GPU.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.75Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.752.00Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.4Latency ratio (r )S = 1(b) Relationship between r\u2113 and rfor LAS-RegNetY-800MF blocks on the Nvidia Jetson TX2 GPU.Figure 5: Latency prediction results of LAS-ResNet blocks on V100 (a) and LAS-RegNet blocks onTX2 (b). For both networks, we plot the relationship between the latency ratio r\u2113 and the activationrate r for the blocks in 4 stages with the convolutional stride 1. The practical ef\ufb01ciency is onlyimproved when r\u2113 <1. Note that S = 1can harm the practical latency even for a small r(reducedcomputation), while a larger Swill alleviate this problem. See detailed analysis in Sec. 4.2.where \u03c4 is the Softmax temperature. Following the common practice [ 31, 8], we let \u03c4 decreaseexponentially from 5.0 to 0.1 in training to facilitate the optimization of maskers.Training objective. The FLOPs of each spatial-wise dynamic convolutional block can be calculatedbased on our de\ufb01ned activation rate r[31]. Then we can obtain the FLOPs of the overall dynamicnetwork Fdyn. Let Fstat denotes the FLOPs of its static counterpart. We optimize their ratio toapproximate a target 0 <t< 1:LFLOPs = (FdynFstat\u2212t)2.In addition, we de\ufb01ne loss item Lbounds asin [31] to constrain the upper bound and the lower bound of activation rates in early training epochs.We further propose to leverage the static counterparts of our dynamic networks as \u201cteachers\u201d to guidethe optimization procedure. Let y and y\u2032denote the output logits of a dynamic model (\u201cstudent\u201d)and its \u201cteacher\u201d, respectively. Our \ufb01nal loss can be written asL= Ltask + \u03b1(LFLOPs + Lbounds) +\u03b2T2 \xb7KL(\u03c3(y/T)||\u03c3(y\u2032/T)), (2)where Ltask represents the task-related loss, e.g., cross-entropy loss in image classi\ufb01cation. KL(\xb7||\xb7)denotes the Kullback\u2013Leibler divergence, and \u03b1,\u03b2 are the coef\ufb01cients balancing these items. We use\u03c3to denote the log-Softmax function, and T is the temperature for computing KL-divergence.4 ExperimentsIn this section, we \ufb01rst introduce the experiment settings in Sec. 4.1. Then the latency of differentgranularity settings are analyzed in Sec. 4.2. The performance of our LASNet on ImageNet is furtherevaluated in Sec. 4.3, followed by the ablation studies in Sec. 4.4. Visualization results are illustratedin Sec. 4.5, and we \ufb01nally validate our method on the object detection task (Sec. 4.6). The results onthe instance segmentation task are presented in . For simplicity, we add \u201cLAS-\u201d as a pre\ufb01x beforemodel names to denote our LASNet, e.g., LAS-ResNet-50.4.1 Experiment settingsLatency prediction. Various types of hardware platforms are tested, including a server GPU (TeslaV100), a desktop GPU (GTX1080) and edge devices ( e.g., Nvidia Nano and Jetson TX2). Themajor properties considered by our latency prediction model include the number of processingengines (#PE), the \ufb02oating-point computation in a processing engine (#FP32), the frequency and thebandwidth. It can be observed that the server GPUs generally have a larger #PE than the IoT devices.The batch size is set as 1 for all dynamic models and computing platforms.Image classi\ufb01cation. The image classi\ufb01cation experiments are conducted on the ImageNet [ 4]dataset. Following [31], we initialize the backbone parameter from a pre-trained checkpoint4, and4We use the torchvision pre-trained models at https://pytorch.org/vision/stable/models.html.6--- Page 7 ---1 2 4 7 8 14 28Spatial granularity (S)12345Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.51.01.52.02.53.03.54.0Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.51.01.52.02.53.0Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(a) Relationship between r\u2113 and Sfor LAS-ResNet blocks on V100.1 2 4 7 8 14 28Spatial granularity (S)0.250.500.751.001.251.501.75Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.250.500.751.001.251.501.752.00Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.20.40.60.81.01.21.41.6Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(b) Relationship between r\u2113 and Sfor LAS-RegNetY-800MF blocks on Nvidia Jetson TX2 GPU.Figure 6: The relationship between the latency ratio r\u2113 and the spatial granularity S.\ufb01netune the whole network for 100 epochs with the loss function in Eq. (2). We \ufb01x \u03b1= 10,\u03b2 = 0.5and T = 4.0 for all dynamic models. More details are provided in Appendix B.4.2 Latency prediction resultsIn this subsection, we present the latency prediction results of the spatial-wise dynamic convolutionalblocks in two different models: LAS-ResNet-101 [11] (on V100) and LAS-RegNetY-800MF [25] (onTX2). All the blocks have the bottleneck structure with different channel numbers and convolutiongroups, and the RegNetY is equipped with Squeeze-and-Excitation (SE) [14] modules.We \ufb01rst de\ufb01ne \u2113dyn as the latency of a spatial-wise dynamic convolutional block, and \u2113stat as that ofa static block without a masker. Their ratio is denoted as r\u2113 = \u2113dyn\u2113stat. We investigate the relationshipbetween r\u2113 and the activation rate r(cf. Sec. 3.5) for different granularity settings. The results inFigure 5 demonstrate that: 1) even equipped with our special optimization on the scheduling strategies,pixel-level spatially adaptive inference (S=1) cannot always improve the practical ef\ufb01ciency. Such\ufb01ne-grained adaptive inference is adopted by most previous works [31, 35], and our result can explainthe reason why they can only achieve realistic speedup on less powerful CPUs [35] or specializeddevices [3]; 2) a proper granularity S >1 effectively alleviates this problem on both hardwaredevices. By setting S >1, realistic speedup could be achieved with larger activation rates.The latency prediction results are further used to seek for a preferable granularity setting for the\ufb01rst 3 stages (we \ufb01x S = 1 for the last stage, where the feature resolution is 7 \xd77). Therefore,we plot the relationship between r\u2113 and S in Figure 10. It can be observed that: 1) r\u2113 generallydecreases with Sincreasing for a given r; 2) an overly largeS(less \ufb02exible adaptive inference) bringsinsigni\ufb01cant improvement on both devices. Especially, enlarging S from 8 to 28 in the \ufb01rst stageof a LAS-ResNet brings very little improvement on V100. Based on the results in Figure 10, wecan trade off between \ufb02exibility and ef\ufb01ciency by selecting appropriate Sfor different models andhardware devices. For example, we can simply set Snet=8-4-7-15 in a LAS-ResNet-101 to achiverealistic speedup. The accuracy-latency plots in Figure 7 also validate this observation. More resultsof our latency prediction model on the desktop-level GPU, GTX1080, are presented in Appendix C.1.4.3 ImageNet classi\ufb01cation resultsWe now empirically evaluate our proposed LASNet on the ImageNet dataset. The network perfor-mance is measured in terms of the trade-off between classi\ufb01cation accuracy and inference ef\ufb01ciency.Both theoretical (i.e. FLOPs) and practical ef\ufb01ciency (i.e. latency) are tested in our experiments.5We use this form to represnet the Ssettings for the 4 stages of a network.7--- Page 8 ---2 3 4 5 6 7 8FLOPs (x1e9)73747576777879Accuracy (%)static ResNetsConvAIG-ResNet-101SkipNet-ResNet-101BAS-ResNet-50DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)15 20 25 30 35 40Latency on Nvidia Jetson TX2 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)1.5 2.0 2.5 3.0 3.5Latency on Nvidia Tesla V100 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)(a) LAS-ResNet results.0.2 0.3 0.4 0.5 0.6 0.7 0.8FLOPs (x1e9)717273747576Accuracy (%)RegNets-XRegNets-YMobileNets-v2ShuffleNets-v2CondenseNetsLAS-RegNetY-400MF (Snet=1-1-1-1)LAS-RegNetY-400MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)3 4 5 6 7 8Latency on Nvidia Jetson TX2 GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-XRegNets-YMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)8 10 12 14 16 18 20Latency on Nvidia nano GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-YRegNets-XMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)(b) LAS-RegNetY results.Figure 7: Experimental results on ImageNet. The proposed coarse-grained spatially adaptive inferenceis tested on standard ResNets (a) and lightweight RegNets (b).4.3.1 Standard baseline comparison: ResNetsWe \ufb01rst establish our LASNet based on the standard ResNets [ 11]. Speci\ufb01cally, we build LAS-ResNet-50 and LAS-ResNet-101 by plugging our maskers in the two common ResNet structures.The baselines include various types of dynamic inference approaches: 1) layer skipping (SkipNet[32] and Conv-AIG [30]); 2) channel skipping (BAS [1]); and 3) pixel-level spatial-wise dynamicnetwork (DynConv [31]). For our LASNet, we compare various settings of the spatial granularitySnet. We set training targets (cf. Sec. 3.5) t\u2208{0,4,0.5,0.6,0.7}for our dynamic models to evaluatetheir performance in different sparsity regimes. We apply the same operator fusion (Sec. 3.4) for bothour models and the compared baselines [30, 31] for fair comparison.Results are presented in Figure 7 (a). On the left we plot the relationship of accuracy v.s. FLOPs. Itcan be observed that our LAS-ResNets with different granularity settings signi\ufb01cantly outperformthe competing dynamic neural networks. Surprisingly, coarse-grained spatially adaptive inference(Snet=4-4-2-1 and Snet=8-4-7-1 for the 4 stages) can achieve even higher accuracy when consumingsimilar FLOPs on ResNets, despite the sacri\ufb01ced \ufb02exibility compared toSnet=1-1-1-1. We conjecturethat a larger Sis also bene\ufb01cial to the optimization of maskers.We compare the practical latency of three granularity settings in Figure 7 (a) predicted by our latencyprediction model (middle on TX2 and right on V100). We can witness that although they achievecomparable theoretical ef\ufb01ciency (Figure 7 (a) left), larger Sis more hardware-friendly comparedto the \ufb01nest granularity. For example, the inference latency of LAS-ResNet-101 ( Snet=1-1-1-1)is signi\ufb01cantly higher than the ResNet-101 baseline on V100 (Figure 7 (a) right), even though itstheoretical computation is much smaller than that of the static model. However, larger granularities(Snet=4-4-2-1 and Snet=8-4-7-1) can effectively improve the inference latency due to its lower burdenon the memory access. Remarkably, the latency of ResNet-101 could be reduced by 36% and 46% onV100 and TX2 respectively without sacri\ufb01cing the accuracy when t=0.4. The classi\ufb01cation accuracyis increased by 1.9% with similar inference ef\ufb01ciency. It can be observed that the realistic speedupratio r\u2113 is more close to the theoretical FLOPs ratio target ton the less powerful TX2, because thelatency is computation-bounded (i.e. the latency is mainly spent on computation) on such IoT devices.In contrast, there is a larger gap between practical and theoretical ef\ufb01ciency on the more powerfulV100, as the latency is bounded by the memory access cost.4.3.2 Lightweight baseline comparison: RegNetsWe further evaluate our LASNet in lightweight CNN architectures,i.e. RegNets-Y [25]. Two differentsized models are tested: RegNetY-400MF and RegNetY-800MF. Compared baselines include othertypes of ef\ufb01cient models, e.g., MobileNets-v2 [28], Shuf\ufb02etNets-v2 [23] and CondenseNets [16].The results are presented in Figure 7 (b). The x-axis for the three sub-\ufb01gures are the FLOPs, thelatency on TX2, and the latency on the Nvidia nano GPU, respectively. We can observe that our8--- Page 9 ---1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2Latency on Nvidia Tesla V100 GPU (ms)76.577.077.578.078.579.0Accuracy (%) static ResNetsLAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=4-4-7-1)LAS-ResNet-101 (Snet=8-4-7-1)Figure 8: Ablation studies on S. Figure 9: Visualization results.method outperforms various types of static models in terms of the trade-off between accuracy andef\ufb01ciency. More results on image classi\ufb01cation are provided in Appendix C.2.4.4 Ablation studiesWe conduct ablation studies to validate the effectiveness of our coarse-grained spatially adaptiveinference (Sec. 3.2) and operator fusion operations (Sec. 3.4).More granularities settings. We test various granularity settings on LAS-ResNet-101 to examinethe effects of Sin different stages. The results on the Tesla-V100 GPU are presented in Figure 8. Itcan be found that the \ufb01nest granularity (Snet=1-1-1-1) leads to substantial inef\ufb01ciency despite thereduced FLOPs (cf. Figure 7 (a) left). Coarse-grained spatially adaptive inference in the \ufb01rst twostages (Snet=4-4-1-1) effectively reduces the inference latency. We further increase Sin the thirdstage to 2 and 7, and this procedure consistently improves the realistic ef\ufb01ciency on the V100 GPU.This trend also holds on the GTX 1080 GPU (see the results in Appendix C.2.Table 1: Ablation studies on operator fusion.Masker- Gather- Scatter- LatencyConv1x1 Conv3x3 Add (\xb5s)   163.2   90.1   86.7   71.4Operator fusion. We investigate the effect of ouroperator fusion introduced in Sec. 3.4. One convolu-tional block in the \ufb01rst stage of a LAS-ResNet-101(S=4, r=0.6) is tested. The results in Table 1 vali-date that every step of operator fusion bene\ufb01ts thepractical latency of a block, as the overhead onmemory access is effectively reduced. Especially,the fusion of the masker operation and the \ufb01rst con-volution is crucial to reducing the latency.4.5 VisualizationWe visualize the masks generated by our masker in the third block of a LAS-ResNet-101 (Snet=4-4-2-1) in Figure 12. The brilliant areas correspond to the locations of 1 elements in a mask, andthe computation on the dimmed regions is skipped by our dynamic model. We can witness that themasker accurately locate the most task-related regions (even the tiny aircraft at the corner), whichhelps reduce the unnecessary computation on background areas. These resultsalso suggest that forthe \ufb01rst stage, the granularity S=4 is suf\ufb01ciently \ufb02exible to recognize the important regions, anda win-win can be achieved between accuracy and ef\ufb01ciency. Interestingly, the masker could selectsome objects that are not labeled for the sample, e.g., the \ufb02ower beside the hummingbird and thehuman holding the camera. This indicates that our spatial-wise dynamic networks can automaticallyrecognize the regions with semantics, and their capability is not limited by the classi\ufb01cation labels.This property is helpful in some downstream tasks, such as object detection (Sec. 4.6) and instancesegmentation (Sec. 4.7), which require detecting multiple classes and objects in an image. Morevisualization results could be found in Appendix C.3.4.6 COCO Object detectionWe further evaluate our LASNet on the COCO [22] object detection task. The mean average precision(mAP), the average backbone FLOPs, and the average backbone latency on the validation set areused to measure the network performance. We test two commonly used detection frameworks: FasterR-CNN [27] with Feature Pyramid Network [20] and RetinaNet [21]. Thanks to the generality ofour method, we can conveniently replace the backbones with ours pre-trained on ImageNet, andthe whole models are \ufb01netuned on COCO with the standard setting for 12 epochs (see detailed9--- Page 10 ---Table 2: Object detection results on the COCO dataset.Detection Backbone Backbone Backbone Latency (ms)mAP (%)Framework FLOPs (G) V100 GTX1080 TX2Faster R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 39.4LAS-ResNet-101 (Snet=4-4-2-1,t=0.6) 90.7 33.8 90.6 524.9 40.3LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 79.3 30.7 82.5 477.1 39.8LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.5 29.0 79.9 464.8 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 67.9 25.3 69.1 401.3 39.5RetinaNetResNet-101 (Baseline) 141.2 39.5 119.3 729.4 38.5LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 77.8 30.4 81.8 472.6 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.4 28.9 79.9 464.8 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 66.4 25.3 69.1 401.3 38.9Table 3: Instance Segmentation results on the COCO dataset.Segmentation Backbone Backbone Backbone Latency (ms)APmask(%) APbox(%)Framework FLOPs (G) V100 GTX1080 TX2Mask R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 36.1 40.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 80.5 31.1 83.3 481.9 37.0 41.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.4) 69.2 27.9 74.8 431.6 36.1 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 68.8 25.8 70.9 411.8 36.2 40.0setup in Appendix B.3). The input images are resized to a short side of 800 and a long side notexceeding 1333. The results of our LAS-ResNet-101 with different Snet settings are presented inTable 2. We can observe from the results that when setting the training target as 0.4, the latency ofour LAS-ResNet-101 with Snet=4-4-7-1 is signi\ufb01cantly lower than the static baseline on all deviceswithout sacri\ufb01cing mAP in both detection frameworks. With a larger training targets, our LASNetcan increase the mAP by 0.9% and 0.8% in Faster R-CNN and RetinaNet respectively, while stillbeing faster than the baseline method.4.7 COCO instance segmentationWe also present the results of instance segmentation on COCO, which demonstrate the effectivenessof our LASNet on the dense prediction task. From the results in Table 3, we can observe that whensetting the training target as 0.4, the Mask R-CNN [ 10] models (Snet=4-4-2-1 and Snet=4-4-7-1)runs faster on all tested hardware devices without sacri\ufb01cing the performance. With a training targetof 0.5, the APmask and APbox of the Mask R-CNN model could be increased by 0.9% and 1.0%respectively while still running faster than the baseline method.5 ConclusionIn this paper, we propose to build latency-aware spatial-wise dynamic networks (LASNet) under theguidance of a latency prediction model. By simultaneously considering the algorithm, the schedulingstrategy and the hardware properties, we can ef\ufb01ciently estimate the practical latency of spatial-wise dynamic operators on arbitrary computing platforms. Based on the empirical analysis on therelationship between the latency and the granularity of spatially adaptive inference, we optimizeboth the algorithm and the scheduling strategies to achieve realistic speedup on many multi-coreprocessors, e.g., the Tesla V100 GPU and the Jetson TX2 GPU. Experiments on image classi\ufb01cation,object detection and instance segmentation tasks validate that the proposed method signi\ufb01cantlyimproves the practical ef\ufb01ciency of deep CNNs, and outperforms various competing approaches.AcknowledgementThis work is supported in part by the National Key R&D Program of China under Grant2020AAA0105200, the National Natural Science Foundation of China under Grants 62022048and the Tsinghua University-China Mobile Communications Group Co.,Ltd. Joint Institute..10--- Page 11 ---References[1] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping for learningconditional channel gated networks. In ICLR, 2020.[2] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, MeghanCowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. Tvm: An automated end-to-end optimizingcompiler for deep learning. In OSDI, 2018.[3] Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian Verhelst. Proces-sor architecture optimization for spatially dynamic neural networks. In VLSI-SoC, 2021.[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In CVPR, 2009.[5] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicatednetwork with less inference complexity. In CVPR, 2017.[6] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov,and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In CVPR,2017.[7] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamicneural networks: A survey. TPAMI, 2021.[8] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, and Haojun Jiang. Spatiallyadaptive feature re\ufb01nement for ef\ufb01cient inference. TIP, 2021.[9] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfen Cao, Wenhui Huang,Chao Deng, and Gao Huang. Learning to weight samples for dynamic early-exiting networks.In ECCV, 2022.[10] Kaiming He, Georgia Gkioxari, Piotr Doll\xe1r, and Ross Girshick. Mask r-cnn. In Proceedings ofthe IEEE international conference on computer vision, pages 2961\u20132969, 2017.[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In CVPR, 2016.[12] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach .Elsevier, 2011.[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, TobiasWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ef\ufb01cient convolutional neuralnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.[15] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Wein-berger. Multi-scale dense networks for resource ef\ufb01cient image classi\ufb01cation. In ICLR, 2018.[16] Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: Anef\ufb01cient densenet using learned group convolutions. In CVPR, 2018.[17] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Weinberger.Densely connected convolutional networks. TPAMI, 2019.[18] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.In ICLR, 2017.[19] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In NeurIPS, 2017.[20] Tsung-Yi Lin, Piotr Doll\xe1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.Feature pyramid networks for object detection. In CVPR, 2017.[21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Focal loss for denseobject detection. In ICCV, 2017.11--- Page 12 ---[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,Piotr Doll\xe1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,2014.[23] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shuf\ufb02enet v2: Practical guidelinesfor ef\ufb01cient cnn architecture design. In ECCV, 2018.[24] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuousrelaxation of discrete random variables. In ICLR, 2017.[25] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Design-ing network design spaces. In CVPR, 2020.[26] Mengye Ren, Andrei Pokrovsky, Bin Yang, and Raquel Urtasun. SBNet: Sparse Blocks Networkfor Fast Inference. CVPR, 2018.[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-timeobject detection with region proposal networks. In NeurIPS, 2015.[28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.[29] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, andQuoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820\u20132828,2019.[30] Andreas Veit and Serge Belongie. Convolutional networks with adaptive inference graphs. InECCV, 2018.[31] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: Exploiting Spatial Sparsity forFaster Inference. In CVPR, 2020.[32] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learningdynamic routing in convolutional networks. In ECCV, 2018.[33] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus:a dynamic approach to reducing spatial redundancy in image classi\ufb01cation. In NeurIPS, 2020.[34] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, YuandongTian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware ef\ufb01cient convnetdesign via differentiable neural architecture search. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 10734\u201310742, 2019.[35] Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptiveinference with stochastic feature sampling and interpolation. In ECCV, 2020.[36] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptivenetworks for ef\ufb01cient inference. In CVPR, 2020.[37] Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan, and Yunhe Wang.Dynamic resolution network. NeurIPS, 2021.12--- Page 13 ---AppendixA Latency prediction model.As the dynamic operators in our method have not been supported by current deep learning libraries,we propose a latency prediction model to ef\ufb01ciently estimate the real latency of these operators onhardware device. The inputs of the latency prediction model include: 1) the structural con\ufb01gurationof a convolutional block, 2) its activation rate rwhich decides the computation amount, 3) the spatialgranularity S, and 4) the hardware properties mentioned in Table 4. The latency of a dynamic blockis predicted as follows.Input/output shape de\ufb01nition. The \ufb01rst step of predicting the latency of an operation is to calculatethe shape of input and output. Taking the gather-conv2 operation as an example, the input of thisoperation is the activation with the shape of Cin \xd7H \xd7W, where Cin is the number of inputchannels, and H and W are the resolution of the feature map. The shape of the output tensor isP \xd7Cout \xd7S\xd7S, where P is the number of output patches, Cout is the number of output channelsand Sis the spatial granularity. Note that P is obtained based on the output of our maskers.Operation-to-hardware mapping. Next, we map the operations to hardware. As is mentioned in thepaper, we model a hardware device as multiple processing engines (PEs). We assign the computationof each element in the output feature map to a PE. Speci\ufb01cally, we consecutively split the outputfeature map into multiple tiles. The shape of each tile is TP \xd7TC \xd7TS1 \xd7TS2. These split tilesare assigned to multiple PEs. The computation of the elements in each tile is executed in a PE. Wecan con\ufb01gure different shapes of tiles. In order to determine the optimal shape of the tile, we makea search space of different tile shapes. The tile shape has 4 dimensions. The candidates of eachdimension are power-of-2 and do not exceed the corresponding dimension of the feature map.Latency estimation. Then, we evaluate the latency of each tile shape in the search space and selectthe optimal tile shape with the lowest latency. The latency includes the data movement latency andthe computation latency: \u2113= \u2113data + \u2113computation.1) Data movement latency \u2113data. The estimation of the latency for data movement requires us tomodel the memory system of a hardware device. We model the memory system of hardware as athree-level architecture [12]: off-chip memory, on-chip global memory, and local memory in PE.The input data and weight data are \ufb01rst transferred from the off-chip memory to the on-chip globalmemory. We assume the hardware can make full use of the off-chip memory bandwidth to simplifythe latency prediction model.After that, the data used to compute the output tiles is transferred from on-chip global memory tothe local m, Your response: ```ADD_PAPER\n2210.06223v1\nThis paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach guided by a novel latency prediction model that accounts for algorithm design, scheduling strategies, and hardware properties. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method achieves these gains through optimized spatial granularity selection and operator fusion techniques, demonstrating significant improvements over pixel-level dynamic networks in practical deployment scenarios.\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\n~~~~~~~~~~\nCurrent Step #11, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Conditionally Deep Hybrid Neural NetworksAcross Edge and CloudYinghan Long*, Indranil Chakraborty *, Kaushik Roy(* Authors contributed equally to the work)School of Electrical and Computer Engineering, Purdue Universitylong273@purdue.edu, ichakra@purdue.edu, kaushik@purdue.eduAbstract\u2014The pervasiveness of \u201cInternet-of-Things\u201d in ourdaily life has led to a recent surge in fog computing, encompassinga collaboration of cloud computing and edge intelligence. To thateffect, deep learning has been a major driving force towardsenabling such intelligent systems. However, growing model sizesin deep learning pose a signi\ufb01cant challenge towards deploymentin resource-constrained edge devices. Moreover, in a distributedintelligence environment, ef\ufb01cient workload distribution is neces-sary between edge and cloud systems. To address these challenges,we propose a conditionally deep hybrid neural network forenabling AI-based fog computing. The proposed network can bedeployed in a distributed manner, consisting of quantized layersand early exits at the edge and full-precision layers on the cloud.During inference, if an early exit has high con\ufb01dence in the clas-si\ufb01cation results, it would allow samples to exit at the edge, andthe deeper layers on the cloud are activated conditionally, whichcan lead to improved energy ef\ufb01ciency and inference latency. Weperform an extensive design space exploration with the goal ofminimizing energy consumption at the edge while achieving stateof the art classi\ufb01cation accuracies on image classi\ufb01cation tasks.We show that with binarized layers at the edge, the proposedconditional hybrid network can process 65% of inferences atthe edge, leading to 5.5 \xd7 computational energy reduction withminimal accuracy degradation on CIFAR-10 dataset. For themore complex dataset CIFAR-100, we observe that the proposednetwork with 4-bit quantization at the edge achieves 52% earlyclassi\ufb01cation at the edge with 4.8\xd7 energy reduction. The analysisgives us insights on designing ef\ufb01cient hybrid networks whichachieve signi\ufb01cantly higher energy ef\ufb01ciency than full-precisionnetworks for edge-cloud based distributed intelligence systems.Index Terms\u2014Conditional deep learning, quantized neuralnetwork, fog computingI. I NTRODUCTIONThe age of \u201cInternet-of-Things\u201d (IoT) has touched humanlives in an unprecedented manner by endowing us withremarkable connectivity and autonomous intelligent systems[1]. These bene\ufb01ts come with the necessity of processinghumongous amounts of heterogeneous data obtained from theenvironment. The ability of deep learning to reliably processsuch data has resulted in it playing a dominant role in awide range of Arti\ufb01cial Intelligence (AI) applications, includ-ing image classi\ufb01cation [2]\u2013[4], natural language processing[5], and object detection [6]. As the third wave of arti\ufb01cialintelligence (AI) is accelerating, researchers are incorporatingThe research was funded in part by C-BRIC, one of six centers in JUMP, aSemiconductor Research Corporation (SRC) program sponsored by DARPA,the National Science Foundation, Intel Corporation, Vannevar Bush FacultyFellowship, and the U.K. Ministry of Defence under Agreement NumberW911NF-16-3-0001.AI into IoT applications. Traditionally, these cognitive appli-cations perform compute-intensive tasks in a centralized hubwhich collects data from thousands of connected edge devices.However, in the recent past, there have been rising concerns ofdata security due to over-centralization of information, and thecontinuous communication overhead between the edge devicesand the cloud drastically increases the power consumption ofworld-wide internet. Moreover, real-time AI systems such ashealth monitoring [7] and autonomous driving [8] require fastprocessing. Hence, to truly realize the potential of AI in IoT,it is necessary to enable intelligence at the edge [9].On the other hand, the resource constraints of edge deviceslimit us from deploying large deep learning models at theedge. With the growing complexity of tasks, there has beena considerable increase in the sizes of deep neural networks(DNNs). This requires DNNs to be trained on powerfulGraphic Processing Units (GPUs) or Tensor Processing Units(TPUs) [10]. Edge devices, however, are resource-constrainedand implementing DNNs on such devices jeopardizes therequirement of long battery life for stable operation. Thishas propelled the search for techniques that enable energy-ef\ufb01cient implementations of deep learning models. Besides,researchers are investigating ef\ufb01cient techniques to enabledistributed intelligence at the near-user edge device and end-user cloud [11].A popular approach to reduce model complexity and im-prove computational ef\ufb01ciency is to quantize the weights andactivations of neural networks by representing 32-bit \ufb02oating-point values with lower bit-depth \ufb01xed point [12] or evenbinary values [13] [14]. Compared to full-precision networks,binary neural networks are fast and ef\ufb01cient, but this comeswith a degradation in performance. For example, binarizingboth inputs and weights of a residual network(ResNet) causes18% reduction in top-1 accuracy on ImageNet and 9% onCIFAR10. In order to overcome this challenge, there have beenvarious approaches towards improving quantization schemes[15], [16] as well as proposing training algorithms for net-works with increased bit-precision [17]\u2013[19]. An alternativeapproach towards preserving the performance while achievinga signi\ufb01cant energy-ef\ufb01ciency is designing hybrid networkarchitectures [20], [21], which consist of both quantized andhigh-precision layers. Since high-precision layers are power-hungry, such computations can be performed in the cloud.In this work, we propose a design methodology to buildconditionally deep hybrid networks for distributed intelligenceacross an edge-cloud system. In such a network, quantizedarXiv:2005.10851v1  [cs.LG]  21 May 2020--- Page 2 ---Full-precisionNetworkQuantizedNetworkfcQuantizedNetwork+Early exitsDeep NeuralNetworkHybrid NeuralNetworkConditionally DeepHybrid NetworkEdgefcFull-precisionNetworkfcCloudFull-precisionNetworkHybridQuantizationConditionalDeep\xa0LearningEarly resultsFinal resultsInputsInputsEasy Hard Inputs:Hard Easy Fig. 1. Overview of our design methodology for Conditionally Deep Hybrid Networks. From left to right, we show how a standard deep neural network isdeveloped into a Conditionally Deep Hybrid Network and distributed to an edge-cloud system.layers of the network are deployed in the edge device, whilehigh-precision layers are performed on the cloud. Further, byusing the Conditional Deep Learning (CDL) approach [22],our network leverages the dif\ufb01culty of inputs to classify therelatively easy samples with early exits and conditionallyactivate the deeper layers. This technique is particularly suitedfor a distributed intelligence system where we can process theeasy inputs at the energy-ef\ufb01cient edge device with quantizednetworks while only the inputs that are close to the decisionboundary are sent to the cloud. As a result, both communica-tion cost and computation cost can be improved. Through anextensive analysis of hybrid network architectures, we identifythe tradeoffs in terms of energy, latency, accuracy as well asbit-precision of the layers for optimal hybrid network design.The contributions of our work are as follows:\u2022 We propose a methodology for designing hybrid precisionneural networks for edge-cloud processing systems.\u2022 We engineer early exiting strategies to conditionally acti-vate the deeper layers of the neural networks for enforcingenergy-ef\ufb01ciency in edge devices.\u2022 We evaluate the proposed methodology on state-of-artnetwork architectures. We show that by modifying net-work con\ufb01gurations such as bit-precision at the edge,early exiting is triggered in most cases.The paper is organized as follows. The next section gives anoverview of the design methodology for conditionally deephybrid neural networks. Section III reports the experimentresults and shows how to adjust network con\ufb01gurations toachieve a better performance-vs-energy trade-off.II. H YBRID NETWORK DESIGN WITH CONDITIONAL DEEPLEARNINGIn this section, we propose a methodology for designinghybrid neural networks and explore strategies to enable earlyclassi\ufb01cation in the edge network based on the dif\ufb01culty ofinputs to achieve early processing of data and reduction inenergy consumption.A. Hybrid Neural NetworksWe propose a hybrid neural network where a N-layernetwork is divided into two sections, M layers where thebit-precision of the weights and inputs is p 1 and N \u2212Mlayers where the corresponding bit-precisions are p 2 wherep 2 \u226bp 1. This is illustrated in Fig. 1. Generally, p 2 can beconsidered as a 32-bit \ufb02oat as the primary objective for ourdesign is to preserve the accuracy of the neural network. Thedesign parameters p 1 and M can be varied to identify theoptimal division for the proposed hybrid networks consideringthe tradeoff between performance and energy consumption.The low-precision section of a hybrid CNN consists ofQuanConv layers by which we quantize both the weights andinputs. Fig. 2 illustrates that a QuanConv layer is comprisedof a batch normalization layer, an activation layer, and aconvolution layer using quantized weights in order. In thecase of a ResNet [23], the residual connections are kept full-precision to mitigate the negative effect of quantized inputswith a small overhead. A batch normalization layer is appliedbefore quantization to ensure that inputs hold zero mean [14].Then the activation layer quantizes inputs using the followingscheme. For an input matrix I \u2208 R c\xd7wi\xd7hi , where c,w,hare the number channels, width and height respectively, thecorresponding binary input matrix I B \u2208 {+1 , \u22121 }c\xd7wi\xd7hiand quantized input matrix I Q is given byI B = sign( I ); I Q = v 1(\u230aI + v 0v 1\xb7Z\u2309/Z \u2212v 2) (1)where Z = 2 p \u22121 is the number of quantization levels andv 0,v 1,v 2 are constants.The quantization of weights is performed for each layer ina CNN. Let us represent the kernel as W \u2208R c\xd7w\xd7h, wherec,w,h are the number channels and width and height of thekernel respectively. If both weights and inputs are extremelyquantized (binarized), dot product operations in convolutionscan be implemented ef\ufb01ciently using XNOR and bit-counting--- Page 3 ---operations. To approximate the convolution operation, weestimate the real-value weight matrix W using a quantizedmatrix WQ and a scaling factor \u03b1 as proposed in [14].I\u2217W \u2248(IQ \u2217WQ)\u03b1; \u03b1= \u2225W\u2225l1n (2)WQ = v1(\u230aW + v0v1\xb7Z\u2309/Z\u2212v2) (3)where n= c\xd7w\xd7h is a constant. The precision of weightsand activations of the high precision section of a hybrid CNNare kept 32-bit \ufb02oating point. We adopt the training algorithmproposed by Rastegari et al [14] to train the networks withquantization.QuanConv QuanConv QuanConv ... Residual connection... BatchNormQuantizedActivationConvolutionQuanConv Fig. 2. Quantized convolutional layers in a ResNetIn a hybrid neural network, the edge network must sendextracted features corresponding to all samples to the cloudfor further processing, which takes communication energy andtime. In the next subsection, we will introduce how to enablethe edge to analyze easy data independently.B. Conditional Exit in Deep Hybrid NetworksEnabling distributed intelligence using hybrid networks inan AI-based fog computing system requires data processingboth at the edge and the cloud. Since the dif\ufb01culty of examplesin a dataset often exhibits an inherent variability, we apply theconcept of conditional deep learning [22] on hybrid networksto enable classi\ufb01cation of relatively simpler images at the low-precision edge, while the more dif\ufb01cult ones are passed to thehigh-precision deeper layers of the network in the cloud. Suchconditional activation of deeper layers in the context of hybridnetworks provides both communicational and computationalenergy ef\ufb01ciency and run-time improvements over standardnetworks. Each early exit at the edge is a linear classi\ufb01er thatconsists of an average pooling layer and a fully connectedlayer. Typically, such an exit needs to be placed at the end ofthe low-precision section of the hybrid network, i.e, the partof the network being implemented on edge. However, moreexits can be added in the low-precision section to facilitateearly classi\ufb01cation of easier examples. In Fig. 3, we show anexample of adding three early exits. The input to an earlyexit is the output from a convolutional layer, and all the earlyexits use full-precision weights to ensure no computation erroroccurs in the classi\ufb01cation layer.During the training process, although the early exits havealready provided labels, the rest of the network would stillbe activated during training to optimize all exits based on theCloudEdgeQuanConv Layers 1QuanConv Layers 2Conv Layers 3-2Conv Pooling & fcPooling & fcPooling & fcEarly Exit 0Early Exit 1Final ExitQuanConv Layers 3-1Pooling & fcEarly Exit 2Fig. 3. An example of a hybrid neural network with three early exits. Blocksin orange are quantized. The sizes of early exits differ corresponding to theirlocations.whole training set. Since there are more than one labels givenby our network system, we need to modify the optimizationobjective to train both the hybrid network and additionalexits. There are two optional training strategies: 1) SeparateOptimization, 2) Joint optimization.1) Separate optimization: In this approach, we \ufb01rst trainthe hybrid network without any early exits, hence, the sameoptimization objective can be used. Then we \ufb01x the parametersof all convolutional layers and train one early exit at a time.Let us denote the ground-truth label vector as yand the outputlabel given by early exit k as \u02c6yk. The optimization objectiveof the kth early exit isLk( \u02c6yk,y) =\u2212\u2211c\u2208Cyc log \u02c6ykc (4)where \u02c6ykc is a vector containing predicted probabilities for allpossible class labels and C represents the set of all classes.The backward propagation function will be called for k timeswith respect to Lk. Since parameters of convolutional layersdo not require gradient, the backward propagations in earlyexits are independent of each other.2) Joint optimization: In this approach, we use a jointoptimization technique to train the network along with earlyexits [24]. The total loss is calculated as the weighted sum ofthe loss functions of each exit branch.L(\u02c6y,y) =K\u2211k=1\u03bbkLk(\u02c6yk,y) (5)where K is the total number of exits and \u03bbk is set empiricallyand sum to 1. By joint optimization, the gradients used toupdate weights of the convolutional layers become\u2202L\u2202w =K\u2211k=1\u03bbk\u2202Lk\u2202w (6)Compared to separate optimization, training the entire networktogether optimizes the weights of the convolutional layersbased on the losses of both the \ufb01nal exit and early exits,--- Page 4 ---so it enables more samples to exit early and enlarges theenergy saving. On the other hand, separate optimization ismore \ufb02exible because it allows users to add early exits to atrained network. We will compare the results corresponding tothese two optimization strategies in section III. Because ourobjective is to make as many inferences to \ufb01nish at the edgeas possible, we will use joint optimization for the rest of ourexperiments.Algorithm 1: Inference using a conditionally deep hybridneural networkInput: Samples XOutput: Classi\ufb01cation results \u02c6Yfor sample i doO1[i] = Forward( X[i],Wt[1]);for layer l = 2 to M do//Propagate at the edgeOl[i] = Forward( Ol\u22121[i],Wt[l]);if Classi\ufb01er k locates at layer l then\u02c6Yk[i] = Classi\ufb01er[k]( Ol[i]);if Entropy(\u02c6Yk[i])<Threshold k then\u02c6Y[i] = \u02c6Yk[i];break; //take early exitendendendif \u02c6Y[i] not found then\u02c6Y[i] = Forward(OM [i], Wt[M : N]);//Continuepropagating on the cloudendendOnce the hybrid deep network is trained, we can utilizequantized weights and early exits to allow inference of easysamples to \ufb01nish at the edge. Algorithm 1 shows the inferenceprocess. To classify a sample, we activate the layers of the deepnetwork conditionally based on the entropy of the predictionresult calculated byentropy(\u02c6y) =\u2212\u2211c\u2208C\u02c6yc log \u02c6ycIf the entropy is lower than the threshold, it means the earlyexit has high con\ufb01dence about correctly labeling this sample,so the prediction can be returned without activating laterlayers. The thresholds for early exits are set empirically.C. Design Considerations of Conditionally Deep Hybrid Net-work1) Effect of bit-precision: When conditional deep learningis applied to a hybrid network, the effect of bit-depth on thetotal energy consumption of the edge-cloud system becomestwofold. Although the energy consumed by an operation isproportional to the bit-depth, increasing the bit-depth meanshaving a more precise network at the edge, so the numberof activated high-precision operations would be fewer thanksto early exiting. As a result, using more bits for quantizationdoes not necessarily lead to higher energy consumption. Toachieve better performance-vs-energy trade-off, we will adjustthe bit-depth depending on the complexity of datasets.2) Effect of the number of layers on edge: In a conditionallydeep hybrid network, the inference accuracy is not commensu-rate with the proportion of quantized layers due to additionalexits. For simplicity, suppose that we have only one earlyexit located after the last layer at the edge. The predictionability of the \ufb01nal exit on the cloud would be weakened tothe degree corresponding to the number of quantized layers.However, the overall accuracy of the hybrid network dependson the accuracy of all exits, and that of the early exit isaffected by the number of layers before it and the numberof parameters contained in its fully-connected layer. As thenumber of quantized layers increases, the early exit can inferbased on the high-level features extracted by a deeper network,which makes the inference task easier and potentially allowsmore samples to exit. Nevertheless, convolutional layers indifferent divisions of a ResNet have three distinct output sizes,and hence the number of parameters contained in the earlyexit drops signi\ufb01cantly when moving to the later division.Therefore, it is important to \ufb01nd the optimal division of ahybrid network.3) Effect of the number of exits: Using multiple exits atthe edge can allow inference to terminate as soon as possible,thus the latency can be further reduced. In spite of that,whether it can also provide further energy saving to the edgedepends on whether the power consumption of an earlier exitis smaller than the total power of convolutional layers and thelater exit located in the next division of the network. Becauseconvolutional layers are quantized but exits are not, their powerconsumption becomes a considerable part. If the number ofclasses in the dataset is large and the network is not verydeep, using more than one early exits is likely to consumemore power and hence is not desired.III. E XPERIMENTSTo validate the effectiveness of our model, we conductseveral sets of experiments using PyTorch. We report resultsof ResNet32 with varying hybrid con\ufb01gurations and condi-tional deep learning over CIFAR10 and CIFAR100 datasetsto explore the design space and evaluate the scalability. Weestimate the total energy consumption of inference by mul-tiplying the number of full-precision and binary operationswith corresponding energy listed in Table. I. Convolutions ofmatrices are implemented as multiply-and-add (MAC). For allour analysis, we have not included the communication energybetween the edge and the cloud.Operation Energy (pJ) Operation Energy (pJ)32-bit Addition 0.9 Binary MAC 0.232-bit MAC 4.6 Memory access per bit 2.5TABLE IENERGY CONSUMPTION CHART--- Page 5 ---A. Hybrid NetworkWe present the inference accuracy and energy results forhybrid networks in Fig. 4 using dashed lines. \u201c10E + 20C\u201drepresents a hybrid network with 10 quantized layers at theedge and 20 layers at the cloud. When we binarize the \ufb01rst10 layers of the network, the energy reduction with respect toa full-precision ResNet-32 is 1.5\xd7for both CIFAR10 and CI-FAR100 with accuracy losses of 1.5% and 2.1% respectively.On binarization of 20 layers, the energy saving becomes 2.6\xd7.a)b)CIFAR-10CIFAR-100Edge QuantizationEdge QuantizationFig. 4. a) Accuracy vs Energy plot for CIFAR-10 showing 20E+10C con\ufb01gachieves 5.5x energy bene\ufb01t over a full-precision standalone network on cloud.b)Accuracy vs Energy plot for CIFAR-100 showing 4-bit network at edge(24E+6C con\ufb01g) achieves 4.8x energy bene\ufb01t over a full-precision standalonenetwork on cloud. Various con\ufb01gurations have been explored with differentedge and cloud layers.B. Conditionally Deep Hybrid NetworkWe apply conditional deep learning on hybrid networks byadding an early exit after the last quantized layer at the edge.The thresholds for entropy to determine whether inference can\ufb01nish at the edge are set to 0.5 for CIFAR10 and 1.0 forCIFAR100. In Fig. 4(a), we show the energy consumptionand accuracy on CIFAR10 with and without an early exit. Theenergy saving of binarizing 20 convolutional layers becomes5.5\xd7after adding an early exit, which is two times larger thanbefore, and the accuracy remains the same. In comparison witha full-precision network with conditional exiting, it is 3.9 \xd7more energy ef\ufb01cient due to hybrid quantization. Next, weevaluate the different con\ufb01gurations of the conditional deephybrid network as mentioned in Section II.1) Separate Optimization VS Joint Optimization: In Table.II, we compare the results corresponding to these two differentoptimization strategies. For joint optimization, we set theweights \u03bbk used to calculate the joint loss empirically to 0.6for the early exit because giving more weight to the earlyexit will encourage more discriminative feature learning inearly layers [24]. With the same hybrid network con\ufb01guration(16 E + 14 C), joint optimization enables more samples to\ufb01nish inference at the early exit and yields higher accuracy.It con\ufb01rms our assumption that using joint optimization canmake the edge network more con\ufb01dent and make the edge-cloud system more energy ef\ufb01cient.Percentage of early exiting (%) Accuracy(%)Separate Joint Separate JointCIFAR10 24.1 52.8 88.72 89.16CIFAR100 17.3 24.6 61.80 62.50TABLE IICOMPARISON BETWEEN SEPARATE OPTIMIZATION AND JOINTOPTIMIZATION2) Effects of hybrid bit-depth: Unlike CIFAR10 of whichmost samples can be con\ufb01dently inferred by the binarizednetwork at the edge, CIFAR100 is more complex so thebinarized network is only con\ufb01dent on less than 30% of testsamples as illustrated in Fig. 5. To make the network atthe edge more con\ufb01dent, we use 2-bit or 4-bit quantizationinstead of binarization. For each quantization option, we varythe number of quantized layers and plot the performance ofconditionally deep hybrid networks in Fig. 4(b). Points in thetop-left corner are corresponding to optimal networks sincethey achieve high accuracy and low inference energy. Theexperiment results show that quantizing the network with morebits can achieve better accuracy without trading off energyef\ufb01ciency. 4-bit and 2-bit quantization only cause 1.4% and3.0% accuracy degradation respectively, while energy savingscan be 4.8\xd7and 5.3\xd7compared to a full-precision network.In Table. III, we show the inference energy consumed by theedge and the cloud. From the top left corner to the bottomright corner, the cloud energy decreases rapidly while the edgeenergy does not change much because we keep the \ufb01rst layerat the edge full-precision and it dominates the edge energy.Energy (mJ) 1-bit 2-bit 4-bitedge cloud edge cloud edge cloud10 E + 20 C 261 2269 295 2108 363 205916 E + 14 C 213 1440 264 1168 365 108920 E + 10 C 236 991 299 805 424 72624 E + 6 C 203 645 278 478 429 406TABLE IIIINFERENCE ENERGY AT THE EDGE AND CLOUD3) Effect of the number of layers on edge: From Fig. 4(b),we can see that as the number of binary layers increases, thehybrid network becomes less accurate. However, this is nottrue if early exits are added, as we have discussed in SectionII.C. In Fig. 4, we \ufb01nd the sweet spots which provides the bestperformance is using 16 binary layers and 14 full-precisionones. We also have discussed that the precision of the edgenetwork depends on the number of parameters contained in theearly exit. In Fig. 5, we show that in most cases, the number ofcorrect predictions \ufb01nish at the edge increases with the number--- Page 6 ---Fig. 5. Accuracy and Percentage of early exiting samples at the edge forCIFAR-100 dataset on Resnet-32of quantized layers, however, when the early exit is locatedafter 24 binary layers, its weight matrix of size 256 \xd7100is not robust enough, and hence the number of early exitingsamples on CIFAR100 abnormally decreases. In the case of2-bit or 4-bit quantization, the network is more robust, so theeffect of the decrease in the number of parameters is not assevere as that on binarized networks.% of exiting at edge Accuracy Energy(mJ)# of early exits 1 2 1 2 1 220 E + 10 C 59.3 38.6, 19.7 88.61 85.98 729 724TABLE IVEFFECT OF THE NUMBER OF EARLY EXITS ON CIFAR104) Effects of the number of exits: In Table. IV, we comparethe results of hybrid networks with one or two early exits onCIFAR10. We evenly distribute two exits: one is at the end ofthe edge, the other is put in the middle. We notice that addingmore exits has relatively minor effects on the total energy.This is because the full-precision layers of the network aremuch more power-hungry than the binarized ones, and the totalenergy of inference is dominated by the energy of samplesthat cannot be handled at the edge. As illustrated in Table.IV, adding more exits does not enable more samples to skipthe full-precision layers, so the cloud energy is not reduced.Besides, using two early exits causes performance degradationdue to the complexity of training. For CIFAR100, since thenumber of classes is 10 times larger, adding the other earlyexit in Conv 1 would lead to even higher energy consumptionthan activating binarized Conv 2 and the early exit in Conv 2.Moreover, if two exits are close, the number of exiting samplesat the later one would be small because their con\ufb01dences aresimilar. Therefore, using one early exit at the edge is enoughin most cases.IV. C ONCLUSIONIn conclusion, we applied conditional deep learning onpartially quantized neural networks to simulate neural networksystems distributed at the edge and cloud. We found that thejoint optimization strategy can enable the edge to handle moresamples. We showed that conditionally deep hybrid networksachieve much better performance-vs-energy trade-offs thanquantized networks without early exits. By modifying the bit-depth and number of quantized layers at the edge, we foundthe optimal con\ufb01gurations for hybrid networks, which allow65% of CIFAR10 samples and 52% of CIFAR100 samples toexit early and achieve around \ufb01ve times energy reduction.REFERENCES[1] M. Chiang and T. Zhang, \u201cFog and iot: An overview of researchopportunities,\u201d IEEE IoT Journal , vol. 3, pp. 854\u2013864, Dec 2016.[2] Krizhevsky et al. , \u201cImagenet classi\ufb01cation with deep convolutionalneural networks,\u201d in Advances in neural information processing systems,pp. 1097\u20131105, 2012.[3] Szegedy et al. , \u201cGoing deeper with convolutions,\u201d in Proceedings ofCVPR, pp. 1\u20139, 2015.[4] He et al., \u201cDeep residual learning for image recognition,\u201d inProceedingsof CVPR, pp. 770\u2013778, 2016.[5] T. Mikolov et al., \u201cDistributed representations of words and phrases andtheir compositionality,\u201d in Advances in neural information processingsystems, pp. 3111\u20133119, 2013.[6] R. B. Girshick, \u201cFast R-CNN,\u201d CoRR, vol. abs/1504.08083, 2015.[7] M. Hassanalieragh et al. , \u201cHealth monitoring and management usinginternet-of-things (iot) sensing with cloud-based processing: Opportuni-ties and challenges,\u201d in 2015 IEEE International Conference on ServicesComputing, pp. 285\u2013292, June 2015.[8] M. Teichmann et al., \u201cMultinet: Real-time joint semantic reasoning forautonomous driving,\u201d in 2018 IEEE Intelligent Vehicles Symposium (IV),pp. 1013\u20131020, June 2018.[9] H. Li et al. , \u201cLearning iot in edge: Deep learning for the internet ofthings with edge computing,\u201d IEEE Network , vol. 32, pp. 96\u2013101, Jan2018.[10] N. P. Jouppi et al., \u201cIn-datacenter performance analysis of a tensor pro-cessing unit,\u201d in 2017 ACM/IEEE 44th Annual International Symposiumon Computer Architecture (ISCA) , pp. 1\u201312, IEEE, 2017.[11] S. Teerapittayanon et al. , \u201cDistributed deep neural networks over thecloud, the edge and end devices,\u201d in 2017 IEEE 37th InternationalConference on Distributed Computing Systems (ICDCS) , pp. 328\u2013339,June 2017.[12] B. Jacob et al. , \u201cQuantization and training of neural networks foref\ufb01cient integer-arithmetic-only inference,\u201d CoRR, vol. abs/1712.05877,2017.[13] M. Courbariaux and Y . Bengio, \u201cBinarynet: Training deep neural net-works with weights and activations constrained to +1 or -1,\u201d CoRR,vol. abs/1602.02830, 2016.[14] M. Rastegari et al. , \u201cXnor-net: Imagenet classi\ufb01cation using binaryconvolutional neural networks,\u201d CoRR, vol. abs/1603.05279, 2016.[15] S. Zhou et al., \u201cDorefa-net: Training low bitwidth convolutional neuralnetworks with low bitwidth gradients,\u201darXiv preprint arXiv:1606.06160,2016.[16] S.-C. Zhou et al. , \u201cBalanced quantization: An effective and ef\ufb01cientapproach to quantized neural networks,\u201d Journal of Computer Scienceand Technology, vol. 32, no. 4, pp. 667\u2013682, 2017.[17] D. Zhang, J. Yang, D. Ye, and G. Hua, \u201cLq-nets: Learned quantizationfor highly accurate and compact deep neural networks,\u201d in Proceedingsof ECCV, pp. 365\u2013382, 2018.[18] S. Jung, C. Son, S. Lee, J. Son, Y . Kwak, J.-J. Han, and C. Choi,\u201cJoint training of low-precision neural network with quantization intervalparameters,\u201d arXiv preprint arXiv:1808.05779 , 2018.[19] J. Choi et al. , \u201cPact: Parameterized clipping activation for quantizedneural networks,\u201d arXiv preprint arXiv:1805.06085 , 2018.[20] I. Chakraborty et al. , \u201cPca-driven hybrid network design for enablingintelligence at the edge,\u201d arXiv preprint arXiv:1906.01493 , 2019.[21] Prabhu et al. , \u201cHybrid binary networks: Optimizing for accuracy, ef\ufb01-ciency and memory,\u201d in 2018 IEEE Winter Conference on Applicationsof Computer Vision (WACV) , pp. 821\u2013829, IEEE, 2018.[22] P. Panda et al. , \u201cConditional deep learning for energy-ef\ufb01cient andenhanced pattern recognition,\u201d CoRR, vol. abs/1509.08971, 2015.[23] K. He et al. , \u201cDeep residual learning for image recognition,\u201d CVPR,pp. 770\u2013778, 2015.[24] S. Teerapittayanon et al., \u201cBranchynet: Fast inference via early exitingfrom deep neural networks,\u201d CoRR, vol. abs/1709.01686, 2017.\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2005.10851v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 13:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #11, Phase: literature review, Feedback: 3\n--- Page 1 ---Conditionally Deep Hybrid Neural NetworksAcross Edge and CloudYinghan Long*, Indranil Chakraborty *, Kaushik Roy(* Authors contributed equally to the work)School of Electrical and Computer Engineering, Purdue Universitylong273@purdue.edu, ichakra@purdue.edu, kaushik@purdue.eduAbstract\u2014The pervasiveness of \u201cInternet-of-Things\u201d in ourdaily life has led to a recent surge in fog computing, encompassinga collaboration of cloud computing and edge intelligence. To thateffect, deep learning has been a major driving force towardsenabling such intelligent systems. However, growing model sizesin deep learning pose a signi\ufb01cant challenge towards deploymentin resource-constrained edge devices. Moreover, in a distributedintelligence environment, ef\ufb01cient workload distribution is neces-sary between edge and cloud systems. To address these challenges,we propose a conditionally deep hybrid neural network forenabling AI-based fog computing. The proposed network can bedeployed in a distributed manner, consisting of quantized layersand early exits at the edge and full-precision layers on the cloud.During inference, if an early exit has high con\ufb01dence in the clas-si\ufb01cation results, it would allow samples to exit at the edge, andthe deeper layers on the cloud are activated conditionally, whichcan lead to improved energy ef\ufb01ciency and inference latency. Weperform an extensive design space exploration with the goal ofminimizing energy consumption at the edge while achieving stateof the art classi\ufb01cation accuracies on image classi\ufb01cation tasks.We show that with binarized layers at the edge, the proposedconditional hybrid network can process 65% of inferences atthe edge, leading to 5.5 \xd7 computational energy reduction withminimal accuracy degradation on CIFAR-10 dataset. For themore complex dataset CIFAR-100, we observe that the proposednetwork with 4-bit quantization at the edge achieves 52% earlyclassi\ufb01cation at the edge with 4.8\xd7 energy reduction. The analysisgives us insights on designing ef\ufb01cient hybrid networks whichachieve signi\ufb01cantly higher energy ef\ufb01ciency than full-precisionnetworks for edge-cloud based distributed intelligence systems.Index Terms\u2014Conditional deep learning, quantized neuralnetwork, fog computingI. I NTRODUCTIONThe age of \u201cInternet-of-Things\u201d (IoT) has touched humanlives in an unprecedented manner by endowing us withremarkable connectivity and autonomous intelligent systems[1]. These bene\ufb01ts come with the necessity of processinghumongous amounts of heterogeneous data obtained from theenvironment. The ability of deep learning to reliably processsuch data has resulted in it playing a dominant role in awide range of Arti\ufb01cial Intelligence (AI) applications, includ-ing image classi\ufb01cation [2]\u2013[4], natural language processing[5], and object detection [6]. As the third wave of arti\ufb01cialintelligence (AI) is accelerating, researchers are incorporatingThe research was funded in part by C-BRIC, one of six centers in JUMP, aSemiconductor Research Corporation (SRC) program sponsored by DARPA,the National Science Foundation, Intel Corporation, Vannevar Bush FacultyFellowship, and the U.K. Ministry of Defence under Agreement NumberW911NF-16-3-0001.AI into IoT applications. Traditionally, these cognitive appli-cations perform compute-intensive tasks in a centralized hubwhich collects data from thousands of connected edge devices.However, in the recent past, there have been rising concerns ofdata security due to over-centralization of information, and thecontinuous communication overhead between the edge devicesand the cloud drastically increases the power consumption ofworld-wide internet. Moreover, real-time AI systems such ashealth monitoring [7] and autonomous driving [8] require fastprocessing. Hence, to truly realize the potential of AI in IoT,it is necessary to enable intelligence at the edge [9].On the other hand, the resource constraints of edge deviceslimit us from deploying large deep learning models at theedge. With the growing complexity of tasks, there has beena considerable increase in the sizes of deep neural networks(DNNs). This requires DNNs to be trained on powerfulGraphic Processing Units (GPUs) or Tensor Processing Units(TPUs) [10]. Edge devices, however, are resource-constrainedand implementing DNNs on such devices jeopardizes therequirement of long battery life for stable operation. Thishas propelled the search for techniques that enable energy-ef\ufb01cient implementations of deep learning models. Besides,researchers are investigating ef\ufb01cient techniques to enabledistributed intelligence at the near-user edge device and end-user cloud [11].A popular approach to reduce model complexity and im-prove computational ef\ufb01ciency is to quantize the weights andactivations of neural networks by representing 32-bit \ufb02oating-point values with lower bit-depth \ufb01xed point [12] or evenbinary values [13] [14]. Compared to full-precision networks,binary neural networks are fast and ef\ufb01cient, but this comeswith a degradation in performance. For example, binarizingboth inputs and weights of a residual network(ResNet) causes18% reduction in top-1 accuracy on ImageNet and 9% onCIFAR10. In order to overcome this challenge, there have beenvarious approaches towards improving quantization schemes[15], [16] as well as proposing training algorithms for net-works with increased bit-precision [17]\u2013[19]. An alternativeapproach towards preserving the performance while achievinga signi\ufb01cant energy-ef\ufb01ciency is designing hybrid networkarchitectures [20], [21], which consist of both quantized andhigh-precision layers. Since high-precision layers are power-hungry, such computations can be performed in the cloud.In this work, we propose a design methodology to buildconditionally deep hybrid networks for distributed intelligenceacross an edge-cloud system. In such a network, quantizedarXiv:2005.10851v1  [cs.LG]  21 May 2020--- Page 2 ---Full-precisionNetworkQuantizedNetworkfcQuantizedNetwork+Early exitsDeep NeuralNetworkHybrid NeuralNetworkConditionally DeepHybrid NetworkEdgefcFull-precisionNetworkfcCloudFull-precisionNetworkHybridQuantizationConditionalDeep\xa0LearningEarly resultsFinal resultsInputsInputsEasy Hard Inputs:Hard Easy Fig. 1. Overview of our design methodology for Conditionally Deep Hybrid Networks. From left to right, we show how a standard deep neural network isdeveloped into a Conditionally Deep Hybrid Network and distributed to an edge-cloud system.layers of the network are deployed in the edge device, whilehigh-precision layers are performed on the cloud. Further, byusing the Conditional Deep Learning (CDL) approach [22],our network leverages the dif\ufb01culty of inputs to classify therelatively easy samples with early exits and conditionallyactivate the deeper layers. This technique is particularly suitedfor a distributed intelligence system where we can process theeasy inputs at the energy-ef\ufb01cient edge device with quantizednetworks while only the inputs that are close to the decisionboundary are sent to the cloud. As a result, both communica-tion cost and computation cost can be improved. Through anextensive analysis of hybrid network architectures, we identifythe tradeoffs in terms of energy, latency, accuracy as well asbit-precision of the layers for optimal hybrid network design.The contributions of our work are as follows:\u2022 We propose a methodology for designing hybrid precisionneural networks for edge-cloud processing systems.\u2022 We engineer early exiting strategies to conditionally acti-vate the deeper layers of the neural networks for enforcingenergy-ef\ufb01ciency in edge devices.\u2022 We evaluate the proposed methodology on state-of-artnetwork architectures. We show that by modifying net-work con\ufb01gurations such as bit-precision at the edge,early exiting is triggered in most cases.The paper is organized as follows. The next section gives anoverview of the design methodology for conditionally deephybrid neural networks. Section III reports the experimentresults and shows how to adjust network con\ufb01gurations toachieve a better performance-vs-energy trade-off.II. H YBRID NETWORK DESIGN WITH CONDITIONAL DEEPLEARNINGIn this section, we propose a methodology for designinghybrid neural networks and explore strategies to enable earlyclassi\ufb01cation in the edge network based on the dif\ufb01culty ofinputs to achieve early processing of data and reduction inenergy consumption.A. Hybrid Neural NetworksWe propose a hybrid neural network where a N-layernetwork is divided into two sections, M layers where thebit-precision of the weights and inputs is p 1 and N \u2212Mlayers where the corresponding bit-precisions are p 2 wherep 2 \u226bp 1. This is illustrated in Fig. 1. Generally, p 2 can beconsidered as a 32-bit \ufb02oat as the primary objective for ourdesign is to preserve the accuracy of the neural network. Thedesign parameters p 1 and M can be varied to identify theoptimal division for the proposed hybrid networks consideringthe tradeoff between performance and energy consumption.The low-precision section of a hybrid CNN consists ofQuanConv layers by which we quantize both the weights andinputs. Fig. 2 illustrates that a QuanConv layer is comprisedof a batch normalization layer, an activation layer, and aconvolution layer using quantized weights in order. In thecase of a ResNet [23], the residual connections are kept full-precision to mitigate the negative effect of quantized inputswith a small overhead. A batch normalization layer is appliedbefore quantization to ensure that inputs hold zero mean [14].Then the activation layer quantizes inputs using the followingscheme. For an input matrix I \u2208 R c\xd7wi\xd7hi , where c,w,hare the number channels, width and height respectively, thecorresponding binary input matrix I B \u2208 {+1 , \u22121 }c\xd7wi\xd7hiand quantized input matrix I Q is given byI B = sign( I ); I Q = v 1(\u230aI + v 0v 1\xb7Z\u2309/Z \u2212v 2) (1)where Z = 2 p \u22121 is the number of quantization levels andv 0,v 1,v 2 are constants.The quantization of weights is performed for each layer ina CNN. Let us represent the kernel as W \u2208R c\xd7w\xd7h, wherec,w,h are the number channels and width and height of thekernel respectively. If both weights and inputs are extremelyquantized (binarized), dot product operations in convolutionscan be implemented ef\ufb01ciently using XNOR and bit-counting--- Page 3 ---operations. To approximate the convolution operation, weestimate the real-value weight matrix W using a quantizedmatrix WQ and a scaling factor \u03b1 as proposed in [14].I\u2217W \u2248(IQ \u2217WQ)\u03b1; \u03b1= \u2225W\u2225l1n (2)WQ = v1(\u230aW + v0v1\xb7Z\u2309/Z\u2212v2) (3)where n= c\xd7w\xd7h is a constant. The precision of weightsand activations of the high precision section of a hybrid CNNare kept 32-bit \ufb02oating point. We adopt the training algorithmproposed by Rastegari et al [14] to train the networks withquantization.QuanConv QuanConv QuanConv ... Residual connection... BatchNormQuantizedActivationConvolutionQuanConv Fig. 2. Quantized convolutional layers in a ResNetIn a hybrid neural network, the edge network must sendextracted features corresponding to all samples to the cloudfor further processing, which takes communication energy andtime. In the next subsection, we will introduce how to enablethe edge to analyze easy data independently.B. Conditional Exit in Deep Hybrid NetworksEnabling distributed intelligence using hybrid networks inan AI-based fog computing system requires data processingboth at the edge and the cloud. Since the dif\ufb01culty of examplesin a dataset often exhibits an inherent variability, we apply theconcept of conditional deep learning [22] on hybrid networksto enable classi\ufb01cation of relatively simpler images at the low-precision edge, while the more dif\ufb01cult ones are passed to thehigh-precision deeper layers of the network in the cloud. Suchconditional activation of deeper layers in the context of hybridnetworks provides both communicational and computationalenergy ef\ufb01ciency and run-time improvements over standardnetworks. Each early exit at the edge is a linear classi\ufb01er thatconsists of an average pooling layer and a fully connectedlayer. Typically, such an exit needs to be placed at the end ofthe low-precision section of the hybrid network, i.e, the partof the network being implemented on edge. However, moreexits can be added in the low-precision section to facilitateearly classi\ufb01cation of easier examples. In Fig. 3, we show anexample of adding three early exits. The input to an earlyexit is the output from a convolutional layer, and all the earlyexits use full-precision weights to ensure no computation erroroccurs in the classi\ufb01cation layer.During the training process, although the early exits havealready provided labels, the rest of the network would stillbe activated during training to optimize all exits based on theCloudEdgeQuanConv Layers 1QuanConv Layers 2Conv Layers 3-2Conv Pooling & fcPooling & fcPooling & fcEarly Exit 0Early Exit 1Final ExitQuanConv Layers 3-1Pooling & fcEarly Exit 2Fig. 3. An example of a hybrid neural network with three early exits. Blocksin orange are quantized. The sizes of early exits differ corresponding to theirlocations.whole training set. Since there are more than one labels givenby our network system, we need to modify the optimizationobjective to train both the hybrid network and additionalexits. There are two optional training strategies: 1) SeparateOptimization, 2) Joint optimization.1) Separate optimization: In this approach, we \ufb01rst trainthe hybrid network without any early exits, hence, the sameoptimization objective can be used. Then we \ufb01x the parametersof all convolutional layers and train one early exit at a time.Let us denote the ground-truth label vector as yand the outputlabel given by early exit k as \u02c6yk. The optimization objectiveof the kth early exit isLk( \u02c6yk,y) =\u2212\u2211c\u2208Cyc log \u02c6ykc (4)where \u02c6ykc is a vector containing predicted probabilities for allpossible class labels and C represents the set of all classes.The backward propagation function will be called for k timeswith respect to Lk. Since parameters of convolutional layersdo not require gradient, the backward propagations in earlyexits are independent of each other.2) Joint optimization: In this approach, we use a jointoptimization technique to train the network along with earlyexits [24]. The total loss is calculated as the weighted sum ofthe loss functions of each exit branch.L(\u02c6y,y) =K\u2211k=1\u03bbkLk(\u02c6yk,y) (5)where K is the total number of exits and \u03bbk is set empiricallyand sum to 1. By joint optimization, the gradients used toupdate weights of the convolutional layers become\u2202L\u2202w =K\u2211k=1\u03bbk\u2202Lk\u2202w (6)Compared to separate optimization, training the entire networktogether optimizes the weights of the convolutional layersbased on the losses of both the \ufb01nal exit and early exits,--- Page 4 ---so it enables more samples to exit early and enlarges theenergy saving. On the other hand, separate optimization ismore \ufb02exible because it allows users to add early exits to atrained network. We will compare the results corresponding tothese two optimization strategies in section III. Because ourobjective is to make as many inferences to \ufb01nish at the edgeas possible, we will use joint optimization for the rest of ourexperiments.Algorithm 1: Inference using a conditionally deep hybridneural networkInput: Samples XOutput: Classi\ufb01cation results \u02c6Yfor sample i doO1[i] = Forward( X[i],Wt[1]);for layer l = 2 to M do//Propagate at the edgeOl[i] = Forward( Ol\u22121[i],Wt[l]);if Classi\ufb01er k locates at layer l then\u02c6Yk[i] = Classi\ufb01er[k]( Ol[i]);if Entropy(\u02c6Yk[i])<Threshold k then\u02c6Y[i] = \u02c6Yk[i];break; //take early exitendendendif \u02c6Y[i] not found then\u02c6Y[i] = Forward(OM [i], Wt[M : N]);//Continuepropagating on the cloudendendOnce the hybrid deep network is trained, we can utilizequantized weights and early exits to allow inference of easysamples to \ufb01nish at the edge. Algorithm 1 shows the inferenceprocess. To classify a sample, we activate the layers of the deepnetwork conditionally based on the entropy of the predictionresult calculated byentropy(\u02c6y) =\u2212\u2211c\u2208C\u02c6yc log \u02c6ycIf the entropy is lower than the threshold, it means the earlyexit has high con\ufb01dence about correctly labeling this sample,so the prediction can be returned without activating laterlayers. The thresholds for early exits are set empirically.C. Design Considerations of Conditionally Deep Hybrid Net-work1) Effect of bit-precision: When conditional deep learningis applied to a hybrid network, the effect of bit-depth on thetotal energy consumption of the edge-cloud system becomestwofold. Although the energy consumed by an operation isproportional to the bit-depth, increasing the bit-depth meanshaving a more precise network at the edge, so the numberof activated high-precision operations would be fewer thanksto early exiting. As a result, using more bits for quantizationdoes not necessarily lead to higher energy consumption. Toachieve better performance-vs-energy trade-off, we will adjustthe bit-depth depending on the complexity of datasets.2) Effect of the number of layers on edge: In a conditionallydeep hybrid network, the inference accuracy is not commensu-rate with the proportion of quantized layers due to additionalexits. For simplicity, suppose that we have only one earlyexit located after the last layer at the edge. The predictionability of the \ufb01nal exit on the cloud would be weakened tothe degree corresponding to the number of quantized layers.However, the overall accuracy of the hybrid network dependson the accuracy of all exits, and that of the early exit isaffected by the number of layers before it and the numberof parameters contained in its fully-connected layer. As thenumber of quantized layers increases, the early exit can inferbased on the high-level features extracted by a deeper network,which makes the inference task easier and potentially allowsmore samples to exit. Nevertheless, convolutional layers indifferent divisions of a ResNet have three distinct output sizes,and hence the number of parameters contained in the earlyexit drops signi\ufb01cantly when moving to the later division.Therefore, it is important to \ufb01nd the optimal division of ahybrid network.3) Effect of the number of exits: Using multiple exits atthe edge can allow inference to terminate as soon as possible,thus the latency can be further reduced. In spite of that,whether it can also provide further energy saving to the edgedepends on whether the power consumption of an earlier exitis smaller than the total power of convolutional layers and thelater exit located in the next division of the network. Becauseconvolutional layers are quantized but exits are not, their powerconsumption becomes a considerable part. If the number ofclasses in the dataset is large and the network is not verydeep, using more than one early exits is likely to consumemore power and hence is not desired.III. E XPERIMENTSTo validate the effectiveness of our model, we conductseveral sets of experiments using PyTorch. We report resultsof ResNet32 with varying hybrid con\ufb01gurations and condi-tional deep learning over CIFAR10 and CIFAR100 datasetsto explore the design space and evaluate the scalability. Weestimate the total energy consumption of inference by mul-tiplying the number of full-precision and binary operationswith corresponding energy listed in Table. I. Convolutions ofmatrices are implemented as multiply-and-add (MAC). For allour analysis, we have not included the communication energybetween the edge and the cloud.Operation Energy (pJ) Operation Energy (pJ)32-bit Addition 0.9 Binary MAC 0.232-bit MAC 4.6 Memory access per bit 2.5TABLE IENERGY CONSUMPTION CHART--- Page 5 ---A. Hybrid NetworkWe present the inference accuracy and energy results forhybrid networks in Fig. 4 using dashed lines. \u201c10E + 20C\u201drepresents a hybrid network with 10 quantized layers at theedge and 20 layers at the cloud. When we binarize the \ufb01rst10 layers of the network, the energy reduction with respect toa full-precision ResNet-32 is 1.5\xd7for both CIFAR10 and CI-FAR100 with accuracy losses of 1.5% and 2.1% respectively.On binarization of 20 layers, the energy saving becomes 2.6\xd7.a)b)CIFAR-10CIFAR-100Edge QuantizationEdge QuantizationFig. 4. a) Accuracy vs Energy plot for CIFAR-10 showing 20E+10C con\ufb01gachieves 5.5x energy bene\ufb01t over a full-precision standalone network on cloud.b)Accuracy vs Energy plot for CIFAR-100 showing 4-bit network at edge(24E+6C con\ufb01g) achieves 4.8x energy bene\ufb01t over a full-precision standalonenetwork on cloud. Various con\ufb01gurations have been explored with differentedge and cloud layers.B. Conditionally Deep Hybrid NetworkWe apply conditional deep learning on hybrid networks byadding an early exit after the last quantized layer at the edge.The thresholds for entropy to determine whether inference can\ufb01nish at the edge are set to 0.5 for CIFAR10 and 1.0 forCIFAR100. In Fig. 4(a), we show the energy consumptionand accuracy on CIFAR10 with and without an early exit. Theenergy saving of binarizing 20 convolutional layers becomes5.5\xd7after adding an early exit, which is two times larger thanbefore, and the accuracy remains the same. In comparison witha full-precision network with conditional exiting, it is 3.9 \xd7more energy ef\ufb01cient due to hybrid quantization. Next, weevaluate the different con\ufb01gurations of the conditional deephybrid network as mentioned in Section II.1) Separate Optimization VS Joint Optimization: In Table.II, we compare the results corresponding to these two differentoptimization strategies. For joint optimization, we set theweights \u03bbk used to calculate the joint loss empirically to 0.6for the early exit because giving more weight to the earlyexit will encourage more discriminative feature learning inearly layers [24]. With the same hybrid network con\ufb01guration(16 E + 14 C), joint optimization enables more samples to\ufb01nish inference at the early exit and yields higher accuracy.It con\ufb01rms our assumption that using joint optimization canmake the edge network more con\ufb01dent and make the edge-cloud system more energy ef\ufb01cient.Percentage of early exiting (%) Accuracy(%)Separate Joint Separate JointCIFAR10 24.1 52.8 88.72 89.16CIFAR100 17.3 24.6 61.80 62.50TABLE IICOMPARISON BETWEEN SEPARATE OPTIMIZATION AND JOINTOPTIMIZATION2) Effects of hybrid bit-depth: Unlike CIFAR10 of whichmost samples can be con\ufb01dently inferred by the binarizednetwork at the edge, CIFAR100 is more complex so thebinarized network is only con\ufb01dent on less than 30% of testsamples as illustrated in Fig. 5. To make the network atthe edge more con\ufb01dent, we use 2-bit or 4-bit quantizationinstead of binarization. For each quantization option, we varythe number of quantized layers and plot the performance ofconditionally deep hybrid networks in Fig. 4(b). Points in thetop-left corner are corresponding to optimal networks sincethey achieve high accuracy and low inference energy. Theexperiment results show that quantizing the network with morebits can achieve better accuracy without trading off energyef\ufb01ciency. 4-bit and 2-bit quantization only cause 1.4% and3.0% accuracy degradation respectively, while energy savingscan be 4.8\xd7and 5.3\xd7compared to a full-precision network.In Table. III, we show the inference energy consumed by theedge and the cloud. From the top left corner to the bottomright corner, the cloud energy decreases rapidly while the edgeenergy does not change much because we keep the \ufb01rst layerat the edge full-precision and it dominates the edge energy.Energy (mJ) 1-bit 2-bit 4-bitedge cloud edge cloud edge cloud10 E + 20 C 261 2269 295 2108 363 205916 E + 14 C 213 1440 264 1168 365 108920 E + 10 C 236 991 299 805 424 72624 E + 6 C 203 645 278 478 429 406TABLE IIIINFERENCE ENERGY AT THE EDGE AND CLOUD3) Effect of the number of layers on edge: From Fig. 4(b),we can see that as the number of binary layers increases, thehybrid network becomes less accurate. However, this is nottrue if early exits are added, as we have discussed in SectionII.C. In Fig. 4, we \ufb01nd the sweet spots which provides the bestperformance is using 16 binary layers and 14 full-precisionones. We also have discussed that the precision of the edgenetwork depends on the number of parameters contained in theearly exit. In Fig. 5, we show that in most cases, the number ofcorrect predictions \ufb01nish at the edge increases with the number--- Page 6 ---Fig. 5. Accuracy and Percentage of early exiting samples at the edge forCIFAR-100 dataset on Resnet-32of quantized layers, however, when the early exit is locatedafter 24 binary layers, its weight matrix of size 256 \xd7100is not robust enough, and hence the number of early exitingsamples on CIFAR100 abnormally decreases. In the case of2-bit or 4-bit quantization, the network is more robust, so theeffect of the decrease in the number of parameters is not assevere as that on binarized networks.% of exiting at edge Accuracy Energy(mJ)# of early exits 1 2 1 2 1 220 E + 10 C 59.3 38.6, 19.7 88.61 85.98 729 724TABLE IVEFFECT OF THE NUMBER OF EARLY EXITS ON CIFAR104) Effects of the number of exits: In Table. IV, we comparethe results of hybrid networks with one or two early exits onCIFAR10. We evenly distribute two exits: one is at the end ofthe edge, the other is put in the middle. We notice that addingmore exits has relatively minor effects on the total energy.This is because the full-precision layers of the network aremuch more power-hungry than the binarized ones, and the totalenergy of inference is dominated by the energy of samplesthat cannot be handled at the edge. As illustrated in Table.IV, adding more exits does not enable more samples to skipthe full-precision layers, so the cloud energy is not reduced.Besides, using two early exits causes performance degradationdue to the complexity of training. For CIFAR100, since thenumber of classes is 10 times larger, adding the other earlyexit in Conv 1 would lead to even higher energy consumptionthan activating binarized Conv 2 and the early exit in Conv 2.Moreover, if two exits are close, the number of exiting samplesat the later one would be small because their con\ufb01dences aresimilar. Therefore, using one early exit at the edge is enoughin most cases.IV. C ONCLUSIONIn conclusion, we applied conditional deep learning onpartially quantized neural networks to simulate neural networksystems distributed at the edge and cloud. We found that thejoint optimization strategy can enable the edge to handle moresamples. We showed that conditionally deep hybrid networksachieve much better performance-vs-energy trade-offs thanquantized networks without early exits. By modifying the bit-depth and number of quantized layers at the edge, we foundthe optimal con\ufb01gurations for hybrid networks, which allow65% of CIFAR10 samples and 52% of CIFAR100 samples toexit early and achieve around \ufb01ve times energy reduction.REFERENCES[1] M. Chiang and T. Zhang, \u201cFog and iot: An overview of researchopportunities,\u201d IEEE IoT Journal , vol. 3, pp. 854\u2013864, Dec 2016.[2] Krizhevsky et al. , \u201cImagenet classi\ufb01cation with deep convolutionalneural networks,\u201d in Advances in neural information processing systems,pp. 1097\u20131105, 2012.[3] Szegedy et al. , \u201cGoing deeper with convolutions,\u201d in Proceedings ofCVPR, pp. 1\u20139, 2015.[4] He et al., \u201cDeep residual learning for image recognition,\u201d inProceedingsof CVPR, pp. 770\u2013778, 2016.[5] T. Mikolov et al., \u201cDistributed representations of words and phrases andtheir compositionality,\u201d in Advances in neural information processingsystems, pp. 3111\u20133119, 2013.[6] R. B. Girshick, \u201cFast R-CNN,\u201d CoRR, vol. abs/1504.08083, 2015.[7] M. Hassanalieragh et al. , \u201cHealth monitoring and management usinginternet-of-things (iot) sensing with cloud-based processing: Opportuni-ties and challenges,\u201d in 2015 IEEE International Conference on ServicesComputing, pp. 285\u2013292, June 2015.[8] M. Teichmann et al., \u201cMultinet: Real-time joint semantic reasoning forautonomous driving,\u201d in 2018 IEEE Intelligent Vehicles Symposium (IV),pp. 1013\u20131020, June 2018.[9] H. Li et al. , \u201cLearning iot in edge: Deep learning for the internet ofthings with edge computing,\u201d IEEE Network , vol. 32, pp. 96\u2013101, Jan2018.[10] N. P. Jouppi et al., \u201cIn-datacenter performance analysis of a tensor pro-cessing unit,\u201d in 2017 ACM/IEEE 44th Annual International Symposiumon Computer Architecture (ISCA) , pp. 1\u201312, IEEE, 2017.[11] S. Teerapittayanon et al. , \u201cDistributed deep neural networks over thecloud, the edge and end devices,\u201d in 2017 IEEE 37th InternationalConference on Distributed Computing Systems (ICDCS) , pp. 328\u2013339,June 2017.[12] B. Jacob et al. , \u201cQuantization and training of neural networks foref\ufb01cient integer-arithmetic-only inference,\u201d CoRR, vol. abs/1712.05877,2017.[13] M. Courbariaux and Y . Bengio, \u201cBinarynet: Training deep neural net-works with weights and activations constrained to +1 or -1,\u201d CoRR,vol. abs/1602.02830, 2016.[14] M. Rastegari et al. , \u201cXnor-net: Imagenet classi\ufb01cation using binaryconvolutional neural networks,\u201d CoRR, vol. abs/1603.05279, 2016.[15] S. Zhou et al., \u201cDorefa-net: Training low bitwidth convolutional neuralnetworks with low bitwidth gradients,\u201darXiv preprint arXiv:1606.06160,2016.[16] S.-C. Zhou et al. , \u201cBalanced quantization: An effective and ef\ufb01cientapproach to quantized neural networks,\u201d Journal of Computer Scienceand Technology, vol. 32, no. 4, pp. 667\u2013682, 2017.[17] D. Zhang, J. Yang, D. Ye, and G. Hua, \u201cLq-nets: Learned quantizationfor highly accurate and compact deep neural networks,\u201d in Proceedingsof ECCV, pp. 365\u2013382, 2018.[18] S. Jung, C. Son, S. Lee, J. Son, Y . Kwak, J.-J. Han, and C. Choi,\u201cJoint training of low-precision neural network with quantization intervalparameters,\u201d arXiv preprint arXiv:1808.05779 , 2018.[19] J. Choi et al. , \u201cPact: Parameterized clipping activation for quantizedneural networks,\u201d arXiv preprint arXiv:1805.06085 , 2018.[20] I. Chakraborty et al. , \u201cPca-driven hybrid network design for enablingintelligence at the edge,\u201d arXiv preprint arXiv:1906.01493 , 2019.[21] Prabhu et al. , \u201cHybrid binary networks: Optimizing for accuracy, ef\ufb01-ciency and memory,\u201d in 2018 IEEE Winter Conference on Applicationsof Computer Vision (WACV) , pp. 821\u2013829, IEEE, 2018.[22] P. Panda et al. , \u201cConditional deep learning for energy-ef\ufb01cient andenhanced pattern recognition,\u201d CoRR, vol. abs/1509.08971, 2015.[23] K. He et al. , \u201cDeep residual learning for image recognition,\u201d CVPR,pp. 770\u2013778, 2015.[24] S. Teerapittayanon et al., \u201cBranchynet: Fast inference via early exitingfrom deep neural networks,\u201d CoRR, vol. abs/1709.01686, 2017., Your response: ```ADD_PAPER\n2005.10851v1\nThis paper presents a conditionally deep hybrid neural network for edge-cloud collaborative inference, combining quantized layers at the edge with full-precision layers in the cloud. The key innovation is an early-exit mechanism that allows easy samples to be classified at the edge, reducing both computation and communication costs. Experimental results show the approach achieves 5.5x energy reduction on CIFAR-10 (processing 65% of samples at edge) and 4.8x reduction on CIFAR-100 while maintaining accuracy. The hybrid quantization strategy and joint optimization of early exits demonstrate significant improvements over full-precision networks for distributed intelligence systems.\n```\n~~~~~~~~~~\nCurrent Step #12, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2005.10851v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2005.10851v1\nThis paper presents a conditionally deep hybrid neural network for edge-cloud collaborative inference, combining quantized layers at the edge with full-precision layers in the cloud. The key innovation is an early-exit mechanism that allows easy samples to be classified at the edge, reducing both computation and communication costs. Experimental results show the approach achieves 5.5x energy reduction on CIFAR-10 (processing 65% of samples at edge) and 4.8x reduction on CIFAR-100 while maintaining accuracy. The hybrid quantization strategy and joint optimization of early exits demonstrate significant improvements over full-precision networks for distributed intelligence systems.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 14:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #11, Phase: literature review, Feedback: 3\n--- Page 1 ---Conditionally Deep Hybrid Neural NetworksAcross Edge and CloudYinghan Long*, Indranil Chakraborty *, Kaushik Roy(* Authors contributed equally to the work)School of Electrical and Computer Engineering, Purdue Universitylong273@purdue.edu, ichakra@purdue.edu, kaushik@purdue.eduAbstract\u2014The pervasiveness of \u201cInternet-of-Things\u201d in ourdaily life has led to a recent surge in fog computing, encompassinga collaboration of cloud computing and edge intelligence. To thateffect, deep learning has been a major driving force towardsenabling such intelligent systems. However, growing model sizesin deep learning pose a signi\ufb01cant challenge towards deploymentin resource-constrained edge devices. Moreover, in a distributedintelligence environment, ef\ufb01cient workload distribution is neces-sary between edge and cloud systems. To address these challenges,we propose a conditionally deep hybrid neural network forenabling AI-based fog computing. The proposed network can bedeployed in a distributed manner, consisting of quantized layersand early exits at the edge and full-precision layers on the cloud.During inference, if an early exit has high con\ufb01dence in the clas-si\ufb01cation results, it would allow samples to exit at the edge, andthe deeper layers on the cloud are activated conditionally, whichcan lead to improved energy ef\ufb01ciency and inference latency. Weperform an extensive design space exploration with the goal ofminimizing energy consumption at the edge while achieving stateof the art classi\ufb01cation accuracies on image classi\ufb01cation tasks.We show that with binarized layers at the edge, the proposedconditional hybrid network can process 65% of inferences atthe edge, leading to 5.5 \xd7 computational energy reduction withminimal accuracy degradation on CIFAR-10 dataset. For themore complex dataset CIFAR-100, we observe that the proposednetwork with 4-bit quantization at the edge achieves 52% earlyclassi\ufb01cation at the edge with 4.8\xd7 energy reduction. The analysisgives us insights on designing ef\ufb01cient hybrid networks whichachieve signi\ufb01cantly higher energy ef\ufb01ciency than full-precisionnetworks for edge-cloud based distributed intelligence systems.Index Terms\u2014Conditional deep learning, quantized neuralnetwork, fog computingI. I NTRODUCTIONThe age of \u201cInternet-of-Things\u201d (IoT) has touched humanlives in an unprecedented manner by endowing us withremarkable connectivity and autonomous intelligent systems[1]. These bene\ufb01ts come with the necessity of processinghumongous amounts of heterogeneous data obtained from theenvironment. The ability of deep learning to reliably processsuch data has resulted in it playing a dominant role in awide range of Arti\ufb01cial Intelligence (AI) applications, includ-ing image classi\ufb01cation [2]\u2013[4], natural language processing[5], and object detection [6]. As the third wave of arti\ufb01cialintelligence (AI) is accelerating, researchers are incorporatingThe research was funded in part by C-BRIC, one of six centers in JUMP, aSemiconductor Research Corporation (SRC) program sponsored by DARPA,the National Science Foundation, Intel Corporation, Vannevar Bush FacultyFellowship, and the U.K. Ministry of Defence under Agreement NumberW911NF-16-3-0001.AI into IoT applications. Traditionally, these cognitive appli-cations perform compute-intensive tasks in a centralized hubwhich collects data from thousands of connected edge devices.However, in the recent past, there have been rising concerns ofdata security due to over-centralization of information, and thecontinuous communication overhead between the edge devicesand the cloud drastically increases the power consumption ofworld-wide internet. Moreover, real-time AI systems such ashealth monitoring [7] and autonomous driving [8] require fastprocessing. Hence, to truly realize the potential of AI in IoT,it is necessary to enable intelligence at the edge [9].On the other hand, the resource constraints of edge deviceslimit us from deploying large deep learning models at theedge. With the growing complexity of tasks, there has beena considerable increase in the sizes of deep neural networks(DNNs). This requires DNNs to be trained on powerfulGraphic Processing Units (GPUs) or Tensor Processing Units(TPUs) [10]. Edge devices, however, are resource-constrainedand implementing DNNs on such devices jeopardizes therequirement of long battery life for stable operation. Thishas propelled the search for techniques that enable energy-ef\ufb01cient implementations of deep learning models. Besides,researchers are investigating ef\ufb01cient techniques to enabledistributed intelligence at the near-user edge device and end-user cloud [11].A popular approach to reduce model complexity and im-prove computational ef\ufb01ciency is to quantize the weights andactivations of neural networks by representing 32-bit \ufb02oating-point values with lower bit-depth \ufb01xed point [12] or evenbinary values [13] [14]. Compared to full-precision networks,binary neural networks are fast and ef\ufb01cient, but this comeswith a degradation in performance. For example, binarizingboth inputs and weights of a residual network(ResNet) causes18% reduction in top-1 accuracy on ImageNet and 9% onCIFAR10. In order to overcome this challenge, there have beenvarious approaches towards improving quantization schemes[15], [16] as well as proposing training algorithms for net-works with increased bit-precision [17]\u2013[19]. An alternativeapproach towards preserving the performance while achievinga signi\ufb01cant energy-ef\ufb01ciency is designing hybrid networkarchitectures [20], [21], which consist of both quantized andhigh-precision layers. Since high-precision layers are power-hungry, such computations can be performed in the cloud.In this work, we propose a design methodology to buildconditionally deep hybrid networks for distributed intelligenceacross an edge-cloud system. In such a network, quantizedarXiv:2005.10851v1  [cs.LG]  21 May 2020--- Page 2 ---Full-precisionNetworkQuantizedNetworkfcQuantizedNetwork+Early exitsDeep NeuralNetworkHybrid NeuralNetworkConditionally DeepHybrid NetworkEdgefcFull-precisionNetworkfcCloudFull-precisionNetworkHybridQuantizationConditionalDeep\xa0LearningEarly resultsFinal resultsInputsInputsEasy Hard Inputs:Hard Easy Fig. 1. Overview of our design methodology for Conditionally Deep Hybrid Networks. From left to right, we show how a standard deep neural network isdeveloped into a Conditionally Deep Hybrid Network and distributed to an edge-cloud system.layers of the network are deployed in the edge device, whilehigh-precision layers are performed on the cloud. Further, byusing the Conditional Deep Learning (CDL) approach [22],our network leverages the dif\ufb01culty of inputs to classify therelatively easy samples with early exits and conditionallyactivate the deeper layers. This technique is particularly suitedfor a distributed intelligence system where we can process theeasy inputs at the energy-ef\ufb01cient edge device with quantizednetworks while only the inputs that are close to the decisionboundary are sent to the cloud. As a result, both communica-tion cost and computation cost can be improved. Through anextensive analysis of hybrid network architectures, we identifythe tradeoffs in terms of energy, latency, accuracy as well asbit-precision of the layers for optimal hybrid network design.The contributions of our work are as follows:\u2022 We propose a methodology for designing hybrid precisionneural networks for edge-cloud processing systems.\u2022 We engineer early exiting strategies to conditionally acti-vate the deeper layers of the neural networks for enforcingenergy-ef\ufb01ciency in edge devices.\u2022 We evaluate the proposed methodology on state-of-artnetwork architectures. We show that by modifying net-work con\ufb01gurations such as bit-precision at the edge,early exiting is triggered in most cases.The paper is organized as follows. The next section gives anoverview of the design methodology for conditionally deephybrid neural networks. Section III reports the experimentresults and shows how to adjust network con\ufb01gurations toachieve a better performance-vs-energy trade-off.II. H YBRID NETWORK DESIGN WITH CONDITIONAL DEEPLEARNINGIn this section, we propose a methodology for designinghybrid neural networks and explore strategies to enable earlyclassi\ufb01cation in the edge network based on the dif\ufb01culty ofinputs to achieve early processing of data and reduction inenergy consumption.A. Hybrid Neural NetworksWe propose a hybrid neural network where a N-layernetwork is divided into two sections, M layers where thebit-precision of the weights and inputs is p 1 and N \u2212Mlayers where the corresponding bit-precisions are p 2 wherep 2 \u226bp 1. This is illustrated in Fig. 1. Generally, p 2 can beconsidered as a 32-bit \ufb02oat as the primary objective for ourdesign is to preserve the accuracy of the neural network. Thedesign parameters p 1 and M can be varied to identify theoptimal division for the proposed hybrid networks consideringthe tradeoff between performance and energy consumption.The low-precision section of a hybrid CNN consists ofQuanConv layers by which we quantize both the weights andinputs. Fig. 2 illustrates that a QuanConv layer is comprisedof a batch normalization layer, an activation layer, and aconvolution layer using quantized weights in order. In thecase of a ResNet [23], the residual connections are kept full-precision to mitigate the negative effect of quantized inputswith a small overhead. A batch normalization layer is appliedbefore quantization to ensure that inputs hold zero mean [14].Then the activation layer quantizes inputs using the followingscheme. For an input matrix I \u2208 R c\xd7wi\xd7hi , where c,w,hare the number channels, width and height respectively, thecorresponding binary input matrix I B \u2208 {+1 , \u22121 }c\xd7wi\xd7hiand quantized input matrix I Q is given byI B = sign( I ); I Q = v 1(\u230aI + v 0v 1\xb7Z\u2309/Z \u2212v 2) (1)where Z = 2 p \u22121 is the number of quantization levels andv 0,v 1,v 2 are constants.The quantization of weights is performed for each layer ina CNN. Let us represent the kernel as W \u2208R c\xd7w\xd7h, wherec,w,h are the number channels and width and height of thekernel respectively. If both weights and inputs are extremelyquantized (binarized), dot product operations in convolutionscan be implemented ef\ufb01ciently using XNOR and bit-counting--- Page 3 ---operations. To approximate the convolution operation, weestimate the real-value weight matrix W using a quantizedmatrix WQ and a scaling factor \u03b1 as proposed in [14].I\u2217W \u2248(IQ \u2217WQ)\u03b1; \u03b1= \u2225W\u2225l1n (2)WQ = v1(\u230aW + v0v1\xb7Z\u2309/Z\u2212v2) (3)where n= c\xd7w\xd7h is a constant. The precision of weightsand activations of the high precision section of a hybrid CNNare kept 32-bit \ufb02oating point. We adopt the training algorithmproposed by Rastegari et al [14] to train the networks withquantization.QuanConv QuanConv QuanConv ... Residual connection... BatchNormQuantizedActivationConvolutionQuanConv Fig. 2. Quantized convolutional layers in a ResNetIn a hybrid neural network, the edge network must sendextracted features corresponding to all samples to the cloudfor further processing, which takes communication energy andtime. In the next subsection, we will introduce how to enablethe edge to analyze easy data independently.B. Conditional Exit in Deep Hybrid NetworksEnabling distributed intelligence using hybrid networks inan AI-based fog computing system requires data processingboth at the edge and the cloud. Since the dif\ufb01culty of examplesin a dataset often exhibits an inherent variability, we apply theconcept of conditional deep learning [22] on hybrid networksto enable classi\ufb01cation of relatively simpler images at the low-precision edge, while the more dif\ufb01cult ones are passed to thehigh-precision deeper layers of the network in the cloud. Suchconditional activation of deeper layers in the context of hybridnetworks provides both communicational and computationalenergy ef\ufb01ciency and run-time improvements over standardnetworks. Each early exit at the edge is a linear classi\ufb01er thatconsists of an average pooling layer and a fully connectedlayer. Typically, such an exit needs to be placed at the end ofthe low-precision section of the hybrid network, i.e, the partof the network being implemented on edge. However, moreexits can be added in the low-precision section to facilitateearly classi\ufb01cation of easier examples. In Fig. 3, we show anexample of adding three early exits. The input to an earlyexit is the output from a convolutional layer, and all the earlyexits use full-precision weights to ensure no computation erroroccurs in the classi\ufb01cation layer.During the training process, although the early exits havealready provided labels, the rest of the network would stillbe activated during training to optimize all exits based on theCloudEdgeQuanConv Layers 1QuanConv Layers 2Conv Layers 3-2Conv Pooling & fcPooling & fcPooling & fcEarly Exit 0Early Exit 1Final ExitQuanConv Layers 3-1Pooling & fcEarly Exit 2Fig. 3. An example of a hybrid neural network with three early exits. Blocksin orange are quantized. The sizes of early exits differ corresponding to theirlocations.whole training set. Since there are more than one labels givenby our network system, we need to modify the optimizationobjective to train both the hybrid network and additionalexits. There are two optional training strategies: 1) SeparateOptimization, 2) Joint optimization.1) Separate optimization: In this approach, we \ufb01rst trainthe hybrid network without any early exits, hence, the sameoptimization objective can be used. Then we \ufb01x the parametersof all convolutional layers and train one early exit at a time.Let us denote the ground-truth label vector as yand the outputlabel given by early exit k as \u02c6yk. The optimization objectiveof the kth early exit isLk( \u02c6yk,y) =\u2212\u2211c\u2208Cyc log \u02c6ykc (4)where \u02c6ykc is a vector containing predicted probabilities for allpossible class labels and C represents the set of all classes.The backward propagation function will be called for k timeswith respect to Lk. Since parameters of convolutional layersdo not require gradient, the backward propagations in earlyexits are independent of each other.2) Joint optimization: In this approach, we use a jointoptimization technique to train the network along with earlyexits [24]. The total loss is calculated as the weighted sum ofthe loss functions of each exit branch.L(\u02c6y,y) =K\u2211k=1\u03bbkLk(\u02c6yk,y) (5)where K is the total number of exits and \u03bbk is set empiricallyand sum to 1. By joint optimization, the gradients used toupdate weights of the convolutional layers become\u2202L\u2202w =K\u2211k=1\u03bbk\u2202Lk\u2202w (6)Compared to separate optimization, training the entire networktogether optimizes the weights of the convolutional layersbased on the losses of both the \ufb01nal exit and early exits,--- Page 4 ---so it enables more samples to exit early and enlarges theenergy saving. On the other hand, separate optimization ismore \ufb02exible because it allows users to add early exits to atrained network. We will compare the results corresponding tothese two optimization strategies in section III. Because ourobjective is to make as many inferences to \ufb01nish at the edgeas possible, we will use joint optimization for the rest of ourexperiments.Algorithm 1: Inference using a conditionally deep hybridneural networkInput: Samples XOutput: Classi\ufb01cation results \u02c6Yfor sample i doO1[i] = Forward( X[i],Wt[1]);for layer l = 2 to M do//Propagate at the edgeOl[i] = Forward( Ol\u22121[i],Wt[l]);if Classi\ufb01er k locates at layer l then\u02c6Yk[i] = Classi\ufb01er[k]( Ol[i]);if Entropy(\u02c6Yk[i])<Threshold k then\u02c6Y[i] = \u02c6Yk[i];break; //take early exitendendendif \u02c6Y[i] not found then\u02c6Y[i] = Forward(OM [i], Wt[M : N]);//Continuepropagating on the cloudendendOnce the hybrid deep network is trained, we can utilizequantized weights and early exits to allow inference of easysamples to \ufb01nish at the edge. Algorithm 1 shows the inferenceprocess. To classify a sample, we activate the layers of the deepnetwork conditionally based on the entropy of the predictionresult calculated byentropy(\u02c6y) =\u2212\u2211c\u2208C\u02c6yc log \u02c6ycIf the entropy is lower than the threshold, it means the earlyexit has high con\ufb01dence about correctly labeling this sample,so the prediction can be returned without activating laterlayers. The thresholds for early exits are set empirically.C. Design Considerations of Conditionally Deep Hybrid Net-work1) Effect of bit-precision: When conditional deep learningis applied to a hybrid network, the effect of bit-depth on thetotal energy consumption of the edge-cloud system becomestwofold. Although the energy consumed by an operation isproportional to the bit-depth, increasing the bit-depth meanshaving a more precise network at the edge, so the numberof activated high-precision operations would be fewer thanksto early exiting. As a result, using more bits for quantizationdoes not necessarily lead to higher energy consumption. Toachieve better performance-vs-energy trade-off, we will adjustthe bit-depth depending on the complexity of datasets.2) Effect of the number of layers on edge: In a conditionallydeep hybrid network, the inference accuracy is not commensu-rate with the proportion of quantized layers due to additionalexits. For simplicity, suppose that we have only one earlyexit located after the last layer at the edge. The predictionability of the \ufb01nal exit on the cloud would be weakened tothe degree corresponding to the number of quantized layers.However, the overall accuracy of the hybrid network dependson the accuracy of all exits, and that of the early exit isaffected by the number of layers before it and the numberof parameters contained in its fully-connected layer. As thenumber of quantized layers increases, the early exit can inferbased on the high-level features extracted by a deeper network,which makes the inference task easier and potentially allowsmore samples to exit. Nevertheless, convolutional layers indifferent divisions of a ResNet have three distinct output sizes,and hence the number of parameters contained in the earlyexit drops signi\ufb01cantly when moving to the later division.Therefore, it is important to \ufb01nd the optimal division of ahybrid network.3) Effect of the number of exits: Using multiple exits atthe edge can allow inference to terminate as soon as possible,thus the latency can be further reduced. In spite of that,whether it can also provide further energy saving to the edgedepends on whether the power consumption of an earlier exitis smaller than the total power of convolutional layers and thelater exit located in the next division of the network. Becauseconvolutional layers are quantized but exits are not, their powerconsumption becomes a considerable part. If the number ofclasses in the dataset is large and the network is not verydeep, using more than one early exits is likely to consumemore power and hence is not desired.III. E XPERIMENTSTo validate the effectiveness of our model, we conductseveral sets of experiments using PyTorch. We report resultsof ResNet32 with varying hybrid con\ufb01gurations and condi-tional deep learning over CIFAR10 and CIFAR100 datasetsto explore the design space and evaluate the scalability. Weestimate the total energy consumption of inference by mul-tiplying the number of full-precision and binary operationswith corresponding energy listed in Table. I. Convolutions ofmatrices are implemented as multiply-and-add (MAC). For allour analysis, we have not included the communication energybetween the edge and the cloud.Operation Energy (pJ) Operation Energy (pJ)32-bit Addition 0.9 Binary MAC 0.232-bit MAC 4.6 Memory access per bit 2.5TABLE IENERGY CONSUMPTION CHART--- Page 5 ---A. Hybrid NetworkWe present the inference accuracy and energy results forhybrid networks in Fig. 4 using dashed lines. \u201c10E + 20C\u201drepresents a hybrid network with 10 quantized layers at theedge and 20 layers at the cloud. When we binarize the \ufb01rst10 layers of the network, the energy reduction with respect toa full-precision ResNet-32 is 1.5\xd7for both CIFAR10 and CI-FAR100 with accuracy losses of 1.5% and 2.1% respectively.On binarization of 20 layers, the energy saving becomes 2.6\xd7.a)b)CIFAR-10CIFAR-100Edge QuantizationEdge QuantizationFig. 4. a) Accuracy vs Energy plot for CIFAR-10 showing 20E+10C con\ufb01gachieves 5.5x energy bene\ufb01t over a full-precision standalone network on cloud.b)Accuracy vs Energy plot for CIFAR-100 showing 4-bit network at edge(24E+6C con\ufb01g) achieves 4.8x energy bene\ufb01t over a full-precision standalonenetwork on cloud. Various con\ufb01gurations have been explored with differentedge and cloud layers.B. Conditionally Deep Hybrid NetworkWe apply conditional deep learning on hybrid networks byadding an early exit after the last quantized layer at the edge.The thresholds for entropy to determine whether inference can\ufb01nish at the edge are set to 0.5 for CIFAR10 and 1.0 forCIFAR100. In Fig. 4(a), we show the energy consumptionand accuracy on CIFAR10 with and without an early exit. Theenergy saving of binarizing 20 convolutional layers becomes5.5\xd7after adding an early exit, which is two times larger thanbefore, and the accuracy remains the same. In comparison witha full-precision network with conditional exiting, it is 3.9 \xd7more energy ef\ufb01cient due to hybrid quantization. Next, weevaluate the different con\ufb01gurations of the conditional deephybrid network as mentioned in Section II.1) Separate Optimization VS Joint Optimization: In Table.II, we compare the results corresponding to these two differentoptimization strategies. For joint optimization, we set theweights \u03bbk used to calculate the joint loss empirically to 0.6for the early exit because giving more weight to the earlyexit will encourage more discriminative feature learning inearly layers [24]. With the same hybrid network con\ufb01guration(16 E + 14 C), joint optimization enables more samples to\ufb01nish inference at the early exit and yields higher accuracy.It con\ufb01rms our assumption that using joint optimization canmake the edge network more con\ufb01dent and make the edge-cloud system more energy ef\ufb01cient.Percentage of early exiting (%) Accuracy(%)Separate Joint Separate JointCIFAR10 24.1 52.8 88.72 89.16CIFAR100 17.3 24.6 61.80 62.50TABLE IICOMPARISON BETWEEN SEPARATE OPTIMIZATION AND JOINTOPTIMIZATION2) Effects of hybrid bit-depth: Unlike CIFAR10 of whichmost samples can be con\ufb01dently inferred by the binarizednetwork at the edge, CIFAR100 is more complex so thebinarized network is only con\ufb01dent on less than 30% of testsamples as illustrated in Fig. 5. To make the network atthe edge more con\ufb01dent, we use 2-bit or 4-bit quantizationinstead of binarization. For each quantization option, we varythe number of quantized layers and plot the performance ofconditionally deep hybrid networks in Fig. 4(b). Points in thetop-left corner are corresponding to optimal networks sincethey achieve high accuracy and low inference energy. Theexperiment results show that quantizing the network with morebits can achieve better accuracy without trading off energyef\ufb01ciency. 4-bit and 2-bit quantization only cause 1.4% and3.0% accuracy degradation respectively, while energy savingscan be 4.8\xd7and 5.3\xd7compared to a full-precision network.In Table. III, we show the inference energy consumed by theedge and the cloud. From the top left corner to the bottomright corner, the cloud energy decreases rapidly while the edgeenergy does not change much because we keep the \ufb01rst layerat the edge full-precision and it dominates the edge energy.Energy (mJ) 1-bit 2-bit 4-bitedge cloud edge cloud edge cloud10 E + 20 C 261 2269 295 2108 363 205916 E + 14 C 213 1440 264 1168 365 108920 E + 10 C 236 991 299 805 424 72624 E + 6 C 203 645 278 478 429 406TABLE IIIINFERENCE ENERGY AT THE EDGE AND CLOUD3) Effect of the number of layers on edge: From Fig. 4(b),we can see that as the number of binary layers increases, thehybrid network becomes less accurate. However, this is nottrue if early exits are added, as we have discussed in SectionII.C. In Fig. 4, we \ufb01nd the sweet spots which provides the bestperformance is using 16 binary layers and 14 full-precisionones. We also have discussed that the precision of the edgenetwork depends on the number of parameters contained in theearly exit. In Fig. 5, we show that in most cases, the number ofcorrect predictions \ufb01nish at the edge increases with the number--- Page 6 ---Fig. 5. Accuracy and Percentage of early exiting samples at the edge forCIFAR-100 dataset on Resnet-32of quantized layers, however, when the early exit is locatedafter 24 binary layers, its weight matrix of size 256 \xd7100is not robust enough, and hence the number of early exitingsamples on CIFAR100 abnormally decreases. In the case of2-bit or 4-bit quantization, the network is more robust, so theeffect of the decrease in the number of parameters is not assevere as that on binarized networks.% of exiting at edge Accuracy Energy(mJ)# of early exits 1 2 1 2 1 220 E + 10 C 59.3 38.6, 19.7 88.61 85.98 729 724TABLE IVEFFECT OF THE NUMBER OF EARLY EXITS ON CIFAR104) Effects of the number of exits: In Table. IV, we comparethe results of hybrid networks with one or two early exits onCIFAR10. We evenly distribute two exits: one is at the end ofthe edge, the other is put in the middle. We notice that addingmore exits has relatively minor effects on the total energy.This is because the full-precision layers of the network aremuch more power-hungry than the binarized ones, and the totalenergy of inference is dominated by the energy of samplesthat cannot be handled at the edge. As illustrated in Table.IV, adding more exits does not enable more samples to skipthe full-precision layers, so the cloud energy is not reduced.Besides, using two early exits causes performance degradationdue to the complexity of training. For CIFAR100, since thenumber of classes is 10 times larger, adding the other earlyexit in Conv 1 would lead to even higher energy consumptionthan activating binarized Conv 2 and the early exit in Conv 2.Moreover, if two exits are close, the number of exiting samplesat the later one would be small because their con\ufb01dences aresimilar. Therefore, using one early exit at the edge is enoughin most cases.IV. C ONCLUSIONIn conclusion, we applied conditional deep learning onpartially quantized neural networks to simulate neural networksystems distributed at the edge and cloud. We found that thejoint optimization strategy can enable the edge to handle moresamples. We showed that conditionally deep hybrid networksachieve much better performance-vs-energy trade-offs thanquantized networks without early exits. By modifying the bit-depth and number of quantized layers at the edge, we foundthe optimal con\ufb01gurations for hybrid networks, which allow65% of CIFAR10 samples and 52% of CIFAR100 samples toexit early and achieve around \ufb01ve times energy reduction.REFERENCES[1] M. Chiang and T. Zhang, \u201cFog and iot: An overview of researchopportunities,\u201d IEEE IoT Journal , vol. 3, pp. 854\u2013864, Dec 2016.[2] Krizhevsky et al. , \u201cImagenet classi\ufb01cation with deep convolutionalneural networks,\u201d in Advances in neural information processing systems,pp. 1097\u20131105, 2012.[3] Szegedy et al. , \u201cGoing deeper with convolutions,\u201d in Proceedings ofCVPR, pp. 1\u20139, 2015.[4] He et al., \u201cDeep residual learning for image recognition,\u201d inProceedingsof CVPR, pp. 770\u2013778, 2016.[5] T. Mikolov et al., \u201cDistributed representations of words and phrases andtheir compositionality,\u201d in Advances in neural information processingsystems, pp. 3111\u20133119, 2013.[6] R. B. Girshick, \u201cFast R-CNN,\u201d CoRR, vol. abs/1504.08083, 2015.[7] M. Hassanalieragh et al. , \u201cHealth monitoring and management usinginternet-of-things (iot) sensing with cloud-based processing: Opportuni-ties and challenges,\u201d in 2015 IEEE International Conference on ServicesComputing, pp. 285\u2013292, June 2015.[8] M. Teichmann et al., \u201cMultinet: Real-time joint semantic reasoning forautonomous driving,\u201d in 2018 IEEE Intelligent Vehicles Symposium (IV),pp. 1013\u20131020, June 2018.[9] H. Li et al. , \u201cLearning iot in edge: Deep learning for the internet ofthings with edge computing,\u201d IEEE Network , vol. 32, pp. 96\u2013101, Jan2018.[10] N. P. Jouppi et al., \u201cIn-datacenter performance analysis of a tensor pro-cessing unit,\u201d in 2017 ACM/IEEE 44th Annual International Symposiumon Computer Architecture (ISCA) , pp. 1\u201312, IEEE, 2017.[11] S. Teerapittayanon et al. , \u201cDistributed deep neural networks over thecloud, the edge and end devices,\u201d in 2017 IEEE 37th InternationalConference on Distributed Computing Systems (ICDCS) , pp. 328\u2013339,June 2017.[12] B. Jacob et al. , \u201cQuantization and training of neural networks foref\ufb01cient integer-arithmetic-only inference,\u201d CoRR, vol. abs/1712.05877,2017.[13] M. Courbariaux and Y . Bengio, \u201cBinarynet: Training deep neural net-works with weights and activations constrained to +1 or -1,\u201d CoRR,vol. abs/1602.02830, 2016.[14] M. Rastegari et al. , \u201cXnor-net: Imagenet classi\ufb01cation using binaryconvolutional neural networks,\u201d CoRR, vol. abs/1603.05279, 2016.[15] S. Zhou et al., \u201cDorefa-net: Training low bitwidth convolutional neuralnetworks with low bitwidth gradients,\u201darXiv preprint arXiv:1606.06160,2016.[16] S.-C. Zhou et al. , \u201cBalanced quantization: An effective and ef\ufb01cientapproach to quantized neural networks,\u201d Journal of Computer Scienceand Technology, vol. 32, no. 4, pp. 667\u2013682, 2017.[17] D. Zhang, J. Yang, D. Ye, and G. Hua, \u201cLq-nets: Learned quantizationfor highly accurate and compact deep neural networks,\u201d in Proceedingsof ECCV, pp. 365\u2013382, 2018.[18] S. Jung, C. Son, S. Lee, J. Son, Y . Kwak, J.-J. Han, and C. Choi,\u201cJoint training of low-precision neural network with quantization intervalparameters,\u201d arXiv preprint arXiv:1808.05779 , 2018.[19] J. Choi et al. , \u201cPact: Parameterized clipping activation for quantizedneural networks,\u201d arXiv preprint arXiv:1805.06085 , 2018.[20] I. Chakraborty et al. , \u201cPca-driven hybrid network design for enablingintelligence at the edge,\u201d arXiv preprint arXiv:1906.01493 , 2019.[21] Prabhu et al. , \u201cHybrid binary networks: Optimizing for accuracy, ef\ufb01-ciency and memory,\u201d in 2018 IEEE Winter Conference on Applicationsof Computer Vision (WACV) , pp. 821\u2013829, IEEE, 2018.[22] P. Panda et al. , \u201cConditional deep learning for energy-ef\ufb01cient andenhanced pattern recognition,\u201d CoRR, vol. abs/1509.08971, 2015.[23] K. He et al. , \u201cDeep residual learning for image recognition,\u201d CVPR,pp. 770\u2013778, 2015.[24] S. Teerapittayanon et al., \u201cBranchynet: Fast inference via early exitingfrom deep neural networks,\u201d CoRR, vol. abs/1709.01686, 2017., Your response: ```ADD_PAPER\n2005.10851v1\nThis paper presents a conditionally deep hybrid neural network for edge-cloud collaborative inference, combining quantized layers at the edge with full-precision layers in the cloud. The key innovation is an early-exit mechanism that allows easy samples to be classified at the edge, reducing both computation and communication costs. Experimental results show the approach achieves 5.5x energy reduction on CIFAR-10 (processing 65% of samples at edge) and 4.8x reduction on CIFAR-100 while maintaining accuracy. The hybrid quantization strategy and joint optimization of early exits demonstrate significant improvements over full-precision networks for distributed intelligence systems.\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\n~~~~~~~~~~\nCurrent Step #13, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification model compression\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 15:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #11, Phase: literature review, Feedback: 3\n--- Page 1 ---Conditionally Deep Hybrid Neural NetworksAcross Edge and CloudYinghan Long*, Indranil Chakraborty *, Kaushik Roy(* Authors contributed equally to the work)School of Electrical and Computer Engineering, Purdue Universitylong273@purdue.edu, ichakra@purdue.edu, kaushik@purdue.eduAbstract\u2014The pervasiveness of \u201cInternet-of-Things\u201d in ourdaily life has led to a recent surge in fog computing, encompassinga collaboration of cloud computing and edge intelligence. To thateffect, deep learning has been a major driving force towardsenabling such intelligent systems. However, growing model sizesin deep learning pose a signi\ufb01cant challenge towards deploymentin resource-constrained edge devices. Moreover, in a distributedintelligence environment, ef\ufb01cient workload distribution is neces-sary between edge and cloud systems. To address these challenges,we propose a conditionally deep hybrid neural network forenabling AI-based fog computing. The proposed network can bedeployed in a distributed manner, consisting of quantized layersand early exits at the edge and full-precision layers on the cloud.During inference, if an early exit has high con\ufb01dence in the clas-si\ufb01cation results, it would allow samples to exit at the edge, andthe deeper layers on the cloud are activated conditionally, whichcan lead to improved energy ef\ufb01ciency and inference latency. Weperform an extensive design space exploration with the goal ofminimizing energy consumption at the edge while achieving stateof the art classi\ufb01cation accuracies on image classi\ufb01cation tasks.We show that with binarized layers at the edge, the proposedconditional hybrid network can process 65% of inferences atthe edge, leading to 5.5 \xd7 computational energy reduction withminimal accuracy degradation on CIFAR-10 dataset. For themore complex dataset CIFAR-100, we observe that the proposednetwork with 4-bit quantization at the edge achieves 52% earlyclassi\ufb01cation at the edge with 4.8\xd7 energy reduction. The analysisgives us insights on designing ef\ufb01cient hybrid networks whichachieve signi\ufb01cantly higher energy ef\ufb01ciency than full-precisionnetworks for edge-cloud based distributed intelligence systems.Index Terms\u2014Conditional deep learning, quantized neuralnetwork, fog computingI. I NTRODUCTIONThe age of \u201cInternet-of-Things\u201d (IoT) has touched humanlives in an unprecedented manner by endowing us withremarkable connectivity and autonomous intelligent systems[1]. These bene\ufb01ts come with the necessity of processinghumongous amounts of heterogeneous data obtained from theenvironment. The ability of deep learning to reliably processsuch data has resulted in it playing a dominant role in awide range of Arti\ufb01cial Intelligence (AI) applications, includ-ing image classi\ufb01cation [2]\u2013[4], natural language processing[5], and object detection [6]. As the third wave of arti\ufb01cialintelligence (AI) is accelerating, researchers are incorporatingThe research was funded in part by C-BRIC, one of six centers in JUMP, aSemiconductor Research Corporation (SRC) program sponsored by DARPA,the National Science Foundation, Intel Corporation, Vannevar Bush FacultyFellowship, and the U.K. Ministry of Defence under Agreement NumberW911NF-16-3-0001.AI into IoT applications. Traditionally, these cognitive appli-cations perform compute-intensive tasks in a centralized hubwhich collects data from thousands of connected edge devices.However, in the recent past, there have been rising concerns ofdata security due to over-centralization of information, and thecontinuous communication overhead between the edge devicesand the cloud drastically increases the power consumption ofworld-wide internet. Moreover, real-time AI systems such ashealth monitoring [7] and autonomous driving [8] require fastprocessing. Hence, to truly realize the potential of AI in IoT,it is necessary to enable intelligence at the edge [9].On the other hand, the resource constraints of edge deviceslimit us from deploying large deep learning models at theedge. With the growing complexity of tasks, there has beena considerable increase in the sizes of deep neural networks(DNNs). This requires DNNs to be trained on powerfulGraphic Processing Units (GPUs) or Tensor Processing Units(TPUs) [10]. Edge devices, however, are resource-constrainedand implementing DNNs on such devices jeopardizes therequirement of long battery life for stable operation. Thishas propelled the search for techniques that enable energy-ef\ufb01cient implementations of deep learning models. Besides,researchers are investigating ef\ufb01cient techniques to enabledistributed intelligence at the near-user edge device and end-user cloud [11].A popular approach to reduce model complexity and im-prove computational ef\ufb01ciency is to quantize the weights andactivations of neural networks by representing 32-bit \ufb02oating-point values with lower bit-depth \ufb01xed point [12] or evenbinary values [13] [14]. Compared to full-precision networks,binary neural networks are fast and ef\ufb01cient, but this comeswith a degradation in performance. For example, binarizingboth inputs and weights of a residual network(ResNet) causes18% reduction in top-1 accuracy on ImageNet and 9% onCIFAR10. In order to overcome this challenge, there have beenvarious approaches towards improving quantization schemes[15], [16] as well as proposing training algorithms for net-works with increased bit-precision [17]\u2013[19]. An alternativeapproach towards preserving the performance while achievinga signi\ufb01cant energy-ef\ufb01ciency is designing hybrid networkarchitectures [20], [21], which consist of both quantized andhigh-precision layers. Since high-precision layers are power-hungry, such computations can be performed in the cloud.In this work, we propose a design methodology to buildconditionally deep hybrid networks for distributed intelligenceacross an edge-cloud system. In such a network, quantizedarXiv:2005.10851v1  [cs.LG]  21 May 2020--- Page 2 ---Full-precisionNetworkQuantizedNetworkfcQuantizedNetwork+Early exitsDeep NeuralNetworkHybrid NeuralNetworkConditionally DeepHybrid NetworkEdgefcFull-precisionNetworkfcCloudFull-precisionNetworkHybridQuantizationConditionalDeep\xa0LearningEarly resultsFinal resultsInputsInputsEasy Hard Inputs:Hard Easy Fig. 1. Overview of our design methodology for Conditionally Deep Hybrid Networks. From left to right, we show how a standard deep neural network isdeveloped into a Conditionally Deep Hybrid Network and distributed to an edge-cloud system.layers of the network are deployed in the edge device, whilehigh-precision layers are performed on the cloud. Further, byusing the Conditional Deep Learning (CDL) approach [22],our network leverages the dif\ufb01culty of inputs to classify therelatively easy samples with early exits and conditionallyactivate the deeper layers. This technique is particularly suitedfor a distributed intelligence system where we can process theeasy inputs at the energy-ef\ufb01cient edge device with quantizednetworks while only the inputs that are close to the decisionboundary are sent to the cloud. As a result, both communica-tion cost and computation cost can be improved. Through anextensive analysis of hybrid network architectures, we identifythe tradeoffs in terms of energy, latency, accuracy as well asbit-precision of the layers for optimal hybrid network design.The contributions of our work are as follows:\u2022 We propose a methodology for designing hybrid precisionneural networks for edge-cloud processing systems.\u2022 We engineer early exiting strategies to conditionally acti-vate the deeper layers of the neural networks for enforcingenergy-ef\ufb01ciency in edge devices.\u2022 We evaluate the proposed methodology on state-of-artnetwork architectures. We show that by modifying net-work con\ufb01gurations such as bit-precision at the edge,early exiting is triggered in most cases.The paper is organized as follows. The next section gives anoverview of the design methodology for conditionally deephybrid neural networks. Section III reports the experimentresults and shows how to adjust network con\ufb01gurations toachieve a better performance-vs-energy trade-off.II. H YBRID NETWORK DESIGN WITH CONDITIONAL DEEPLEARNINGIn this section, we propose a methodology for designinghybrid neural networks and explore strategies to enable earlyclassi\ufb01cation in the edge network based on the dif\ufb01culty ofinputs to achieve early processing of data and reduction inenergy consumption.A. Hybrid Neural NetworksWe propose a hybrid neural network where a N-layernetwork is divided into two sections, M layers where thebit-precision of the weights and inputs is p 1 and N \u2212Mlayers where the corresponding bit-precisions are p 2 wherep 2 \u226bp 1. This is illustrated in Fig. 1. Generally, p 2 can beconsidered as a 32-bit \ufb02oat as the primary objective for ourdesign is to preserve the accuracy of the neural network. Thedesign parameters p 1 and M can be varied to identify theoptimal division for the proposed hybrid networks consideringthe tradeoff between performance and energy consumption.The low-precision section of a hybrid CNN consists ofQuanConv layers by which we quantize both the weights andinputs. Fig. 2 illustrates that a QuanConv layer is comprisedof a batch normalization layer, an activation layer, and aconvolution layer using quantized weights in order. In thecase of a ResNet [23], the residual connections are kept full-precision to mitigate the negative effect of quantized inputswith a small overhead. A batch normalization layer is appliedbefore quantization to ensure that inputs hold zero mean [14].Then the activation layer quantizes inputs using the followingscheme. For an input matrix I \u2208 R c\xd7wi\xd7hi , where c,w,hare the number channels, width and height respectively, thecorresponding binary input matrix I B \u2208 {+1 , \u22121 }c\xd7wi\xd7hiand quantized input matrix I Q is given byI B = sign( I ); I Q = v 1(\u230aI + v 0v 1\xb7Z\u2309/Z \u2212v 2) (1)where Z = 2 p \u22121 is the number of quantization levels andv 0,v 1,v 2 are constants.The quantization of weights is performed for each layer ina CNN. Let us represent the kernel as W \u2208R c\xd7w\xd7h, wherec,w,h are the number channels and width and height of thekernel respectively. If both weights and inputs are extremelyquantized (binarized), dot product operations in convolutionscan be implemented ef\ufb01ciently using XNOR and bit-counting--- Page 3 ---operations. To approximate the convolution operation, weestimate the real-value weight matrix W using a quantizedmatrix WQ and a scaling factor \u03b1 as proposed in [14].I\u2217W \u2248(IQ \u2217WQ)\u03b1; \u03b1= \u2225W\u2225l1n (2)WQ = v1(\u230aW + v0v1\xb7Z\u2309/Z\u2212v2) (3)where n= c\xd7w\xd7h is a constant. The precision of weightsand activations of the high precision section of a hybrid CNNare kept 32-bit \ufb02oating point. We adopt the training algorithmproposed by Rastegari et al [14] to train the networks withquantization.QuanConv QuanConv QuanConv ... Residual connection... BatchNormQuantizedActivationConvolutionQuanConv Fig. 2. Quantized convolutional layers in a ResNetIn a hybrid neural network, the edge network must sendextracted features corresponding to all samples to the cloudfor further processing, which takes communication energy andtime. In the next subsection, we will introduce how to enablethe edge to analyze easy data independently.B. Conditional Exit in Deep Hybrid NetworksEnabling distributed intelligence using hybrid networks inan AI-based fog computing system requires data processingboth at the edge and the cloud. Since the dif\ufb01culty of examplesin a dataset often exhibits an inherent variability, we apply theconcept of conditional deep learning [22] on hybrid networksto enable classi\ufb01cation of relatively simpler images at the low-precision edge, while the more dif\ufb01cult ones are passed to thehigh-precision deeper layers of the network in the cloud. Suchconditional activation of deeper layers in the context of hybridnetworks provides both communicational and computationalenergy ef\ufb01ciency and run-time improvements over standardnetworks. Each early exit at the edge is a linear classi\ufb01er thatconsists of an average pooling layer and a fully connectedlayer. Typically, such an exit needs to be placed at the end ofthe low-precision section of the hybrid network, i.e, the partof the network being implemented on edge. However, moreexits can be added in the low-precision section to facilitateearly classi\ufb01cation of easier examples. In Fig. 3, we show anexample of adding three early exits. The input to an earlyexit is the output from a convolutional layer, and all the earlyexits use full-precision weights to ensure no computation erroroccurs in the classi\ufb01cation layer.During the training process, although the early exits havealready provided labels, the rest of the network would stillbe activated during training to optimize all exits based on theCloudEdgeQuanConv Layers 1QuanConv Layers 2Conv Layers 3-2Conv Pooling & fcPooling & fcPooling & fcEarly Exit 0Early Exit 1Final ExitQuanConv Layers 3-1Pooling & fcEarly Exit 2Fig. 3. An example of a hybrid neural network with three early exits. Blocksin orange are quantized. The sizes of early exits differ corresponding to theirlocations.whole training set. Since there are more than one labels givenby our network system, we need to modify the optimizationobjective to train both the hybrid network and additionalexits. There are two optional training strategies: 1) SeparateOptimization, 2) Joint optimization.1) Separate optimization: In this approach, we \ufb01rst trainthe hybrid network without any early exits, hence, the sameoptimization objective can be used. Then we \ufb01x the parametersof all convolutional layers and train one early exit at a time.Let us denote the ground-truth label vector as yand the outputlabel given by early exit k as \u02c6yk. The optimization objectiveof the kth early exit isLk( \u02c6yk,y) =\u2212\u2211c\u2208Cyc log \u02c6ykc (4)where \u02c6ykc is a vector containing predicted probabilities for allpossible class labels and C represents the set of all classes.The backward propagation function will be called for k timeswith respect to Lk. Since parameters of convolutional layersdo not require gradient, the backward propagations in earlyexits are independent of each other.2) Joint optimization: In this approach, we use a jointoptimization technique to train the network along with earlyexits [24]. The total loss is calculated as the weighted sum ofthe loss functions of each exit branch.L(\u02c6y,y) =K\u2211k=1\u03bbkLk(\u02c6yk,y) (5)where K is the total number of exits and \u03bbk is set empiricallyand sum to 1. By joint optimization, the gradients used toupdate weights of the convolutional layers become\u2202L\u2202w =K\u2211k=1\u03bbk\u2202Lk\u2202w (6)Compared to separate optimization, training the entire networktogether optimizes the weights of the convolutional layersbased on the losses of both the \ufb01nal exit and early exits,--- Page 4 ---so it enables more samples to exit early and enlarges theenergy saving. On the other hand, separate optimization ismore \ufb02exible because it allows users to add early exits to atrained network. We will compare the results corresponding tothese two optimization strategies in section III. Because ourobjective is to make as many inferences to \ufb01nish at the edgeas possible, we will use joint optimization for the rest of ourexperiments.Algorithm 1: Inference using a conditionally deep hybridneural networkInput: Samples XOutput: Classi\ufb01cation results \u02c6Yfor sample i doO1[i] = Forward( X[i],Wt[1]);for layer l = 2 to M do//Propagate at the edgeOl[i] = Forward( Ol\u22121[i],Wt[l]);if Classi\ufb01er k locates at layer l then\u02c6Yk[i] = Classi\ufb01er[k]( Ol[i]);if Entropy(\u02c6Yk[i])<Threshold k then\u02c6Y[i] = \u02c6Yk[i];break; //take early exitendendendif \u02c6Y[i] not found then\u02c6Y[i] = Forward(OM [i], Wt[M : N]);//Continuepropagating on the cloudendendOnce the hybrid deep network is trained, we can utilizequantized weights and early exits to allow inference of easysamples to \ufb01nish at the edge. Algorithm 1 shows the inferenceprocess. To classify a sample, we activate the layers of the deepnetwork conditionally based on the entropy of the predictionresult calculated byentropy(\u02c6y) =\u2212\u2211c\u2208C\u02c6yc log \u02c6ycIf the entropy is lower than the threshold, it means the earlyexit has high con\ufb01dence about correctly labeling this sample,so the prediction can be returned without activating laterlayers. The thresholds for early exits are set empirically.C. Design Considerations of Conditionally Deep Hybrid Net-work1) Effect of bit-precision: When conditional deep learningis applied to a hybrid network, the effect of bit-depth on thetotal energy consumption of the edge-cloud system becomestwofold. Although the energy consumed by an operation isproportional to the bit-depth, increasing the bit-depth meanshaving a more precise network at the edge, so the numberof activated high-precision operations would be fewer thanksto early exiting. As a result, using more bits for quantizationdoes not necessarily lead to higher energy consumption. Toachieve better performance-vs-energy trade-off, we will adjustthe bit-depth depending on the complexity of datasets.2) Effect of the number of layers on edge: In a conditionallydeep hybrid network, the inference accuracy is not commensu-rate with the proportion of quantized layers due to additionalexits. For simplicity, suppose that we have only one earlyexit located after the last layer at the edge. The predictionability of the \ufb01nal exit on the cloud would be weakened tothe degree corresponding to the number of quantized layers.However, the overall accuracy of the hybrid network dependson the accuracy of all exits, and that of the early exit isaffected by the number of layers before it and the numberof parameters contained in its fully-connected layer. As thenumber of quantized layers increases, the early exit can inferbased on the high-level features extracted by a deeper network,which makes the inference task easier and potentially allowsmore samples to exit. Nevertheless, convolutional layers indifferent divisions of a ResNet have three distinct output sizes,and hence the number of parameters contained in the earlyexit drops signi\ufb01cantly when moving to the later division.Therefore, it is important to \ufb01nd the optimal division of ahybrid network.3) Effect of the number of exits: Using multiple exits atthe edge can allow inference to terminate as soon as possible,thus the latency can be further reduced. In spite of that,whether it can also provide further energy saving to the edgedepends on whether the power consumption of an earlier exitis smaller than the total power of convolutional layers and thelater exit located in the next division of the network. Becauseconvolutional layers are quantized but exits are not, their powerconsumption becomes a considerable part. If the number ofclasses in the dataset is large and the network is not verydeep, using more than one early exits is likely to consumemore power and hence is not desired.III. E XPERIMENTSTo validate the effectiveness of our model, we conductseveral sets of experiments using PyTorch. We report resultsof ResNet32 with varying hybrid con\ufb01gurations and condi-tional deep learning over CIFAR10 and CIFAR100 datasetsto explore the design space and evaluate the scalability. Weestimate the total energy consumption of inference by mul-tiplying the number of full-precision and binary operationswith corresponding energy listed in Table. I. Convolutions ofmatrices are implemented as multiply-and-add (MAC). For allour analysis, we have not included the communication energybetween the edge and the cloud.Operation Energy (pJ) Operation Energy (pJ)32-bit Addition 0.9 Binary MAC 0.232-bit MAC 4.6 Memory access per bit 2.5TABLE IENERGY CONSUMPTION CHART--- Page 5 ---A. Hybrid NetworkWe present the inference accuracy and energy results forhybrid networks in Fig. 4 using dashed lines. \u201c10E + 20C\u201drepresents a hybrid network with 10 quantized layers at theedge and 20 layers at the cloud. When we binarize the \ufb01rst10 layers of the network, the energy reduction with respect toa full-precision ResNet-32 is 1.5\xd7for both CIFAR10 and CI-FAR100 with accuracy losses of 1.5% and 2.1% respectively.On binarization of 20 layers, the energy saving becomes 2.6\xd7.a)b)CIFAR-10CIFAR-100Edge QuantizationEdge QuantizationFig. 4. a) Accuracy vs Energy plot for CIFAR-10 showing 20E+10C con\ufb01gachieves 5.5x energy bene\ufb01t over a full-precision standalone network on cloud.b)Accuracy vs Energy plot for CIFAR-100 showing 4-bit network at edge(24E+6C con\ufb01g) achieves 4.8x energy bene\ufb01t over a full-precision standalonenetwork on cloud. Various con\ufb01gurations have been explored with differentedge and cloud layers.B. Conditionally Deep Hybrid NetworkWe apply conditional deep learning on hybrid networks byadding an early exit after the last quantized layer at the edge.The thresholds for entropy to determine whether inference can\ufb01nish at the edge are set to 0.5 for CIFAR10 and 1.0 forCIFAR100. In Fig. 4(a), we show the energy consumptionand accuracy on CIFAR10 with and without an early exit. Theenergy saving of binarizing 20 convolutional layers becomes5.5\xd7after adding an early exit, which is two times larger thanbefore, and the accuracy remains the same. In comparison witha full-precision network with conditional exiting, it is 3.9 \xd7more energy ef\ufb01cient due to hybrid quantization. Next, weevaluate the different con\ufb01gurations of the conditional deephybrid network as mentioned in Section II.1) Separate Optimization VS Joint Optimization: In Table.II, we compare the results corresponding to these two differentoptimization strategies. For joint optimization, we set theweights \u03bbk used to calculate the joint loss empirically to 0.6for the early exit because giving more weight to the earlyexit will encourage more discriminative feature learning inearly layers [24]. With the same hybrid network con\ufb01guration(16 E + 14 C), joint optimization enables more samples to\ufb01nish inference at the early exit and yields higher accuracy.It con\ufb01rms our assumption that using joint optimization canmake the edge network more con\ufb01dent and make the edge-cloud system more energy ef\ufb01cient.Percentage of early exiting (%) Accuracy(%)Separate Joint Separate JointCIFAR10 24.1 52.8 88.72 89.16CIFAR100 17.3 24.6 61.80 62.50TABLE IICOMPARISON BETWEEN SEPARATE OPTIMIZATION AND JOINTOPTIMIZATION2) Effects of hybrid bit-depth: Unlike CIFAR10 of whichmost samples can be con\ufb01dently inferred by the binarizednetwork at the edge, CIFAR100 is more complex so thebinarized network is only con\ufb01dent on less than 30% of testsamples as illustrated in Fig. 5. To make the network atthe edge more con\ufb01dent, we use 2-bit or 4-bit quantizationinstead of binarization. For each quantization option, we varythe number of quantized layers and plot the performance ofconditionally deep hybrid networks in Fig. 4(b). Points in thetop-left corner are corresponding to optimal networks sincethey achieve high accuracy and low inference energy. Theexperiment results show that quantizing the network with morebits can achieve better accuracy without trading off energyef\ufb01ciency. 4-bit and 2-bit quantization only cause 1.4% and3.0% accuracy degradation respectively, while energy savingscan be 4.8\xd7and 5.3\xd7compared to a full-precision network.In Table. III, we show the inference energy consumed by theedge and the cloud. From the top left corner to the bottomright corner, the cloud energy decreases rapidly while the edgeenergy does not change much because we keep the \ufb01rst layerat the edge full-precision and it dominates the edge energy.Energy (mJ) 1-bit 2-bit 4-bitedge cloud edge cloud edge cloud10 E + 20 C 261 2269 295 2108 363 205916 E + 14 C 213 1440 264 1168 365 108920 E + 10 C 236 991 299 805 424 72624 E + 6 C 203 645 278 478 429 406TABLE IIIINFERENCE ENERGY AT THE EDGE AND CLOUD3) Effect of the number of layers on edge: From Fig. 4(b),we can see that as the number of binary layers increases, thehybrid network becomes less accurate. However, this is nottrue if early exits are added, as we have discussed in SectionII.C. In Fig. 4, we \ufb01nd the sweet spots which provides the bestperformance is using 16 binary layers and 14 full-precisionones. We also have discussed that the precision of the edgenetwork depends on the number of parameters contained in theearly exit. In Fig. 5, we show that in most cases, the number ofcorrect predictions \ufb01nish at the edge increases with the number--- Page 6 ---Fig. 5. Accuracy and Percentage of early exiting samples at the edge forCIFAR-100 dataset on Resnet-32of quantized layers, however, when the early exit is locatedafter 24 binary layers, its weight matrix of size 256 \xd7100is not robust enough, and hence the number of early exitingsamples on CIFAR100 abnormally decreases. In the case of2-bit or 4-bit quantization, the network is more robust, so theeffect of the decrease in the number of parameters is not assevere as that on binarized networks.% of exiting at edge Accuracy Energy(mJ)# of early exits 1 2 1 2 1 220 E + 10 C 59.3 38.6, 19.7 88.61 85.98 729 724TABLE IVEFFECT OF THE NUMBER OF EARLY EXITS ON CIFAR104) Effects of the number of exits: In Table. IV, we comparethe results of hybrid networks with one or two early exits onCIFAR10. We evenly distribute two exits: one is at the end ofthe edge, the other is put in the middle. We notice that addingmore exits has relatively minor effects on the total energy.This is because the full-precision layers of the network aremuch more power-hungry than the binarized ones, and the totalenergy of inference is dominated by the energy of samplesthat cannot be handled at the edge. As illustrated in Table.IV, adding more exits does not enable more samples to skipthe full-precision layers, so the cloud energy is not reduced.Besides, using two early exits causes performance degradationdue to the complexity of training. For CIFAR100, since thenumber of classes is 10 times larger, adding the other earlyexit in Conv 1 would lead to even higher energy consumptionthan activating binarized Conv 2 and the early exit in Conv 2.Moreover, if two exits are close, the number of exiting samplesat the later one would be small because their con\ufb01dences aresimilar. Therefore, using one early exit at the edge is enoughin most cases.IV. C ONCLUSIONIn conclusion, we applied conditional deep learning onpartially quantized neural networks to simulate neural networksystems distributed at the edge and cloud. We found that thejoint optimization strategy can enable the edge to handle moresamples. We showed that conditionally deep hybrid networksachieve much better performance-vs-energy trade-offs thanquantized networks without early exits. By modifying the bit-depth and number of quantized layers at the edge, we foundthe optimal con\ufb01gurations for hybrid networks, which allow65% of CIFAR10 samples and 52% of CIFAR100 samples toexit early and achieve around \ufb01ve times energy reduction.REFERENCES[1] M. Chiang and T. Zhang, \u201cFog and iot: An overview of researchopportunities,\u201d IEEE IoT Journal , vol. 3, pp. 854\u2013864, Dec 2016.[2] Krizhevsky et al. , \u201cImagenet classi\ufb01cation with deep convolutionalneural networks,\u201d in Advances in neural information processing systems,pp. 1097\u20131105, 2012.[3] Szegedy et al. , \u201cGoing deeper with convolutions,\u201d in Proceedings ofCVPR, pp. 1\u20139, 2015.[4] He et al., \u201cDeep residual learning for image recognition,\u201d inProceedingsof CVPR, pp. 770\u2013778, 2016.[5] T. Mikolov et al., \u201cDistributed representations of words and phrases andtheir compositionality,\u201d in Advances in neural information processingsystems, pp. 3111\u20133119, 2013.[6] R. B. Girshick, \u201cFast R-CNN,\u201d CoRR, vol. abs/1504.08083, 2015.[7] M. Hassanalieragh et al. , \u201cHealth monitoring and management usinginternet-of-things (iot) sensing with cloud-based processing: Opportuni-ties and challenges,\u201d in 2015 IEEE International Conference on ServicesComputing, pp. 285\u2013292, June 2015.[8] M. Teichmann et al., \u201cMultinet: Real-time joint semantic reasoning forautonomous driving,\u201d in 2018 IEEE Intelligent Vehicles Symposium (IV),pp. 1013\u20131020, June 2018.[9] H. Li et al. , \u201cLearning iot in edge: Deep learning for the internet ofthings with edge computing,\u201d IEEE Network , vol. 32, pp. 96\u2013101, Jan2018.[10] N. P. Jouppi et al., \u201cIn-datacenter performance analysis of a tensor pro-cessing unit,\u201d in 2017 ACM/IEEE 44th Annual International Symposiumon Computer Architecture (ISCA) , pp. 1\u201312, IEEE, 2017.[11] S. Teerapittayanon et al. , \u201cDistributed deep neural networks over thecloud, the edge and end devices,\u201d in 2017 IEEE 37th InternationalConference on Distributed Computing Systems (ICDCS) , pp. 328\u2013339,June 2017.[12] B. Jacob et al. , \u201cQuantization and training of neural networks foref\ufb01cient integer-arithmetic-only inference,\u201d CoRR, vol. abs/1712.05877,2017.[13] M. Courbariaux and Y . Bengio, \u201cBinarynet: Training deep neural net-works with weights and activations constrained to +1 or -1,\u201d CoRR,vol. abs/1602.02830, 2016.[14] M. Rastegari et al. , \u201cXnor-net: Imagenet classi\ufb01cation using binaryconvolutional neural networks,\u201d CoRR, vol. abs/1603.05279, 2016.[15] S. Zhou et al., \u201cDorefa-net: Training low bitwidth convolutional neuralnetworks with low bitwidth gradients,\u201darXiv preprint arXiv:1606.06160,2016.[16] S.-C. Zhou et al. , \u201cBalanced quantization: An effective and ef\ufb01cientapproach to quantized neural networks,\u201d Journal of Computer Scienceand Technology, vol. 32, no. 4, pp. 667\u2013682, 2017.[17] D. Zhang, J. Yang, D. Ye, and G. Hua, \u201cLq-nets: Learned quantizationfor highly accurate and compact deep neural networks,\u201d in Proceedingsof ECCV, pp. 365\u2013382, 2018.[18] S. Jung, C. Son, S. Lee, J. Son, Y . Kwak, J.-J. Han, and C. Choi,\u201cJoint training of low-precision neural network with quantization intervalparameters,\u201d arXiv preprint arXiv:1808.05779 , 2018.[19] J. Choi et al. , \u201cPact: Parameterized clipping activation for quantizedneural networks,\u201d arXiv preprint arXiv:1805.06085 , 2018.[20] I. Chakraborty et al. , \u201cPca-driven hybrid network design for enablingintelligence at the edge,\u201d arXiv preprint arXiv:1906.01493 , 2019.[21] Prabhu et al. , \u201cHybrid binary networks: Optimizing for accuracy, ef\ufb01-ciency and memory,\u201d in 2018 IEEE Winter Conference on Applicationsof Computer Vision (WACV) , pp. 821\u2013829, IEEE, 2018.[22] P. Panda et al. , \u201cConditional deep learning for energy-ef\ufb01cient andenhanced pattern recognition,\u201d CoRR, vol. abs/1509.08971, 2015.[23] K. He et al. , \u201cDeep residual learning for image recognition,\u201d CVPR,pp. 770\u2013778, 2015.[24] S. Teerapittayanon et al., \u201cBranchynet: Fast inference via early exitingfrom deep neural networks,\u201d CoRR, vol. abs/1709.01686, 2017., Your response: ```ADD_PAPER\n2005.10851v1\nThis paper presents a conditionally deep hybrid neural network for edge-cloud collaborative inference, combining quantized layers at the edge with full-precision layers in the cloud. The key innovation is an early-exit mechanism that allows easy samples to be classified at the edge, reducing both computation and communication costs. Experimental results show the approach achieves 5.5x energy reduction on CIFAR-10 (processing 65% of samples at edge) and 4.8x reduction on CIFAR-100 while maintaining accuracy. The hybrid quantization strategy and joint optimization of early exits demonstrate significant improvements over full-precision networks for distributed intelligence systems.\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\n~~~~~~~~~~\nCurrent Step #14, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---1Edge AI: Evaluation of Model CompressionTechniques for Convolutional Neural NetworksSamer Francy, Member, IEEE,Raghubir Singh, Member, IEEEAbstract\u2014This work evaluates the compression techniqueson ConvNeXt models in image classification tasks using theCIFAR-10 dataset. Structured pruning, unstructured pruning,and dynamic quantization methods are evaluated to reduce modelsize and computational complexity while maintaining accuracy.The experiments, conducted on cloud-based platforms and edgedevice, assess the performance of these techniques. Results showsignificant reductions in model size, with up to 75% reduc-tion achieved using structured pruning techniques. Additionally,dynamic quantization achieves a reduction of up to 95% inthe number of parameters. Fine-tuned models exhibit improvedcompression performance, indicating the benefits of pre-trainingin conjunction with compression techniques. Unstructured prun-ing methods reveal trends in accuracy and compression, withlimited reductions in computational complexity. The combinationof OTOV3 pruning and dynamic quantization further enhancescompression performance, resulting 89.7% reduction in size, 95%reduction with number of parameters and MACs, and 3.8%increase with accuracy. The deployment of the final compressedmodel on edge device demonstrates high accuracy 92.5% and lowinference time 20 ms, validating the effectiveness of compressiontechniques for real-world edge computing applications.Index Terms\u2014edge AI, ConvNeXt, CNN, pruning, quantiza-tion, compression, OTO.I. I NTRODUCTIONEDGE devices such as Internet of Things (IoT) are becom-ing increasingly important and widely used in our dailylives and industrial facilities. IoT is a network of things thatempowered by sensors, identifiers, software intelligence, andinternet connectivity, it can be considered as the intersectionof the internet, things/objects (anything/everything), and data[1]. The number of these devices is expected to increase evenmore [2]. These devices have the potential to perform complexArtificial Intelligence (AI) tasks locally, without relying heav-ily on cloud infrastructure [3]. The rapid advancement of AIhas led to the development of complex deep learning modelsthat show high performance in different domains. DeployingAI models on edge devices has many advantages such as lowlatency, privacy and data security, bandwidth optimization, andreduced network dependence. Low latency is achieved due toreal-time processing by instant data analysis on edge withoutwaiting for remote server processing, this data analysis on theedge reduces transmitting data to the cloud which enhancessecurity against breaches, reduces the bandwidth consumption,and reduces network dependence.Samer Francy is with the Department of Computer Science, University ofBath, Bath, England (e-mail: samer.francy@bath.edu).Raghubir Singh is with the Department of Computer Science, Universityof Bath, Bath, England (e-mail: rs3022@bath.ac.uk).A. Overview of Edge AIEdge AI represents a paradigm shift in the way AI isimplemented in the context of the IoT. It capitalizes on thecapabilities of IoT devices, enhancing real-time processing,analytics, and decision-making directly at the edge of thenetwork. The IoT architecture, which is the foundation forEdge AI, typically involves three core layers [1]. The layersare perceptual layer, where data is collected from varioussensors and devices, network layer, where data is transmittedand routed through this layer, which is responsible for commu-nication between devices and cloud services, and applicationlayer, which processes and utilizes the data, providing insightsand enabling actions.B. Convolutional Neural Networks (CNNs)CNN models are subsets of Deep Neural Networks (DNN)models. CNN models are effective for image and video-relatedtasks due to their ability to learn relevant features from thedata by recognizing patterns, shapes, and structures in images,which is challenging for traditional machine learning models,that\u2019s why they are used for computer vision tasks such asimage classification, object detection, and image segmentation[4].1) CNN Architecture: In general CNN models consist ofbelow parts:\u2022 Input Image : Pixels form the binary basis of computerimages, while the human visual system operates throughneurons with receptive fields. Similarly, CNNs functionwithin their receptive areas, starting with simple patternsand advancing to more complex ones, making CNNs apromising tool for computer vision [4].\u2022 Convolutional Layer : A convolutional layer in a CNNuses a small filter (e.g., 3x3 or 5x5) that slides over theinput image. At each position, it multiplies its valueswith the overlapping image pixels and sums the results toproduce an output. This sliding operation helps identifylocal features like edges and colors, building a hierar-chical representation. The depth of the filter matches theinput image\u2019s channels (e.g., 3 for RGB images). Stackingmultiple filters allows the network to learn features atdifferent abstraction levels [4].\u2022 Pooling Layer : Pooling reduces the spatial size of fea-ture maps. This not only lowers computational demandsbut also helps in extracting position and orientation-independent features essential for training. Two com-mon pooling methods are maximum pooling and averagepooling. In maximum pooling, a small kernel (e.g., 2x2)arXiv:2409.02134v1  [cs.LG]  2 Sep 2024--- Page 2 ---2Fig. 1. Unbalanced Demand For Computation (Left) and Memory (Right) in AlexNet [5].selects the maximum value within its range and places itin the output. In average pooling, a similar-sized kernelcomputes the average value within its area for eachchannel, maintaining the same depth. Pooling simplifiescomputation and weight requirements, with maximumpooling being the most commonly used method [4].\u2022 Activation Function : The activation function, applies amathematical operation to the filter\u2019s output to concludethe output of the network. The common choice is the Rec-tified Linear Unit (ReLU). They fall into two categories,linear and non-linear activation functions [4].\u2022 Fully Connected Layer : It functions as a feed-forwardneural network (NN) typically situated at the network\u2019slower layers. It receives input from the output of the lastpooling or convolutional layer, which is flattened into aone-dimensional vector, enabling it to learn complex datarelationships [4].2) Computation and Memory Demands: In CNNs, unbal-ance exists in resource demands between the layers. Convolu-tional layers primarily serve as feature extractors and heavilydominate the computational workload. In the case of AlexNet,for instance, the convolutional layers account for just 2 millionweights but demand a substantial 1.33 Giga Operations PerSecond (GOPS) of computation. In contrast, fully connectedlayers function as classifiers, accumulating information forhigh-level decisions, and bear the weight of the network witharound 59 million parameters, yet they contribute significantlyless to computation, requiring only 0.12 GOPS. This obviouscontrast in resource allocation (Figure 1) highlights the unbal-anced demands between these two layers in CNNs [5].3) Key CNN Architectures:In 1989, the use of a NN archi-tecture with convolutional layers for recognizing handwrittendigits in the context of ZIP code recognition was introduced[6], That architecture consisted of input layer, 3 hidden layers,and output layer. Since then, CNN models have developed(Figure 2) and became much deeper.4) CNN on Edge: Deploying CNN models on edge hasa wide range of practical and industrial applications acrossvarious sectors. Here are some specific examples:\u2022 Surveillance and Security : It can perform real-time ob-ject detection and facial recognition for security monitor-ing, identifying intruders, and managing access control.Face identification was deployed using VGGFace [7].Video analysis was deployed using YOLOX [8]. Infraredand Visible Image Fusion for security systems was de-ployed using DenseNet [9]. Human action recognitionapplications were deployed using ResNet [10].\u2022 Manufacturing and Quality Control : It can inspectproducts on assembly lines for defects, ensuring qualitycontrol and minimizing errors. Real-time detection ofsteel strip surface defects was deployed using Faster R-CNN model [11].\u2022 Agriculture: Drones can monitor crops, detect pests,diseases, and nutrient deficiencies, enabling precisionagriculture. Identifying rice leaf diseases in natural en-vironments was deployed using GoogLeNet [12]. Pepperleaf disease identification was deployed using GoogLeNet[13]. Detection for insect pests was deployed on YOLOX[14].\u2022 Healthcare and Wearables : Wearable devices can con-tinuously monitor vital signs, detect anomalies, and evendiagnose certain health conditions. Medical diagnosis(Covid and Lung Disease Detection) was deployed usingVGG, MobileNet, and AlexNet [15]. Automatically diag-nose pneumonia and COVID-19 from chest X-ray imageswas deployed on DenseNet [16]. Medical applications(e.g., COVID-19 detection, cardiomegaly diagnosis, braintumor classification) were deployed using ResNet [10].\u2022 Energy Management: It can monitor energy usage, opti-mize consumption patterns, and identify areas for energyefficiency improvements. Wind Turbine Maintenance andfault diagnosis was deployed using AlexNet [17].\u2022 Environmental Monitoring : It can monitor air quality,pollution levels, and weather conditions, providing valu-able insights for urban planning. A smartphone app toperform fine-grained classification of animals in the wildwas deployed using AlexNet, GoogLeNet, and ResNet[18]. Identification of mosquito species was deployedusing AlexNet, DenseNet, Inception, ResNet, and VGG[19].--- Page 3 ---3Fig. 2. Evolution of Key CNN Architectures Over Time.\u2022 Logistics and Inventory Management : It can automatepackage sorting, inventory tracking, and warehouse man-agement. Mobile robot to map its surroundings while de-tecting objects and people was deployed using AlexNet,GoogLeNet, and ResNet [18].\u2022 Autonomous Vehicles : Vehicles can process real-timedata from cameras and sensors using CNNs, aiding inautonomous navigation and collision avoidance. Instanceobjects detection system for intelligent service robots wasdeployed using Alexnet [20]. Advanced driving assistancesystems (ADASs) and automated vehicles (A Vs) weredeployed using Faster R-CNN [21].Deploying CNNs on edge devices presents significant chal-lenges mainly due to the limited computational resources,constrained memory, and power consumption constraints in-herent to these devices. CNN models, known for their depthand complexity, often demand substantial computational powerand memory, which may exceed the capabilities of edgehardware. Hence, compressing the model before deploymentbecomes imperative. Model compression techniques aim toreduce the size of the CNN model while preserving its perfor-mance, thereby enabling efficient utilization of computationalresources and memory on edge devices. By compressingthe model, we can mitigate the challenges associated withdeploying CNNs on edge devices, ensuring that they caneffectively perform tasks such as real-time image processing,object detection, and classification within resource-constrainedenvironments.With the enormous number of compression techniques pro-posed for CNNs, the rapid evolution of CNN architectures hascreated a gap in the field. This dynamic shift in architecturedesign requires an evaluation of existing compression methods,particularly in light of the demand to make these advancedCNN models suitable for deployment on edge devices. AsCNN designs continue to advance, the challenge lies inadapting compression techniques to smoothly integrate withthese modern architectures. This evaluation (either for eachindividual techniques or combined with each other) becomesimportant, as it not only ensures the continued relevance ofcompression techniques but also addresses the urgent need tomake resource-intensive CNN models accessible and deploy-able on edge devices.This work aims to evaluate CNN compression techniquesthat assure appropriate performance on edge devices. In thesubsequent sections, this work reveals in a structured mannerto evaluate the compression techniques for CNN models.section 2 provides a detailed review of related work, offeringinsights into prior research and establishing a foundationalunderstanding of the topic. section 3 explains the methodologyemployed in conducting the experiments, describing the designand execution of the study. Following this, section 4 presentsthe experimental results and analyzes the findings to recognizetrends and implications. Section 5 critically evaluates the re-sults. Section 6 draws conclusions regarding the effectivenessand significance of the compression techniques. This organizedframework aims to comprehensively explore and contribute tothe field of model compression for efficient deployment inresource-constrained environments.II. R ELATED WORKWithin the context of edge AI, it is important to address thecritical need for model compression. The resource constrainednature of these devices requires more efficient AI modelsby minimizing memory and computational demands, ensur-ing faster inference speeds, and enhancing energy efficiency.Below will explore various model compression techniques andtheir implications for edge AI applications.A. PruningPruning is a key technique in DNN, aimed at enhancingefficiency and model generalization. It involves the removalof redundant components, such as parameters, neurons, filters,or entire layers, leading to several advantages. By reducingunnecessary parameters, it cuts down on storage require-ments, and important for models deployed on devices withlimited memory. Furthermore, it streamlines computational--- Page 4 ---4Fig. 3. Weight Pruning (a) and Neuron Pruning (b). x: input, w: weight. [22].complexity during inference, resulting in faster predictions andlower power consumption. Pruning also mitigates overfittingby simplifying the network. Various pruning techniques, likeweight pruning, neuron pruning, filter pruning, and layerpruning, offer different levels of granularity in componentremoval. Whether applied during or after training, pruningenables the creation of more compact and efficient CNNmodels tailored to specific needs, effectively balancing modelsize, computational efficiency, and accuracy. Weight pruningsets weight connections in the network to zero if they fallbelow a predefined threshold or are considered redundant.Neuron pruning focuses on removing entire neurons if they arefound to be redundant. Layer pruning allows for the removalof entire layers that are considered less important [22].1) Pruning For Fully Connected Layer: Fully connectedlayers are dense that makes the layer with high memorydemand. Pruning them effectively reduces the memory burdenand reduce size of the model.It involves selectively removing weight connections andneurons to reduce the model\u2019s complexity while preservingperformance. In a typical feed-forward NN, inputs are multi-plied by corresponding weights, and a linear sum is computedat each neuron, which is then transformed by an activationfunction. As shown in Figure 3, a network with 3 inputneurons, 2 hidden neurons, and 1 output neuron may havemultiple weight connections. Pruning can be applied to elim-inate specific weight connections or entire neurons. By doingso, the total number of weight connections can be significantlyreduced, leading to a more compact network. The concept ofpruning was first introduced by [6], who proposed removingweights based on their saliency, with small-magnitude weightshaving less impact on training error. The process involvesiterative retraining to regain accuracy, and the technique isknown as \u2019Optimal Brain Damage (OBD)\u2019 where the secondderivative of the objective function with respect to parametersis used to calculate the small saliency, facilitating informedpruning decisions. Since then, other pruning approaches havebeen introduced for fully connected layers [22].2) Pruning For Convolutional Layer: Each convolutionallayer typically consists of numerous filters that makes the layerwith high computational demand. Pruning these less significantfilters directly from the convolutional layer effectively reducesthe computational burden and speeds up the model. Inspired byearly pruning methods, new approaches have been introducedto be used to prune convolutional layers [22].Bayesian was used to decide what to prune and the level ofpruning, in this context involves employing scale mixtures ofnormals as priors for parameter weights in LeNet and VGG[23]. Differential evolution based layer-wise weight methodalongside three other pruning techniques (Naive Cut, Itera-tive Pruning, and Multi-Objective NN Pruning) was used toprune LeNet, AlexNet, and VGG16 [24]. Two fully connectedlayers are removed from the AlexNet architecture, and BatchNormalization (BN) is introduced to mitigate overfitting [20].Filters Similarity in Consecutive Layers (FSCL) for CNNs wasused to reduce the number of filters while preserving impor-tant filters, ultimately improving model efficiency for VGG,GoogLeNet, and ResNet [25]. Structured pruning throughsparsity-induced pruning was used to enhance the real-timeimplementation of the DEtection TRansformer (DETR) [26].Structured pruning was used to compress YOLOX, this processincluded sparse training to prune unimportant channels, withfine-tuning to recover accuracy [14]. Evolutionary approachto filter pruning involved sequential application of multiplepruners in a specific order to sparsify LeNet and VGG-19while maintaining model accuracy [27]. Multilayer networkswere used to represent and compress ResNets, it involvedcreating class networks, calculating arc weights, and forminga multilayer network. The overall degree of nodes in themultilayer network is used to select a subset of nodes forcompression, and convolutional layers are pruned [10]. Tooptimize the Fused-DenseNet-Tiny model for efficient detec-tion of COVID-19 and pneumonia in chest X-ray images,three steps were implemented including removing insignif-icant weights, discarding pruning casings, and applying acompression algorithm [16]. Deep Scalable Zerotree-based(DeepSZ) framework was used to address resource constraints--- Page 5 ---5by achieving significant compression for LeNet, AlexNet,and VGG while maintaining acceptable inference accuracy[28]. Compressing without retraining that was used withResNet, AlexNet, VGGNet and SqueezeNet. It focused onconvolutional and fully connected layers, while maintainingor improving classification accuracy [29].B. QuantizationQuantization plays an important role in addressing theresource-intensive nature of CNNs. By reducing the bit pre-cision of model parameters, quantization not only conservesmemory and energy but also enhances inference speed, makingit an essential technique for deploying CNNs in resource-constrained environments such as edge devices. Weight clus-tering takes quantization to a more advanced level by orga-nizing weights into clusters, where each cluster shares thesame weight value. This approach minimizes the need forfine-tuning individual weights and can lead to substantialreductions in memory and computational overhead [22].Single Level Quantization (SLQ) and Multiple Level Quan-tization (MLQ) technique were used to quantize AlexNet,VGG, GoogleNet, and ResNet to the deployment of thesemodels on resource-constrained mobile devices like mobilephones and drones [30].C. Low-Rank Decomposition/FactorizationIt is a compression technique used with feed-forward NNsand CNNs, to reduce the size of weight matrices whilepreserving model performance. Singular Value Decomposition(SVD) is a popular factorization scheme that decomposes aweight matrix A into three smaller matrices: U, S, and V T . Urepresents the left singular vectors, S is a diagonal matrix ofsingular values, and V T is the transpose of the right singularvectors. This factorization offers several advantages, such asreduced storage requirements, which is crucial for memory-constrained environments, and accelerated inference, espe-cially in CNNs, as smaller matrices can be convolved faster.Low-rank factorization can be applied to fully connected andconvolutional layers, making models more storage-efficientand faster without sacrificing performance. Careful selectionof the rank is essential for achieving a balance between sizereduction and model accuracy. Later, more approaches havebeen introduced [22].Tucker decomposition for weight tensors was used tooptimizes weight tensor dimensions of LeNet and ResNetmodels [31]. Low-rank decomposition was used as an efficientmethod for compressing AlexNet, VGG, and ResNet withoutthe need for fine-tuning to significantly reduce model sizeand computational complexity to make them more suitablefor resource-constrained mobile and embedded devices [32].Hardware-Aware Automatic Low-Rank Compression frame-work HALOC was used to compress ResNet, VGG andMobileNet, with the goal of efficiently exploring the structure-level redundancy in NNs by integrating principles from neuralarchitecture search (NAS) [33]. Automatically DifferentiableTensor Network (ADTN) method was used to significantlyreduce the number of parameters of fully connected NN,LeNet, and VGG while maintaining or enhancing the per-formance [34]. Joint Matrix Decomposition, specifically JointSVD (JSVD) was used to address the challenge of deployingResNet with numerous parameters on resource-constrainedplatforms. It included Right JSVD, Left JSVD, and BinaryJSVD algorithms [35]. Tensor Ring Networks (TR-Nets) wasused as a method to effectively factorize LeNet and ResNet,thereby reducing computational and memory requirements[36]. Tucker decomposition with rank selection and fine tuningwas used as a one-shot whole network compression schemefor deploying AlexNet, VGG, and GoogLeNet on mobiledevices while maintaining reasonable accuracy [37]. TensorDynamic Low-Rank Training (TDLRT) was used to createa training algorithm with VGG and AlexNet that maintainshigh model performance while significantly reducing memoryrequirements for convolutional layers [38].D. Knowledge Distillation (KD)It is a technique used to transfer the knowledge learnedby a larger, more complex model (the teacher model) to asmaller and lighter model (the student model). The primarygoal of KD is to enable the student model to benefit from thegeneralization capabilities of the teacher model while beingmore lightweight in terms of parameters and computations.This technique helps to recover the accuracy drop occurs dueto implementing other compression techniques.Knowledge transfer and distillation, initially introduced by[39], aimed to compress large ensemble models into smaller,faster counterparts with minimal performance loss. [40] ex-tended this concept by empirically demonstrating that theintricate knowledge within larger DNNs could be effectivelytransferred to smaller, shallower models, yielding comparableaccuracy. This involved training a large DNN and transfer-ring its knowledge to a shallower network while minimizingthe squared difference between the logits produced by thetwo models. These foundational ideas produced knowledgedistillation, a widely used technique for training efficientmodels by transferring knowledge from larger ones. Later,more approaches have been introduced [22].KD was used to improve the compression of LeNet andResNet models when fresh training data is scarce, primarilythrough the use of synthetic data generated by GenerativeAdversarial Networks (GANs) [41]. To fuse information frominfrared and visible images while reducing DenseNet complex-ity and improving inference speed. Insights from pre-trainedteacher models are transferred to the smaller student model[9]. KD was used to develop a lightweight mosquito speciesidentification model (EfficientNet) that balances efficiency andaccuracy through the compression [42].E. Mixed TechniquesDifferent compression techniques are often combined andused together to achieve more effective and comprehensivemodel compression. Each compression technique targets spe-cific aspects of the model, such as reducing model size,computation complexity, or memory footprint.--- Page 6 ---6In-Parallel Pruning-Quantization CLIP-Q method combinesnetwork pruning and weight quantization was used to com-press AlexNet, GoogLeNet, and ResNet [18]. Pruning andquantization were used to optimize the compression ofAlexNet and reduce the number of parameters significantlywhile maintaining accuracy to be implemented on Field-Programmable Gate Array (FPGA) [5]. Pruning, quantization,and Huffman encoding combined with adversarial trainingwere used to enhance the robustness and compression ofAlexNet while also addressing the model vulnerability toadversarial attacks [43]. Pruning and quantization were usedto compress VGG and ResNet for remote sensing imageclassification, balancing computational complexity constraintswhile preserving model accuracy [44]. Low-rank decompo-sition and quantization were used to compress ResNet andMobileNet, and reduce the computational complexity whilepreserving high performance [45]. Pruning, quantization, andchanging the model architecture were used to design a com-pact SqueezeNet with competitive accuracy while significantlyreducing the number of parameters [46]. Quantization andpruning were used to develop an effective model compressionframework for ResNet and MobileNet. The objective was tooptimize the allocation of compression ratios to minimizeperformance degradation while reducing model size [47]. Jointquantization and pruning were used to develop a post-trainingmodel size compression method that efficiently combines lossyand lossless compression techniques to reduce the size ofResNet, MobileNet, RegNet, MNasNet, and YOLOv5 withoutsacrificing accuracy [48].F . Other TechniquesDepthwise separable convolutions was used to improvesteel strip defect detection by creating a real-time and ef-ficient model while maintaining high accuracy using FasterR-CNN [11]. Deferential Evolution was used to develop anefficient and optimized AlexNet, VGG, and MobileNet forCovid and liver disease detection [15]. Genetic Algorithmwas used to reduce the storage space and inference timeof VGG, ResNet, AlexNet, and SqueezeNet models [49].Factorization (changing kernel size) was used to improvethe accuracy and computing efficiency of pepper leaf diseasedetection using GoogLeNet, specifically for the agriculturalindustry [13]. Flexible and Separable Convolution (FSConv)was used to reduce computational costs without compromisingthe accuracy of VGG, ResNet, Faster R-CNN and RetinaNet[50]. Efficient Layer Compression (ELC) was used to enhancethe computational efficiency of VGG, ResNet, and ConvNeXtwhile preserving their representation capabilities [51].III. D ESIGN OF THE EXPERIMENTSThe experiments aimed to evaluate various compressiontechniques, namely pruning and quantization, on differenttypes of ConvNext [52] model. The experiments includedtraining, fine-tuning, and evaluating of models using CIFAR-10 dataset. The setup involved conducting experiments bothon cloud-based platforms and on edge devices to evaluate theperformance of the compressed models.Fig. 4. Block modifications and resulted specifications. (a) is a ResNeXtblock; in (b) we create an inverted bottleneck block and in (c) the positionof the spatial depthwise conv layer is moved up [52].A. ConvNeXt [52]Is a modern CNN family produced as a journey of graduallymodernize a standard ResNet toward the design of a visionTransformer. The journey starts from a ResNet-50 model, intoa CNN architecture that mirrors some aspects of Transformers,particularly Swin Transformers. The roadmap:1) Training Techniques: Vision Transformer training pro-cedures were used to train ResNet-50 model, this includedextending the training to 300 epochs (90 epochs originally),using AdamW optimizer, and data augmentation techniques(Mixup, Cutmix, RandAugment, Random Erasing, and regu-larization schemes including Stochastic Depth).2) Macro Design: Number of blocks in each stage wasadjusted from (3, 4, 6, 3) to (3, 3, 9, 3) and the stem wasreplaced with a patchify layer implemented using a 4x4, stride4 convolutional layer (non-overlapping convolution).3) ResNeXt-ify: ResNeXt approach was adopted which isutilize grouped convolutions, where convolutional filters aredivided into groups, each handling a subset of input channels,a variation of grouped convolution known as depthwise convo-lution was adopted, and the network\u2019s width was expanded byincreasing the number of channels in the convolutional layers.4) Inverted Bottleneck: The hidden dimension of the MLPblock was changed to be four times wider than the inputdimension as shown in Figure 4 (a and b)5) Large Kernel Sizes: The position of the convolutionallayer is moved up and the kernel size was changed from (3x3)to (7x7) as shown in Figure 4 (a and c).B. Micro DesignReplacing ReLU with Gaussian Error Linear Unit (GELU),fewer normalization layers, Substituting Batch Normalization(BN) with Layer Normalization (LN), and introducing separatedownsampling layers as shown in Figure 5.C. Compression Techniques1) Pruning: Different pruning techniques have been usedincluding structured and unstructured techniques.\u2022 Only Train Once (OTO) [53]: OTO version 3 (OTOV3)is automated framework for structured pruning which in-volves removing entire structures or groups of parametersfrom a DNN. OTOv3 begins by analyzing the depen-dencies between the vertices of the target DNN. Thisanalysis involves identifying accessory, Shape-Dependent--- Page 7 ---7Fig. 5. Block designs for a ResNet, a Swin Transformer, and a ConvNeXt.Swin Transformer\u2019s block is more sophisticated due to the presence of multiplespecialized modules and two residual connections [52].(SD) joint, and unknown vertices that are adjacent andestablishing their interdependencies. The goal is to formnode groups based on these dependencies, laying thefoundation for identifying interdependent vertices duringstructured pruning.Using the information gathered from the dependencyanalysis, OTOv3 constructs a pruning dependency graph.This graph represents the interdependencies between ver-tices, with vertices in the same node group indicatingtheir interdependency during structured pruning. Thepruning dependency graph ensures the validity of theproduced subnetwork by preserving essential connectionsbetween vertices.OTOv3 partitions the trainable variables of the DNNinto Pruning Zero-Invariant Groups (PZIGs) based on thepruning dependency graph. PZIGs consist of pairwisetrainable variables grouped together, with each grouprepresenting a potential pruning structure. Node groupsadjacent to the DNN output and containing unknownvertices are excluded from forming PZIGs to preserveoutput shapes and ensure model robustness as shown inFigure 6.To jointly search for redundant pruning structures andtrain the remaining groups for optimal performance,OTOv3 employs the Dual Half-Space Projected Gradient(DHSPG) algorithm. DHSPG minimizes the objectivefunction while introducing a sparsity constraint to identifyredundant groups for removal. It employs saliency-drivenredundant identification and a hybrid training paradigmto control sparsity and achieve better generalization per-formance as shown in Figure 7.\u2022 L1 Unstructured [54] L1 unstructured pruning is atechnique used in machine learning, to reduce the sizeof neural networks by eliminating less important con-nections. Each weight in the network is assigned a scorebased on its magnitude. This score reflects the importanceof the weight in the network\u2019s performance. In l1 pruning,this score is often the absolute value of the weight.A threshold is set, typically by selecting the top x% ofweights based on their magnitude scores. The thresholddetermines which weights will be pruned and which willbe retained.Weights that fall below the threshold are pruned, meaningthey are set to zero and effectively removed from thenetwork. This results in a sparser network architecturewith fewer connections.\u2022 Random Unstructured [55] Similar to l1 unstructuredpruning, random unstructured pruning is also a techniqueused in machine learning, to reduce the size of neuralnetworks by eliminating less important connections. Thedifference is the pruned weight are selected randomlyinstead of using l1 to decide the importance of theweights.2) Dynamic Quantization [30]:Dynamic quantization is anapproach aimed at optimizing the deployment of neural net-works by reducing the precision of the weights. Unlike tradi-tional quantization methods that apply a fixed quantization bit-width across all layers of the network, dynamic quantizationadapts the quantization bit-width for each layer individuallybased on its representation abilities and capacities. This isachieved through the use of a bit-width controller module,which employs a policy gradient-based training approach tolearn the optimal bit-width for each layer. By dynamicallyadjusting the quantization bit-width, dynamic quantization canstrike a balance between maintaining accuracy and reducingmemory size and computational costs.D. CIFAR-10 [56]CIFAR-10 is a dataset used for computer vision and ma-chine learning research, offering a rich resource for trainingand evaluating image classification algorithms. Comprising60,000 32x32 RGB color images across 10 distinct classes(Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse,Ship, and Truck), CIFAR-10 facilitates comprehensive explo-ration of diverse visual concepts. With each class containing6,000 images and a balanced distribution across the dataset,CIFAR-10 presents a well-structured foundation for model de-velopment. Its division into 50,000 training images and 10,000test images, further segmented into multiple batches, enablesstrict evaluation and benchmarking of algorithms. In terms ofcomputational requirements, CIFAR-10 generally requires lesscomputation compared to CIFAR-100 and ImageNet due to itssmaller image size and fewer classes which makes it suitablefor experiments with limited computational resources.E. Experiment SetupTwo types of experiments have been conducted, cloud-basedexperiments that focused on compressing the models and eval-uating the techniques and edge-based experiment experimentto evaluate the performance of one of the compressed models.--- Page 8 ---8Fig. 6. Automated PZIG Partition [53].Fig. 7. Search Direction in DHSPG [53].1) Cloud-based Experiment Setup: Google Colab Pro+was used to utilize GPU resources (NVIDIA A100 and V100Tensor Core GPUs), facilitating accelerated model trainingand evaluation and background execution. The integrationwith Google Drive reduced overheads associated with upload-ing and downloading model data to and from cloud. Theevaluation framework was designed to profile the originalmodel, compress it, profile the compressed model, and conductcomparison between the measurements before and after thecompression as shown in Figure 8.This profiling process involved measuring several key met-rics:\u2022 Accuracy: The classification accuracy achieved by themodel on the validation dataset.\u2022 Model Size: The size of the model in megabytes (MB).\u2022 Number of Parameters : The total count of trainableparameters in the model, measured in millions (M).\u2022 Number of MACs : The number of multiply-accumulateoperations performed during inference, measured in mil-lions (M).\u2022 Number of Non-Zero Parameters : The count of non-zero parameters in the model, essential for pruning-basedtechniques.2) Edge-based Experiment Setup: A compressed modelwas deployed on edge with CPU (11th Gen Intel(R) Core(TM)i7-1165G7 @ 2.80GHz 2.80 GHz), RAM (16GB), and laptopintegrated camera.2 samples from each of CIFAR-10 classes have been se-lected randomly from the internet, printed on A4 papers, andplaced in front of the camera to measure the accuracy and theinference time.--- Page 9 ---9Fig. 8. Compression Evaluation Framework.TABLE IOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT TINY, SMALL , BASE AND LARGE .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)Tiny Full 63.81 106.26 26.53 86.88 18.41Compressed 63.81 41.31 10.29 47.80 10.30Small Full 63.48 188.89 47.16 169.28 29.55Compressed 63.48 48.04 11.94 68.24 11.96Base Full 61.22 334.28 83.50 299.20 52.22Compressed 61.22 84.12 20.96 119.40 20.98Large Full 63.40 748.82 187.12 669.30 116.93Compressed 63.40 187.32 46.75 264.69 46.78IV. R UNNING THE EXPERIMENTS AND EXPERIMENTALRESULTSA. Cloud-Base Experimentsdifferent experiments have been conducted on cloud to eval-uate different compressing techniques and different versions ofConvNeXt model.1) Evaluate OTOV3 on Untrained Torch ConvNext Tiny,Small, Base, and Large: Untrained ConvNeXt tiny, small,base, and large have been imported from Torch and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and 200epochs were used for training and pruning. OTOV3 achievedhigh performance (Table I) with reducing the model size(61% for tiny and 75% for small, base, and large), numberof parameters (61% for tiny and 75% for small, base, andlarge), and MACs (45% for tiny and 60% for small, base,and large) as shown in Figure 9. Meanwhile OTOV3 wasable to increase both the full and compressed model accuracythrough the training and pruning without any accuracy dropafter pruning comparing to the full model.To investigate the effect of OTOV3 on the model archi-tecture, a comparison has been conducted between ConvNeXtsmall before and after compression. The Torch implementationof the model consist of many CNBlocks, each CNBlock con-sist of Conv2d, Permute, LayerNorm, Linear, GELU, Linear,and Permute layers. As shown in Figure 10, OTOV3 reducednumber of output features of the Linear layer (sequence 3)and the input features of the next Linear layer (sequence 5)and considering the big number of CNBlock in the modelarchitecture, the reduction in model size and number ofparameters after compression is justified as shown in TableII.2) Evaluate OTOV3 on Untrained ConvNext Small (Torchvs. TIMM): Two untrained ConvNeXt small have been im-ported, one from Torch and one from TIMM [57] and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and200 epochs were used for training and pruning. Although thecompression performance was same with size reduction (75%)but the accuracy after 200 epochs was less for Torch model(63%) comparing to TIMM model (73%) as shown in Figure11.To investigate the accuracy performance of OTOV3 withTorch and Timm ConvNeXt Small, a comparison has beenconducted between the two model architectures. The Torchmodel uses the CNBlock structure, which includes additionaloperations such as Permute and varying StochasticDepth prob-abilities. The TIMM model follows a simpler structure withConv2d and LayerNorm, lacking the additional complexitiesintroduced by CNBlock and associated operations in theTorch model as shown in Figure 12 which effects OTOV3performance regarding the accuracy.3) Evaluate OTOV3 on Fine-Tuned Torch ConvNext Small:A pre-trained ConvNeXt small have been imported from Torchand fine-tuned on CIFAR-10 with 100 epochs, the accuracyreached 89.5%. This fine-tuned ConvNeXt small will be usedfor the rest of cloud-base experiments. This model was usedto evaluate OTOV3, CIFAR-10 was used for training andevaluation, and 200 epochs were used for training and pruning.OTOV3 achieved high performance (Table III) 74% reductionin model size and number of parameters, 60% reduction inMACs, and 3.8% increase with accuracy as shown Figure13. The accuracy of the full model in (Table III) (92.86%)is different that the accuracy of the original model used inthe experiment (89.5%), that because OTOV3 trained the full--- Page 10 ---10Fig. 9. OTOV3 Compression Performance with ConvNeXt Tiny, Small, Base and Large.Fig. 10. Comparison Between ConvNeXt Small Full (Left) and Compressed (Right).Fig. 11. OTOV3 Comparison Between Torch and Timm ConvNeXt Small.model during the process which increased the model accuracy.4) Evaluate Unstructured Pruning: The Fine-tuned Con-vNext Small was used to evaluate Pytorch L1 UnstructuredPruning and Random Unstructured Pruning by using differentcombinations of weights pruning percentages for linear (.1 to.9) and convolutional (.1 to .9) layers. In both experiments,the accuracy and the number of non-zero parameters weredropping as the values of weights pruning percentages for bothlinear and convolutional amounts were increasing as shown inFigure 14 a and b. Although the accuracy dropped but themodel size, number of parameters, and MACs didn\u2019t changeas these techniques zero the weights instead of removing them.5) Evaluate Dynamic Quantization: The Fine-tuned Con-vNext Small was used to evaluate Pytorch dynamic quantiza-tion, 8-bit integer was used during the experiment and CIFAR-10 was used for evaluation. Dynamic quantization achievedhigh performance (Table IV) 71% reduction in model size,95% reduction with number of parameters and MACs, and0.1% drop with accuracy as shown in Figure 156) Evaluate Combination of OTOV3 and Dynamic Quan-tization: Two compression stages were evaluated together asone compression pipeline, OTOV3 and Dynamic Quantization.The compressed ConvNeXt small model that was produced inexperiment 4.1.3 (OTOV3) was furtherly compressed using--- Page 11 ---11Fig. 12. Comparison Between ConvNeXt Small TIMM (Left) and Torch (Right).Fig. 13. OTOV3 Compression Performance with ConvNeXt Small Tuned.Pytorch dynamic quantization using 8-bits integer (similar toexperiment 4.1.5). The pruned models using OTOV2 [58]had dependencies on Open Nural Network Exchange (ONNX)[59] which made it not applicable to be combined with othercompression technique like quantization. In OTOv3, there wasengineering changes produced the pruned model directly inPytorch format, which enhanced the flexibility to be combinedwith quantization as this experiment did [53].Pruning using OTOV3 and Quantization using Pytorchdynamic quantization achieved high performance (Table V)89.7% reduction in model size, 95% reduction with numberof parameters and MACs, and 3.8% increase with accuracy asshown in 16.B. Edge-Base ExperimentThe final compressed ConvNeXt Small model in experimentIV .A.6 (OTV3 and Dynamic Quantization) was deployed onedge and the printed samples used to measure the accuracyand inference time by placing them in front of the camera.The compressed model achieved 92.5% accuracy and 20msinference time. Figure 17 shows samples of the output.V. A NALYSIS OF EXPERIMENTAL RESULTSHere, the experimental results obtained from running aseries of experiments will be analyzed, the experiments aimedat evaluating various compression techniques applied to Con-vNeXt models. The experiments were designed to evaluate theperformance of techniques such as pruning and quantizationon different sizes of ConvNeXt models, with a focus on bothcloud-based compression experiments and edge-based deploy-ment experiment. The analysis will be conducted with respectto the work\u2019s aim of evaluating CNN compression techniquesthat assure appropriate performance (size and inference time)on edge devices and resource-constrained environments.A. OTOV3 Evaluation on Untrained ConvNeXt ModelsThe evaluation of OTOV3 on untrained ConvNeXt modelsdemonstrated its effectiveness in achieving substantial com-pression while increasing model accuracy for both full andcompressed models. Across varying sizes of ConvNeXt mod-els, OTOV3 consistently produced impressive reductions inmodel size, number of parameters, and MACs, highlighting itsability to prune redundant structures effectively. This suggests--- Page 12 ---12Fig. 14. Unstructured Pruning Performance with ConvMeXt Small.TABLE IICOMPARISON FOR NUMBER OF OUTPUT FEATURES AND INPUTFEATURES OF THE LINEAR LAYERS IN THE CNB LOCKS BEFORE ANDAFTER OTOV3 C OMPRESSION .Layer Index Input & Output FeaturesSequential CNBlock Full Model CompressedModel1 0 384 3841 1 384 3841 2 384 3843 0 768 7673 1 768 7043 2 768 7265 0 1536 2515 1 1536 3495 2 1536 2425 3 1536 3785 4 1536 2935 5 1536 3775 6 1536 3405 7 1536 4005 8 1536 3945 9 1536 4785 10 1536 4145 11 1536 4245 12 1536 4105 13 1536 3185 14 1536 4885 15 1536 4885 16 1536 4025 17 1536 2465 18 1536 4025 19 1536 4585 20 1536 3235 21 1536 4195 22 1536 4465 23 1536 4445 24 1536 4415 25 1536 4685 26 1536 10707 0 3072 2087 1 3072 2547 2 3072 262Fig. 15. Dynamic Quantization Compression Performance with ConvNeXtSmall Tuned.Fig. 16. OTOV3 and Dynamic Quantization Compression Performance withConvNeXt Small Tuned.--- Page 13 ---13TABLE IIIOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 92.86 188.89 47.16 169.28 29.80Compressed 92.86 50.03 12.44 67.41 12.46TABLE IVDYNAMIC QUANTIZATION COMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 89.53 188.89 47.16 169.28 47.17Compressed 89.40 54.21 2.15 7.27 2.17TABLE VOTOV3 AND DYNAMIC QUANTIZATION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy % Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number of NonZeroParameters (M)Full 89.53 188.89 47.16 169.28 47.17Phase 1 (OTOV3) 92.86 50.03 12.44 67.41 12.46Phase 2 (Dynamic Quantization) 92.93 19.39 2.15 7.27 2.17Fig. 17. Samples From the Compressed Model Output on Edge.that OTOV3 efficiently identifies and removes unnecessaryparameters from the models, leading to more streamlinedarchitectures without compromising predictive performance.However, an unexpected observation arose when comparingthe accuracy of the Torch implementation with that of theTIMM implementation of ConvNeXt Small. The Torch im-plementation exhibited lower accuracy compared to its TIMMcounterpart, indicating that OTOV3\u2019s performance regardingaccuracy may be influenced by the details of the modelarchitecture. This unexpected result suggests that differenttraining strategies or adjustments may be necessary to optimizeOTOV3\u2019s performance across various model implementations,emphasizing the importance of considering architectural dif-ferences when applying compression techniques like OTOV3.B. OTOV3 Evaluation on Fine-Tuned ConvNeXt ModelsWhen evaluating OTOV3\u2019s performance on a fine-tunedConvNeXt model, notable improvements in compression per-formance were observed, confirming its effectiveness in re-ducing model size, parameters, and MACs while marginallyenhancing accuracy. This outcome highlights the potential ofintegrating fine-tuning with structured pruning techniques toachieve even greater optimization of model performance. Thefine-tuned model displayed enhanced compression capabilitiescompared to untrained models, suggesting that pre-trainingcan significantly increase the effectiveness of compressiontechniques. This finding highlights the importance of lever-aging pre-existing knowledge within models to maximize thebenefits of compression, ultimately resulting in CNN modelswith higher performance.C. Unstructured Pruning TechniquesDuring the evaluation of l1 unstructured and random un-structured pruning techniques, expected trends were observedregarding accuracy and compression. As the pruning per-centages increased for both linear and convolutional layers,a corresponding decrease in accuracy was noted, while themodel size, parameters, and MACs remained unaltered. Thisoutcome aligns with the inherent nature of unstructured prun-ing, wherein weights are zeroed out but not entirely eliminated,resulting in sparse models without substantial reductions incomputational complexity. However, the lack of significantreductions in computational complexity may constrain their ef-fectiveness, particularly in resource-constrained environmentswhere efficient utilization of computational resources is essen-tial. This highlights the importance of considering the trade-offs between model compression and computational efficiencywhen selecting pruning techniques for deployment in real---- Page 14 ---14TABLE VIOTOV3 AND DYNAMIC QUANTIZATION REDUCTION NUMBERS WITH CONV NEXT SMALL TUNED .Technique AccuracyChange Size Reduction ParametersReductionMACsReductionOTOV3 3.75% 73.51% 73.62% 60.18%Dynamic Quantization -0.15% 71.30% 95.43% 95.71%OTOV3 and DynamicQuantization 3.80% 89.74% 95.43% 95.71%world applications, especially in edge computing scenarioswhere computational resources are limited.D. Dynamic QuantizationDynamic quantization emerged as a highly effective tech-nique for model compression, demonstrating remarkable re-ductions in model size, parameters, and \n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2409.02134v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 16:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #14, Phase: literature review, Feedback: 3\n--- Page 1 ---1Edge AI: Evaluation of Model CompressionTechniques for Convolutional Neural NetworksSamer Francy, Member, IEEE,Raghubir Singh, Member, IEEEAbstract\u2014This work evaluates the compression techniqueson ConvNeXt models in image classification tasks using theCIFAR-10 dataset. Structured pruning, unstructured pruning,and dynamic quantization methods are evaluated to reduce modelsize and computational complexity while maintaining accuracy.The experiments, conducted on cloud-based platforms and edgedevice, assess the performance of these techniques. Results showsignificant reductions in model size, with up to 75% reduc-tion achieved using structured pruning techniques. Additionally,dynamic quantization achieves a reduction of up to 95% inthe number of parameters. Fine-tuned models exhibit improvedcompression performance, indicating the benefits of pre-trainingin conjunction with compression techniques. Unstructured prun-ing methods reveal trends in accuracy and compression, withlimited reductions in computational complexity. The combinationof OTOV3 pruning and dynamic quantization further enhancescompression performance, resulting 89.7% reduction in size, 95%reduction with number of parameters and MACs, and 3.8%increase with accuracy. The deployment of the final compressedmodel on edge device demonstrates high accuracy 92.5% and lowinference time 20 ms, validating the effectiveness of compressiontechniques for real-world edge computing applications.Index Terms\u2014edge AI, ConvNeXt, CNN, pruning, quantiza-tion, compression, OTO.I. I NTRODUCTIONEDGE devices such as Internet of Things (IoT) are becom-ing increasingly important and widely used in our dailylives and industrial facilities. IoT is a network of things thatempowered by sensors, identifiers, software intelligence, andinternet connectivity, it can be considered as the intersectionof the internet, things/objects (anything/everything), and data[1]. The number of these devices is expected to increase evenmore [2]. These devices have the potential to perform complexArtificial Intelligence (AI) tasks locally, without relying heav-ily on cloud infrastructure [3]. The rapid advancement of AIhas led to the development of complex deep learning modelsthat show high performance in different domains. DeployingAI models on edge devices has many advantages such as lowlatency, privacy and data security, bandwidth optimization, andreduced network dependence. Low latency is achieved due toreal-time processing by instant data analysis on edge withoutwaiting for remote server processing, this data analysis on theedge reduces transmitting data to the cloud which enhancessecurity against breaches, reduces the bandwidth consumption,and reduces network dependence.Samer Francy is with the Department of Computer Science, University ofBath, Bath, England (e-mail: samer.francy@bath.edu).Raghubir Singh is with the Department of Computer Science, Universityof Bath, Bath, England (e-mail: rs3022@bath.ac.uk).A. Overview of Edge AIEdge AI represents a paradigm shift in the way AI isimplemented in the context of the IoT. It capitalizes on thecapabilities of IoT devices, enhancing real-time processing,analytics, and decision-making directly at the edge of thenetwork. The IoT architecture, which is the foundation forEdge AI, typically involves three core layers [1]. The layersare perceptual layer, where data is collected from varioussensors and devices, network layer, where data is transmittedand routed through this layer, which is responsible for commu-nication between devices and cloud services, and applicationlayer, which processes and utilizes the data, providing insightsand enabling actions.B. Convolutional Neural Networks (CNNs)CNN models are subsets of Deep Neural Networks (DNN)models. CNN models are effective for image and video-relatedtasks due to their ability to learn relevant features from thedata by recognizing patterns, shapes, and structures in images,which is challenging for traditional machine learning models,that\u2019s why they are used for computer vision tasks such asimage classification, object detection, and image segmentation[4].1) CNN Architecture: In general CNN models consist ofbelow parts:\u2022 Input Image : Pixels form the binary basis of computerimages, while the human visual system operates throughneurons with receptive fields. Similarly, CNNs functionwithin their receptive areas, starting with simple patternsand advancing to more complex ones, making CNNs apromising tool for computer vision [4].\u2022 Convolutional Layer : A convolutional layer in a CNNuses a small filter (e.g., 3x3 or 5x5) that slides over theinput image. At each position, it multiplies its valueswith the overlapping image pixels and sums the results toproduce an output. This sliding operation helps identifylocal features like edges and colors, building a hierar-chical representation. The depth of the filter matches theinput image\u2019s channels (e.g., 3 for RGB images). Stackingmultiple filters allows the network to learn features atdifferent abstraction levels [4].\u2022 Pooling Layer : Pooling reduces the spatial size of fea-ture maps. This not only lowers computational demandsbut also helps in extracting position and orientation-independent features essential for training. Two com-mon pooling methods are maximum pooling and averagepooling. In maximum pooling, a small kernel (e.g., 2x2)arXiv:2409.02134v1  [cs.LG]  2 Sep 2024--- Page 2 ---2Fig. 1. Unbalanced Demand For Computation (Left) and Memory (Right) in AlexNet [5].selects the maximum value within its range and places itin the output. In average pooling, a similar-sized kernelcomputes the average value within its area for eachchannel, maintaining the same depth. Pooling simplifiescomputation and weight requirements, with maximumpooling being the most commonly used method [4].\u2022 Activation Function : The activation function, applies amathematical operation to the filter\u2019s output to concludethe output of the network. The common choice is the Rec-tified Linear Unit (ReLU). They fall into two categories,linear and non-linear activation functions [4].\u2022 Fully Connected Layer : It functions as a feed-forwardneural network (NN) typically situated at the network\u2019slower layers. It receives input from the output of the lastpooling or convolutional layer, which is flattened into aone-dimensional vector, enabling it to learn complex datarelationships [4].2) Computation and Memory Demands: In CNNs, unbal-ance exists in resource demands between the layers. Convolu-tional layers primarily serve as feature extractors and heavilydominate the computational workload. In the case of AlexNet,for instance, the convolutional layers account for just 2 millionweights but demand a substantial 1.33 Giga Operations PerSecond (GOPS) of computation. In contrast, fully connectedlayers function as classifiers, accumulating information forhigh-level decisions, and bear the weight of the network witharound 59 million parameters, yet they contribute significantlyless to computation, requiring only 0.12 GOPS. This obviouscontrast in resource allocation (Figure 1) highlights the unbal-anced demands between these two layers in CNNs [5].3) Key CNN Architectures:In 1989, the use of a NN archi-tecture with convolutional layers for recognizing handwrittendigits in the context of ZIP code recognition was introduced[6], That architecture consisted of input layer, 3 hidden layers,and output layer. Since then, CNN models have developed(Figure 2) and became much deeper.4) CNN on Edge: Deploying CNN models on edge hasa wide range of practical and industrial applications acrossvarious sectors. Here are some specific examples:\u2022 Surveillance and Security : It can perform real-time ob-ject detection and facial recognition for security monitor-ing, identifying intruders, and managing access control.Face identification was deployed using VGGFace [7].Video analysis was deployed using YOLOX [8]. Infraredand Visible Image Fusion for security systems was de-ployed using DenseNet [9]. Human action recognitionapplications were deployed using ResNet [10].\u2022 Manufacturing and Quality Control : It can inspectproducts on assembly lines for defects, ensuring qualitycontrol and minimizing errors. Real-time detection ofsteel strip surface defects was deployed using Faster R-CNN model [11].\u2022 Agriculture: Drones can monitor crops, detect pests,diseases, and nutrient deficiencies, enabling precisionagriculture. Identifying rice leaf diseases in natural en-vironments was deployed using GoogLeNet [12]. Pepperleaf disease identification was deployed using GoogLeNet[13]. Detection for insect pests was deployed on YOLOX[14].\u2022 Healthcare and Wearables : Wearable devices can con-tinuously monitor vital signs, detect anomalies, and evendiagnose certain health conditions. Medical diagnosis(Covid and Lung Disease Detection) was deployed usingVGG, MobileNet, and AlexNet [15]. Automatically diag-nose pneumonia and COVID-19 from chest X-ray imageswas deployed on DenseNet [16]. Medical applications(e.g., COVID-19 detection, cardiomegaly diagnosis, braintumor classification) were deployed using ResNet [10].\u2022 Energy Management: It can monitor energy usage, opti-mize consumption patterns, and identify areas for energyefficiency improvements. Wind Turbine Maintenance andfault diagnosis was deployed using AlexNet [17].\u2022 Environmental Monitoring : It can monitor air quality,pollution levels, and weather conditions, providing valu-able insights for urban planning. A smartphone app toperform fine-grained classification of animals in the wildwas deployed using AlexNet, GoogLeNet, and ResNet[18]. Identification of mosquito species was deployedusing AlexNet, DenseNet, Inception, ResNet, and VGG[19].--- Page 3 ---3Fig. 2. Evolution of Key CNN Architectures Over Time.\u2022 Logistics and Inventory Management : It can automatepackage sorting, inventory tracking, and warehouse man-agement. Mobile robot to map its surroundings while de-tecting objects and people was deployed using AlexNet,GoogLeNet, and ResNet [18].\u2022 Autonomous Vehicles : Vehicles can process real-timedata from cameras and sensors using CNNs, aiding inautonomous navigation and collision avoidance. Instanceobjects detection system for intelligent service robots wasdeployed using Alexnet [20]. Advanced driving assistancesystems (ADASs) and automated vehicles (A Vs) weredeployed using Faster R-CNN [21].Deploying CNNs on edge devices presents significant chal-lenges mainly due to the limited computational resources,constrained memory, and power consumption constraints in-herent to these devices. CNN models, known for their depthand complexity, often demand substantial computational powerand memory, which may exceed the capabilities of edgehardware. Hence, compressing the model before deploymentbecomes imperative. Model compression techniques aim toreduce the size of the CNN model while preserving its perfor-mance, thereby enabling efficient utilization of computationalresources and memory on edge devices. By compressingthe model, we can mitigate the challenges associated withdeploying CNNs on edge devices, ensuring that they caneffectively perform tasks such as real-time image processing,object detection, and classification within resource-constrainedenvironments.With the enormous number of compression techniques pro-posed for CNNs, the rapid evolution of CNN architectures hascreated a gap in the field. This dynamic shift in architecturedesign requires an evaluation of existing compression methods,particularly in light of the demand to make these advancedCNN models suitable for deployment on edge devices. AsCNN designs continue to advance, the challenge lies inadapting compression techniques to smoothly integrate withthese modern architectures. This evaluation (either for eachindividual techniques or combined with each other) becomesimportant, as it not only ensures the continued relevance ofcompression techniques but also addresses the urgent need tomake resource-intensive CNN models accessible and deploy-able on edge devices.This work aims to evaluate CNN compression techniquesthat assure appropriate performance on edge devices. In thesubsequent sections, this work reveals in a structured mannerto evaluate the compression techniques for CNN models.section 2 provides a detailed review of related work, offeringinsights into prior research and establishing a foundationalunderstanding of the topic. section 3 explains the methodologyemployed in conducting the experiments, describing the designand execution of the study. Following this, section 4 presentsthe experimental results and analyzes the findings to recognizetrends and implications. Section 5 critically evaluates the re-sults. Section 6 draws conclusions regarding the effectivenessand significance of the compression techniques. This organizedframework aims to comprehensively explore and contribute tothe field of model compression for efficient deployment inresource-constrained environments.II. R ELATED WORKWithin the context of edge AI, it is important to address thecritical need for model compression. The resource constrainednature of these devices requires more efficient AI modelsby minimizing memory and computational demands, ensur-ing faster inference speeds, and enhancing energy efficiency.Below will explore various model compression techniques andtheir implications for edge AI applications.A. PruningPruning is a key technique in DNN, aimed at enhancingefficiency and model generalization. It involves the removalof redundant components, such as parameters, neurons, filters,or entire layers, leading to several advantages. By reducingunnecessary parameters, it cuts down on storage require-ments, and important for models deployed on devices withlimited memory. Furthermore, it streamlines computational--- Page 4 ---4Fig. 3. Weight Pruning (a) and Neuron Pruning (b). x: input, w: weight. [22].complexity during inference, resulting in faster predictions andlower power consumption. Pruning also mitigates overfittingby simplifying the network. Various pruning techniques, likeweight pruning, neuron pruning, filter pruning, and layerpruning, offer different levels of granularity in componentremoval. Whether applied during or after training, pruningenables the creation of more compact and efficient CNNmodels tailored to specific needs, effectively balancing modelsize, computational efficiency, and accuracy. Weight pruningsets weight connections in the network to zero if they fallbelow a predefined threshold or are considered redundant.Neuron pruning focuses on removing entire neurons if they arefound to be redundant. Layer pruning allows for the removalof entire layers that are considered less important [22].1) Pruning For Fully Connected Layer: Fully connectedlayers are dense that makes the layer with high memorydemand. Pruning them effectively reduces the memory burdenand reduce size of the model.It involves selectively removing weight connections andneurons to reduce the model\u2019s complexity while preservingperformance. In a typical feed-forward NN, inputs are multi-plied by corresponding weights, and a linear sum is computedat each neuron, which is then transformed by an activationfunction. As shown in Figure 3, a network with 3 inputneurons, 2 hidden neurons, and 1 output neuron may havemultiple weight connections. Pruning can be applied to elim-inate specific weight connections or entire neurons. By doingso, the total number of weight connections can be significantlyreduced, leading to a more compact network. The concept ofpruning was first introduced by [6], who proposed removingweights based on their saliency, with small-magnitude weightshaving less impact on training error. The process involvesiterative retraining to regain accuracy, and the technique isknown as \u2019Optimal Brain Damage (OBD)\u2019 where the secondderivative of the objective function with respect to parametersis used to calculate the small saliency, facilitating informedpruning decisions. Since then, other pruning approaches havebeen introduced for fully connected layers [22].2) Pruning For Convolutional Layer: Each convolutionallayer typically consists of numerous filters that makes the layerwith high computational demand. Pruning these less significantfilters directly from the convolutional layer effectively reducesthe computational burden and speeds up the model. Inspired byearly pruning methods, new approaches have been introducedto be used to prune convolutional layers [22].Bayesian was used to decide what to prune and the level ofpruning, in this context involves employing scale mixtures ofnormals as priors for parameter weights in LeNet and VGG[23]. Differential evolution based layer-wise weight methodalongside three other pruning techniques (Naive Cut, Itera-tive Pruning, and Multi-Objective NN Pruning) was used toprune LeNet, AlexNet, and VGG16 [24]. Two fully connectedlayers are removed from the AlexNet architecture, and BatchNormalization (BN) is introduced to mitigate overfitting [20].Filters Similarity in Consecutive Layers (FSCL) for CNNs wasused to reduce the number of filters while preserving impor-tant filters, ultimately improving model efficiency for VGG,GoogLeNet, and ResNet [25]. Structured pruning throughsparsity-induced pruning was used to enhance the real-timeimplementation of the DEtection TRansformer (DETR) [26].Structured pruning was used to compress YOLOX, this processincluded sparse training to prune unimportant channels, withfine-tuning to recover accuracy [14]. Evolutionary approachto filter pruning involved sequential application of multiplepruners in a specific order to sparsify LeNet and VGG-19while maintaining model accuracy [27]. Multilayer networkswere used to represent and compress ResNets, it involvedcreating class networks, calculating arc weights, and forminga multilayer network. The overall degree of nodes in themultilayer network is used to select a subset of nodes forcompression, and convolutional layers are pruned [10]. Tooptimize the Fused-DenseNet-Tiny model for efficient detec-tion of COVID-19 and pneumonia in chest X-ray images,three steps were implemented including removing insignif-icant weights, discarding pruning casings, and applying acompression algorithm [16]. Deep Scalable Zerotree-based(DeepSZ) framework was used to address resource constraints--- Page 5 ---5by achieving significant compression for LeNet, AlexNet,and VGG while maintaining acceptable inference accuracy[28]. Compressing without retraining that was used withResNet, AlexNet, VGGNet and SqueezeNet. It focused onconvolutional and fully connected layers, while maintainingor improving classification accuracy [29].B. QuantizationQuantization plays an important role in addressing theresource-intensive nature of CNNs. By reducing the bit pre-cision of model parameters, quantization not only conservesmemory and energy but also enhances inference speed, makingit an essential technique for deploying CNNs in resource-constrained environments such as edge devices. Weight clus-tering takes quantization to a more advanced level by orga-nizing weights into clusters, where each cluster shares thesame weight value. This approach minimizes the need forfine-tuning individual weights and can lead to substantialreductions in memory and computational overhead [22].Single Level Quantization (SLQ) and Multiple Level Quan-tization (MLQ) technique were used to quantize AlexNet,VGG, GoogleNet, and ResNet to the deployment of thesemodels on resource-constrained mobile devices like mobilephones and drones [30].C. Low-Rank Decomposition/FactorizationIt is a compression technique used with feed-forward NNsand CNNs, to reduce the size of weight matrices whilepreserving model performance. Singular Value Decomposition(SVD) is a popular factorization scheme that decomposes aweight matrix A into three smaller matrices: U, S, and V T . Urepresents the left singular vectors, S is a diagonal matrix ofsingular values, and V T is the transpose of the right singularvectors. This factorization offers several advantages, such asreduced storage requirements, which is crucial for memory-constrained environments, and accelerated inference, espe-cially in CNNs, as smaller matrices can be convolved faster.Low-rank factorization can be applied to fully connected andconvolutional layers, making models more storage-efficientand faster without sacrificing performance. Careful selectionof the rank is essential for achieving a balance between sizereduction and model accuracy. Later, more approaches havebeen introduced [22].Tucker decomposition for weight tensors was used tooptimizes weight tensor dimensions of LeNet and ResNetmodels [31]. Low-rank decomposition was used as an efficientmethod for compressing AlexNet, VGG, and ResNet withoutthe need for fine-tuning to significantly reduce model sizeand computational complexity to make them more suitablefor resource-constrained mobile and embedded devices [32].Hardware-Aware Automatic Low-Rank Compression frame-work HALOC was used to compress ResNet, VGG andMobileNet, with the goal of efficiently exploring the structure-level redundancy in NNs by integrating principles from neuralarchitecture search (NAS) [33]. Automatically DifferentiableTensor Network (ADTN) method was used to significantlyreduce the number of parameters of fully connected NN,LeNet, and VGG while maintaining or enhancing the per-formance [34]. Joint Matrix Decomposition, specifically JointSVD (JSVD) was used to address the challenge of deployingResNet with numerous parameters on resource-constrainedplatforms. It included Right JSVD, Left JSVD, and BinaryJSVD algorithms [35]. Tensor Ring Networks (TR-Nets) wasused as a method to effectively factorize LeNet and ResNet,thereby reducing computational and memory requirements[36]. Tucker decomposition with rank selection and fine tuningwas used as a one-shot whole network compression schemefor deploying AlexNet, VGG, and GoogLeNet on mobiledevices while maintaining reasonable accuracy [37]. TensorDynamic Low-Rank Training (TDLRT) was used to createa training algorithm with VGG and AlexNet that maintainshigh model performance while significantly reducing memoryrequirements for convolutional layers [38].D. Knowledge Distillation (KD)It is a technique used to transfer the knowledge learnedby a larger, more complex model (the teacher model) to asmaller and lighter model (the student model). The primarygoal of KD is to enable the student model to benefit from thegeneralization capabilities of the teacher model while beingmore lightweight in terms of parameters and computations.This technique helps to recover the accuracy drop occurs dueto implementing other compression techniques.Knowledge transfer and distillation, initially introduced by[39], aimed to compress large ensemble models into smaller,faster counterparts with minimal performance loss. [40] ex-tended this concept by empirically demonstrating that theintricate knowledge within larger DNNs could be effectivelytransferred to smaller, shallower models, yielding comparableaccuracy. This involved training a large DNN and transfer-ring its knowledge to a shallower network while minimizingthe squared difference between the logits produced by thetwo models. These foundational ideas produced knowledgedistillation, a widely used technique for training efficientmodels by transferring knowledge from larger ones. Later,more approaches have been introduced [22].KD was used to improve the compression of LeNet andResNet models when fresh training data is scarce, primarilythrough the use of synthetic data generated by GenerativeAdversarial Networks (GANs) [41]. To fuse information frominfrared and visible images while reducing DenseNet complex-ity and improving inference speed. Insights from pre-trainedteacher models are transferred to the smaller student model[9]. KD was used to develop a lightweight mosquito speciesidentification model (EfficientNet) that balances efficiency andaccuracy through the compression [42].E. Mixed TechniquesDifferent compression techniques are often combined andused together to achieve more effective and comprehensivemodel compression. Each compression technique targets spe-cific aspects of the model, such as reducing model size,computation complexity, or memory footprint.--- Page 6 ---6In-Parallel Pruning-Quantization CLIP-Q method combinesnetwork pruning and weight quantization was used to com-press AlexNet, GoogLeNet, and ResNet [18]. Pruning andquantization were used to optimize the compression ofAlexNet and reduce the number of parameters significantlywhile maintaining accuracy to be implemented on Field-Programmable Gate Array (FPGA) [5]. Pruning, quantization,and Huffman encoding combined with adversarial trainingwere used to enhance the robustness and compression ofAlexNet while also addressing the model vulnerability toadversarial attacks [43]. Pruning and quantization were usedto compress VGG and ResNet for remote sensing imageclassification, balancing computational complexity constraintswhile preserving model accuracy [44]. Low-rank decompo-sition and quantization were used to compress ResNet andMobileNet, and reduce the computational complexity whilepreserving high performance [45]. Pruning, quantization, andchanging the model architecture were used to design a com-pact SqueezeNet with competitive accuracy while significantlyreducing the number of parameters [46]. Quantization andpruning were used to develop an effective model compressionframework for ResNet and MobileNet. The objective was tooptimize the allocation of compression ratios to minimizeperformance degradation while reducing model size [47]. Jointquantization and pruning were used to develop a post-trainingmodel size compression method that efficiently combines lossyand lossless compression techniques to reduce the size ofResNet, MobileNet, RegNet, MNasNet, and YOLOv5 withoutsacrificing accuracy [48].F . Other TechniquesDepthwise separable convolutions was used to improvesteel strip defect detection by creating a real-time and ef-ficient model while maintaining high accuracy using FasterR-CNN [11]. Deferential Evolution was used to develop anefficient and optimized AlexNet, VGG, and MobileNet forCovid and liver disease detection [15]. Genetic Algorithmwas used to reduce the storage space and inference timeof VGG, ResNet, AlexNet, and SqueezeNet models [49].Factorization (changing kernel size) was used to improvethe accuracy and computing efficiency of pepper leaf diseasedetection using GoogLeNet, specifically for the agriculturalindustry [13]. Flexible and Separable Convolution (FSConv)was used to reduce computational costs without compromisingthe accuracy of VGG, ResNet, Faster R-CNN and RetinaNet[50]. Efficient Layer Compression (ELC) was used to enhancethe computational efficiency of VGG, ResNet, and ConvNeXtwhile preserving their representation capabilities [51].III. D ESIGN OF THE EXPERIMENTSThe experiments aimed to evaluate various compressiontechniques, namely pruning and quantization, on differenttypes of ConvNext [52] model. The experiments includedtraining, fine-tuning, and evaluating of models using CIFAR-10 dataset. The setup involved conducting experiments bothon cloud-based platforms and on edge devices to evaluate theperformance of the compressed models.Fig. 4. Block modifications and resulted specifications. (a) is a ResNeXtblock; in (b) we create an inverted bottleneck block and in (c) the positionof the spatial depthwise conv layer is moved up [52].A. ConvNeXt [52]Is a modern CNN family produced as a journey of graduallymodernize a standard ResNet toward the design of a visionTransformer. The journey starts from a ResNet-50 model, intoa CNN architecture that mirrors some aspects of Transformers,particularly Swin Transformers. The roadmap:1) Training Techniques: Vision Transformer training pro-cedures were used to train ResNet-50 model, this includedextending the training to 300 epochs (90 epochs originally),using AdamW optimizer, and data augmentation techniques(Mixup, Cutmix, RandAugment, Random Erasing, and regu-larization schemes including Stochastic Depth).2) Macro Design: Number of blocks in each stage wasadjusted from (3, 4, 6, 3) to (3, 3, 9, 3) and the stem wasreplaced with a patchify layer implemented using a 4x4, stride4 convolutional layer (non-overlapping convolution).3) ResNeXt-ify: ResNeXt approach was adopted which isutilize grouped convolutions, where convolutional filters aredivided into groups, each handling a subset of input channels,a variation of grouped convolution known as depthwise convo-lution was adopted, and the network\u2019s width was expanded byincreasing the number of channels in the convolutional layers.4) Inverted Bottleneck: The hidden dimension of the MLPblock was changed to be four times wider than the inputdimension as shown in Figure 4 (a and b)5) Large Kernel Sizes: The position of the convolutionallayer is moved up and the kernel size was changed from (3x3)to (7x7) as shown in Figure 4 (a and c).B. Micro DesignReplacing ReLU with Gaussian Error Linear Unit (GELU),fewer normalization layers, Substituting Batch Normalization(BN) with Layer Normalization (LN), and introducing separatedownsampling layers as shown in Figure 5.C. Compression Techniques1) Pruning: Different pruning techniques have been usedincluding structured and unstructured techniques.\u2022 Only Train Once (OTO) [53]: OTO version 3 (OTOV3)is automated framework for structured pruning which in-volves removing entire structures or groups of parametersfrom a DNN. OTOv3 begins by analyzing the depen-dencies between the vertices of the target DNN. Thisanalysis involves identifying accessory, Shape-Dependent--- Page 7 ---7Fig. 5. Block designs for a ResNet, a Swin Transformer, and a ConvNeXt.Swin Transformer\u2019s block is more sophisticated due to the presence of multiplespecialized modules and two residual connections [52].(SD) joint, and unknown vertices that are adjacent andestablishing their interdependencies. The goal is to formnode groups based on these dependencies, laying thefoundation for identifying interdependent vertices duringstructured pruning.Using the information gathered from the dependencyanalysis, OTOv3 constructs a pruning dependency graph.This graph represents the interdependencies between ver-tices, with vertices in the same node group indicatingtheir interdependency during structured pruning. Thepruning dependency graph ensures the validity of theproduced subnetwork by preserving essential connectionsbetween vertices.OTOv3 partitions the trainable variables of the DNNinto Pruning Zero-Invariant Groups (PZIGs) based on thepruning dependency graph. PZIGs consist of pairwisetrainable variables grouped together, with each grouprepresenting a potential pruning structure. Node groupsadjacent to the DNN output and containing unknownvertices are excluded from forming PZIGs to preserveoutput shapes and ensure model robustness as shown inFigure 6.To jointly search for redundant pruning structures andtrain the remaining groups for optimal performance,OTOv3 employs the Dual Half-Space Projected Gradient(DHSPG) algorithm. DHSPG minimizes the objectivefunction while introducing a sparsity constraint to identifyredundant groups for removal. It employs saliency-drivenredundant identification and a hybrid training paradigmto control sparsity and achieve better generalization per-formance as shown in Figure 7.\u2022 L1 Unstructured [54] L1 unstructured pruning is atechnique used in machine learning, to reduce the sizeof neural networks by eliminating less important con-nections. Each weight in the network is assigned a scorebased on its magnitude. This score reflects the importanceof the weight in the network\u2019s performance. In l1 pruning,this score is often the absolute value of the weight.A threshold is set, typically by selecting the top x% ofweights based on their magnitude scores. The thresholddetermines which weights will be pruned and which willbe retained.Weights that fall below the threshold are pruned, meaningthey are set to zero and effectively removed from thenetwork. This results in a sparser network architecturewith fewer connections.\u2022 Random Unstructured [55] Similar to l1 unstructuredpruning, random unstructured pruning is also a techniqueused in machine learning, to reduce the size of neuralnetworks by eliminating less important connections. Thedifference is the pruned weight are selected randomlyinstead of using l1 to decide the importance of theweights.2) Dynamic Quantization [30]:Dynamic quantization is anapproach aimed at optimizing the deployment of neural net-works by reducing the precision of the weights. Unlike tradi-tional quantization methods that apply a fixed quantization bit-width across all layers of the network, dynamic quantizationadapts the quantization bit-width for each layer individuallybased on its representation abilities and capacities. This isachieved through the use of a bit-width controller module,which employs a policy gradient-based training approach tolearn the optimal bit-width for each layer. By dynamicallyadjusting the quantization bit-width, dynamic quantization canstrike a balance between maintaining accuracy and reducingmemory size and computational costs.D. CIFAR-10 [56]CIFAR-10 is a dataset used for computer vision and ma-chine learning research, offering a rich resource for trainingand evaluating image classification algorithms. Comprising60,000 32x32 RGB color images across 10 distinct classes(Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse,Ship, and Truck), CIFAR-10 facilitates comprehensive explo-ration of diverse visual concepts. With each class containing6,000 images and a balanced distribution across the dataset,CIFAR-10 presents a well-structured foundation for model de-velopment. Its division into 50,000 training images and 10,000test images, further segmented into multiple batches, enablesstrict evaluation and benchmarking of algorithms. In terms ofcomputational requirements, CIFAR-10 generally requires lesscomputation compared to CIFAR-100 and ImageNet due to itssmaller image size and fewer classes which makes it suitablefor experiments with limited computational resources.E. Experiment SetupTwo types of experiments have been conducted, cloud-basedexperiments that focused on compressing the models and eval-uating the techniques and edge-based experiment experimentto evaluate the performance of one of the compressed models.--- Page 8 ---8Fig. 6. Automated PZIG Partition [53].Fig. 7. Search Direction in DHSPG [53].1) Cloud-based Experiment Setup: Google Colab Pro+was used to utilize GPU resources (NVIDIA A100 and V100Tensor Core GPUs), facilitating accelerated model trainingand evaluation and background execution. The integrationwith Google Drive reduced overheads associated with upload-ing and downloading model data to and from cloud. Theevaluation framework was designed to profile the originalmodel, compress it, profile the compressed model, and conductcomparison between the measurements before and after thecompression as shown in Figure 8.This profiling process involved measuring several key met-rics:\u2022 Accuracy: The classification accuracy achieved by themodel on the validation dataset.\u2022 Model Size: The size of the model in megabytes (MB).\u2022 Number of Parameters : The total count of trainableparameters in the model, measured in millions (M).\u2022 Number of MACs : The number of multiply-accumulateoperations performed during inference, measured in mil-lions (M).\u2022 Number of Non-Zero Parameters : The count of non-zero parameters in the model, essential for pruning-basedtechniques.2) Edge-based Experiment Setup: A compressed modelwas deployed on edge with CPU (11th Gen Intel(R) Core(TM)i7-1165G7 @ 2.80GHz 2.80 GHz), RAM (16GB), and laptopintegrated camera.2 samples from each of CIFAR-10 classes have been se-lected randomly from the internet, printed on A4 papers, andplaced in front of the camera to measure the accuracy and theinference time.--- Page 9 ---9Fig. 8. Compression Evaluation Framework.TABLE IOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT TINY, SMALL , BASE AND LARGE .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)Tiny Full 63.81 106.26 26.53 86.88 18.41Compressed 63.81 41.31 10.29 47.80 10.30Small Full 63.48 188.89 47.16 169.28 29.55Compressed 63.48 48.04 11.94 68.24 11.96Base Full 61.22 334.28 83.50 299.20 52.22Compressed 61.22 84.12 20.96 119.40 20.98Large Full 63.40 748.82 187.12 669.30 116.93Compressed 63.40 187.32 46.75 264.69 46.78IV. R UNNING THE EXPERIMENTS AND EXPERIMENTALRESULTSA. Cloud-Base Experimentsdifferent experiments have been conducted on cloud to eval-uate different compressing techniques and different versions ofConvNeXt model.1) Evaluate OTOV3 on Untrained Torch ConvNext Tiny,Small, Base, and Large: Untrained ConvNeXt tiny, small,base, and large have been imported from Torch and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and 200epochs were used for training and pruning. OTOV3 achievedhigh performance (Table I) with reducing the model size(61% for tiny and 75% for small, base, and large), numberof parameters (61% for tiny and 75% for small, base, andlarge), and MACs (45% for tiny and 60% for small, base,and large) as shown in Figure 9. Meanwhile OTOV3 wasable to increase both the full and compressed model accuracythrough the training and pruning without any accuracy dropafter pruning comparing to the full model.To investigate the effect of OTOV3 on the model archi-tecture, a comparison has been conducted between ConvNeXtsmall before and after compression. The Torch implementationof the model consist of many CNBlocks, each CNBlock con-sist of Conv2d, Permute, LayerNorm, Linear, GELU, Linear,and Permute layers. As shown in Figure 10, OTOV3 reducednumber of output features of the Linear layer (sequence 3)and the input features of the next Linear layer (sequence 5)and considering the big number of CNBlock in the modelarchitecture, the reduction in model size and number ofparameters after compression is justified as shown in TableII.2) Evaluate OTOV3 on Untrained ConvNext Small (Torchvs. TIMM): Two untrained ConvNeXt small have been im-ported, one from Torch and one from TIMM [57] and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and200 epochs were used for training and pruning. Although thecompression performance was same with size reduction (75%)but the accuracy after 200 epochs was less for Torch model(63%) comparing to TIMM model (73%) as shown in Figure11.To investigate the accuracy performance of OTOV3 withTorch and Timm ConvNeXt Small, a comparison has beenconducted between the two model architectures. The Torchmodel uses the CNBlock structure, which includes additionaloperations such as Permute and varying StochasticDepth prob-abilities. The TIMM model follows a simpler structure withConv2d and LayerNorm, lacking the additional complexitiesintroduced by CNBlock and associated operations in theTorch model as shown in Figure 12 which effects OTOV3performance regarding the accuracy.3) Evaluate OTOV3 on Fine-Tuned Torch ConvNext Small:A pre-trained ConvNeXt small have been imported from Torchand fine-tuned on CIFAR-10 with 100 epochs, the accuracyreached 89.5%. This fine-tuned ConvNeXt small will be usedfor the rest of cloud-base experiments. This model was usedto evaluate OTOV3, CIFAR-10 was used for training andevaluation, and 200 epochs were used for training and pruning.OTOV3 achieved high performance (Table III) 74% reductionin model size and number of parameters, 60% reduction inMACs, and 3.8% increase with accuracy as shown Figure13. The accuracy of the full model in (Table III) (92.86%)is different that the accuracy of the original model used inthe experiment (89.5%), that because OTOV3 trained the full--- Page 10 ---10Fig. 9. OTOV3 Compression Performance with ConvNeXt Tiny, Small, Base and Large.Fig. 10. Comparison Between ConvNeXt Small Full (Left) and Compressed (Right).Fig. 11. OTOV3 Comparison Between Torch and Timm ConvNeXt Small.model during the process which increased the model accuracy.4) Evaluate Unstructured Pruning: The Fine-tuned Con-vNext Small was used to evaluate Pytorch L1 UnstructuredPruning and Random Unstructured Pruning by using differentcombinations of weights pruning percentages for linear (.1 to.9) and convolutional (.1 to .9) layers. In both experiments,the accuracy and the number of non-zero parameters weredropping as the values of weights pruning percentages for bothlinear and convolutional amounts were increasing as shown inFigure 14 a and b. Although the accuracy dropped but themodel size, number of parameters, and MACs didn\u2019t changeas these techniques zero the weights instead of removing them.5) Evaluate Dynamic Quantization: The Fine-tuned Con-vNext Small was used to evaluate Pytorch dynamic quantiza-tion, 8-bit integer was used during the experiment and CIFAR-10 was used for evaluation. Dynamic quantization achievedhigh performance (Table IV) 71% reduction in model size,95% reduction with number of parameters and MACs, and0.1% drop with accuracy as shown in Figure 156) Evaluate Combination of OTOV3 and Dynamic Quan-tization: Two compression stages were evaluated together asone compression pipeline, OTOV3 and Dynamic Quantization.The compressed ConvNeXt small model that was produced inexperiment 4.1.3 (OTOV3) was furtherly compressed using--- Page 11 ---11Fig. 12. Comparison Between ConvNeXt Small TIMM (Left) and Torch (Right).Fig. 13. OTOV3 Compression Performance with ConvNeXt Small Tuned.Pytorch dynamic quantization using 8-bits integer (similar toexperiment 4.1.5). The pruned models using OTOV2 [58]had dependencies on Open Nural Network Exchange (ONNX)[59] which made it not applicable to be combined with othercompression technique like quantization. In OTOv3, there wasengineering changes produced the pruned model directly inPytorch format, which enhanced the flexibility to be combinedwith quantization as this experiment did [53].Pruning using OTOV3 and Quantization using Pytorchdynamic quantization achieved high performance (Table V)89.7% reduction in model size, 95% reduction with numberof parameters and MACs, and 3.8% increase with accuracy asshown in 16.B. Edge-Base ExperimentThe final compressed ConvNeXt Small model in experimentIV .A.6 (OTV3 and Dynamic Quantization) was deployed onedge and the printed samples used to measure the accuracyand inference time by placing them in front of the camera.The compressed model achieved 92.5% accuracy and 20msinference time. Figure 17 shows samples of the output.V. A NALYSIS OF EXPERIMENTAL RESULTSHere, the experimental results obtained from running aseries of experiments will be analyzed, the experiments aimedat evaluating various compression techniques applied to Con-vNeXt models. The experiments were designed to evaluate theperformance of techniques such as pruning and quantizationon different sizes of ConvNeXt models, with a focus on bothcloud-based compression experiments and edge-based deploy-ment experiment. The analysis will be conducted with respectto the work\u2019s aim of evaluating CNN compression techniquesthat assure appropriate performance (size and inference time)on edge devices and resource-constrained environments.A. OTOV3 Evaluation on Untrained ConvNeXt ModelsThe evaluation of OTOV3 on untrained ConvNeXt modelsdemonstrated its effectiveness in achieving substantial com-pression while increasing model accuracy for both full andcompressed models. Across varying sizes of ConvNeXt mod-els, OTOV3 consistently produced impressive reductions inmodel size, number of parameters, and MACs, highlighting itsability to prune redundant structures effectively. This suggests--- Page 12 ---12Fig. 14. Unstructured Pruning Performance with ConvMeXt Small.TABLE IICOMPARISON FOR NUMBER OF OUTPUT FEATURES AND INPUTFEATURES OF THE LINEAR LAYERS IN THE CNB LOCKS BEFORE ANDAFTER OTOV3 C OMPRESSION .Layer Index Input & Output FeaturesSequential CNBlock Full Model CompressedModel1 0 384 3841 1 384 3841 2 384 3843 0 768 7673 1 768 7043 2 768 7265 0 1536 2515 1 1536 3495 2 1536 2425 3 1536 3785 4 1536 2935 5 1536 3775 6 1536 3405 7 1536 4005 8 1536 3945 9 1536 4785 10 1536 4145 11 1536 4245 12 1536 4105 13 1536 3185 14 1536 4885 15 1536 4885 16 1536 4025 17 1536 2465 18 1536 4025 19 1536 4585 20 1536 3235 21 1536 4195 22 1536 4465 23 1536 4445 24 1536 4415 25 1536 4685 26 1536 10707 0 3072 2087 1 3072 2547 2 3072 262Fig. 15. Dynamic Quantization Compression Performance with ConvNeXtSmall Tuned.Fig. 16. OTOV3 and Dynamic Quantization Compression Performance withConvNeXt Small Tuned.--- Page 13 ---13TABLE IIIOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 92.86 188.89 47.16 169.28 29.80Compressed 92.86 50.03 12.44 67.41 12.46TABLE IVDYNAMIC QUANTIZATION COMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 89.53 188.89 47.16 169.28 47.17Compressed 89.40 54.21 2.15 7.27 2.17TABLE VOTOV3 AND DYNAMIC QUANTIZATION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy % Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number of NonZeroParameters (M)Full 89.53 188.89 47.16 169.28 47.17Phase 1 (OTOV3) 92.86 50.03 12.44 67.41 12.46Phase 2 (Dynamic Quantization) 92.93 19.39 2.15 7.27 2.17Fig. 17. Samples From the Compressed Model Output on Edge.that OTOV3 efficiently identifies and removes unnecessaryparameters from the models, leading to more streamlinedarchitectures without compromising predictive performance.However, an unexpected observation arose when comparingthe accuracy of the Torch implementation with that of theTIMM implementation of ConvNeXt Small. The Torch im-plementation exhibited lower accuracy compared to its TIMMcounterpart, indicating that OTOV3\u2019s performance regardingaccuracy may be influenced by the details of the modelarchitecture. This unexpected result suggests that differenttraining strategies or adjustments may be necessary to optimizeOTOV3\u2019s performance across various model implementations,emphasizing the importance of considering architectural dif-ferences when applying compression techniques like OTOV3.B. OTOV3 Evaluation on Fine-Tuned ConvNeXt ModelsWhen evaluating OTOV3\u2019s performance on a fine-tunedConvNeXt model, notable improvements in compression per-formance were observed, confirming its effectiveness in re-ducing model size, parameters, and MACs while marginallyenhancing accuracy. This outcome highlights the potential ofintegrating fine-tuning with structured pruning techniques toachieve even greater optimization of model performance. Thefine-tuned model displayed enhanced compression capabilitiescompared to untrained models, suggesting that pre-trainingcan significantly increase the effectiveness of compressiontechniques. This finding highlights the importance of lever-aging pre-existing knowledge within models to maximize thebenefits of compression, ultimately resulting in CNN modelswith higher performance.C. Unstructured Pruning TechniquesDuring the evaluation of l1 unstructured and random un-structured pruning techniques, expected trends were observedregarding accuracy and compression. As the pruning per-centages increased for both linear and convolutional layers,a corresponding decrease in accuracy was noted, while themodel size, parameters, and MACs remained unaltered. Thisoutcome aligns with the inherent nature of unstructured prun-ing, wherein weights are zeroed out but not entirely eliminated,resulting in sparse models without substantial reductions incomputational complexity. However, the lack of significantreductions in computational complexity may constrain their ef-fectiveness, particularly in resource-constrained environmentswhere efficient utilization of computational resources is essen-tial. This highlights the importance of considering the trade-offs between model compression and computational efficiencywhen selecting pruning techniques for deployment in real---- Page 14 ---14TABLE VIOTOV3 AND DYNAMIC QUANTIZATION REDUCTION NUMBERS WITH CONV NEXT SMALL TUNED .Technique AccuracyChange Size Reduction ParametersReductionMACsReductionOTOV3 3.75% 73.51% 73.62% 60.18%Dynamic Quantization -0.15% 71.30% 95.43% 95.71%OTOV3 and DynamicQuantization 3.80% 89.74% 95.43% 95.71%world applications, especially in edge computing scenarioswhere computational resources are limited.D. Dynamic QuantizationDynamic quantization emerged as a highly effective tech-nique for model compression, demonstrating remarkable re-ductions in model size, parameters, and, Your response: ```SUMMARY\nedge device image classification model compression quantization\n```\n~~~~~~~~~~\nCurrent Step #15, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification model compression quantization\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 17:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #14, Phase: literature review, Feedback: 3\n--- Page 1 ---1Edge AI: Evaluation of Model CompressionTechniques for Convolutional Neural NetworksSamer Francy, Member, IEEE,Raghubir Singh, Member, IEEEAbstract\u2014This work evaluates the compression techniqueson ConvNeXt models in image classification tasks using theCIFAR-10 dataset. Structured pruning, unstructured pruning,and dynamic quantization methods are evaluated to reduce modelsize and computational complexity while maintaining accuracy.The experiments, conducted on cloud-based platforms and edgedevice, assess the performance of these techniques. Results showsignificant reductions in model size, with up to 75% reduc-tion achieved using structured pruning techniques. Additionally,dynamic quantization achieves a reduction of up to 95% inthe number of parameters. Fine-tuned models exhibit improvedcompression performance, indicating the benefits of pre-trainingin conjunction with compression techniques. Unstructured prun-ing methods reveal trends in accuracy and compression, withlimited reductions in computational complexity. The combinationof OTOV3 pruning and dynamic quantization further enhancescompression performance, resulting 89.7% reduction in size, 95%reduction with number of parameters and MACs, and 3.8%increase with accuracy. The deployment of the final compressedmodel on edge device demonstrates high accuracy 92.5% and lowinference time 20 ms, validating the effectiveness of compressiontechniques for real-world edge computing applications.Index Terms\u2014edge AI, ConvNeXt, CNN, pruning, quantiza-tion, compression, OTO.I. I NTRODUCTIONEDGE devices such as Internet of Things (IoT) are becom-ing increasingly important and widely used in our dailylives and industrial facilities. IoT is a network of things thatempowered by sensors, identifiers, software intelligence, andinternet connectivity, it can be considered as the intersectionof the internet, things/objects (anything/everything), and data[1]. The number of these devices is expected to increase evenmore [2]. These devices have the potential to perform complexArtificial Intelligence (AI) tasks locally, without relying heav-ily on cloud infrastructure [3]. The rapid advancement of AIhas led to the development of complex deep learning modelsthat show high performance in different domains. DeployingAI models on edge devices has many advantages such as lowlatency, privacy and data security, bandwidth optimization, andreduced network dependence. Low latency is achieved due toreal-time processing by instant data analysis on edge withoutwaiting for remote server processing, this data analysis on theedge reduces transmitting data to the cloud which enhancessecurity against breaches, reduces the bandwidth consumption,and reduces network dependence.Samer Francy is with the Department of Computer Science, University ofBath, Bath, England (e-mail: samer.francy@bath.edu).Raghubir Singh is with the Department of Computer Science, Universityof Bath, Bath, England (e-mail: rs3022@bath.ac.uk).A. Overview of Edge AIEdge AI represents a paradigm shift in the way AI isimplemented in the context of the IoT. It capitalizes on thecapabilities of IoT devices, enhancing real-time processing,analytics, and decision-making directly at the edge of thenetwork. The IoT architecture, which is the foundation forEdge AI, typically involves three core layers [1]. The layersare perceptual layer, where data is collected from varioussensors and devices, network layer, where data is transmittedand routed through this layer, which is responsible for commu-nication between devices and cloud services, and applicationlayer, which processes and utilizes the data, providing insightsand enabling actions.B. Convolutional Neural Networks (CNNs)CNN models are subsets of Deep Neural Networks (DNN)models. CNN models are effective for image and video-relatedtasks due to their ability to learn relevant features from thedata by recognizing patterns, shapes, and structures in images,which is challenging for traditional machine learning models,that\u2019s why they are used for computer vision tasks such asimage classification, object detection, and image segmentation[4].1) CNN Architecture: In general CNN models consist ofbelow parts:\u2022 Input Image : Pixels form the binary basis of computerimages, while the human visual system operates throughneurons with receptive fields. Similarly, CNNs functionwithin their receptive areas, starting with simple patternsand advancing to more complex ones, making CNNs apromising tool for computer vision [4].\u2022 Convolutional Layer : A convolutional layer in a CNNuses a small filter (e.g., 3x3 or 5x5) that slides over theinput image. At each position, it multiplies its valueswith the overlapping image pixels and sums the results toproduce an output. This sliding operation helps identifylocal features like edges and colors, building a hierar-chical representation. The depth of the filter matches theinput image\u2019s channels (e.g., 3 for RGB images). Stackingmultiple filters allows the network to learn features atdifferent abstraction levels [4].\u2022 Pooling Layer : Pooling reduces the spatial size of fea-ture maps. This not only lowers computational demandsbut also helps in extracting position and orientation-independent features essential for training. Two com-mon pooling methods are maximum pooling and averagepooling. In maximum pooling, a small kernel (e.g., 2x2)arXiv:2409.02134v1  [cs.LG]  2 Sep 2024--- Page 2 ---2Fig. 1. Unbalanced Demand For Computation (Left) and Memory (Right) in AlexNet [5].selects the maximum value within its range and places itin the output. In average pooling, a similar-sized kernelcomputes the average value within its area for eachchannel, maintaining the same depth. Pooling simplifiescomputation and weight requirements, with maximumpooling being the most commonly used method [4].\u2022 Activation Function : The activation function, applies amathematical operation to the filter\u2019s output to concludethe output of the network. The common choice is the Rec-tified Linear Unit (ReLU). They fall into two categories,linear and non-linear activation functions [4].\u2022 Fully Connected Layer : It functions as a feed-forwardneural network (NN) typically situated at the network\u2019slower layers. It receives input from the output of the lastpooling or convolutional layer, which is flattened into aone-dimensional vector, enabling it to learn complex datarelationships [4].2) Computation and Memory Demands: In CNNs, unbal-ance exists in resource demands between the layers. Convolu-tional layers primarily serve as feature extractors and heavilydominate the computational workload. In the case of AlexNet,for instance, the convolutional layers account for just 2 millionweights but demand a substantial 1.33 Giga Operations PerSecond (GOPS) of computation. In contrast, fully connectedlayers function as classifiers, accumulating information forhigh-level decisions, and bear the weight of the network witharound 59 million parameters, yet they contribute significantlyless to computation, requiring only 0.12 GOPS. This obviouscontrast in resource allocation (Figure 1) highlights the unbal-anced demands between these two layers in CNNs [5].3) Key CNN Architectures:In 1989, the use of a NN archi-tecture with convolutional layers for recognizing handwrittendigits in the context of ZIP code recognition was introduced[6], That architecture consisted of input layer, 3 hidden layers,and output layer. Since then, CNN models have developed(Figure 2) and became much deeper.4) CNN on Edge: Deploying CNN models on edge hasa wide range of practical and industrial applications acrossvarious sectors. Here are some specific examples:\u2022 Surveillance and Security : It can perform real-time ob-ject detection and facial recognition for security monitor-ing, identifying intruders, and managing access control.Face identification was deployed using VGGFace [7].Video analysis was deployed using YOLOX [8]. Infraredand Visible Image Fusion for security systems was de-ployed using DenseNet [9]. Human action recognitionapplications were deployed using ResNet [10].\u2022 Manufacturing and Quality Control : It can inspectproducts on assembly lines for defects, ensuring qualitycontrol and minimizing errors. Real-time detection ofsteel strip surface defects was deployed using Faster R-CNN model [11].\u2022 Agriculture: Drones can monitor crops, detect pests,diseases, and nutrient deficiencies, enabling precisionagriculture. Identifying rice leaf diseases in natural en-vironments was deployed using GoogLeNet [12]. Pepperleaf disease identification was deployed using GoogLeNet[13]. Detection for insect pests was deployed on YOLOX[14].\u2022 Healthcare and Wearables : Wearable devices can con-tinuously monitor vital signs, detect anomalies, and evendiagnose certain health conditions. Medical diagnosis(Covid and Lung Disease Detection) was deployed usingVGG, MobileNet, and AlexNet [15]. Automatically diag-nose pneumonia and COVID-19 from chest X-ray imageswas deployed on DenseNet [16]. Medical applications(e.g., COVID-19 detection, cardiomegaly diagnosis, braintumor classification) were deployed using ResNet [10].\u2022 Energy Management: It can monitor energy usage, opti-mize consumption patterns, and identify areas for energyefficiency improvements. Wind Turbine Maintenance andfault diagnosis was deployed using AlexNet [17].\u2022 Environmental Monitoring : It can monitor air quality,pollution levels, and weather conditions, providing valu-able insights for urban planning. A smartphone app toperform fine-grained classification of animals in the wildwas deployed using AlexNet, GoogLeNet, and ResNet[18]. Identification of mosquito species was deployedusing AlexNet, DenseNet, Inception, ResNet, and VGG[19].--- Page 3 ---3Fig. 2. Evolution of Key CNN Architectures Over Time.\u2022 Logistics and Inventory Management : It can automatepackage sorting, inventory tracking, and warehouse man-agement. Mobile robot to map its surroundings while de-tecting objects and people was deployed using AlexNet,GoogLeNet, and ResNet [18].\u2022 Autonomous Vehicles : Vehicles can process real-timedata from cameras and sensors using CNNs, aiding inautonomous navigation and collision avoidance. Instanceobjects detection system for intelligent service robots wasdeployed using Alexnet [20]. Advanced driving assistancesystems (ADASs) and automated vehicles (A Vs) weredeployed using Faster R-CNN [21].Deploying CNNs on edge devices presents significant chal-lenges mainly due to the limited computational resources,constrained memory, and power consumption constraints in-herent to these devices. CNN models, known for their depthand complexity, often demand substantial computational powerand memory, which may exceed the capabilities of edgehardware. Hence, compressing the model before deploymentbecomes imperative. Model compression techniques aim toreduce the size of the CNN model while preserving its perfor-mance, thereby enabling efficient utilization of computationalresources and memory on edge devices. By compressingthe model, we can mitigate the challenges associated withdeploying CNNs on edge devices, ensuring that they caneffectively perform tasks such as real-time image processing,object detection, and classification within resource-constrainedenvironments.With the enormous number of compression techniques pro-posed for CNNs, the rapid evolution of CNN architectures hascreated a gap in the field. This dynamic shift in architecturedesign requires an evaluation of existing compression methods,particularly in light of the demand to make these advancedCNN models suitable for deployment on edge devices. AsCNN designs continue to advance, the challenge lies inadapting compression techniques to smoothly integrate withthese modern architectures. This evaluation (either for eachindividual techniques or combined with each other) becomesimportant, as it not only ensures the continued relevance ofcompression techniques but also addresses the urgent need tomake resource-intensive CNN models accessible and deploy-able on edge devices.This work aims to evaluate CNN compression techniquesthat assure appropriate performance on edge devices. In thesubsequent sections, this work reveals in a structured mannerto evaluate the compression techniques for CNN models.section 2 provides a detailed review of related work, offeringinsights into prior research and establishing a foundationalunderstanding of the topic. section 3 explains the methodologyemployed in conducting the experiments, describing the designand execution of the study. Following this, section 4 presentsthe experimental results and analyzes the findings to recognizetrends and implications. Section 5 critically evaluates the re-sults. Section 6 draws conclusions regarding the effectivenessand significance of the compression techniques. This organizedframework aims to comprehensively explore and contribute tothe field of model compression for efficient deployment inresource-constrained environments.II. R ELATED WORKWithin the context of edge AI, it is important to address thecritical need for model compression. The resource constrainednature of these devices requires more efficient AI modelsby minimizing memory and computational demands, ensur-ing faster inference speeds, and enhancing energy efficiency.Below will explore various model compression techniques andtheir implications for edge AI applications.A. PruningPruning is a key technique in DNN, aimed at enhancingefficiency and model generalization. It involves the removalof redundant components, such as parameters, neurons, filters,or entire layers, leading to several advantages. By reducingunnecessary parameters, it cuts down on storage require-ments, and important for models deployed on devices withlimited memory. Furthermore, it streamlines computational--- Page 4 ---4Fig. 3. Weight Pruning (a) and Neuron Pruning (b). x: input, w: weight. [22].complexity during inference, resulting in faster predictions andlower power consumption. Pruning also mitigates overfittingby simplifying the network. Various pruning techniques, likeweight pruning, neuron pruning, filter pruning, and layerpruning, offer different levels of granularity in componentremoval. Whether applied during or after training, pruningenables the creation of more compact and efficient CNNmodels tailored to specific needs, effectively balancing modelsize, computational efficiency, and accuracy. Weight pruningsets weight connections in the network to zero if they fallbelow a predefined threshold or are considered redundant.Neuron pruning focuses on removing entire neurons if they arefound to be redundant. Layer pruning allows for the removalof entire layers that are considered less important [22].1) Pruning For Fully Connected Layer: Fully connectedlayers are dense that makes the layer with high memorydemand. Pruning them effectively reduces the memory burdenand reduce size of the model.It involves selectively removing weight connections andneurons to reduce the model\u2019s complexity while preservingperformance. In a typical feed-forward NN, inputs are multi-plied by corresponding weights, and a linear sum is computedat each neuron, which is then transformed by an activationfunction. As shown in Figure 3, a network with 3 inputneurons, 2 hidden neurons, and 1 output neuron may havemultiple weight connections. Pruning can be applied to elim-inate specific weight connections or entire neurons. By doingso, the total number of weight connections can be significantlyreduced, leading to a more compact network. The concept ofpruning was first introduced by [6], who proposed removingweights based on their saliency, with small-magnitude weightshaving less impact on training error. The process involvesiterative retraining to regain accuracy, and the technique isknown as \u2019Optimal Brain Damage (OBD)\u2019 where the secondderivative of the objective function with respect to parametersis used to calculate the small saliency, facilitating informedpruning decisions. Since then, other pruning approaches havebeen introduced for fully connected layers [22].2) Pruning For Convolutional Layer: Each convolutionallayer typically consists of numerous filters that makes the layerwith high computational demand. Pruning these less significantfilters directly from the convolutional layer effectively reducesthe computational burden and speeds up the model. Inspired byearly pruning methods, new approaches have been introducedto be used to prune convolutional layers [22].Bayesian was used to decide what to prune and the level ofpruning, in this context involves employing scale mixtures ofnormals as priors for parameter weights in LeNet and VGG[23]. Differential evolution based layer-wise weight methodalongside three other pruning techniques (Naive Cut, Itera-tive Pruning, and Multi-Objective NN Pruning) was used toprune LeNet, AlexNet, and VGG16 [24]. Two fully connectedlayers are removed from the AlexNet architecture, and BatchNormalization (BN) is introduced to mitigate overfitting [20].Filters Similarity in Consecutive Layers (FSCL) for CNNs wasused to reduce the number of filters while preserving impor-tant filters, ultimately improving model efficiency for VGG,GoogLeNet, and ResNet [25]. Structured pruning throughsparsity-induced pruning was used to enhance the real-timeimplementation of the DEtection TRansformer (DETR) [26].Structured pruning was used to compress YOLOX, this processincluded sparse training to prune unimportant channels, withfine-tuning to recover accuracy [14]. Evolutionary approachto filter pruning involved sequential application of multiplepruners in a specific order to sparsify LeNet and VGG-19while maintaining model accuracy [27]. Multilayer networkswere used to represent and compress ResNets, it involvedcreating class networks, calculating arc weights, and forminga multilayer network. The overall degree of nodes in themultilayer network is used to select a subset of nodes forcompression, and convolutional layers are pruned [10]. Tooptimize the Fused-DenseNet-Tiny model for efficient detec-tion of COVID-19 and pneumonia in chest X-ray images,three steps were implemented including removing insignif-icant weights, discarding pruning casings, and applying acompression algorithm [16]. Deep Scalable Zerotree-based(DeepSZ) framework was used to address resource constraints--- Page 5 ---5by achieving significant compression for LeNet, AlexNet,and VGG while maintaining acceptable inference accuracy[28]. Compressing without retraining that was used withResNet, AlexNet, VGGNet and SqueezeNet. It focused onconvolutional and fully connected layers, while maintainingor improving classification accuracy [29].B. QuantizationQuantization plays an important role in addressing theresource-intensive nature of CNNs. By reducing the bit pre-cision of model parameters, quantization not only conservesmemory and energy but also enhances inference speed, makingit an essential technique for deploying CNNs in resource-constrained environments such as edge devices. Weight clus-tering takes quantization to a more advanced level by orga-nizing weights into clusters, where each cluster shares thesame weight value. This approach minimizes the need forfine-tuning individual weights and can lead to substantialreductions in memory and computational overhead [22].Single Level Quantization (SLQ) and Multiple Level Quan-tization (MLQ) technique were used to quantize AlexNet,VGG, GoogleNet, and ResNet to the deployment of thesemodels on resource-constrained mobile devices like mobilephones and drones [30].C. Low-Rank Decomposition/FactorizationIt is a compression technique used with feed-forward NNsand CNNs, to reduce the size of weight matrices whilepreserving model performance. Singular Value Decomposition(SVD) is a popular factorization scheme that decomposes aweight matrix A into three smaller matrices: U, S, and V T . Urepresents the left singular vectors, S is a diagonal matrix ofsingular values, and V T is the transpose of the right singularvectors. This factorization offers several advantages, such asreduced storage requirements, which is crucial for memory-constrained environments, and accelerated inference, espe-cially in CNNs, as smaller matrices can be convolved faster.Low-rank factorization can be applied to fully connected andconvolutional layers, making models more storage-efficientand faster without sacrificing performance. Careful selectionof the rank is essential for achieving a balance between sizereduction and model accuracy. Later, more approaches havebeen introduced [22].Tucker decomposition for weight tensors was used tooptimizes weight tensor dimensions of LeNet and ResNetmodels [31]. Low-rank decomposition was used as an efficientmethod for compressing AlexNet, VGG, and ResNet withoutthe need for fine-tuning to significantly reduce model sizeand computational complexity to make them more suitablefor resource-constrained mobile and embedded devices [32].Hardware-Aware Automatic Low-Rank Compression frame-work HALOC was used to compress ResNet, VGG andMobileNet, with the goal of efficiently exploring the structure-level redundancy in NNs by integrating principles from neuralarchitecture search (NAS) [33]. Automatically DifferentiableTensor Network (ADTN) method was used to significantlyreduce the number of parameters of fully connected NN,LeNet, and VGG while maintaining or enhancing the per-formance [34]. Joint Matrix Decomposition, specifically JointSVD (JSVD) was used to address the challenge of deployingResNet with numerous parameters on resource-constrainedplatforms. It included Right JSVD, Left JSVD, and BinaryJSVD algorithms [35]. Tensor Ring Networks (TR-Nets) wasused as a method to effectively factorize LeNet and ResNet,thereby reducing computational and memory requirements[36]. Tucker decomposition with rank selection and fine tuningwas used as a one-shot whole network compression schemefor deploying AlexNet, VGG, and GoogLeNet on mobiledevices while maintaining reasonable accuracy [37]. TensorDynamic Low-Rank Training (TDLRT) was used to createa training algorithm with VGG and AlexNet that maintainshigh model performance while significantly reducing memoryrequirements for convolutional layers [38].D. Knowledge Distillation (KD)It is a technique used to transfer the knowledge learnedby a larger, more complex model (the teacher model) to asmaller and lighter model (the student model). The primarygoal of KD is to enable the student model to benefit from thegeneralization capabilities of the teacher model while beingmore lightweight in terms of parameters and computations.This technique helps to recover the accuracy drop occurs dueto implementing other compression techniques.Knowledge transfer and distillation, initially introduced by[39], aimed to compress large ensemble models into smaller,faster counterparts with minimal performance loss. [40] ex-tended this concept by empirically demonstrating that theintricate knowledge within larger DNNs could be effectivelytransferred to smaller, shallower models, yielding comparableaccuracy. This involved training a large DNN and transfer-ring its knowledge to a shallower network while minimizingthe squared difference between the logits produced by thetwo models. These foundational ideas produced knowledgedistillation, a widely used technique for training efficientmodels by transferring knowledge from larger ones. Later,more approaches have been introduced [22].KD was used to improve the compression of LeNet andResNet models when fresh training data is scarce, primarilythrough the use of synthetic data generated by GenerativeAdversarial Networks (GANs) [41]. To fuse information frominfrared and visible images while reducing DenseNet complex-ity and improving inference speed. Insights from pre-trainedteacher models are transferred to the smaller student model[9]. KD was used to develop a lightweight mosquito speciesidentification model (EfficientNet) that balances efficiency andaccuracy through the compression [42].E. Mixed TechniquesDifferent compression techniques are often combined andused together to achieve more effective and comprehensivemodel compression. Each compression technique targets spe-cific aspects of the model, such as reducing model size,computation complexity, or memory footprint.--- Page 6 ---6In-Parallel Pruning-Quantization CLIP-Q method combinesnetwork pruning and weight quantization was used to com-press AlexNet, GoogLeNet, and ResNet [18]. Pruning andquantization were used to optimize the compression ofAlexNet and reduce the number of parameters significantlywhile maintaining accuracy to be implemented on Field-Programmable Gate Array (FPGA) [5]. Pruning, quantization,and Huffman encoding combined with adversarial trainingwere used to enhance the robustness and compression ofAlexNet while also addressing the model vulnerability toadversarial attacks [43]. Pruning and quantization were usedto compress VGG and ResNet for remote sensing imageclassification, balancing computational complexity constraintswhile preserving model accuracy [44]. Low-rank decompo-sition and quantization were used to compress ResNet andMobileNet, and reduce the computational complexity whilepreserving high performance [45]. Pruning, quantization, andchanging the model architecture were used to design a com-pact SqueezeNet with competitive accuracy while significantlyreducing the number of parameters [46]. Quantization andpruning were used to develop an effective model compressionframework for ResNet and MobileNet. The objective was tooptimize the allocation of compression ratios to minimizeperformance degradation while reducing model size [47]. Jointquantization and pruning were used to develop a post-trainingmodel size compression method that efficiently combines lossyand lossless compression techniques to reduce the size ofResNet, MobileNet, RegNet, MNasNet, and YOLOv5 withoutsacrificing accuracy [48].F . Other TechniquesDepthwise separable convolutions was used to improvesteel strip defect detection by creating a real-time and ef-ficient model while maintaining high accuracy using FasterR-CNN [11]. Deferential Evolution was used to develop anefficient and optimized AlexNet, VGG, and MobileNet forCovid and liver disease detection [15]. Genetic Algorithmwas used to reduce the storage space and inference timeof VGG, ResNet, AlexNet, and SqueezeNet models [49].Factorization (changing kernel size) was used to improvethe accuracy and computing efficiency of pepper leaf diseasedetection using GoogLeNet, specifically for the agriculturalindustry [13]. Flexible and Separable Convolution (FSConv)was used to reduce computational costs without compromisingthe accuracy of VGG, ResNet, Faster R-CNN and RetinaNet[50]. Efficient Layer Compression (ELC) was used to enhancethe computational efficiency of VGG, ResNet, and ConvNeXtwhile preserving their representation capabilities [51].III. D ESIGN OF THE EXPERIMENTSThe experiments aimed to evaluate various compressiontechniques, namely pruning and quantization, on differenttypes of ConvNext [52] model. The experiments includedtraining, fine-tuning, and evaluating of models using CIFAR-10 dataset. The setup involved conducting experiments bothon cloud-based platforms and on edge devices to evaluate theperformance of the compressed models.Fig. 4. Block modifications and resulted specifications. (a) is a ResNeXtblock; in (b) we create an inverted bottleneck block and in (c) the positionof the spatial depthwise conv layer is moved up [52].A. ConvNeXt [52]Is a modern CNN family produced as a journey of graduallymodernize a standard ResNet toward the design of a visionTransformer. The journey starts from a ResNet-50 model, intoa CNN architecture that mirrors some aspects of Transformers,particularly Swin Transformers. The roadmap:1) Training Techniques: Vision Transformer training pro-cedures were used to train ResNet-50 model, this includedextending the training to 300 epochs (90 epochs originally),using AdamW optimizer, and data augmentation techniques(Mixup, Cutmix, RandAugment, Random Erasing, and regu-larization schemes including Stochastic Depth).2) Macro Design: Number of blocks in each stage wasadjusted from (3, 4, 6, 3) to (3, 3, 9, 3) and the stem wasreplaced with a patchify layer implemented using a 4x4, stride4 convolutional layer (non-overlapping convolution).3) ResNeXt-ify: ResNeXt approach was adopted which isutilize grouped convolutions, where convolutional filters aredivided into groups, each handling a subset of input channels,a variation of grouped convolution known as depthwise convo-lution was adopted, and the network\u2019s width was expanded byincreasing the number of channels in the convolutional layers.4) Inverted Bottleneck: The hidden dimension of the MLPblock was changed to be four times wider than the inputdimension as shown in Figure 4 (a and b)5) Large Kernel Sizes: The position of the convolutionallayer is moved up and the kernel size was changed from (3x3)to (7x7) as shown in Figure 4 (a and c).B. Micro DesignReplacing ReLU with Gaussian Error Linear Unit (GELU),fewer normalization layers, Substituting Batch Normalization(BN) with Layer Normalization (LN), and introducing separatedownsampling layers as shown in Figure 5.C. Compression Techniques1) Pruning: Different pruning techniques have been usedincluding structured and unstructured techniques.\u2022 Only Train Once (OTO) [53]: OTO version 3 (OTOV3)is automated framework for structured pruning which in-volves removing entire structures or groups of parametersfrom a DNN. OTOv3 begins by analyzing the depen-dencies between the vertices of the target DNN. Thisanalysis involves identifying accessory, Shape-Dependent--- Page 7 ---7Fig. 5. Block designs for a ResNet, a Swin Transformer, and a ConvNeXt.Swin Transformer\u2019s block is more sophisticated due to the presence of multiplespecialized modules and two residual connections [52].(SD) joint, and unknown vertices that are adjacent andestablishing their interdependencies. The goal is to formnode groups based on these dependencies, laying thefoundation for identifying interdependent vertices duringstructured pruning.Using the information gathered from the dependencyanalysis, OTOv3 constructs a pruning dependency graph.This graph represents the interdependencies between ver-tices, with vertices in the same node group indicatingtheir interdependency during structured pruning. Thepruning dependency graph ensures the validity of theproduced subnetwork by preserving essential connectionsbetween vertices.OTOv3 partitions the trainable variables of the DNNinto Pruning Zero-Invariant Groups (PZIGs) based on thepruning dependency graph. PZIGs consist of pairwisetrainable variables grouped together, with each grouprepresenting a potential pruning structure. Node groupsadjacent to the DNN output and containing unknownvertices are excluded from forming PZIGs to preserveoutput shapes and ensure model robustness as shown inFigure 6.To jointly search for redundant pruning structures andtrain the remaining groups for optimal performance,OTOv3 employs the Dual Half-Space Projected Gradient(DHSPG) algorithm. DHSPG minimizes the objectivefunction while introducing a sparsity constraint to identifyredundant groups for removal. It employs saliency-drivenredundant identification and a hybrid training paradigmto control sparsity and achieve better generalization per-formance as shown in Figure 7.\u2022 L1 Unstructured [54] L1 unstructured pruning is atechnique used in machine learning, to reduce the sizeof neural networks by eliminating less important con-nections. Each weight in the network is assigned a scorebased on its magnitude. This score reflects the importanceof the weight in the network\u2019s performance. In l1 pruning,this score is often the absolute value of the weight.A threshold is set, typically by selecting the top x% ofweights based on their magnitude scores. The thresholddetermines which weights will be pruned and which willbe retained.Weights that fall below the threshold are pruned, meaningthey are set to zero and effectively removed from thenetwork. This results in a sparser network architecturewith fewer connections.\u2022 Random Unstructured [55] Similar to l1 unstructuredpruning, random unstructured pruning is also a techniqueused in machine learning, to reduce the size of neuralnetworks by eliminating less important connections. Thedifference is the pruned weight are selected randomlyinstead of using l1 to decide the importance of theweights.2) Dynamic Quantization [30]:Dynamic quantization is anapproach aimed at optimizing the deployment of neural net-works by reducing the precision of the weights. Unlike tradi-tional quantization methods that apply a fixed quantization bit-width across all layers of the network, dynamic quantizationadapts the quantization bit-width for each layer individuallybased on its representation abilities and capacities. This isachieved through the use of a bit-width controller module,which employs a policy gradient-based training approach tolearn the optimal bit-width for each layer. By dynamicallyadjusting the quantization bit-width, dynamic quantization canstrike a balance between maintaining accuracy and reducingmemory size and computational costs.D. CIFAR-10 [56]CIFAR-10 is a dataset used for computer vision and ma-chine learning research, offering a rich resource for trainingand evaluating image classification algorithms. Comprising60,000 32x32 RGB color images across 10 distinct classes(Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse,Ship, and Truck), CIFAR-10 facilitates comprehensive explo-ration of diverse visual concepts. With each class containing6,000 images and a balanced distribution across the dataset,CIFAR-10 presents a well-structured foundation for model de-velopment. Its division into 50,000 training images and 10,000test images, further segmented into multiple batches, enablesstrict evaluation and benchmarking of algorithms. In terms ofcomputational requirements, CIFAR-10 generally requires lesscomputation compared to CIFAR-100 and ImageNet due to itssmaller image size and fewer classes which makes it suitablefor experiments with limited computational resources.E. Experiment SetupTwo types of experiments have been conducted, cloud-basedexperiments that focused on compressing the models and eval-uating the techniques and edge-based experiment experimentto evaluate the performance of one of the compressed models.--- Page 8 ---8Fig. 6. Automated PZIG Partition [53].Fig. 7. Search Direction in DHSPG [53].1) Cloud-based Experiment Setup: Google Colab Pro+was used to utilize GPU resources (NVIDIA A100 and V100Tensor Core GPUs), facilitating accelerated model trainingand evaluation and background execution. The integrationwith Google Drive reduced overheads associated with upload-ing and downloading model data to and from cloud. Theevaluation framework was designed to profile the originalmodel, compress it, profile the compressed model, and conductcomparison between the measurements before and after thecompression as shown in Figure 8.This profiling process involved measuring several key met-rics:\u2022 Accuracy: The classification accuracy achieved by themodel on the validation dataset.\u2022 Model Size: The size of the model in megabytes (MB).\u2022 Number of Parameters : The total count of trainableparameters in the model, measured in millions (M).\u2022 Number of MACs : The number of multiply-accumulateoperations performed during inference, measured in mil-lions (M).\u2022 Number of Non-Zero Parameters : The count of non-zero parameters in the model, essential for pruning-basedtechniques.2) Edge-based Experiment Setup: A compressed modelwas deployed on edge with CPU (11th Gen Intel(R) Core(TM)i7-1165G7 @ 2.80GHz 2.80 GHz), RAM (16GB), and laptopintegrated camera.2 samples from each of CIFAR-10 classes have been se-lected randomly from the internet, printed on A4 papers, andplaced in front of the camera to measure the accuracy and theinference time.--- Page 9 ---9Fig. 8. Compression Evaluation Framework.TABLE IOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT TINY, SMALL , BASE AND LARGE .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)Tiny Full 63.81 106.26 26.53 86.88 18.41Compressed 63.81 41.31 10.29 47.80 10.30Small Full 63.48 188.89 47.16 169.28 29.55Compressed 63.48 48.04 11.94 68.24 11.96Base Full 61.22 334.28 83.50 299.20 52.22Compressed 61.22 84.12 20.96 119.40 20.98Large Full 63.40 748.82 187.12 669.30 116.93Compressed 63.40 187.32 46.75 264.69 46.78IV. R UNNING THE EXPERIMENTS AND EXPERIMENTALRESULTSA. Cloud-Base Experimentsdifferent experiments have been conducted on cloud to eval-uate different compressing techniques and different versions ofConvNeXt model.1) Evaluate OTOV3 on Untrained Torch ConvNext Tiny,Small, Base, and Large: Untrained ConvNeXt tiny, small,base, and large have been imported from Torch and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and 200epochs were used for training and pruning. OTOV3 achievedhigh performance (Table I) with reducing the model size(61% for tiny and 75% for small, base, and large), numberof parameters (61% for tiny and 75% for small, base, andlarge), and MACs (45% for tiny and 60% for small, base,and large) as shown in Figure 9. Meanwhile OTOV3 wasable to increase both the full and compressed model accuracythrough the training and pruning without any accuracy dropafter pruning comparing to the full model.To investigate the effect of OTOV3 on the model archi-tecture, a comparison has been conducted between ConvNeXtsmall before and after compression. The Torch implementationof the model consist of many CNBlocks, each CNBlock con-sist of Conv2d, Permute, LayerNorm, Linear, GELU, Linear,and Permute layers. As shown in Figure 10, OTOV3 reducednumber of output features of the Linear layer (sequence 3)and the input features of the next Linear layer (sequence 5)and considering the big number of CNBlock in the modelarchitecture, the reduction in model size and number ofparameters after compression is justified as shown in TableII.2) Evaluate OTOV3 on Untrained ConvNext Small (Torchvs. TIMM): Two untrained ConvNeXt small have been im-ported, one from Torch and one from TIMM [57] and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and200 epochs were used for training and pruning. Although thecompression performance was same with size reduction (75%)but the accuracy after 200 epochs was less for Torch model(63%) comparing to TIMM model (73%) as shown in Figure11.To investigate the accuracy performance of OTOV3 withTorch and Timm ConvNeXt Small, a comparison has beenconducted between the two model architectures. The Torchmodel uses the CNBlock structure, which includes additionaloperations such as Permute and varying StochasticDepth prob-abilities. The TIMM model follows a simpler structure withConv2d and LayerNorm, lacking the additional complexitiesintroduced by CNBlock and associated operations in theTorch model as shown in Figure 12 which effects OTOV3performance regarding the accuracy.3) Evaluate OTOV3 on Fine-Tuned Torch ConvNext Small:A pre-trained ConvNeXt small have been imported from Torchand fine-tuned on CIFAR-10 with 100 epochs, the accuracyreached 89.5%. This fine-tuned ConvNeXt small will be usedfor the rest of cloud-base experiments. This model was usedto evaluate OTOV3, CIFAR-10 was used for training andevaluation, and 200 epochs were used for training and pruning.OTOV3 achieved high performance (Table III) 74% reductionin model size and number of parameters, 60% reduction inMACs, and 3.8% increase with accuracy as shown Figure13. The accuracy of the full model in (Table III) (92.86%)is different that the accuracy of the original model used inthe experiment (89.5%), that because OTOV3 trained the full--- Page 10 ---10Fig. 9. OTOV3 Compression Performance with ConvNeXt Tiny, Small, Base and Large.Fig. 10. Comparison Between ConvNeXt Small Full (Left) and Compressed (Right).Fig. 11. OTOV3 Comparison Between Torch and Timm ConvNeXt Small.model during the process which increased the model accuracy.4) Evaluate Unstructured Pruning: The Fine-tuned Con-vNext Small was used to evaluate Pytorch L1 UnstructuredPruning and Random Unstructured Pruning by using differentcombinations of weights pruning percentages for linear (.1 to.9) and convolutional (.1 to .9) layers. In both experiments,the accuracy and the number of non-zero parameters weredropping as the values of weights pruning percentages for bothlinear and convolutional amounts were increasing as shown inFigure 14 a and b. Although the accuracy dropped but themodel size, number of parameters, and MACs didn\u2019t changeas these techniques zero the weights instead of removing them.5) Evaluate Dynamic Quantization: The Fine-tuned Con-vNext Small was used to evaluate Pytorch dynamic quantiza-tion, 8-bit integer was used during the experiment and CIFAR-10 was used for evaluation. Dynamic quantization achievedhigh performance (Table IV) 71% reduction in model size,95% reduction with number of parameters and MACs, and0.1% drop with accuracy as shown in Figure 156) Evaluate Combination of OTOV3 and Dynamic Quan-tization: Two compression stages were evaluated together asone compression pipeline, OTOV3 and Dynamic Quantization.The compressed ConvNeXt small model that was produced inexperiment 4.1.3 (OTOV3) was furtherly compressed using--- Page 11 ---11Fig. 12. Comparison Between ConvNeXt Small TIMM (Left) and Torch (Right).Fig. 13. OTOV3 Compression Performance with ConvNeXt Small Tuned.Pytorch dynamic quantization using 8-bits integer (similar toexperiment 4.1.5). The pruned models using OTOV2 [58]had dependencies on Open Nural Network Exchange (ONNX)[59] which made it not applicable to be combined with othercompression technique like quantization. In OTOv3, there wasengineering changes produced the pruned model directly inPytorch format, which enhanced the flexibility to be combinedwith quantization as this experiment did [53].Pruning using OTOV3 and Quantization using Pytorchdynamic quantization achieved high performance (Table V)89.7% reduction in model size, 95% reduction with numberof parameters and MACs, and 3.8% increase with accuracy asshown in 16.B. Edge-Base ExperimentThe final compressed ConvNeXt Small model in experimentIV .A.6 (OTV3 and Dynamic Quantization) was deployed onedge and the printed samples used to measure the accuracyand inference time by placing them in front of the camera.The compressed model achieved 92.5% accuracy and 20msinference time. Figure 17 shows samples of the output.V. A NALYSIS OF EXPERIMENTAL RESULTSHere, the experimental results obtained from running aseries of experiments will be analyzed, the experiments aimedat evaluating various compression techniques applied to Con-vNeXt models. The experiments were designed to evaluate theperformance of techniques such as pruning and quantizationon different sizes of ConvNeXt models, with a focus on bothcloud-based compression experiments and edge-based deploy-ment experiment. The analysis will be conducted with respectto the work\u2019s aim of evaluating CNN compression techniquesthat assure appropriate performance (size and inference time)on edge devices and resource-constrained environments.A. OTOV3 Evaluation on Untrained ConvNeXt ModelsThe evaluation of OTOV3 on untrained ConvNeXt modelsdemonstrated its effectiveness in achieving substantial com-pression while increasing model accuracy for both full andcompressed models. Across varying sizes of ConvNeXt mod-els, OTOV3 consistently produced impressive reductions inmodel size, number of parameters, and MACs, highlighting itsability to prune redundant structures effectively. This suggests--- Page 12 ---12Fig. 14. Unstructured Pruning Performance with ConvMeXt Small.TABLE IICOMPARISON FOR NUMBER OF OUTPUT FEATURES AND INPUTFEATURES OF THE LINEAR LAYERS IN THE CNB LOCKS BEFORE ANDAFTER OTOV3 C OMPRESSION .Layer Index Input & Output FeaturesSequential CNBlock Full Model CompressedModel1 0 384 3841 1 384 3841 2 384 3843 0 768 7673 1 768 7043 2 768 7265 0 1536 2515 1 1536 3495 2 1536 2425 3 1536 3785 4 1536 2935 5 1536 3775 6 1536 3405 7 1536 4005 8 1536 3945 9 1536 4785 10 1536 4145 11 1536 4245 12 1536 4105 13 1536 3185 14 1536 4885 15 1536 4885 16 1536 4025 17 1536 2465 18 1536 4025 19 1536 4585 20 1536 3235 21 1536 4195 22 1536 4465 23 1536 4445 24 1536 4415 25 1536 4685 26 1536 10707 0 3072 2087 1 3072 2547 2 3072 262Fig. 15. Dynamic Quantization Compression Performance with ConvNeXtSmall Tuned.Fig. 16. OTOV3 and Dynamic Quantization Compression Performance withConvNeXt Small Tuned.--- Page 13 ---13TABLE IIIOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 92.86 188.89 47.16 169.28 29.80Compressed 92.86 50.03 12.44 67.41 12.46TABLE IVDYNAMIC QUANTIZATION COMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 89.53 188.89 47.16 169.28 47.17Compressed 89.40 54.21 2.15 7.27 2.17TABLE VOTOV3 AND DYNAMIC QUANTIZATION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy % Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number of NonZeroParameters (M)Full 89.53 188.89 47.16 169.28 47.17Phase 1 (OTOV3) 92.86 50.03 12.44 67.41 12.46Phase 2 (Dynamic Quantization) 92.93 19.39 2.15 7.27 2.17Fig. 17. Samples From the Compressed Model Output on Edge.that OTOV3 efficiently identifies and removes unnecessaryparameters from the models, leading to more streamlinedarchitectures without compromising predictive performance.However, an unexpected observation arose when comparingthe accuracy of the Torch implementation with that of theTIMM implementation of ConvNeXt Small. The Torch im-plementation exhibited lower accuracy compared to its TIMMcounterpart, indicating that OTOV3\u2019s performance regardingaccuracy may be influenced by the details of the modelarchitecture. This unexpected result suggests that differenttraining strategies or adjustments may be necessary to optimizeOTOV3\u2019s performance across various model implementations,emphasizing the importance of considering architectural dif-ferences when applying compression techniques like OTOV3.B. OTOV3 Evaluation on Fine-Tuned ConvNeXt ModelsWhen evaluating OTOV3\u2019s performance on a fine-tunedConvNeXt model, notable improvements in compression per-formance were observed, confirming its effectiveness in re-ducing model size, parameters, and MACs while marginallyenhancing accuracy. This outcome highlights the potential ofintegrating fine-tuning with structured pruning techniques toachieve even greater optimization of model performance. Thefine-tuned model displayed enhanced compression capabilitiescompared to untrained models, suggesting that pre-trainingcan significantly increase the effectiveness of compressiontechniques. This finding highlights the importance of lever-aging pre-existing knowledge within models to maximize thebenefits of compression, ultimately resulting in CNN modelswith higher performance.C. Unstructured Pruning TechniquesDuring the evaluation of l1 unstructured and random un-structured pruning techniques, expected trends were observedregarding accuracy and compression. As the pruning per-centages increased for both linear and convolutional layers,a corresponding decrease in accuracy was noted, while themodel size, parameters, and MACs remained unaltered. Thisoutcome aligns with the inherent nature of unstructured prun-ing, wherein weights are zeroed out but not entirely eliminated,resulting in sparse models without substantial reductions incomputational complexity. However, the lack of significantreductions in computational complexity may constrain their ef-fectiveness, particularly in resource-constrained environmentswhere efficient utilization of computational resources is essen-tial. This highlights the importance of considering the trade-offs between model compression and computational efficiencywhen selecting pruning techniques for deployment in real---- Page 14 ---14TABLE VIOTOV3 AND DYNAMIC QUANTIZATION REDUCTION NUMBERS WITH CONV NEXT SMALL TUNED .Technique AccuracyChange Size Reduction ParametersReductionMACsReductionOTOV3 3.75% 73.51% 73.62% 60.18%Dynamic Quantization -0.15% 71.30% 95.43% 95.71%OTOV3 and DynamicQuantization 3.80% 89.74% 95.43% 95.71%world applications, especially in edge computing scenarioswhere computational resources are limited.D. Dynamic QuantizationDynamic quantization emerged as a highly effective tech-nique for model compression, demonstrating remarkable re-ductions in model size, parameters, and, Your response: ```SUMMARY\nedge device image classification model compression quantization\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\n~~~~~~~~~~\nCurrent Step #16, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 18:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #14, Phase: literature review, Feedback: 3\n--- Page 1 ---1Edge AI: Evaluation of Model CompressionTechniques for Convolutional Neural NetworksSamer Francy, Member, IEEE,Raghubir Singh, Member, IEEEAbstract\u2014This work evaluates the compression techniqueson ConvNeXt models in image classification tasks using theCIFAR-10 dataset. Structured pruning, unstructured pruning,and dynamic quantization methods are evaluated to reduce modelsize and computational complexity while maintaining accuracy.The experiments, conducted on cloud-based platforms and edgedevice, assess the performance of these techniques. Results showsignificant reductions in model size, with up to 75% reduc-tion achieved using structured pruning techniques. Additionally,dynamic quantization achieves a reduction of up to 95% inthe number of parameters. Fine-tuned models exhibit improvedcompression performance, indicating the benefits of pre-trainingin conjunction with compression techniques. Unstructured prun-ing methods reveal trends in accuracy and compression, withlimited reductions in computational complexity. The combinationof OTOV3 pruning and dynamic quantization further enhancescompression performance, resulting 89.7% reduction in size, 95%reduction with number of parameters and MACs, and 3.8%increase with accuracy. The deployment of the final compressedmodel on edge device demonstrates high accuracy 92.5% and lowinference time 20 ms, validating the effectiveness of compressiontechniques for real-world edge computing applications.Index Terms\u2014edge AI, ConvNeXt, CNN, pruning, quantiza-tion, compression, OTO.I. I NTRODUCTIONEDGE devices such as Internet of Things (IoT) are becom-ing increasingly important and widely used in our dailylives and industrial facilities. IoT is a network of things thatempowered by sensors, identifiers, software intelligence, andinternet connectivity, it can be considered as the intersectionof the internet, things/objects (anything/everything), and data[1]. The number of these devices is expected to increase evenmore [2]. These devices have the potential to perform complexArtificial Intelligence (AI) tasks locally, without relying heav-ily on cloud infrastructure [3]. The rapid advancement of AIhas led to the development of complex deep learning modelsthat show high performance in different domains. DeployingAI models on edge devices has many advantages such as lowlatency, privacy and data security, bandwidth optimization, andreduced network dependence. Low latency is achieved due toreal-time processing by instant data analysis on edge withoutwaiting for remote server processing, this data analysis on theedge reduces transmitting data to the cloud which enhancessecurity against breaches, reduces the bandwidth consumption,and reduces network dependence.Samer Francy is with the Department of Computer Science, University ofBath, Bath, England (e-mail: samer.francy@bath.edu).Raghubir Singh is with the Department of Computer Science, Universityof Bath, Bath, England (e-mail: rs3022@bath.ac.uk).A. Overview of Edge AIEdge AI represents a paradigm shift in the way AI isimplemented in the context of the IoT. It capitalizes on thecapabilities of IoT devices, enhancing real-time processing,analytics, and decision-making directly at the edge of thenetwork. The IoT architecture, which is the foundation forEdge AI, typically involves three core layers [1]. The layersare perceptual layer, where data is collected from varioussensors and devices, network layer, where data is transmittedand routed through this layer, which is responsible for commu-nication between devices and cloud services, and applicationlayer, which processes and utilizes the data, providing insightsand enabling actions.B. Convolutional Neural Networks (CNNs)CNN models are subsets of Deep Neural Networks (DNN)models. CNN models are effective for image and video-relatedtasks due to their ability to learn relevant features from thedata by recognizing patterns, shapes, and structures in images,which is challenging for traditional machine learning models,that\u2019s why they are used for computer vision tasks such asimage classification, object detection, and image segmentation[4].1) CNN Architecture: In general CNN models consist ofbelow parts:\u2022 Input Image : Pixels form the binary basis of computerimages, while the human visual system operates throughneurons with receptive fields. Similarly, CNNs functionwithin their receptive areas, starting with simple patternsand advancing to more complex ones, making CNNs apromising tool for computer vision [4].\u2022 Convolutional Layer : A convolutional layer in a CNNuses a small filter (e.g., 3x3 or 5x5) that slides over theinput image. At each position, it multiplies its valueswith the overlapping image pixels and sums the results toproduce an output. This sliding operation helps identifylocal features like edges and colors, building a hierar-chical representation. The depth of the filter matches theinput image\u2019s channels (e.g., 3 for RGB images). Stackingmultiple filters allows the network to learn features atdifferent abstraction levels [4].\u2022 Pooling Layer : Pooling reduces the spatial size of fea-ture maps. This not only lowers computational demandsbut also helps in extracting position and orientation-independent features essential for training. Two com-mon pooling methods are maximum pooling and averagepooling. In maximum pooling, a small kernel (e.g., 2x2)arXiv:2409.02134v1  [cs.LG]  2 Sep 2024--- Page 2 ---2Fig. 1. Unbalanced Demand For Computation (Left) and Memory (Right) in AlexNet [5].selects the maximum value within its range and places itin the output. In average pooling, a similar-sized kernelcomputes the average value within its area for eachchannel, maintaining the same depth. Pooling simplifiescomputation and weight requirements, with maximumpooling being the most commonly used method [4].\u2022 Activation Function : The activation function, applies amathematical operation to the filter\u2019s output to concludethe output of the network. The common choice is the Rec-tified Linear Unit (ReLU). They fall into two categories,linear and non-linear activation functions [4].\u2022 Fully Connected Layer : It functions as a feed-forwardneural network (NN) typically situated at the network\u2019slower layers. It receives input from the output of the lastpooling or convolutional layer, which is flattened into aone-dimensional vector, enabling it to learn complex datarelationships [4].2) Computation and Memory Demands: In CNNs, unbal-ance exists in resource demands between the layers. Convolu-tional layers primarily serve as feature extractors and heavilydominate the computational workload. In the case of AlexNet,for instance, the convolutional layers account for just 2 millionweights but demand a substantial 1.33 Giga Operations PerSecond (GOPS) of computation. In contrast, fully connectedlayers function as classifiers, accumulating information forhigh-level decisions, and bear the weight of the network witharound 59 million parameters, yet they contribute significantlyless to computation, requiring only 0.12 GOPS. This obviouscontrast in resource allocation (Figure 1) highlights the unbal-anced demands between these two layers in CNNs [5].3) Key CNN Architectures:In 1989, the use of a NN archi-tecture with convolutional layers for recognizing handwrittendigits in the context of ZIP code recognition was introduced[6], That architecture consisted of input layer, 3 hidden layers,and output layer. Since then, CNN models have developed(Figure 2) and became much deeper.4) CNN on Edge: Deploying CNN models on edge hasa wide range of practical and industrial applications acrossvarious sectors. Here are some specific examples:\u2022 Surveillance and Security : It can perform real-time ob-ject detection and facial recognition for security monitor-ing, identifying intruders, and managing access control.Face identification was deployed using VGGFace [7].Video analysis was deployed using YOLOX [8]. Infraredand Visible Image Fusion for security systems was de-ployed using DenseNet [9]. Human action recognitionapplications were deployed using ResNet [10].\u2022 Manufacturing and Quality Control : It can inspectproducts on assembly lines for defects, ensuring qualitycontrol and minimizing errors. Real-time detection ofsteel strip surface defects was deployed using Faster R-CNN model [11].\u2022 Agriculture: Drones can monitor crops, detect pests,diseases, and nutrient deficiencies, enabling precisionagriculture. Identifying rice leaf diseases in natural en-vironments was deployed using GoogLeNet [12]. Pepperleaf disease identification was deployed using GoogLeNet[13]. Detection for insect pests was deployed on YOLOX[14].\u2022 Healthcare and Wearables : Wearable devices can con-tinuously monitor vital signs, detect anomalies, and evendiagnose certain health conditions. Medical diagnosis(Covid and Lung Disease Detection) was deployed usingVGG, MobileNet, and AlexNet [15]. Automatically diag-nose pneumonia and COVID-19 from chest X-ray imageswas deployed on DenseNet [16]. Medical applications(e.g., COVID-19 detection, cardiomegaly diagnosis, braintumor classification) were deployed using ResNet [10].\u2022 Energy Management: It can monitor energy usage, opti-mize consumption patterns, and identify areas for energyefficiency improvements. Wind Turbine Maintenance andfault diagnosis was deployed using AlexNet [17].\u2022 Environmental Monitoring : It can monitor air quality,pollution levels, and weather conditions, providing valu-able insights for urban planning. A smartphone app toperform fine-grained classification of animals in the wildwas deployed using AlexNet, GoogLeNet, and ResNet[18]. Identification of mosquito species was deployedusing AlexNet, DenseNet, Inception, ResNet, and VGG[19].--- Page 3 ---3Fig. 2. Evolution of Key CNN Architectures Over Time.\u2022 Logistics and Inventory Management : It can automatepackage sorting, inventory tracking, and warehouse man-agement. Mobile robot to map its surroundings while de-tecting objects and people was deployed using AlexNet,GoogLeNet, and ResNet [18].\u2022 Autonomous Vehicles : Vehicles can process real-timedata from cameras and sensors using CNNs, aiding inautonomous navigation and collision avoidance. Instanceobjects detection system for intelligent service robots wasdeployed using Alexnet [20]. Advanced driving assistancesystems (ADASs) and automated vehicles (A Vs) weredeployed using Faster R-CNN [21].Deploying CNNs on edge devices presents significant chal-lenges mainly due to the limited computational resources,constrained memory, and power consumption constraints in-herent to these devices. CNN models, known for their depthand complexity, often demand substantial computational powerand memory, which may exceed the capabilities of edgehardware. Hence, compressing the model before deploymentbecomes imperative. Model compression techniques aim toreduce the size of the CNN model while preserving its perfor-mance, thereby enabling efficient utilization of computationalresources and memory on edge devices. By compressingthe model, we can mitigate the challenges associated withdeploying CNNs on edge devices, ensuring that they caneffectively perform tasks such as real-time image processing,object detection, and classification within resource-constrainedenvironments.With the enormous number of compression techniques pro-posed for CNNs, the rapid evolution of CNN architectures hascreated a gap in the field. This dynamic shift in architecturedesign requires an evaluation of existing compression methods,particularly in light of the demand to make these advancedCNN models suitable for deployment on edge devices. AsCNN designs continue to advance, the challenge lies inadapting compression techniques to smoothly integrate withthese modern architectures. This evaluation (either for eachindividual techniques or combined with each other) becomesimportant, as it not only ensures the continued relevance ofcompression techniques but also addresses the urgent need tomake resource-intensive CNN models accessible and deploy-able on edge devices.This work aims to evaluate CNN compression techniquesthat assure appropriate performance on edge devices. In thesubsequent sections, this work reveals in a structured mannerto evaluate the compression techniques for CNN models.section 2 provides a detailed review of related work, offeringinsights into prior research and establishing a foundationalunderstanding of the topic. section 3 explains the methodologyemployed in conducting the experiments, describing the designand execution of the study. Following this, section 4 presentsthe experimental results and analyzes the findings to recognizetrends and implications. Section 5 critically evaluates the re-sults. Section 6 draws conclusions regarding the effectivenessand significance of the compression techniques. This organizedframework aims to comprehensively explore and contribute tothe field of model compression for efficient deployment inresource-constrained environments.II. R ELATED WORKWithin the context of edge AI, it is important to address thecritical need for model compression. The resource constrainednature of these devices requires more efficient AI modelsby minimizing memory and computational demands, ensur-ing faster inference speeds, and enhancing energy efficiency.Below will explore various model compression techniques andtheir implications for edge AI applications.A. PruningPruning is a key technique in DNN, aimed at enhancingefficiency and model generalization. It involves the removalof redundant components, such as parameters, neurons, filters,or entire layers, leading to several advantages. By reducingunnecessary parameters, it cuts down on storage require-ments, and important for models deployed on devices withlimited memory. Furthermore, it streamlines computational--- Page 4 ---4Fig. 3. Weight Pruning (a) and Neuron Pruning (b). x: input, w: weight. [22].complexity during inference, resulting in faster predictions andlower power consumption. Pruning also mitigates overfittingby simplifying the network. Various pruning techniques, likeweight pruning, neuron pruning, filter pruning, and layerpruning, offer different levels of granularity in componentremoval. Whether applied during or after training, pruningenables the creation of more compact and efficient CNNmodels tailored to specific needs, effectively balancing modelsize, computational efficiency, and accuracy. Weight pruningsets weight connections in the network to zero if they fallbelow a predefined threshold or are considered redundant.Neuron pruning focuses on removing entire neurons if they arefound to be redundant. Layer pruning allows for the removalof entire layers that are considered less important [22].1) Pruning For Fully Connected Layer: Fully connectedlayers are dense that makes the layer with high memorydemand. Pruning them effectively reduces the memory burdenand reduce size of the model.It involves selectively removing weight connections andneurons to reduce the model\u2019s complexity while preservingperformance. In a typical feed-forward NN, inputs are multi-plied by corresponding weights, and a linear sum is computedat each neuron, which is then transformed by an activationfunction. As shown in Figure 3, a network with 3 inputneurons, 2 hidden neurons, and 1 output neuron may havemultiple weight connections. Pruning can be applied to elim-inate specific weight connections or entire neurons. By doingso, the total number of weight connections can be significantlyreduced, leading to a more compact network. The concept ofpruning was first introduced by [6], who proposed removingweights based on their saliency, with small-magnitude weightshaving less impact on training error. The process involvesiterative retraining to regain accuracy, and the technique isknown as \u2019Optimal Brain Damage (OBD)\u2019 where the secondderivative of the objective function with respect to parametersis used to calculate the small saliency, facilitating informedpruning decisions. Since then, other pruning approaches havebeen introduced for fully connected layers [22].2) Pruning For Convolutional Layer: Each convolutionallayer typically consists of numerous filters that makes the layerwith high computational demand. Pruning these less significantfilters directly from the convolutional layer effectively reducesthe computational burden and speeds up the model. Inspired byearly pruning methods, new approaches have been introducedto be used to prune convolutional layers [22].Bayesian was used to decide what to prune and the level ofpruning, in this context involves employing scale mixtures ofnormals as priors for parameter weights in LeNet and VGG[23]. Differential evolution based layer-wise weight methodalongside three other pruning techniques (Naive Cut, Itera-tive Pruning, and Multi-Objective NN Pruning) was used toprune LeNet, AlexNet, and VGG16 [24]. Two fully connectedlayers are removed from the AlexNet architecture, and BatchNormalization (BN) is introduced to mitigate overfitting [20].Filters Similarity in Consecutive Layers (FSCL) for CNNs wasused to reduce the number of filters while preserving impor-tant filters, ultimately improving model efficiency for VGG,GoogLeNet, and ResNet [25]. Structured pruning throughsparsity-induced pruning was used to enhance the real-timeimplementation of the DEtection TRansformer (DETR) [26].Structured pruning was used to compress YOLOX, this processincluded sparse training to prune unimportant channels, withfine-tuning to recover accuracy [14]. Evolutionary approachto filter pruning involved sequential application of multiplepruners in a specific order to sparsify LeNet and VGG-19while maintaining model accuracy [27]. Multilayer networkswere used to represent and compress ResNets, it involvedcreating class networks, calculating arc weights, and forminga multilayer network. The overall degree of nodes in themultilayer network is used to select a subset of nodes forcompression, and convolutional layers are pruned [10]. Tooptimize the Fused-DenseNet-Tiny model for efficient detec-tion of COVID-19 and pneumonia in chest X-ray images,three steps were implemented including removing insignif-icant weights, discarding pruning casings, and applying acompression algorithm [16]. Deep Scalable Zerotree-based(DeepSZ) framework was used to address resource constraints--- Page 5 ---5by achieving significant compression for LeNet, AlexNet,and VGG while maintaining acceptable inference accuracy[28]. Compressing without retraining that was used withResNet, AlexNet, VGGNet and SqueezeNet. It focused onconvolutional and fully connected layers, while maintainingor improving classification accuracy [29].B. QuantizationQuantization plays an important role in addressing theresource-intensive nature of CNNs. By reducing the bit pre-cision of model parameters, quantization not only conservesmemory and energy but also enhances inference speed, makingit an essential technique for deploying CNNs in resource-constrained environments such as edge devices. Weight clus-tering takes quantization to a more advanced level by orga-nizing weights into clusters, where each cluster shares thesame weight value. This approach minimizes the need forfine-tuning individual weights and can lead to substantialreductions in memory and computational overhead [22].Single Level Quantization (SLQ) and Multiple Level Quan-tization (MLQ) technique were used to quantize AlexNet,VGG, GoogleNet, and ResNet to the deployment of thesemodels on resource-constrained mobile devices like mobilephones and drones [30].C. Low-Rank Decomposition/FactorizationIt is a compression technique used with feed-forward NNsand CNNs, to reduce the size of weight matrices whilepreserving model performance. Singular Value Decomposition(SVD) is a popular factorization scheme that decomposes aweight matrix A into three smaller matrices: U, S, and V T . Urepresents the left singular vectors, S is a diagonal matrix ofsingular values, and V T is the transpose of the right singularvectors. This factorization offers several advantages, such asreduced storage requirements, which is crucial for memory-constrained environments, and accelerated inference, espe-cially in CNNs, as smaller matrices can be convolved faster.Low-rank factorization can be applied to fully connected andconvolutional layers, making models more storage-efficientand faster without sacrificing performance. Careful selectionof the rank is essential for achieving a balance between sizereduction and model accuracy. Later, more approaches havebeen introduced [22].Tucker decomposition for weight tensors was used tooptimizes weight tensor dimensions of LeNet and ResNetmodels [31]. Low-rank decomposition was used as an efficientmethod for compressing AlexNet, VGG, and ResNet withoutthe need for fine-tuning to significantly reduce model sizeand computational complexity to make them more suitablefor resource-constrained mobile and embedded devices [32].Hardware-Aware Automatic Low-Rank Compression frame-work HALOC was used to compress ResNet, VGG andMobileNet, with the goal of efficiently exploring the structure-level redundancy in NNs by integrating principles from neuralarchitecture search (NAS) [33]. Automatically DifferentiableTensor Network (ADTN) method was used to significantlyreduce the number of parameters of fully connected NN,LeNet, and VGG while maintaining or enhancing the per-formance [34]. Joint Matrix Decomposition, specifically JointSVD (JSVD) was used to address the challenge of deployingResNet with numerous parameters on resource-constrainedplatforms. It included Right JSVD, Left JSVD, and BinaryJSVD algorithms [35]. Tensor Ring Networks (TR-Nets) wasused as a method to effectively factorize LeNet and ResNet,thereby reducing computational and memory requirements[36]. Tucker decomposition with rank selection and fine tuningwas used as a one-shot whole network compression schemefor deploying AlexNet, VGG, and GoogLeNet on mobiledevices while maintaining reasonable accuracy [37]. TensorDynamic Low-Rank Training (TDLRT) was used to createa training algorithm with VGG and AlexNet that maintainshigh model performance while significantly reducing memoryrequirements for convolutional layers [38].D. Knowledge Distillation (KD)It is a technique used to transfer the knowledge learnedby a larger, more complex model (the teacher model) to asmaller and lighter model (the student model). The primarygoal of KD is to enable the student model to benefit from thegeneralization capabilities of the teacher model while beingmore lightweight in terms of parameters and computations.This technique helps to recover the accuracy drop occurs dueto implementing other compression techniques.Knowledge transfer and distillation, initially introduced by[39], aimed to compress large ensemble models into smaller,faster counterparts with minimal performance loss. [40] ex-tended this concept by empirically demonstrating that theintricate knowledge within larger DNNs could be effectivelytransferred to smaller, shallower models, yielding comparableaccuracy. This involved training a large DNN and transfer-ring its knowledge to a shallower network while minimizingthe squared difference between the logits produced by thetwo models. These foundational ideas produced knowledgedistillation, a widely used technique for training efficientmodels by transferring knowledge from larger ones. Later,more approaches have been introduced [22].KD was used to improve the compression of LeNet andResNet models when fresh training data is scarce, primarilythrough the use of synthetic data generated by GenerativeAdversarial Networks (GANs) [41]. To fuse information frominfrared and visible images while reducing DenseNet complex-ity and improving inference speed. Insights from pre-trainedteacher models are transferred to the smaller student model[9]. KD was used to develop a lightweight mosquito speciesidentification model (EfficientNet) that balances efficiency andaccuracy through the compression [42].E. Mixed TechniquesDifferent compression techniques are often combined andused together to achieve more effective and comprehensivemodel compression. Each compression technique targets spe-cific aspects of the model, such as reducing model size,computation complexity, or memory footprint.--- Page 6 ---6In-Parallel Pruning-Quantization CLIP-Q method combinesnetwork pruning and weight quantization was used to com-press AlexNet, GoogLeNet, and ResNet [18]. Pruning andquantization were used to optimize the compression ofAlexNet and reduce the number of parameters significantlywhile maintaining accuracy to be implemented on Field-Programmable Gate Array (FPGA) [5]. Pruning, quantization,and Huffman encoding combined with adversarial trainingwere used to enhance the robustness and compression ofAlexNet while also addressing the model vulnerability toadversarial attacks [43]. Pruning and quantization were usedto compress VGG and ResNet for remote sensing imageclassification, balancing computational complexity constraintswhile preserving model accuracy [44]. Low-rank decompo-sition and quantization were used to compress ResNet andMobileNet, and reduce the computational complexity whilepreserving high performance [45]. Pruning, quantization, andchanging the model architecture were used to design a com-pact SqueezeNet with competitive accuracy while significantlyreducing the number of parameters [46]. Quantization andpruning were used to develop an effective model compressionframework for ResNet and MobileNet. The objective was tooptimize the allocation of compression ratios to minimizeperformance degradation while reducing model size [47]. Jointquantization and pruning were used to develop a post-trainingmodel size compression method that efficiently combines lossyand lossless compression techniques to reduce the size ofResNet, MobileNet, RegNet, MNasNet, and YOLOv5 withoutsacrificing accuracy [48].F . Other TechniquesDepthwise separable convolutions was used to improvesteel strip defect detection by creating a real-time and ef-ficient model while maintaining high accuracy using FasterR-CNN [11]. Deferential Evolution was used to develop anefficient and optimized AlexNet, VGG, and MobileNet forCovid and liver disease detection [15]. Genetic Algorithmwas used to reduce the storage space and inference timeof VGG, ResNet, AlexNet, and SqueezeNet models [49].Factorization (changing kernel size) was used to improvethe accuracy and computing efficiency of pepper leaf diseasedetection using GoogLeNet, specifically for the agriculturalindustry [13]. Flexible and Separable Convolution (FSConv)was used to reduce computational costs without compromisingthe accuracy of VGG, ResNet, Faster R-CNN and RetinaNet[50]. Efficient Layer Compression (ELC) was used to enhancethe computational efficiency of VGG, ResNet, and ConvNeXtwhile preserving their representation capabilities [51].III. D ESIGN OF THE EXPERIMENTSThe experiments aimed to evaluate various compressiontechniques, namely pruning and quantization, on differenttypes of ConvNext [52] model. The experiments includedtraining, fine-tuning, and evaluating of models using CIFAR-10 dataset. The setup involved conducting experiments bothon cloud-based platforms and on edge devices to evaluate theperformance of the compressed models.Fig. 4. Block modifications and resulted specifications. (a) is a ResNeXtblock; in (b) we create an inverted bottleneck block and in (c) the positionof the spatial depthwise conv layer is moved up [52].A. ConvNeXt [52]Is a modern CNN family produced as a journey of graduallymodernize a standard ResNet toward the design of a visionTransformer. The journey starts from a ResNet-50 model, intoa CNN architecture that mirrors some aspects of Transformers,particularly Swin Transformers. The roadmap:1) Training Techniques: Vision Transformer training pro-cedures were used to train ResNet-50 model, this includedextending the training to 300 epochs (90 epochs originally),using AdamW optimizer, and data augmentation techniques(Mixup, Cutmix, RandAugment, Random Erasing, and regu-larization schemes including Stochastic Depth).2) Macro Design: Number of blocks in each stage wasadjusted from (3, 4, 6, 3) to (3, 3, 9, 3) and the stem wasreplaced with a patchify layer implemented using a 4x4, stride4 convolutional layer (non-overlapping convolution).3) ResNeXt-ify: ResNeXt approach was adopted which isutilize grouped convolutions, where convolutional filters aredivided into groups, each handling a subset of input channels,a variation of grouped convolution known as depthwise convo-lution was adopted, and the network\u2019s width was expanded byincreasing the number of channels in the convolutional layers.4) Inverted Bottleneck: The hidden dimension of the MLPblock was changed to be four times wider than the inputdimension as shown in Figure 4 (a and b)5) Large Kernel Sizes: The position of the convolutionallayer is moved up and the kernel size was changed from (3x3)to (7x7) as shown in Figure 4 (a and c).B. Micro DesignReplacing ReLU with Gaussian Error Linear Unit (GELU),fewer normalization layers, Substituting Batch Normalization(BN) with Layer Normalization (LN), and introducing separatedownsampling layers as shown in Figure 5.C. Compression Techniques1) Pruning: Different pruning techniques have been usedincluding structured and unstructured techniques.\u2022 Only Train Once (OTO) [53]: OTO version 3 (OTOV3)is automated framework for structured pruning which in-volves removing entire structures or groups of parametersfrom a DNN. OTOv3 begins by analyzing the depen-dencies between the vertices of the target DNN. Thisanalysis involves identifying accessory, Shape-Dependent--- Page 7 ---7Fig. 5. Block designs for a ResNet, a Swin Transformer, and a ConvNeXt.Swin Transformer\u2019s block is more sophisticated due to the presence of multiplespecialized modules and two residual connections [52].(SD) joint, and unknown vertices that are adjacent andestablishing their interdependencies. The goal is to formnode groups based on these dependencies, laying thefoundation for identifying interdependent vertices duringstructured pruning.Using the information gathered from the dependencyanalysis, OTOv3 constructs a pruning dependency graph.This graph represents the interdependencies between ver-tices, with vertices in the same node group indicatingtheir interdependency during structured pruning. Thepruning dependency graph ensures the validity of theproduced subnetwork by preserving essential connectionsbetween vertices.OTOv3 partitions the trainable variables of the DNNinto Pruning Zero-Invariant Groups (PZIGs) based on thepruning dependency graph. PZIGs consist of pairwisetrainable variables grouped together, with each grouprepresenting a potential pruning structure. Node groupsadjacent to the DNN output and containing unknownvertices are excluded from forming PZIGs to preserveoutput shapes and ensure model robustness as shown inFigure 6.To jointly search for redundant pruning structures andtrain the remaining groups for optimal performance,OTOv3 employs the Dual Half-Space Projected Gradient(DHSPG) algorithm. DHSPG minimizes the objectivefunction while introducing a sparsity constraint to identifyredundant groups for removal. It employs saliency-drivenredundant identification and a hybrid training paradigmto control sparsity and achieve better generalization per-formance as shown in Figure 7.\u2022 L1 Unstructured [54] L1 unstructured pruning is atechnique used in machine learning, to reduce the sizeof neural networks by eliminating less important con-nections. Each weight in the network is assigned a scorebased on its magnitude. This score reflects the importanceof the weight in the network\u2019s performance. In l1 pruning,this score is often the absolute value of the weight.A threshold is set, typically by selecting the top x% ofweights based on their magnitude scores. The thresholddetermines which weights will be pruned and which willbe retained.Weights that fall below the threshold are pruned, meaningthey are set to zero and effectively removed from thenetwork. This results in a sparser network architecturewith fewer connections.\u2022 Random Unstructured [55] Similar to l1 unstructuredpruning, random unstructured pruning is also a techniqueused in machine learning, to reduce the size of neuralnetworks by eliminating less important connections. Thedifference is the pruned weight are selected randomlyinstead of using l1 to decide the importance of theweights.2) Dynamic Quantization [30]:Dynamic quantization is anapproach aimed at optimizing the deployment of neural net-works by reducing the precision of the weights. Unlike tradi-tional quantization methods that apply a fixed quantization bit-width across all layers of the network, dynamic quantizationadapts the quantization bit-width for each layer individuallybased on its representation abilities and capacities. This isachieved through the use of a bit-width controller module,which employs a policy gradient-based training approach tolearn the optimal bit-width for each layer. By dynamicallyadjusting the quantization bit-width, dynamic quantization canstrike a balance between maintaining accuracy and reducingmemory size and computational costs.D. CIFAR-10 [56]CIFAR-10 is a dataset used for computer vision and ma-chine learning research, offering a rich resource for trainingand evaluating image classification algorithms. Comprising60,000 32x32 RGB color images across 10 distinct classes(Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse,Ship, and Truck), CIFAR-10 facilitates comprehensive explo-ration of diverse visual concepts. With each class containing6,000 images and a balanced distribution across the dataset,CIFAR-10 presents a well-structured foundation for model de-velopment. Its division into 50,000 training images and 10,000test images, further segmented into multiple batches, enablesstrict evaluation and benchmarking of algorithms. In terms ofcomputational requirements, CIFAR-10 generally requires lesscomputation compared to CIFAR-100 and ImageNet due to itssmaller image size and fewer classes which makes it suitablefor experiments with limited computational resources.E. Experiment SetupTwo types of experiments have been conducted, cloud-basedexperiments that focused on compressing the models and eval-uating the techniques and edge-based experiment experimentto evaluate the performance of one of the compressed models.--- Page 8 ---8Fig. 6. Automated PZIG Partition [53].Fig. 7. Search Direction in DHSPG [53].1) Cloud-based Experiment Setup: Google Colab Pro+was used to utilize GPU resources (NVIDIA A100 and V100Tensor Core GPUs), facilitating accelerated model trainingand evaluation and background execution. The integrationwith Google Drive reduced overheads associated with upload-ing and downloading model data to and from cloud. Theevaluation framework was designed to profile the originalmodel, compress it, profile the compressed model, and conductcomparison between the measurements before and after thecompression as shown in Figure 8.This profiling process involved measuring several key met-rics:\u2022 Accuracy: The classification accuracy achieved by themodel on the validation dataset.\u2022 Model Size: The size of the model in megabytes (MB).\u2022 Number of Parameters : The total count of trainableparameters in the model, measured in millions (M).\u2022 Number of MACs : The number of multiply-accumulateoperations performed during inference, measured in mil-lions (M).\u2022 Number of Non-Zero Parameters : The count of non-zero parameters in the model, essential for pruning-basedtechniques.2) Edge-based Experiment Setup: A compressed modelwas deployed on edge with CPU (11th Gen Intel(R) Core(TM)i7-1165G7 @ 2.80GHz 2.80 GHz), RAM (16GB), and laptopintegrated camera.2 samples from each of CIFAR-10 classes have been se-lected randomly from the internet, printed on A4 papers, andplaced in front of the camera to measure the accuracy and theinference time.--- Page 9 ---9Fig. 8. Compression Evaluation Framework.TABLE IOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT TINY, SMALL , BASE AND LARGE .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)Tiny Full 63.81 106.26 26.53 86.88 18.41Compressed 63.81 41.31 10.29 47.80 10.30Small Full 63.48 188.89 47.16 169.28 29.55Compressed 63.48 48.04 11.94 68.24 11.96Base Full 61.22 334.28 83.50 299.20 52.22Compressed 61.22 84.12 20.96 119.40 20.98Large Full 63.40 748.82 187.12 669.30 116.93Compressed 63.40 187.32 46.75 264.69 46.78IV. R UNNING THE EXPERIMENTS AND EXPERIMENTALRESULTSA. Cloud-Base Experimentsdifferent experiments have been conducted on cloud to eval-uate different compressing techniques and different versions ofConvNeXt model.1) Evaluate OTOV3 on Untrained Torch ConvNext Tiny,Small, Base, and Large: Untrained ConvNeXt tiny, small,base, and large have been imported from Torch and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and 200epochs were used for training and pruning. OTOV3 achievedhigh performance (Table I) with reducing the model size(61% for tiny and 75% for small, base, and large), numberof parameters (61% for tiny and 75% for small, base, andlarge), and MACs (45% for tiny and 60% for small, base,and large) as shown in Figure 9. Meanwhile OTOV3 wasable to increase both the full and compressed model accuracythrough the training and pruning without any accuracy dropafter pruning comparing to the full model.To investigate the effect of OTOV3 on the model archi-tecture, a comparison has been conducted between ConvNeXtsmall before and after compression. The Torch implementationof the model consist of many CNBlocks, each CNBlock con-sist of Conv2d, Permute, LayerNorm, Linear, GELU, Linear,and Permute layers. As shown in Figure 10, OTOV3 reducednumber of output features of the Linear layer (sequence 3)and the input features of the next Linear layer (sequence 5)and considering the big number of CNBlock in the modelarchitecture, the reduction in model size and number ofparameters after compression is justified as shown in TableII.2) Evaluate OTOV3 on Untrained ConvNext Small (Torchvs. TIMM): Two untrained ConvNeXt small have been im-ported, one from Torch and one from TIMM [57] and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and200 epochs were used for training and pruning. Although thecompression performance was same with size reduction (75%)but the accuracy after 200 epochs was less for Torch model(63%) comparing to TIMM model (73%) as shown in Figure11.To investigate the accuracy performance of OTOV3 withTorch and Timm ConvNeXt Small, a comparison has beenconducted between the two model architectures. The Torchmodel uses the CNBlock structure, which includes additionaloperations such as Permute and varying StochasticDepth prob-abilities. The TIMM model follows a simpler structure withConv2d and LayerNorm, lacking the additional complexitiesintroduced by CNBlock and associated operations in theTorch model as shown in Figure 12 which effects OTOV3performance regarding the accuracy.3) Evaluate OTOV3 on Fine-Tuned Torch ConvNext Small:A pre-trained ConvNeXt small have been imported from Torchand fine-tuned on CIFAR-10 with 100 epochs, the accuracyreached 89.5%. This fine-tuned ConvNeXt small will be usedfor the rest of cloud-base experiments. This model was usedto evaluate OTOV3, CIFAR-10 was used for training andevaluation, and 200 epochs were used for training and pruning.OTOV3 achieved high performance (Table III) 74% reductionin model size and number of parameters, 60% reduction inMACs, and 3.8% increase with accuracy as shown Figure13. The accuracy of the full model in (Table III) (92.86%)is different that the accuracy of the original model used inthe experiment (89.5%), that because OTOV3 trained the full--- Page 10 ---10Fig. 9. OTOV3 Compression Performance with ConvNeXt Tiny, Small, Base and Large.Fig. 10. Comparison Between ConvNeXt Small Full (Left) and Compressed (Right).Fig. 11. OTOV3 Comparison Between Torch and Timm ConvNeXt Small.model during the process which increased the model accuracy.4) Evaluate Unstructured Pruning: The Fine-tuned Con-vNext Small was used to evaluate Pytorch L1 UnstructuredPruning and Random Unstructured Pruning by using differentcombinations of weights pruning percentages for linear (.1 to.9) and convolutional (.1 to .9) layers. In both experiments,the accuracy and the number of non-zero parameters weredropping as the values of weights pruning percentages for bothlinear and convolutional amounts were increasing as shown inFigure 14 a and b. Although the accuracy dropped but themodel size, number of parameters, and MACs didn\u2019t changeas these techniques zero the weights instead of removing them.5) Evaluate Dynamic Quantization: The Fine-tuned Con-vNext Small was used to evaluate Pytorch dynamic quantiza-tion, 8-bit integer was used during the experiment and CIFAR-10 was used for evaluation. Dynamic quantization achievedhigh performance (Table IV) 71% reduction in model size,95% reduction with number of parameters and MACs, and0.1% drop with accuracy as shown in Figure 156) Evaluate Combination of OTOV3 and Dynamic Quan-tization: Two compression stages were evaluated together asone compression pipeline, OTOV3 and Dynamic Quantization.The compressed ConvNeXt small model that was produced inexperiment 4.1.3 (OTOV3) was furtherly compressed using--- Page 11 ---11Fig. 12. Comparison Between ConvNeXt Small TIMM (Left) and Torch (Right).Fig. 13. OTOV3 Compression Performance with ConvNeXt Small Tuned.Pytorch dynamic quantization using 8-bits integer (similar toexperiment 4.1.5). The pruned models using OTOV2 [58]had dependencies on Open Nural Network Exchange (ONNX)[59] which made it not applicable to be combined with othercompression technique like quantization. In OTOv3, there wasengineering changes produced the pruned model directly inPytorch format, which enhanced the flexibility to be combinedwith quantization as this experiment did [53].Pruning using OTOV3 and Quantization using Pytorchdynamic quantization achieved high performance (Table V)89.7% reduction in model size, 95% reduction with numberof parameters and MACs, and 3.8% increase with accuracy asshown in 16.B. Edge-Base ExperimentThe final compressed ConvNeXt Small model in experimentIV .A.6 (OTV3 and Dynamic Quantization) was deployed onedge and the printed samples used to measure the accuracyand inference time by placing them in front of the camera.The compressed model achieved 92.5% accuracy and 20msinference time. Figure 17 shows samples of the output.V. A NALYSIS OF EXPERIMENTAL RESULTSHere, the experimental results obtained from running aseries of experiments will be analyzed, the experiments aimedat evaluating various compression techniques applied to Con-vNeXt models. The experiments were designed to evaluate theperformance of techniques such as pruning and quantizationon different sizes of ConvNeXt models, with a focus on bothcloud-based compression experiments and edge-based deploy-ment experiment. The analysis will be conducted with respectto the work\u2019s aim of evaluating CNN compression techniquesthat assure appropriate performance (size and inference time)on edge devices and resource-constrained environments.A. OTOV3 Evaluation on Untrained ConvNeXt ModelsThe evaluation of OTOV3 on untrained ConvNeXt modelsdemonstrated its effectiveness in achieving substantial com-pression while increasing model accuracy for both full andcompressed models. Across varying sizes of ConvNeXt mod-els, OTOV3 consistently produced impressive reductions inmodel size, number of parameters, and MACs, highlighting itsability to prune redundant structures effectively. This suggests--- Page 12 ---12Fig. 14. Unstructured Pruning Performance with ConvMeXt Small.TABLE IICOMPARISON FOR NUMBER OF OUTPUT FEATURES AND INPUTFEATURES OF THE LINEAR LAYERS IN THE CNB LOCKS BEFORE ANDAFTER OTOV3 C OMPRESSION .Layer Index Input & Output FeaturesSequential CNBlock Full Model CompressedModel1 0 384 3841 1 384 3841 2 384 3843 0 768 7673 1 768 7043 2 768 7265 0 1536 2515 1 1536 3495 2 1536 2425 3 1536 3785 4 1536 2935 5 1536 3775 6 1536 3405 7 1536 4005 8 1536 3945 9 1536 4785 10 1536 4145 11 1536 4245 12 1536 4105 13 1536 3185 14 1536 4885 15 1536 4885 16 1536 4025 17 1536 2465 18 1536 4025 19 1536 4585 20 1536 3235 21 1536 4195 22 1536 4465 23 1536 4445 24 1536 4415 25 1536 4685 26 1536 10707 0 3072 2087 1 3072 2547 2 3072 262Fig. 15. Dynamic Quantization Compression Performance with ConvNeXtSmall Tuned.Fig. 16. OTOV3 and Dynamic Quantization Compression Performance withConvNeXt Small Tuned.--- Page 13 ---13TABLE IIIOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 92.86 188.89 47.16 169.28 29.80Compressed 92.86 50.03 12.44 67.41 12.46TABLE IVDYNAMIC QUANTIZATION COMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 89.53 188.89 47.16 169.28 47.17Compressed 89.40 54.21 2.15 7.27 2.17TABLE VOTOV3 AND DYNAMIC QUANTIZATION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy % Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number of NonZeroParameters (M)Full 89.53 188.89 47.16 169.28 47.17Phase 1 (OTOV3) 92.86 50.03 12.44 67.41 12.46Phase 2 (Dynamic Quantization) 92.93 19.39 2.15 7.27 2.17Fig. 17. Samples From the Compressed Model Output on Edge.that OTOV3 efficiently identifies and removes unnecessaryparameters from the models, leading to more streamlinedarchitectures without compromising predictive performance.However, an unexpected observation arose when comparingthe accuracy of the Torch implementation with that of theTIMM implementation of ConvNeXt Small. The Torch im-plementation exhibited lower accuracy compared to its TIMMcounterpart, indicating that OTOV3\u2019s performance regardingaccuracy may be influenced by the details of the modelarchitecture. This unexpected result suggests that differenttraining strategies or adjustments may be necessary to optimizeOTOV3\u2019s performance across various model implementations,emphasizing the importance of considering architectural dif-ferences when applying compression techniques like OTOV3.B. OTOV3 Evaluation on Fine-Tuned ConvNeXt ModelsWhen evaluating OTOV3\u2019s performance on a fine-tunedConvNeXt model, notable improvements in compression per-formance were observed, confirming its effectiveness in re-ducing model size, parameters, and MACs while marginallyenhancing accuracy. This outcome highlights the potential ofintegrating fine-tuning with structured pruning techniques toachieve even greater optimization of model performance. Thefine-tuned model displayed enhanced compression capabilitiescompared to untrained models, suggesting that pre-trainingcan significantly increase the effectiveness of compressiontechniques. This finding highlights the importance of lever-aging pre-existing knowledge within models to maximize thebenefits of compression, ultimately resulting in CNN modelswith higher performance.C. Unstructured Pruning TechniquesDuring the evaluation of l1 unstructured and random un-structured pruning techniques, expected trends were observedregarding accuracy and compression. As the pruning per-centages increased for both linear and convolutional layers,a corresponding decrease in accuracy was noted, while themodel size, parameters, and MACs remained unaltered. Thisoutcome aligns with the inherent nature of unstructured prun-ing, wherein weights are zeroed out but not entirely eliminated,resulting in sparse models without substantial reductions incomputational complexity. However, the lack of significantreductions in computational complexity may constrain their ef-fectiveness, particularly in resource-constrained environmentswhere efficient utilization of computational resources is essen-tial. This highlights the importance of considering the trade-offs between model compression and computational efficiencywhen selecting pruning techniques for deployment in real---- Page 14 ---14TABLE VIOTOV3 AND DYNAMIC QUANTIZATION REDUCTION NUMBERS WITH CONV NEXT SMALL TUNED .Technique AccuracyChange Size Reduction ParametersReductionMACsReductionOTOV3 3.75% 73.51% 73.62% 60.18%Dynamic Quantization -0.15% 71.30% 95.43% 95.71%OTOV3 and DynamicQuantization 3.80% 89.74% 95.43% 95.71%world applications, especially in edge computing scenarioswhere computational resources are limited.D. Dynamic QuantizationDynamic quantization emerged as a highly effective tech-nique for model compression, demonstrating remarkable re-ductions in model size, parameters, and, Your response: ```SUMMARY\nedge device image classification model compression quantization\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\n~~~~~~~~~~\nCurrent Step #17, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 19:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\n~~~~~~~~~~\nCurrent Step #18, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification energy-accuracy optimization\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 20:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\n~~~~~~~~~~\nCurrent Step #19, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Ef\ufb01cient Feature Compression forEdge-Cloud SystemsZhihao DuanElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.duan90@purdue.eduFengqing ZhuElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.zhu0@purdue.eduAbstract\u2014Optimizing computation in an edge-cloud system isan important yet challenging problem. In this paper, we considera three way trade-off between bit rate, classi\ufb01cation accuracy,and encoding complexity in an edge-cloud image classi\ufb01cationsystem. Our method includes a new training strategy andan ef\ufb01cient encoder architecture to improve the rate-accuracyperformance. Our design can also be easily scaled accordingto different computation resources on the edge device, taking astep towards achieving rate-accuracy-complexity (RAC) trade-off.Under various settings, our feature coding system consistentlyoutperforms previous methods in terms of the RAC performance.Code is made publicly available 1.Index Terms\u2014Feature compression, Coding for machine vision,Image classi\ufb01cation, Edge-cloud systemI. I NTRODUCTIONMobile devices and wearable sensors are essential gatewaysto many autonomous systems. These edge devices are oftendeployed with vision tasks in applications involving robotics,drones, surveillance cameras, and smart home devices. How-ever, due to constraints on the device size and power supply,edge devices cannot support the real-time execution of neuralnetworks-based computer vision methods, which are typicallycomputationally expensive. A common solution is to takeadvantage of the wireless connection to a powerful cloudcomputing resource, forming an edge-cloud system [1], [2]as illustrated in Fig. 1. In such a system, we obtain data (inour context, images) from edge device sensors (cameras) andsend it to the cloud for task execution.Designing practical methods for edge-cloud systems ischallenging from several aspects. First, in most cases wherethe edge-cloud connection is band-limited, we have constraintson the transmission bit rate, requiring the captured images tobe compressed by an encoder on the edge as ef\ufb01ciently as pos-sible. Second, we also have constraints on the computationalcapacity of the edge device, and thus the encoder cannot betoo complex. Finally, for common vision tasks such as theimage classi\ufb01cation, we also desire high accuracy. Taking allthese into consideration, we face a three-way trade-off betweenencoder complexity, bit rate, and vision task accuracy, whichWe term as the rate-accuracy-complexity (RAC) trade-off.1github.com/duanzhiihao/edge-cloud-racEdge device(sender)Cloud computing(receiver)Bits \u201ccat\u201dRateEncoder complexityAccuracyFig. 1: Illustration of an edge-cloud machine vision system,where three objectives are of interest: encoder computationalcomplexity on the edge device, bit rate in transmission, andtask accuracy in the cloud. In this paper, we present a featurecompression method for the edge-cloud image classi\ufb01cation.A straightforward baseline for RAC trade-off is to com-press and transmit images using different codecs. However,image codecs are typically developed for image reconstruction,making them suboptimal for vision task execution. Recently,research focused on feature compression has gained attention,which can be optimized for different vision tasks [1], [3].Another advantage to transmit features rather than images isprivacy protection. By transmitting features, one could skippixel reconstruction on the cloud and thus reduce potentiallyexposing image contents to the attackers [4].Entropic Student [2] is a recently proposed feature com-pression framework that provides promising rate-accuracyperformance. However, it does not take into account theencoder complexity as part of the performance trade-off. Toextend the scope to the three-way RAC trade-off, we proposea \ufb02exible encoder whose complexity can be easily adjustedfor different device capacities. Together with a new trainingstrategy, we achieve better RAC performance compared toprevious methods, including image coding baselines and thestate-of-the-art Entropic Student model.Our contributions can be summarized as follows:\u2022 We expand the scope of previous feature compressionsystems to include the RAC trade-off.\u2022 We design a \ufb02exible neural network architecture forfeature coding, which is both ef\ufb01cient and achieves goodclassi\ufb01cation performance.\u2022 We conduct extensive experiments to verify the effec-tiveness of our approach, showing its advantages overprevious methods.arXiv:2211.09897v1  [eess.IV]  17 Nov 2022--- Page 2 ---II. R ELATED WORKA. Image CompressionTraditional image coders . Traditional image codecs typi-cally follow a linear transform coding paradigm, such as block-based discrete cosine transform (DCT) and discrete wavelettransform (DWT). Representative codecs include JPEG, WebP,and BPG (i.e., HEVC intra [5]). Their encoding complexityvaries depending on different factors such as the choice of theentropy coding algorithms, number of prediction modes, andhow much optimization is done during training. In general,more complex computation devoted at the encoder leads tobetter compression performance.Learned image coders . With the rise of deep learning,learned image coders (LICs) extend the traditional lineartransform coding paradigm to a non-linear fashion, which isachieved by transforming images using neural networks [6].Despite improved compression performance compared to tra-ditional codecs, LICs typically require more computationalresources due to highly parameterized neural network layersand autoregressive entropy models [7].The paradigm of image coding share a common disadvan-tage: they are designed for image reconstruction and may besuboptimal for vision tasks in most edge-cloud settings.B. Feature compressionVarious methods have been proposed to compress andtransmit features rather than images for machine vision.Hand-crafted feature coding. Earlier works have exploredcompressing deep features using hand-crafted algorithms. Choiand Baji \xb4c use a block-based intra and inter coding algorithmfor video feature compression in [8] and a HEVC-based fea-ture compression method for object detection in [9]. Followingtheir works, methods have been proposed for feature compres-sion using pruning [10], quantization [11], and bidirectionalprediction [10].Learned feature compression . Instead of manually de-signed methods, recent work has shifted attention to end-to-end learned feature compression and showed promisingresults. Singh et al. use a fully factorized entropy model toconstrain the entropy of features, which results in improvedrate-accuracy trade-off over previous methods [3]. Dubois etal. show that, given enough computational capacity, featurescan be extremely (around 1,000\xd7) compressed without affect-ing the vision task accuracy [12]. More recently, Matsubaraet al. train vision models with feature compressor jointly andshow state-of-the-art rate-accuracy performance [2].While existing methods mostly focus on improving therate-accuracy performance, their approach lacks scalabilityfor different encoder complexity. In this work, we extendthe scope to rate-accuracy-complexity (RAC) trade-off byproposing a easily scalable encoder architecture, which can beadapted to edge devices with different computation capacities.III. P RELIMINARIESThe current best method in terms of rate-accuracy perfor-mance, Entropic Student [2], is a good starting point for our\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43f\u0ddc\U0001d466\u0ddc\U0001d466teachLabel \U0001d466Conv5x5, s=2GDNConv5x5, s=2GDNConv2x2, s=1Conv2x2, s=1Inv. GDNConv2x2, s=1Inv. GDNConv2x2, s=1DecoderEncoder ClassifierEntropy modelPre-trained image classifierStage 1 Stage 2(a) Training overview. GDN refers to the generalized divisible nor-malization layer [13], the entropy model is fully factorized, and theclassi\ufb01er is ResNet-50 [14]. See text for description.Edge Cloud\u0ddc\U0001d466Enc. Dec.ClassifierEntropy modelrANS rANSBitsEntropy model(b) Testing (deployment). Entropy coding is done by the range-basedvariant of asymmetric numeral systems (rANS) [15].Fig. 2: Model training (a) and deployment (b) in the EntropicStudent framework [2] for feature compression.proposed work. In this section, we brie\ufb02y summarize it toprovide the background of our method, and we refer interestedreaders to the original paper [2] for more details.The framework for classi\ufb01cation is shown in Fig. 2. Forsplit computing, a feature encoder is designed to be deployedon the edge device, and a decoder together with a classi\ufb01er areexecuted on the cloud. Speci\ufb01cally, a ResNet-50 [14] classi\ufb01eris adopted for image classi\ufb01cation.Training is done in two stages. In the \ufb01rst stage, theclassi\ufb01er is initialized using a pre-trained model and \ufb01xed(i.e., no parameter updating), and the encoder and decoder arelearned to compress images and reconstruct desired features.In the second stage, the encoder is \ufb01xed, while the decoderand the classi\ufb01er are trained to optimize the task accuracy.The training loss functions are:Lstage1 = \u03bblR + lMSE,Lstage2 = \u03bblR + 0.5 \xb7(lKL + lCE), (1)where lR is the bit rate loss estimated by a factorized entropymodel [6], lMSE is the mean squared error (MSE) betweenthe classi\ufb01er\u2019s intermediate features and the ones from a pre-trained teacher model, lKL is the KL divergence loss forknowledge distillation [16], lCE is the standard cross-entropyloss for classi\ufb01cation, and \u03bb is a scalar hyperparameter totrade-off between rate and classi\ufb01cation accuracy.The deployment scenario is shown in Fig. 2b, where theentropy model is shared between the sender (edge) and thereceiver (cloud) for entropy coding, implemented by the range-based asymmetric numeral system (rANS) algorithm [15].Despite its strong rate-accuracy performance [2], the En-tropic Student model only operates at a single encoder com-plexity and is not feasible for RAC trade-off. In the nextsection, we introduce our method, which not only improves therate-accuracy performance, but also enables the RAC trade-off.--- Page 3 ---\u0ddc\U0001d466\u0ddc\U0001d466tLabel \U0001d466Enc. Dec. ClassifierPre-trained Classifier\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43fEntropy modelLearned or fixed(a) Training overview.Conv 8x8, s=8Res. BlockRes. Block\U0001d441 blocks\U0001d467 (b) Encoder architecture.GELUConv 3x3, s=1GELUConv 3x3, s=1 (c) Residual block.GELUConvT5x5Conv 3x3, s=1GELUConv 3x3, s=1GELUConv 1x1, s=1\U0001d467 (d) Decoder architecture.Fig. 3: Summary of proposed method. In (a), we show the training strategy, where the entropy model is fully factorized, andgray blocks indicate frozen models (i.e., no parameter update during training). The classi\ufb01er can either be trained jointly withthe system, or initialized with a pre-trained model and \ufb01xed throughout training. In (b) and (c), we show our encoder anddecoder architecture, respectively. Our encoder network complexity can be tuned by adjusting the number of residual blocks,N. For the classi\ufb01er, we use a truncated version of ResNet-50, following [2].IV. O UR APPROACHInspired by the success of knowledge distillation in the En-tropic Student model, we adopt a similar framework and lossfunctions, shown in Fig. 3a. However, we make modi\ufb01cationsincluding a new training strategy and network architecture, bywhich we are able to realize the RAC trade-off.A. Single-stage TrainingThe Entropic Student method adopts a two-stage trainingstrategy, where the encoder and decoder are trained in the\ufb01rst stage, and the decoder and classi\ufb01er are \ufb01ne-tuned in thesecond stage. We note that this decoupled training strategyrequires more hyperparameter searching, and it is also sub-optimal for rate-accuracy performance since network compo-nents are trained separately. Instead, we propose a simple \ufb01x,i.e., to train the encoder, decoder, and classi\ufb01er in an end-to-end fashion. Our single-stage loss function is as follows:Lsingle = \u03bblR + lMSE + 0.5 \xb7(lKL + lCE), (2)where the terms are de\ufb01ned in Sec. III. We keep the samescaling factor for each loss term as in the Entropic Studentwithout further tuning. Later in Sec. V-D, we show thatour single-stage strategy improves the rate-accuracy trade-offdespite being simpler to implement.B. Network ArchitectureWe introduce an encoder network architecture speci\ufb01callydesigned for ef\ufb01cient and \ufb02exible feature compression, shownin Fig. 3b. We now describe our design choices in details.Downsampling layer. We \ufb01rst noticed that the 5x5 con-volutions for downsampling in previous works (e.g., in theEntropic Student [2] and learned image coders [6], [17]) arecomputationally inef\ufb01cient due to the large overlap betweenconvolutional kernels. To remove this overlap, we adopt asingle patch embedding [18] layer for downsampling, whichcan be equivalently viewed as a convolution with kernel sizeequal to its strides.Network blocks. Instead of feedforward convolutions asin previous works, we adopt residual blocks in our featureencoder, which enables easy adjustment of the encoder com-plexity simply by setting the number of residual blocks, N. Alarger N (i.e., more residual blocks in the encoder) improvesthe rate-accuracy performance while results in higher encodercomplexity. Thus by tuning N, we can trade-off between therate-accuracy performance and the encoding complexity of oursystem. We use a standard design for the residual blocks asshown in Fig. 3c. For non-linearity layers, we use the GeLUactivation [19] instead of GDN [13], as we empirically foundthat the former leads to better performance with the samecomputational complexity.Downsampling ratio. Our \ufb01nal step is to use 8\xd7downsam-pling, instead of the 4\xd7downsampling in Entropic Student,during encoding. Our choice is motivated by the observationthat 8\xd7downsampling is a common choice for various imageprocessing tasks (e.g., in traditional codecs like JPEG) andcomputer vision tasks (e.g., in image object detection [20]and semantic segmentation [21]). Note that in our featuredecoder on the cloud (Fig. 3d), we upsample the feature by2\xd7to recover spatial details of the original image. With thiscon\ufb01guration of the encoder and the decoder, we are able toreduce the encoding latency and improve the average accuracyby a large margin, as discussed later in Sec. V-D.C. Feature Compression for a Fixed Classi\ufb01erSo far, we have considered the joint training setting, wherethe classi\ufb01er and our feature coder are trained jointly. Inaddition, we also examine the setting in which a pre-trainedclassi\ufb01er is available but \ufb01xed, illustrated in Fig. 3a. Thisscenario may happen in practice where there are different edgedevices sharing a single cloud vision model. The edge devicesmay use different feature encoders/decoders according to theircomputational capacity, but the vision model on the cloudshould be \ufb01xed. In this case, we use the same loss functionas in Eq. 2, but we train the encoder and decoder networkswithout updating the classi\ufb01er. We denote the methods in thissetting by cls. \ufb01xed to distinguish them from the joint trainingsetting, where the classi\ufb01er can be trained together with theencoder and the decoder.V. E XPERIMENTSWe compare our method to previous ones for edge-cloudimage classi\ufb01cation. For fair comparison, all methods use theResNet-50 [14] (for image coding methods) or a truncated--- Page 4 ---version of ResNet-50 (for feature coding methods) as theclassi\ufb01er.A. Dataset and MetricsWe use the 1,000-class ImageNet dataset [22] for trainingand testing. Standard data augmentations, including randomresizing and cropping, are applied during training. We use theImageNet val set for testing, in which we resize and center-crop all images such that they have a resolution of 224 \xd7224.All metrics are computed on the 224 \xd7224 val set. Bit rateis measured in terms of bits per pixel (bpp). For classi\ufb01cationaccuracy, we report the top-1 accuracy of the classi\ufb01er predic-tions. For encoding complexity, we focus on the CPU latencysince it well simulates real-world scenarios. All encodinglatencies are measured using an Intel 12700K CPU, averagedover the \ufb01rst 5,000 images in the val set.We also introduce a new metric, Delta-accuracy, whichcomputes the average accuracy improvement (w.r.t. a baselinemethod) over all bit rates. By using this metric, we cansummarize each rate-accuracy curve into a scalar, thus beingable to represent the three-way RAC trade-off in a 2-D plot.Our computation of Delta-accuracy follows the convention ofBD-PSNR and BD-rate [23] except one aspect. Unlike PSNR,the unit of accuracy is percentage (not decibel), so we computeaverage accuracy improvement over the normal bit rate space(instead of the logarithmic rate scale used in BD-PSNR).B. Experiment SettingsWe consider the following baseline methods.Image coding baselines. We compress images using animage codec at the sender side, and we reconstruct images atthe receiver side using the same codec. Then, we run ResNet-50 on the reconstructed images to infer class prediction. Ourselected codecs include handcrafted (e.g., WebP and BPG [5])and learning-based methods (e.g., the Factorized model [6],and the Hyperprior model [17]).Feature coding baseline. We use the existing best method,Entropic Student [2], as a powerful baseline.As we discussed in Sec. IV-C, we group methods into twogroups so that methods within each can be fairly compared.Classi\ufb01er \ufb01xed setting: The pre-trained ResNet-50 is usedand remains \ufb01xed (i.e., no \ufb01ne-tuning). The image codingbaselines all fall in this setting. We label the methods in thissetting as cls. \ufb01xed.Joint training setting: The pre-trained ResNet-50 is jointlytrained with the feature encoder and/or decoder. The EntropicStudent model falls in this setting. We label the methods inthis setting as joint.C. Main Results for RAC Trade-offTo benchmark our method against the baselines in terms ofRAC performance, we \ufb01rst measure the rate-accuracy curvesfor all baselines and our methods. Then, we compute the Delta-accuracy of each method w.r.t. the Entropic Student baseline(whose Delta-accuracy is 0 by de\ufb01nition). We show the Delta-accuracy as a function of encoding latency in Fig. 4. Note that3 4 7 10 14 20 30 40 50Encoding latency on CPU (ms)121086420Delta-accuracy (%)RAC performance on ImageNetFeature coding: ours (joint)Feature coding: Entropic Student [2] (joint)Feature coding: ours (cls. fixed)Image coding: BPG [5] (cls. fixed)Image coding: Hyperpiror model [17] (cls. fixed)Image coding: Factorized model [6] (cls. fixed)Image coding: WebP (cls. fixed)Fig. 4: RAC trade-off, visualized by Delta-accuracy vs en-coding complexity. The Delta-accuracy is w.r.t. the EntropicStudent baseline.the Entropic Student [2] corresponds to a single point on thecomplexity vs. Delta-accuracy plot since its complexity cannotbe tuned. Similarly, the Factorized model [6] and Hyperpriormodel [17] also produce a single point on the this plot. ForWebP and BPG [5] codec, we control the encoding latency bysetting the compression speed option provided by the codecimplementation. For our method, we control the latency bychoosing N \u2208{0,4,8}, where N is the number of residualblocks in the encoder.From Fig. 4, we observe that for classi\ufb01er \ufb01xed methods,our approach clearly outperforms the image coding baselinesby a large margin, demonstrating the effectiveness of featurecoding. This improvement is expected, because our featureencoder and decoder are explicitly trained to maximize classi-\ufb01cation accuracy, instead of pixel reconstruction as in imagecoding baselines. For the joint training setting, our methodachieves higher Delta-accuracy than the Entropic Student base-line with approximately only half of the encoding latency. Inaddition, the \ufb02exible design of our encoder enables encodingcomplexity adjustment, as opposed to the Entropic Student, bywhich only a single complexity can be achieved.D. Ablation StudyTo analyze how each component of our method contributesto the RAC improvement, we start from the Entropic Studentbaseline, and we include our proposed components one ata time. At each step, we show the change of rate-accuracyperformance as well as the encoding complexity.We \ufb01rst compare our single-stage strategy (referred to ascon\ufb01guration A) with the original two-stage one in Fig. 5. InTable I, we also show the encoding complexity and Delta-accuracy. We observe that the single-stage training does notimpair the performance; instead, the average accuracy slightlyincreases by 0.13% with encoding complexity unchanged.Then, we replace the Entropic Student encoder by ourpatch embedding layer and residual blocks, and we show the--- Page 5 ---0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2Bits per pixel (bpp)57.560.062.565.067.570.072.575.0T op-1 acc. (%)Rate-accuracy trade-off on ImageNetOurs C: B + 8x downsamplingOurs B: A + improved architectureOurs A: Baseline + single-stage trainingBaseline: Entropic StudentFig. 5: Ablation study of our approach compared to the En-tropic Student baseline. Trained and tested on ImageNet [22].TABLE I: Ablation study. FLOPs and encoding latency arefor 224x224 resolution inputs. All percentages are w.r.t. to thebaseline, Entropic Student.FLOPs\u2193 Enc. Latency\u2193 Delta-acc.\u2191Baseline: Entropic Student 0.466B 13.8ms 0.0%A: Baseline + single-stage 0.466B (-0.0%) 13.8ms (-0.0%) +0.13%B: A + improved architecture 0.457B (-0.02%) 9.10ms (-34.1%) +0.20%C: B + 8x downsampling 0.489B (+0.05%) 6.70ms (-51.4%) +0.93%resulting system as con\ufb01guration B in Fig. 5 and Table I. Notethat the downsampling ratio ( 4\xd7) is unchanged. We can seethat the improved architecture reduces the encoding latencyby more than 30% and improves the average accuracy by anadditional small percentage ( 0.07%).Finally, we increase the downsampling ratio to 8\xd7 aswe discussed in Sec. IV-B, and we refer to this version ascon\ufb01guration C . By increasing the downsampling ratio, weagain reduce the encoding latency and improve the averageaccuracy by an additional large margin, after which we achievean overall latency reduction of 50% and an average accuracyimprovement of 0.93%. From Fig. 5, we notice that theimprovement is most signi\ufb01cant at lower bit rates. We con-jecture that lower bit rates naturally call for a more compactcompressed representation, which can be achieved by moreaggressive downsampling. For higher bit rate (e.g., more than0.8 bpp), our approach remains on par with the baseline.VI. C ONCLUSIONIn this paper, we present a feature coding system for imageclassi\ufb01cation in edge-cloud systems. We show that by properdesign of the network architecture, existing feature codingmethods can be improved and extended to consider the RACtrade-off. Our system outperforms all previous methods interms of RAC trade-off, which we believe is valuable formany real-world applications deployment. A limitation of oursystem is the requirement of a separate neural network modelfor every rate-complexity pair, which we leave to our futurework to solve.REFERENCES[1] H. Choi and I. V . Baji \xb4c, \u201cDeep feature compression for collaborativeobject detection,\u201d IEEE International Conference on Image Processing ,pp. 3743\u20133747, Oct. 2018.[2] Y . Matsubara, R. Yang, M. Levorato, and S. Mandt, \u201cSupervised com-pression for resource-constrained edge computing systems,\u201d IEEE/CVFWinter Conference on Applications of Computer Vision , pp. 923\u2013933,Jan. 2022.[3] S. Singh, S. Abu-El-Haija, N. Johnston, J. Ball \xb4e, A. Shrivastava, andG. Toderici, \u201cEnd-to-end learning of compressible features,\u201d IEEEInternational Conference on Image Processing , pp. 3349\u20133353, Oct2020.[4] V . Kakkad, M. Patel, and M. Shah, \u201cBiometric authentication and imageencryption for image security in cloud framework,\u201d Multiscale andMultidisciplinary Modeling, Experiments and Design , vol. 2, no. 4, pp.233\u2013248, May 2019.[5] J. Lainema, F. Bossen, W. Han, J. Min, and K. Ugur, \u201cIntra coding of thehevc standard,\u201d IEEE Transactions on Circuits and Systems for VideoTechnology, vol. 22, no. 12, pp. 1792\u20131801, Dec. 2012.[6] J. Ball \xb4e, V . Laparra, and E. Simoncelli, \u201cEnd-to-end optimizationof nonlinear transform codes for perceptual quality,\u201d Picture CodingSymposium, pp. 1\u20135, Dec. 2016.[7] D. Minnen, J. Ball \xb4e, and G. Toderici, \u201cJoint autoregressive and hier-archical priors for learned image compression,\u201d Advances in NeuralInformation Processing Systems, vol. 31, pp. 10 794\u201310 803, Dec. 2018.[8] H. Choi and I. V . Baji \xb4c, \u201cNear-lossless deep feature compression forcollaborative intelligence,\u201d IEEE International Workshop on MultimediaSignal Processing, pp. 1\u20136, Aug. 2018.[9] \u2014\u2014, \u201cDeep feature compression for collaborative object detection,\u201dIEEE International Conference on Image Processing , pp. 3743\u20133747,Oct. 2018.[10] H. Choi, R. A. Cohen, and I. V . Baji \xb4c, \u201cBack-and-forth prediction fordeep tensor compression,\u201d IEEE International Conference on Acoustics,Speech and Signal Processing , pp. 4467\u20134471, May 2020.[11] R. A. Cohen, H. Choi, and I. V . Baji \xb4c, \u201cLightweight compressionof neural network feature tensors for collaborative intelligence,\u201d IEEEInternational Conference on Multimedia and Expo , pp. 1\u20136, Jul. 2020.[12] Y . Dubois, B. Bloem-Reddy, K. Ullrich, and C. J. Maddison, \u201cLossycompression for lossless prediction,\u201d Advances in Neural InformationProcessing Systems, vol. 34, pp. 14 014\u201314 028, Dec. 2021.[13] J. Ball \xb4e, V . Laparra, and E. P. Simoncelli, \u201cDensity modeling ofimages using a generalized normalization transformation,\u201d InternationalConference on Learning Representations , Jul. 2016.[14] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for imagerecognition,\u201d Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 770\u2013778, Jun. 2016.[15] J. Duda, \u201cAsymmetric numeral systems: entropy coding combiningspeed of huffman coding with compression rate of arithmetic coding,\u201darXiv preprint arXiv:1311.2540 , Dec. 2013.[16] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neuralnetwork,\u201d https://arxiv.org/abs/1503.02531, Mar. 2015.[17] J. Ball \xb4e, D. Minnen, S. Singh, S. Hwang, and N. Johnston, \u201cVariationalimage compression with a scale hyperprior,\u201d International Conferenceon Learning Representations , Apr. 2018.[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words:Transformers for image recognition at scale,\u201d International Conferenceon Learning Representations , May. 2021.[19] D. Hendrycks and K. Gimpel, \u201cGaussian error linear units (gelus),\u201d arXivpreprint arXiv:1606.08415, Jun. 2016.[20] J. Redmon and A. Farhadi, \u201cYolov3: An incremental improvement,\u201darXiv preprint arXiv:1804.02767 , Apr. 2018.[21] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsingnetwork,\u201d Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 6230\u20136239, Jul. 2017.[22] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, \u201cImagenet: A large-scale hierarchical image database,\u201d Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition , pp. 248\u2013255, Sep. 2009.[23] G. Bjontegaard, \u201cCalculation of average psnr differences between rd-curves,\u201d Video Coding Experts Group - M33 , Apr. 2001.\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2211.09897v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 21:
\n~~~~~~~~~~\nHistory: Step #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #19, Phase: literature review, Feedback: 3\n--- Page 1 ---Ef\ufb01cient Feature Compression forEdge-Cloud SystemsZhihao DuanElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.duan90@purdue.eduFengqing ZhuElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.zhu0@purdue.eduAbstract\u2014Optimizing computation in an edge-cloud system isan important yet challenging problem. In this paper, we considera three way trade-off between bit rate, classi\ufb01cation accuracy,and encoding complexity in an edge-cloud image classi\ufb01cationsystem. Our method includes a new training strategy andan ef\ufb01cient encoder architecture to improve the rate-accuracyperformance. Our design can also be easily scaled accordingto different computation resources on the edge device, taking astep towards achieving rate-accuracy-complexity (RAC) trade-off.Under various settings, our feature coding system consistentlyoutperforms previous methods in terms of the RAC performance.Code is made publicly available 1.Index Terms\u2014Feature compression, Coding for machine vision,Image classi\ufb01cation, Edge-cloud systemI. I NTRODUCTIONMobile devices and wearable sensors are essential gatewaysto many autonomous systems. These edge devices are oftendeployed with vision tasks in applications involving robotics,drones, surveillance cameras, and smart home devices. How-ever, due to constraints on the device size and power supply,edge devices cannot support the real-time execution of neuralnetworks-based computer vision methods, which are typicallycomputationally expensive. A common solution is to takeadvantage of the wireless connection to a powerful cloudcomputing resource, forming an edge-cloud system [1], [2]as illustrated in Fig. 1. In such a system, we obtain data (inour context, images) from edge device sensors (cameras) andsend it to the cloud for task execution.Designing practical methods for edge-cloud systems ischallenging from several aspects. First, in most cases wherethe edge-cloud connection is band-limited, we have constraintson the transmission bit rate, requiring the captured images tobe compressed by an encoder on the edge as ef\ufb01ciently as pos-sible. Second, we also have constraints on the computationalcapacity of the edge device, and thus the encoder cannot betoo complex. Finally, for common vision tasks such as theimage classi\ufb01cation, we also desire high accuracy. Taking allthese into consideration, we face a three-way trade-off betweenencoder complexity, bit rate, and vision task accuracy, whichWe term as the rate-accuracy-complexity (RAC) trade-off.1github.com/duanzhiihao/edge-cloud-racEdge device(sender)Cloud computing(receiver)Bits \u201ccat\u201dRateEncoder complexityAccuracyFig. 1: Illustration of an edge-cloud machine vision system,where three objectives are of interest: encoder computationalcomplexity on the edge device, bit rate in transmission, andtask accuracy in the cloud. In this paper, we present a featurecompression method for the edge-cloud image classi\ufb01cation.A straightforward baseline for RAC trade-off is to com-press and transmit images using different codecs. However,image codecs are typically developed for image reconstruction,making them suboptimal for vision task execution. Recently,research focused on feature compression has gained attention,which can be optimized for different vision tasks [1], [3].Another advantage to transmit features rather than images isprivacy protection. By transmitting features, one could skippixel reconstruction on the cloud and thus reduce potentiallyexposing image contents to the attackers [4].Entropic Student [2] is a recently proposed feature com-pression framework that provides promising rate-accuracyperformance. However, it does not take into account theencoder complexity as part of the performance trade-off. Toextend the scope to the three-way RAC trade-off, we proposea \ufb02exible encoder whose complexity can be easily adjustedfor different device capacities. Together with a new trainingstrategy, we achieve better RAC performance compared toprevious methods, including image coding baselines and thestate-of-the-art Entropic Student model.Our contributions can be summarized as follows:\u2022 We expand the scope of previous feature compressionsystems to include the RAC trade-off.\u2022 We design a \ufb02exible neural network architecture forfeature coding, which is both ef\ufb01cient and achieves goodclassi\ufb01cation performance.\u2022 We conduct extensive experiments to verify the effec-tiveness of our approach, showing its advantages overprevious methods.arXiv:2211.09897v1  [eess.IV]  17 Nov 2022--- Page 2 ---II. R ELATED WORKA. Image CompressionTraditional image coders . Traditional image codecs typi-cally follow a linear transform coding paradigm, such as block-based discrete cosine transform (DCT) and discrete wavelettransform (DWT). Representative codecs include JPEG, WebP,and BPG (i.e., HEVC intra [5]). Their encoding complexityvaries depending on different factors such as the choice of theentropy coding algorithms, number of prediction modes, andhow much optimization is done during training. In general,more complex computation devoted at the encoder leads tobetter compression performance.Learned image coders . With the rise of deep learning,learned image coders (LICs) extend the traditional lineartransform coding paradigm to a non-linear fashion, which isachieved by transforming images using neural networks [6].Despite improved compression performance compared to tra-ditional codecs, LICs typically require more computationalresources due to highly parameterized neural network layersand autoregressive entropy models [7].The paradigm of image coding share a common disadvan-tage: they are designed for image reconstruction and may besuboptimal for vision tasks in most edge-cloud settings.B. Feature compressionVarious methods have been proposed to compress andtransmit features rather than images for machine vision.Hand-crafted feature coding. Earlier works have exploredcompressing deep features using hand-crafted algorithms. Choiand Baji \xb4c use a block-based intra and inter coding algorithmfor video feature compression in [8] and a HEVC-based fea-ture compression method for object detection in [9]. Followingtheir works, methods have been proposed for feature compres-sion using pruning [10], quantization [11], and bidirectionalprediction [10].Learned feature compression . Instead of manually de-signed methods, recent work has shifted attention to end-to-end learned feature compression and showed promisingresults. Singh et al. use a fully factorized entropy model toconstrain the entropy of features, which results in improvedrate-accuracy trade-off over previous methods [3]. Dubois etal. show that, given enough computational capacity, featurescan be extremely (around 1,000\xd7) compressed without affect-ing the vision task accuracy [12]. More recently, Matsubaraet al. train vision models with feature compressor jointly andshow state-of-the-art rate-accuracy performance [2].While existing methods mostly focus on improving therate-accuracy performance, their approach lacks scalabilityfor different encoder complexity. In this work, we extendthe scope to rate-accuracy-complexity (RAC) trade-off byproposing a easily scalable encoder architecture, which can beadapted to edge devices with different computation capacities.III. P RELIMINARIESThe current best method in terms of rate-accuracy perfor-mance, Entropic Student [2], is a good starting point for our\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43f\u0ddc\U0001d466\u0ddc\U0001d466teachLabel \U0001d466Conv5x5, s=2GDNConv5x5, s=2GDNConv2x2, s=1Conv2x2, s=1Inv. GDNConv2x2, s=1Inv. GDNConv2x2, s=1DecoderEncoder ClassifierEntropy modelPre-trained image classifierStage 1 Stage 2(a) Training overview. GDN refers to the generalized divisible nor-malization layer [13], the entropy model is fully factorized, and theclassi\ufb01er is ResNet-50 [14]. See text for description.Edge Cloud\u0ddc\U0001d466Enc. Dec.ClassifierEntropy modelrANS rANSBitsEntropy model(b) Testing (deployment). Entropy coding is done by the range-basedvariant of asymmetric numeral systems (rANS) [15].Fig. 2: Model training (a) and deployment (b) in the EntropicStudent framework [2] for feature compression.proposed work. In this section, we brie\ufb02y summarize it toprovide the background of our method, and we refer interestedreaders to the original paper [2] for more details.The framework for classi\ufb01cation is shown in Fig. 2. Forsplit computing, a feature encoder is designed to be deployedon the edge device, and a decoder together with a classi\ufb01er areexecuted on the cloud. Speci\ufb01cally, a ResNet-50 [14] classi\ufb01eris adopted for image classi\ufb01cation.Training is done in two stages. In the \ufb01rst stage, theclassi\ufb01er is initialized using a pre-trained model and \ufb01xed(i.e., no parameter updating), and the encoder and decoder arelearned to compress images and reconstruct desired features.In the second stage, the encoder is \ufb01xed, while the decoderand the classi\ufb01er are trained to optimize the task accuracy.The training loss functions are:Lstage1 = \u03bblR + lMSE,Lstage2 = \u03bblR + 0.5 \xb7(lKL + lCE), (1)where lR is the bit rate loss estimated by a factorized entropymodel [6], lMSE is the mean squared error (MSE) betweenthe classi\ufb01er\u2019s intermediate features and the ones from a pre-trained teacher model, lKL is the KL divergence loss forknowledge distillation [16], lCE is the standard cross-entropyloss for classi\ufb01cation, and \u03bb is a scalar hyperparameter totrade-off between rate and classi\ufb01cation accuracy.The deployment scenario is shown in Fig. 2b, where theentropy model is shared between the sender (edge) and thereceiver (cloud) for entropy coding, implemented by the range-based asymmetric numeral system (rANS) algorithm [15].Despite its strong rate-accuracy performance [2], the En-tropic Student model only operates at a single encoder com-plexity and is not feasible for RAC trade-off. In the nextsection, we introduce our method, which not only improves therate-accuracy performance, but also enables the RAC trade-off.--- Page 3 ---\u0ddc\U0001d466\u0ddc\U0001d466tLabel \U0001d466Enc. Dec. ClassifierPre-trained Classifier\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43fEntropy modelLearned or fixed(a) Training overview.Conv 8x8, s=8Res. BlockRes. Block\U0001d441 blocks\U0001d467 (b) Encoder architecture.GELUConv 3x3, s=1GELUConv 3x3, s=1 (c) Residual block.GELUConvT5x5Conv 3x3, s=1GELUConv 3x3, s=1GELUConv 1x1, s=1\U0001d467 (d) Decoder architecture.Fig. 3: Summary of proposed method. In (a), we show the training strategy, where the entropy model is fully factorized, andgray blocks indicate frozen models (i.e., no parameter update during training). The classi\ufb01er can either be trained jointly withthe system, or initialized with a pre-trained model and \ufb01xed throughout training. In (b) and (c), we show our encoder anddecoder architecture, respectively. Our encoder network complexity can be tuned by adjusting the number of residual blocks,N. For the classi\ufb01er, we use a truncated version of ResNet-50, following [2].IV. O UR APPROACHInspired by the success of knowledge distillation in the En-tropic Student model, we adopt a similar framework and lossfunctions, shown in Fig. 3a. However, we make modi\ufb01cationsincluding a new training strategy and network architecture, bywhich we are able to realize the RAC trade-off.A. Single-stage TrainingThe Entropic Student method adopts a two-stage trainingstrategy, where the encoder and decoder are trained in the\ufb01rst stage, and the decoder and classi\ufb01er are \ufb01ne-tuned in thesecond stage. We note that this decoupled training strategyrequires more hyperparameter searching, and it is also sub-optimal for rate-accuracy performance since network compo-nents are trained separately. Instead, we propose a simple \ufb01x,i.e., to train the encoder, decoder, and classi\ufb01er in an end-to-end fashion. Our single-stage loss function is as follows:Lsingle = \u03bblR + lMSE + 0.5 \xb7(lKL + lCE), (2)where the terms are de\ufb01ned in Sec. III. We keep the samescaling factor for each loss term as in the Entropic Studentwithout further tuning. Later in Sec. V-D, we show thatour single-stage strategy improves the rate-accuracy trade-offdespite being simpler to implement.B. Network ArchitectureWe introduce an encoder network architecture speci\ufb01callydesigned for ef\ufb01cient and \ufb02exible feature compression, shownin Fig. 3b. We now describe our design choices in details.Downsampling layer. We \ufb01rst noticed that the 5x5 con-volutions for downsampling in previous works (e.g., in theEntropic Student [2] and learned image coders [6], [17]) arecomputationally inef\ufb01cient due to the large overlap betweenconvolutional kernels. To remove this overlap, we adopt asingle patch embedding [18] layer for downsampling, whichcan be equivalently viewed as a convolution with kernel sizeequal to its strides.Network blocks. Instead of feedforward convolutions asin previous works, we adopt residual blocks in our featureencoder, which enables easy adjustment of the encoder com-plexity simply by setting the number of residual blocks, N. Alarger N (i.e., more residual blocks in the encoder) improvesthe rate-accuracy performance while results in higher encodercomplexity. Thus by tuning N, we can trade-off between therate-accuracy performance and the encoding complexity of oursystem. We use a standard design for the residual blocks asshown in Fig. 3c. For non-linearity layers, we use the GeLUactivation [19] instead of GDN [13], as we empirically foundthat the former leads to better performance with the samecomputational complexity.Downsampling ratio. Our \ufb01nal step is to use 8\xd7downsam-pling, instead of the 4\xd7downsampling in Entropic Student,during encoding. Our choice is motivated by the observationthat 8\xd7downsampling is a common choice for various imageprocessing tasks (e.g., in traditional codecs like JPEG) andcomputer vision tasks (e.g., in image object detection [20]and semantic segmentation [21]). Note that in our featuredecoder on the cloud (Fig. 3d), we upsample the feature by2\xd7to recover spatial details of the original image. With thiscon\ufb01guration of the encoder and the decoder, we are able toreduce the encoding latency and improve the average accuracyby a large margin, as discussed later in Sec. V-D.C. Feature Compression for a Fixed Classi\ufb01erSo far, we have considered the joint training setting, wherethe classi\ufb01er and our feature coder are trained jointly. Inaddition, we also examine the setting in which a pre-trainedclassi\ufb01er is available but \ufb01xed, illustrated in Fig. 3a. Thisscenario may happen in practice where there are different edgedevices sharing a single cloud vision model. The edge devicesmay use different feature encoders/decoders according to theircomputational capacity, but the vision model on the cloudshould be \ufb01xed. In this case, we use the same loss functionas in Eq. 2, but we train the encoder and decoder networkswithout updating the classi\ufb01er. We denote the methods in thissetting by cls. \ufb01xed to distinguish them from the joint trainingsetting, where the classi\ufb01er can be trained together with theencoder and the decoder.V. E XPERIMENTSWe compare our method to previous ones for edge-cloudimage classi\ufb01cation. For fair comparison, all methods use theResNet-50 [14] (for image coding methods) or a truncated--- Page 4 ---version of ResNet-50 (for feature coding methods) as theclassi\ufb01er.A. Dataset and MetricsWe use the 1,000-class ImageNet dataset [22] for trainingand testing. Standard data augmentations, including randomresizing and cropping, are applied during training. We use theImageNet val set for testing, in which we resize and center-crop all images such that they have a resolution of 224 \xd7224.All metrics are computed on the 224 \xd7224 val set. Bit rateis measured in terms of bits per pixel (bpp). For classi\ufb01cationaccuracy, we report the top-1 accuracy of the classi\ufb01er predic-tions. For encoding complexity, we focus on the CPU latencysince it well simulates real-world scenarios. All encodinglatencies are measured using an Intel 12700K CPU, averagedover the \ufb01rst 5,000 images in the val set.We also introduce a new metric, Delta-accuracy, whichcomputes the average accuracy improvement (w.r.t. a baselinemethod) over all bit rates. By using this metric, we cansummarize each rate-accuracy curve into a scalar, thus beingable to represent the three-way RAC trade-off in a 2-D plot.Our computation of Delta-accuracy follows the convention ofBD-PSNR and BD-rate [23] except one aspect. Unlike PSNR,the unit of accuracy is percentage (not decibel), so we computeaverage accuracy improvement over the normal bit rate space(instead of the logarithmic rate scale used in BD-PSNR).B. Experiment SettingsWe consider the following baseline methods.Image coding baselines. We compress images using animage codec at the sender side, and we reconstruct images atthe receiver side using the same codec. Then, we run ResNet-50 on the reconstructed images to infer class prediction. Ourselected codecs include handcrafted (e.g., WebP and BPG [5])and learning-based methods (e.g., the Factorized model [6],and the Hyperprior model [17]).Feature coding baseline. We use the existing best method,Entropic Student [2], as a powerful baseline.As we discussed in Sec. IV-C, we group methods into twogroups so that methods within each can be fairly compared.Classi\ufb01er \ufb01xed setting: The pre-trained ResNet-50 is usedand remains \ufb01xed (i.e., no \ufb01ne-tuning). The image codingbaselines all fall in this setting. We label the methods in thissetting as cls. \ufb01xed.Joint training setting: The pre-trained ResNet-50 is jointlytrained with the feature encoder and/or decoder. The EntropicStudent model falls in this setting. We label the methods inthis setting as joint.C. Main Results for RAC Trade-offTo benchmark our method against the baselines in terms ofRAC performance, we \ufb01rst measure the rate-accuracy curvesfor all baselines and our methods. Then, we compute the Delta-accuracy of each method w.r.t. the Entropic Student baseline(whose Delta-accuracy is 0 by de\ufb01nition). We show the Delta-accuracy as a function of encoding latency in Fig. 4. Note that3 4 7 10 14 20 30 40 50Encoding latency on CPU (ms)121086420Delta-accuracy (%)RAC performance on ImageNetFeature coding: ours (joint)Feature coding: Entropic Student [2] (joint)Feature coding: ours (cls. fixed)Image coding: BPG [5] (cls. fixed)Image coding: Hyperpiror model [17] (cls. fixed)Image coding: Factorized model [6] (cls. fixed)Image coding: WebP (cls. fixed)Fig. 4: RAC trade-off, visualized by Delta-accuracy vs en-coding complexity. The Delta-accuracy is w.r.t. the EntropicStudent baseline.the Entropic Student [2] corresponds to a single point on thecomplexity vs. Delta-accuracy plot since its complexity cannotbe tuned. Similarly, the Factorized model [6] and Hyperpriormodel [17] also produce a single point on the this plot. ForWebP and BPG [5] codec, we control the encoding latency bysetting the compression speed option provided by the codecimplementation. For our method, we control the latency bychoosing N \u2208{0,4,8}, where N is the number of residualblocks in the encoder.From Fig. 4, we observe that for classi\ufb01er \ufb01xed methods,our approach clearly outperforms the image coding baselinesby a large margin, demonstrating the effectiveness of featurecoding. This improvement is expected, because our featureencoder and decoder are explicitly trained to maximize classi-\ufb01cation accuracy, instead of pixel reconstruction as in imagecoding baselines. For the joint training setting, our methodachieves higher Delta-accuracy than the Entropic Student base-line with approximately only half of the encoding latency. Inaddition, the \ufb02exible design of our encoder enables encodingcomplexity adjustment, as opposed to the Entropic Student, bywhich only a single complexity can be achieved.D. Ablation StudyTo analyze how each component of our method contributesto the RAC improvement, we start from the Entropic Studentbaseline, and we include our proposed components one ata time. At each step, we show the change of rate-accuracyperformance as well as the encoding complexity.We \ufb01rst compare our single-stage strategy (referred to ascon\ufb01guration A) with the original two-stage one in Fig. 5. InTable I, we also show the encoding complexity and Delta-accuracy. We observe that the single-stage training does notimpair the performance; instead, the average accuracy slightlyincreases by 0.13% with encoding complexity unchanged.Then, we replace the Entropic Student encoder by ourpatch embedding layer and residual blocks, and we show the--- Page 5 ---0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2Bits per pixel (bpp)57.560.062.565.067.570.072.575.0T op-1 acc. (%)Rate-accuracy trade-off on ImageNetOurs C: B + 8x downsamplingOurs B: A + improved architectureOurs A: Baseline + single-stage trainingBaseline: Entropic StudentFig. 5: Ablation study of our approach compared to the En-tropic Student baseline. Trained and tested on ImageNet [22].TABLE I: Ablation study. FLOPs and encoding latency arefor 224x224 resolution inputs. All percentages are w.r.t. to thebaseline, Entropic Student.FLOPs\u2193 Enc. Latency\u2193 Delta-acc.\u2191Baseline: Entropic Student 0.466B 13.8ms 0.0%A: Baseline + single-stage 0.466B (-0.0%) 13.8ms (-0.0%) +0.13%B: A + improved architecture 0.457B (-0.02%) 9.10ms (-34.1%) +0.20%C: B + 8x downsampling 0.489B (+0.05%) 6.70ms (-51.4%) +0.93%resulting system as con\ufb01guration B in Fig. 5 and Table I. Notethat the downsampling ratio ( 4\xd7) is unchanged. We can seethat the improved architecture reduces the encoding latencyby more than 30% and improves the average accuracy by anadditional small percentage ( 0.07%).Finally, we increase the downsampling ratio to 8\xd7 aswe discussed in Sec. IV-B, and we refer to this version ascon\ufb01guration C . By increasing the downsampling ratio, weagain reduce the encoding latency and improve the averageaccuracy by an additional large margin, after which we achievean overall latency reduction of 50% and an average accuracyimprovement of 0.93%. From Fig. 5, we notice that theimprovement is most signi\ufb01cant at lower bit rates. We con-jecture that lower bit rates naturally call for a more compactcompressed representation, which can be achieved by moreaggressive downsampling. For higher bit rate (e.g., more than0.8 bpp), our approach remains on par with the baseline.VI. C ONCLUSIONIn this paper, we present a feature coding system for imageclassi\ufb01cation in edge-cloud systems. We show that by properdesign of the network architecture, existing feature codingmethods can be improved and extended to consider the RACtrade-off. Our system outperforms all previous methods interms of RAC trade-off, which we believe is valuable formany real-world applications deployment. A limitation of oursystem is the requirement of a separate neural network modelfor every rate-complexity pair, which we leave to our futurework to solve.REFERENCES[1] H. Choi and I. V . Baji \xb4c, \u201cDeep feature compression for collaborativeobject detection,\u201d IEEE International Conference on Image Processing ,pp. 3743\u20133747, Oct. 2018.[2] Y . Matsubara, R. Yang, M. Levorato, and S. Mandt, \u201cSupervised com-pression for resource-constrained edge computing systems,\u201d IEEE/CVFWinter Conference on Applications of Computer Vision , pp. 923\u2013933,Jan. 2022.[3] S. Singh, S. Abu-El-Haija, N. Johnston, J. Ball \xb4e, A. Shrivastava, andG. Toderici, \u201cEnd-to-end learning of compressible features,\u201d IEEEInternational Conference on Image Processing , pp. 3349\u20133353, Oct2020.[4] V . Kakkad, M. Patel, and M. Shah, \u201cBiometric authentication and imageencryption for image security in cloud framework,\u201d Multiscale andMultidisciplinary Modeling, Experiments and Design , vol. 2, no. 4, pp.233\u2013248, May 2019.[5] J. Lainema, F. Bossen, W. Han, J. Min, and K. Ugur, \u201cIntra coding of thehevc standard,\u201d IEEE Transactions on Circuits and Systems for VideoTechnology, vol. 22, no. 12, pp. 1792\u20131801, Dec. 2012.[6] J. Ball \xb4e, V . Laparra, and E. Simoncelli, \u201cEnd-to-end optimizationof nonlinear transform codes for perceptual quality,\u201d Picture CodingSymposium, pp. 1\u20135, Dec. 2016.[7] D. Minnen, J. Ball \xb4e, and G. Toderici, \u201cJoint autoregressive and hier-archical priors for learned image compression,\u201d Advances in NeuralInformation Processing Systems, vol. 31, pp. 10 794\u201310 803, Dec. 2018.[8] H. Choi and I. V . Baji \xb4c, \u201cNear-lossless deep feature compression forcollaborative intelligence,\u201d IEEE International Workshop on MultimediaSignal Processing, pp. 1\u20136, Aug. 2018.[9] \u2014\u2014, \u201cDeep feature compression for collaborative object detection,\u201dIEEE International Conference on Image Processing , pp. 3743\u20133747,Oct. 2018.[10] H. Choi, R. A. Cohen, and I. V . Baji \xb4c, \u201cBack-and-forth prediction fordeep tensor compression,\u201d IEEE International Conference on Acoustics,Speech and Signal Processing , pp. 4467\u20134471, May 2020.[11] R. A. Cohen, H. Choi, and I. V . Baji \xb4c, \u201cLightweight compressionof neural network feature tensors for collaborative intelligence,\u201d IEEEInternational Conference on Multimedia and Expo , pp. 1\u20136, Jul. 2020.[12] Y . Dubois, B. Bloem-Reddy, K. Ullrich, and C. J. Maddison, \u201cLossycompression for lossless prediction,\u201d Advances in Neural InformationProcessing Systems, vol. 34, pp. 14 014\u201314 028, Dec. 2021.[13] J. Ball \xb4e, V . Laparra, and E. P. Simoncelli, \u201cDensity modeling ofimages using a generalized normalization transformation,\u201d InternationalConference on Learning Representations , Jul. 2016.[14] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for imagerecognition,\u201d Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 770\u2013778, Jun. 2016.[15] J. Duda, \u201cAsymmetric numeral systems: entropy coding combiningspeed of huffman coding with compression rate of arithmetic coding,\u201darXiv preprint arXiv:1311.2540 , Dec. 2013.[16] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neuralnetwork,\u201d https://arxiv.org/abs/1503.02531, Mar. 2015.[17] J. Ball \xb4e, D. Minnen, S. Singh, S. Hwang, and N. Johnston, \u201cVariationalimage compression with a scale hyperprior,\u201d International Conferenceon Learning Representations , Apr. 2018.[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words:Transformers for image recognition at scale,\u201d International Conferenceon Learning Representations , May. 2021.[19] D. Hendrycks and K. Gimpel, \u201cGaussian error linear units (gelus),\u201d arXivpreprint arXiv:1606.08415, Jun. 2016.[20] J. Redmon and A. Farhadi, \u201cYolov3: An incremental improvement,\u201darXiv preprint arXiv:1804.02767 , Apr. 2018.[21] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsingnetwork,\u201d Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 6230\u20136239, Jul. 2017.[22] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, \u201cImagenet: A large-scale hierarchical image database,\u201d Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition , pp. 248\u2013255, Sep. 2009.[23] G. Bjontegaard, \u201cCalculation of average psnr differences between rd-curves,\u201d Video Coding Experts Group - M33 , Apr. 2001., Your response: ```ADD_PAPER\n2211.09897v1\nThis paper presents an efficient feature compression method for edge-cloud image classification systems, focusing on optimizing the three-way trade-off between bit rate, classification accuracy, and encoding complexity (RAC trade-off). The authors propose a flexible encoder architecture with residual blocks that can be scaled based on edge device capabilities, along with a single-stage training strategy. Key results show their method achieves 51.4% lower encoding latency and 0.93% higher accuracy compared to prior work (Entropic Student), while supporting adjustable complexity levels. The system demonstrates superior RAC performance across different bit rates, particularly excelling at lower bit rates (0.1-0.8 bpp) where it maintains accuracy while reducing computational overhead.\n```\n~~~~~~~~~~\nCurrent Step #20, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2211.09897v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2211.09897v1\nThis paper presents an efficient feature compression method for edge-cloud image classification systems, focusing on optimizing the three-way trade-off between bit rate, classification accuracy, and encoding complexity (RAC trade-off). The authors propose a flexible encoder architecture with residual blocks that can be scaled based on edge device capabilities, along with a single-stage training strategy. Key results show their method achieves 51.4% lower encoding latency and 0.93% higher accuracy compared to prior work (Entropic Student), while supporting adjustable complexity levels. The system demonstrates superior RAC performance across different bit rates, particularly excelling at lower bit rates (0.1-0.8 bpp) where it maintains accuracy while reducing computational overhead.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 22:
\n~~~~~~~~~~\nHistory: Step #3, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #19, Phase: literature review, Feedback: 3\n--- Page 1 ---Ef\ufb01cient Feature Compression forEdge-Cloud SystemsZhihao DuanElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.duan90@purdue.eduFengqing ZhuElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.zhu0@purdue.eduAbstract\u2014Optimizing computation in an edge-cloud system isan important yet challenging problem. In this paper, we considera three way trade-off between bit rate, classi\ufb01cation accuracy,and encoding complexity in an edge-cloud image classi\ufb01cationsystem. Our method includes a new training strategy andan ef\ufb01cient encoder architecture to improve the rate-accuracyperformance. Our design can also be easily scaled accordingto different computation resources on the edge device, taking astep towards achieving rate-accuracy-complexity (RAC) trade-off.Under various settings, our feature coding system consistentlyoutperforms previous methods in terms of the RAC performance.Code is made publicly available 1.Index Terms\u2014Feature compression, Coding for machine vision,Image classi\ufb01cation, Edge-cloud systemI. I NTRODUCTIONMobile devices and wearable sensors are essential gatewaysto many autonomous systems. These edge devices are oftendeployed with vision tasks in applications involving robotics,drones, surveillance cameras, and smart home devices. How-ever, due to constraints on the device size and power supply,edge devices cannot support the real-time execution of neuralnetworks-based computer vision methods, which are typicallycomputationally expensive. A common solution is to takeadvantage of the wireless connection to a powerful cloudcomputing resource, forming an edge-cloud system [1], [2]as illustrated in Fig. 1. In such a system, we obtain data (inour context, images) from edge device sensors (cameras) andsend it to the cloud for task execution.Designing practical methods for edge-cloud systems ischallenging from several aspects. First, in most cases wherethe edge-cloud connection is band-limited, we have constraintson the transmission bit rate, requiring the captured images tobe compressed by an encoder on the edge as ef\ufb01ciently as pos-sible. Second, we also have constraints on the computationalcapacity of the edge device, and thus the encoder cannot betoo complex. Finally, for common vision tasks such as theimage classi\ufb01cation, we also desire high accuracy. Taking allthese into consideration, we face a three-way trade-off betweenencoder complexity, bit rate, and vision task accuracy, whichWe term as the rate-accuracy-complexity (RAC) trade-off.1github.com/duanzhiihao/edge-cloud-racEdge device(sender)Cloud computing(receiver)Bits \u201ccat\u201dRateEncoder complexityAccuracyFig. 1: Illustration of an edge-cloud machine vision system,where three objectives are of interest: encoder computationalcomplexity on the edge device, bit rate in transmission, andtask accuracy in the cloud. In this paper, we present a featurecompression method for the edge-cloud image classi\ufb01cation.A straightforward baseline for RAC trade-off is to com-press and transmit images using different codecs. However,image codecs are typically developed for image reconstruction,making them suboptimal for vision task execution. Recently,research focused on feature compression has gained attention,which can be optimized for different vision tasks [1], [3].Another advantage to transmit features rather than images isprivacy protection. By transmitting features, one could skippixel reconstruction on the cloud and thus reduce potentiallyexposing image contents to the attackers [4].Entropic Student [2] is a recently proposed feature com-pression framework that provides promising rate-accuracyperformance. However, it does not take into account theencoder complexity as part of the performance trade-off. Toextend the scope to the three-way RAC trade-off, we proposea \ufb02exible encoder whose complexity can be easily adjustedfor different device capacities. Together with a new trainingstrategy, we achieve better RAC performance compared toprevious methods, including image coding baselines and thestate-of-the-art Entropic Student model.Our contributions can be summarized as follows:\u2022 We expand the scope of previous feature compressionsystems to include the RAC trade-off.\u2022 We design a \ufb02exible neural network architecture forfeature coding, which is both ef\ufb01cient and achieves goodclassi\ufb01cation performance.\u2022 We conduct extensive experiments to verify the effec-tiveness of our approach, showing its advantages overprevious methods.arXiv:2211.09897v1  [eess.IV]  17 Nov 2022--- Page 2 ---II. R ELATED WORKA. Image CompressionTraditional image coders . Traditional image codecs typi-cally follow a linear transform coding paradigm, such as block-based discrete cosine transform (DCT) and discrete wavelettransform (DWT). Representative codecs include JPEG, WebP,and BPG (i.e., HEVC intra [5]). Their encoding complexityvaries depending on different factors such as the choice of theentropy coding algorithms, number of prediction modes, andhow much optimization is done during training. In general,more complex computation devoted at the encoder leads tobetter compression performance.Learned image coders . With the rise of deep learning,learned image coders (LICs) extend the traditional lineartransform coding paradigm to a non-linear fashion, which isachieved by transforming images using neural networks [6].Despite improved compression performance compared to tra-ditional codecs, LICs typically require more computationalresources due to highly parameterized neural network layersand autoregressive entropy models [7].The paradigm of image coding share a common disadvan-tage: they are designed for image reconstruction and may besuboptimal for vision tasks in most edge-cloud settings.B. Feature compressionVarious methods have been proposed to compress andtransmit features rather than images for machine vision.Hand-crafted feature coding. Earlier works have exploredcompressing deep features using hand-crafted algorithms. Choiand Baji \xb4c use a block-based intra and inter coding algorithmfor video feature compression in [8] and a HEVC-based fea-ture compression method for object detection in [9]. Followingtheir works, methods have been proposed for feature compres-sion using pruning [10], quantization [11], and bidirectionalprediction [10].Learned feature compression . Instead of manually de-signed methods, recent work has shifted attention to end-to-end learned feature compression and showed promisingresults. Singh et al. use a fully factorized entropy model toconstrain the entropy of features, which results in improvedrate-accuracy trade-off over previous methods [3]. Dubois etal. show that, given enough computational capacity, featurescan be extremely (around 1,000\xd7) compressed without affect-ing the vision task accuracy [12]. More recently, Matsubaraet al. train vision models with feature compressor jointly andshow state-of-the-art rate-accuracy performance [2].While existing methods mostly focus on improving therate-accuracy performance, their approach lacks scalabilityfor different encoder complexity. In this work, we extendthe scope to rate-accuracy-complexity (RAC) trade-off byproposing a easily scalable encoder architecture, which can beadapted to edge devices with different computation capacities.III. P RELIMINARIESThe current best method in terms of rate-accuracy perfor-mance, Entropic Student [2], is a good starting point for our\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43f\u0ddc\U0001d466\u0ddc\U0001d466teachLabel \U0001d466Conv5x5, s=2GDNConv5x5, s=2GDNConv2x2, s=1Conv2x2, s=1Inv. GDNConv2x2, s=1Inv. GDNConv2x2, s=1DecoderEncoder ClassifierEntropy modelPre-trained image classifierStage 1 Stage 2(a) Training overview. GDN refers to the generalized divisible nor-malization layer [13], the entropy model is fully factorized, and theclassi\ufb01er is ResNet-50 [14]. See text for description.Edge Cloud\u0ddc\U0001d466Enc. Dec.ClassifierEntropy modelrANS rANSBitsEntropy model(b) Testing (deployment). Entropy coding is done by the range-basedvariant of asymmetric numeral systems (rANS) [15].Fig. 2: Model training (a) and deployment (b) in the EntropicStudent framework [2] for feature compression.proposed work. In this section, we brie\ufb02y summarize it toprovide the background of our method, and we refer interestedreaders to the original paper [2] for more details.The framework for classi\ufb01cation is shown in Fig. 2. Forsplit computing, a feature encoder is designed to be deployedon the edge device, and a decoder together with a classi\ufb01er areexecuted on the cloud. Speci\ufb01cally, a ResNet-50 [14] classi\ufb01eris adopted for image classi\ufb01cation.Training is done in two stages. In the \ufb01rst stage, theclassi\ufb01er is initialized using a pre-trained model and \ufb01xed(i.e., no parameter updating), and the encoder and decoder arelearned to compress images and reconstruct desired features.In the second stage, the encoder is \ufb01xed, while the decoderand the classi\ufb01er are trained to optimize the task accuracy.The training loss functions are:Lstage1 = \u03bblR + lMSE,Lstage2 = \u03bblR + 0.5 \xb7(lKL + lCE), (1)where lR is the bit rate loss estimated by a factorized entropymodel [6], lMSE is the mean squared error (MSE) betweenthe classi\ufb01er\u2019s intermediate features and the ones from a pre-trained teacher model, lKL is the KL divergence loss forknowledge distillation [16], lCE is the standard cross-entropyloss for classi\ufb01cation, and \u03bb is a scalar hyperparameter totrade-off between rate and classi\ufb01cation accuracy.The deployment scenario is shown in Fig. 2b, where theentropy model is shared between the sender (edge) and thereceiver (cloud) for entropy coding, implemented by the range-based asymmetric numeral system (rANS) algorithm [15].Despite its strong rate-accuracy performance [2], the En-tropic Student model only operates at a single encoder com-plexity and is not feasible for RAC trade-off. In the nextsection, we introduce our method, which not only improves therate-accuracy performance, but also enables the RAC trade-off.--- Page 3 ---\u0ddc\U0001d466\u0ddc\U0001d466tLabel \U0001d466Enc. Dec. ClassifierPre-trained Classifier\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43fEntropy modelLearned or fixed(a) Training overview.Conv 8x8, s=8Res. BlockRes. Block\U0001d441 blocks\U0001d467 (b) Encoder architecture.GELUConv 3x3, s=1GELUConv 3x3, s=1 (c) Residual block.GELUConvT5x5Conv 3x3, s=1GELUConv 3x3, s=1GELUConv 1x1, s=1\U0001d467 (d) Decoder architecture.Fig. 3: Summary of proposed method. In (a), we show the training strategy, where the entropy model is fully factorized, andgray blocks indicate frozen models (i.e., no parameter update during training). The classi\ufb01er can either be trained jointly withthe system, or initialized with a pre-trained model and \ufb01xed throughout training. In (b) and (c), we show our encoder anddecoder architecture, respectively. Our encoder network complexity can be tuned by adjusting the number of residual blocks,N. For the classi\ufb01er, we use a truncated version of ResNet-50, following [2].IV. O UR APPROACHInspired by the success of knowledge distillation in the En-tropic Student model, we adopt a similar framework and lossfunctions, shown in Fig. 3a. However, we make modi\ufb01cationsincluding a new training strategy and network architecture, bywhich we are able to realize the RAC trade-off.A. Single-stage TrainingThe Entropic Student method adopts a two-stage trainingstrategy, where the encoder and decoder are trained in the\ufb01rst stage, and the decoder and classi\ufb01er are \ufb01ne-tuned in thesecond stage. We note that this decoupled training strategyrequires more hyperparameter searching, and it is also sub-optimal for rate-accuracy performance since network compo-nents are trained separately. Instead, we propose a simple \ufb01x,i.e., to train the encoder, decoder, and classi\ufb01er in an end-to-end fashion. Our single-stage loss function is as follows:Lsingle = \u03bblR + lMSE + 0.5 \xb7(lKL + lCE), (2)where the terms are de\ufb01ned in Sec. III. We keep the samescaling factor for each loss term as in the Entropic Studentwithout further tuning. Later in Sec. V-D, we show thatour single-stage strategy improves the rate-accuracy trade-offdespite being simpler to implement.B. Network ArchitectureWe introduce an encoder network architecture speci\ufb01callydesigned for ef\ufb01cient and \ufb02exible feature compression, shownin Fig. 3b. We now describe our design choices in details.Downsampling layer. We \ufb01rst noticed that the 5x5 con-volutions for downsampling in previous works (e.g., in theEntropic Student [2] and learned image coders [6], [17]) arecomputationally inef\ufb01cient due to the large overlap betweenconvolutional kernels. To remove this overlap, we adopt asingle patch embedding [18] layer for downsampling, whichcan be equivalently viewed as a convolution with kernel sizeequal to its strides.Network blocks. Instead of feedforward convolutions asin previous works, we adopt residual blocks in our featureencoder, which enables easy adjustment of the encoder com-plexity simply by setting the number of residual blocks, N. Alarger N (i.e., more residual blocks in the encoder) improvesthe rate-accuracy performance while results in higher encodercomplexity. Thus by tuning N, we can trade-off between therate-accuracy performance and the encoding complexity of oursystem. We use a standard design for the residual blocks asshown in Fig. 3c. For non-linearity layers, we use the GeLUactivation [19] instead of GDN [13], as we empirically foundthat the former leads to better performance with the samecomputational complexity.Downsampling ratio. Our \ufb01nal step is to use 8\xd7downsam-pling, instead of the 4\xd7downsampling in Entropic Student,during encoding. Our choice is motivated by the observationthat 8\xd7downsampling is a common choice for various imageprocessing tasks (e.g., in traditional codecs like JPEG) andcomputer vision tasks (e.g., in image object detection [20]and semantic segmentation [21]). Note that in our featuredecoder on the cloud (Fig. 3d), we upsample the feature by2\xd7to recover spatial details of the original image. With thiscon\ufb01guration of the encoder and the decoder, we are able toreduce the encoding latency and improve the average accuracyby a large margin, as discussed later in Sec. V-D.C. Feature Compression for a Fixed Classi\ufb01erSo far, we have considered the joint training setting, wherethe classi\ufb01er and our feature coder are trained jointly. Inaddition, we also examine the setting in which a pre-trainedclassi\ufb01er is available but \ufb01xed, illustrated in Fig. 3a. Thisscenario may happen in practice where there are different edgedevices sharing a single cloud vision model. The edge devicesmay use different feature encoders/decoders according to theircomputational capacity, but the vision model on the cloudshould be \ufb01xed. In this case, we use the same loss functionas in Eq. 2, but we train the encoder and decoder networkswithout updating the classi\ufb01er. We denote the methods in thissetting by cls. \ufb01xed to distinguish them from the joint trainingsetting, where the classi\ufb01er can be trained together with theencoder and the decoder.V. E XPERIMENTSWe compare our method to previous ones for edge-cloudimage classi\ufb01cation. For fair comparison, all methods use theResNet-50 [14] (for image coding methods) or a truncated--- Page 4 ---version of ResNet-50 (for feature coding methods) as theclassi\ufb01er.A. Dataset and MetricsWe use the 1,000-class ImageNet dataset [22] for trainingand testing. Standard data augmentations, including randomresizing and cropping, are applied during training. We use theImageNet val set for testing, in which we resize and center-crop all images such that they have a resolution of 224 \xd7224.All metrics are computed on the 224 \xd7224 val set. Bit rateis measured in terms of bits per pixel (bpp). For classi\ufb01cationaccuracy, we report the top-1 accuracy of the classi\ufb01er predic-tions. For encoding complexity, we focus on the CPU latencysince it well simulates real-world scenarios. All encodinglatencies are measured using an Intel 12700K CPU, averagedover the \ufb01rst 5,000 images in the val set.We also introduce a new metric, Delta-accuracy, whichcomputes the average accuracy improvement (w.r.t. a baselinemethod) over all bit rates. By using this metric, we cansummarize each rate-accuracy curve into a scalar, thus beingable to represent the three-way RAC trade-off in a 2-D plot.Our computation of Delta-accuracy follows the convention ofBD-PSNR and BD-rate [23] except one aspect. Unlike PSNR,the unit of accuracy is percentage (not decibel), so we computeaverage accuracy improvement over the normal bit rate space(instead of the logarithmic rate scale used in BD-PSNR).B. Experiment SettingsWe consider the following baseline methods.Image coding baselines. We compress images using animage codec at the sender side, and we reconstruct images atthe receiver side using the same codec. Then, we run ResNet-50 on the reconstructed images to infer class prediction. Ourselected codecs include handcrafted (e.g., WebP and BPG [5])and learning-based methods (e.g., the Factorized model [6],and the Hyperprior model [17]).Feature coding baseline. We use the existing best method,Entropic Student [2], as a powerful baseline.As we discussed in Sec. IV-C, we group methods into twogroups so that methods within each can be fairly compared.Classi\ufb01er \ufb01xed setting: The pre-trained ResNet-50 is usedand remains \ufb01xed (i.e., no \ufb01ne-tuning). The image codingbaselines all fall in this setting. We label the methods in thissetting as cls. \ufb01xed.Joint training setting: The pre-trained ResNet-50 is jointlytrained with the feature encoder and/or decoder. The EntropicStudent model falls in this setting. We label the methods inthis setting as joint.C. Main Results for RAC Trade-offTo benchmark our method against the baselines in terms ofRAC performance, we \ufb01rst measure the rate-accuracy curvesfor all baselines and our methods. Then, we compute the Delta-accuracy of each method w.r.t. the Entropic Student baseline(whose Delta-accuracy is 0 by de\ufb01nition). We show the Delta-accuracy as a function of encoding latency in Fig. 4. Note that3 4 7 10 14 20 30 40 50Encoding latency on CPU (ms)121086420Delta-accuracy (%)RAC performance on ImageNetFeature coding: ours (joint)Feature coding: Entropic Student [2] (joint)Feature coding: ours (cls. fixed)Image coding: BPG [5] (cls. fixed)Image coding: Hyperpiror model [17] (cls. fixed)Image coding: Factorized model [6] (cls. fixed)Image coding: WebP (cls. fixed)Fig. 4: RAC trade-off, visualized by Delta-accuracy vs en-coding complexity. The Delta-accuracy is w.r.t. the EntropicStudent baseline.the Entropic Student [2] corresponds to a single point on thecomplexity vs. Delta-accuracy plot since its complexity cannotbe tuned. Similarly, the Factorized model [6] and Hyperpriormodel [17] also produce a single point on the this plot. ForWebP and BPG [5] codec, we control the encoding latency bysetting the compression speed option provided by the codecimplementation. For our method, we control the latency bychoosing N \u2208{0,4,8}, where N is the number of residualblocks in the encoder.From Fig. 4, we observe that for classi\ufb01er \ufb01xed methods,our approach clearly outperforms the image coding baselinesby a large margin, demonstrating the effectiveness of featurecoding. This improvement is expected, because our featureencoder and decoder are explicitly trained to maximize classi-\ufb01cation accuracy, instead of pixel reconstruction as in imagecoding baselines. For the joint training setting, our methodachieves higher Delta-accuracy than the Entropic Student base-line with approximately only half of the encoding latency. Inaddition, the \ufb02exible design of our encoder enables encodingcomplexity adjustment, as opposed to the Entropic Student, bywhich only a single complexity can be achieved.D. Ablation StudyTo analyze how each component of our method contributesto the RAC improvement, we start from the Entropic Studentbaseline, and we include our proposed components one ata time. At each step, we show the change of rate-accuracyperformance as well as the encoding complexity.We \ufb01rst compare our single-stage strategy (referred to ascon\ufb01guration A) with the original two-stage one in Fig. 5. InTable I, we also show the encoding complexity and Delta-accuracy. We observe that the single-stage training does notimpair the performance; instead, the average accuracy slightlyincreases by 0.13% with encoding complexity unchanged.Then, we replace the Entropic Student encoder by ourpatch embedding layer and residual blocks, and we show the--- Page 5 ---0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2Bits per pixel (bpp)57.560.062.565.067.570.072.575.0T op-1 acc. (%)Rate-accuracy trade-off on ImageNetOurs C: B + 8x downsamplingOurs B: A + improved architectureOurs A: Baseline + single-stage trainingBaseline: Entropic StudentFig. 5: Ablation study of our approach compared to the En-tropic Student baseline. Trained and tested on ImageNet [22].TABLE I: Ablation study. FLOPs and encoding latency arefor 224x224 resolution inputs. All percentages are w.r.t. to thebaseline, Entropic Student.FLOPs\u2193 Enc. Latency\u2193 Delta-acc.\u2191Baseline: Entropic Student 0.466B 13.8ms 0.0%A: Baseline + single-stage 0.466B (-0.0%) 13.8ms (-0.0%) +0.13%B: A + improved architecture 0.457B (-0.02%) 9.10ms (-34.1%) +0.20%C: B + 8x downsampling 0.489B (+0.05%) 6.70ms (-51.4%) +0.93%resulting system as con\ufb01guration B in Fig. 5 and Table I. Notethat the downsampling ratio ( 4\xd7) is unchanged. We can seethat the improved architecture reduces the encoding latencyby more than 30% and improves the average accuracy by anadditional small percentage ( 0.07%).Finally, we increase the downsampling ratio to 8\xd7 aswe discussed in Sec. IV-B, and we refer to this version ascon\ufb01guration C . By increasing the downsampling ratio, weagain reduce the encoding latency and improve the averageaccuracy by an additional large margin, after which we achievean overall latency reduction of 50% and an average accuracyimprovement of 0.93%. From Fig. 5, we notice that theimprovement is most signi\ufb01cant at lower bit rates. We con-jecture that lower bit rates naturally call for a more compactcompressed representation, which can be achieved by moreaggressive downsampling. For higher bit rate (e.g., more than0.8 bpp), our approach remains on par with the baseline.VI. C ONCLUSIONIn this paper, we present a feature coding system for imageclassi\ufb01cation in edge-cloud systems. We show that by properdesign of the network architecture, existing feature codingmethods can be improved and extended to consider the RACtrade-off. Our system outperforms all previous methods interms of RAC trade-off, which we believe is valuable formany real-world applications deployment. A limitation of oursystem is the requirement of a separate neural network modelfor every rate-complexity pair, which we leave to our futurework to solve.REFERENCES[1] H. Choi and I. V . Baji \xb4c, \u201cDeep feature compression for collaborativeobject detection,\u201d IEEE International Conference on Image Processing ,pp. 3743\u20133747, Oct. 2018.[2] Y . Matsubara, R. Yang, M. Levorato, and S. Mandt, \u201cSupervised com-pression for resource-constrained edge computing systems,\u201d IEEE/CVFWinter Conference on Applications of Computer Vision , pp. 923\u2013933,Jan. 2022.[3] S. Singh, S. Abu-El-Haija, N. Johnston, J. Ball \xb4e, A. Shrivastava, andG. Toderici, \u201cEnd-to-end learning of compressible features,\u201d IEEEInternational Conference on Image Processing , pp. 3349\u20133353, Oct2020.[4] V . Kakkad, M. Patel, and M. Shah, \u201cBiometric authentication and imageencryption for image security in cloud framework,\u201d Multiscale andMultidisciplinary Modeling, Experiments and Design , vol. 2, no. 4, pp.233\u2013248, May 2019.[5] J. Lainema, F. Bossen, W. Han, J. Min, and K. Ugur, \u201cIntra coding of thehevc standard,\u201d IEEE Transactions on Circuits and Systems for VideoTechnology, vol. 22, no. 12, pp. 1792\u20131801, Dec. 2012.[6] J. Ball \xb4e, V . Laparra, and E. Simoncelli, \u201cEnd-to-end optimizationof nonlinear transform codes for perceptual quality,\u201d Picture CodingSymposium, pp. 1\u20135, Dec. 2016.[7] D. Minnen, J. Ball \xb4e, and G. Toderici, \u201cJoint autoregressive and hier-archical priors for learned image compression,\u201d Advances in NeuralInformation Processing Systems, vol. 31, pp. 10 794\u201310 803, Dec. 2018.[8] H. Choi and I. V . Baji \xb4c, \u201cNear-lossless deep feature compression forcollaborative intelligence,\u201d IEEE International Workshop on MultimediaSignal Processing, pp. 1\u20136, Aug. 2018.[9] \u2014\u2014, \u201cDeep feature compression for collaborative object detection,\u201dIEEE International Conference on Image Processing , pp. 3743\u20133747,Oct. 2018.[10] H. Choi, R. A. Cohen, and I. V . Baji \xb4c, \u201cBack-and-forth prediction fordeep tensor compression,\u201d IEEE International Conference on Acoustics,Speech and Signal Processing , pp. 4467\u20134471, May 2020.[11] R. A. Cohen, H. Choi, and I. V . Baji \xb4c, \u201cLightweight compressionof neural network feature tensors for collaborative intelligence,\u201d IEEEInternational Conference on Multimedia and Expo , pp. 1\u20136, Jul. 2020.[12] Y . Dubois, B. Bloem-Reddy, K. Ullrich, and C. J. Maddison, \u201cLossycompression for lossless prediction,\u201d Advances in Neural InformationProcessing Systems, vol. 34, pp. 14 014\u201314 028, Dec. 2021.[13] J. Ball \xb4e, V . Laparra, and E. P. Simoncelli, \u201cDensity modeling ofimages using a generalized normalization transformation,\u201d InternationalConference on Learning Representations , Jul. 2016.[14] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for imagerecognition,\u201d Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 770\u2013778, Jun. 2016.[15] J. Duda, \u201cAsymmetric numeral systems: entropy coding combiningspeed of huffman coding with compression rate of arithmetic coding,\u201darXiv preprint arXiv:1311.2540 , Dec. 2013.[16] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neuralnetwork,\u201d https://arxiv.org/abs/1503.02531, Mar. 2015.[17] J. Ball \xb4e, D. Minnen, S. Singh, S. Hwang, and N. Johnston, \u201cVariationalimage compression with a scale hyperprior,\u201d International Conferenceon Learning Representations , Apr. 2018.[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words:Transformers for image recognition at scale,\u201d International Conferenceon Learning Representations , May. 2021.[19] D. Hendrycks and K. Gimpel, \u201cGaussian error linear units (gelus),\u201d arXivpreprint arXiv:1606.08415, Jun. 2016.[20] J. Redmon and A. Farhadi, \u201cYolov3: An incremental improvement,\u201darXiv preprint arXiv:1804.02767 , Apr. 2018.[21] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsingnetwork,\u201d Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 6230\u20136239, Jul. 2017.[22] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, \u201cImagenet: A large-scale hierarchical image database,\u201d Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition , pp. 248\u2013255, Sep. 2009.[23] G. Bjontegaard, \u201cCalculation of average psnr differences between rd-curves,\u201d Video Coding Experts Group - M33 , Apr. 2001., Your response: ```ADD_PAPER\n2211.09897v1\nThis paper presents an efficient feature compression method for edge-cloud image classification systems, focusing on optimizing the three-way trade-off between bit rate, classification accuracy, and encoding complexity (RAC trade-off). The authors propose a flexible encoder architecture with residual blocks that can be scaled based on edge device capabilities, along with a single-stage training strategy. Key results show their method achieves 51.4% lower encoding latency and 0.93% higher accuracy compared to prior work (Entropic Student), while supporting adjustable complexity levels. The system demonstrates superior RAC performance across different bit rates, particularly excelling at lower bit rates (0.1-0.8 bpp) where it maintains accuracy while reducing computational overhead.\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\n~~~~~~~~~~\nCurrent Step #21, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification neural architecture search\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 23:
\n~~~~~~~~~~\nHistory: Step #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #19, Phase: literature review, Feedback: 3\n--- Page 1 ---Ef\ufb01cient Feature Compression forEdge-Cloud SystemsZhihao DuanElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.duan90@purdue.eduFengqing ZhuElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.zhu0@purdue.eduAbstract\u2014Optimizing computation in an edge-cloud system isan important yet challenging problem. In this paper, we considera three way trade-off between bit rate, classi\ufb01cation accuracy,and encoding complexity in an edge-cloud image classi\ufb01cationsystem. Our method includes a new training strategy andan ef\ufb01cient encoder architecture to improve the rate-accuracyperformance. Our design can also be easily scaled accordingto different computation resources on the edge device, taking astep towards achieving rate-accuracy-complexity (RAC) trade-off.Under various settings, our feature coding system consistentlyoutperforms previous methods in terms of the RAC performance.Code is made publicly available 1.Index Terms\u2014Feature compression, Coding for machine vision,Image classi\ufb01cation, Edge-cloud systemI. I NTRODUCTIONMobile devices and wearable sensors are essential gatewaysto many autonomous systems. These edge devices are oftendeployed with vision tasks in applications involving robotics,drones, surveillance cameras, and smart home devices. How-ever, due to constraints on the device size and power supply,edge devices cannot support the real-time execution of neuralnetworks-based computer vision methods, which are typicallycomputationally expensive. A common solution is to takeadvantage of the wireless connection to a powerful cloudcomputing resource, forming an edge-cloud system [1], [2]as illustrated in Fig. 1. In such a system, we obtain data (inour context, images) from edge device sensors (cameras) andsend it to the cloud for task execution.Designing practical methods for edge-cloud systems ischallenging from several aspects. First, in most cases wherethe edge-cloud connection is band-limited, we have constraintson the transmission bit rate, requiring the captured images tobe compressed by an encoder on the edge as ef\ufb01ciently as pos-sible. Second, we also have constraints on the computationalcapacity of the edge device, and thus the encoder cannot betoo complex. Finally, for common vision tasks such as theimage classi\ufb01cation, we also desire high accuracy. Taking allthese into consideration, we face a three-way trade-off betweenencoder complexity, bit rate, and vision task accuracy, whichWe term as the rate-accuracy-complexity (RAC) trade-off.1github.com/duanzhiihao/edge-cloud-racEdge device(sender)Cloud computing(receiver)Bits \u201ccat\u201dRateEncoder complexityAccuracyFig. 1: Illustration of an edge-cloud machine vision system,where three objectives are of interest: encoder computationalcomplexity on the edge device, bit rate in transmission, andtask accuracy in the cloud. In this paper, we present a featurecompression method for the edge-cloud image classi\ufb01cation.A straightforward baseline for RAC trade-off is to com-press and transmit images using different codecs. However,image codecs are typically developed for image reconstruction,making them suboptimal for vision task execution. Recently,research focused on feature compression has gained attention,which can be optimized for different vision tasks [1], [3].Another advantage to transmit features rather than images isprivacy protection. By transmitting features, one could skippixel reconstruction on the cloud and thus reduce potentiallyexposing image contents to the attackers [4].Entropic Student [2] is a recently proposed feature com-pression framework that provides promising rate-accuracyperformance. However, it does not take into account theencoder complexity as part of the performance trade-off. Toextend the scope to the three-way RAC trade-off, we proposea \ufb02exible encoder whose complexity can be easily adjustedfor different device capacities. Together with a new trainingstrategy, we achieve better RAC performance compared toprevious methods, including image coding baselines and thestate-of-the-art Entropic Student model.Our contributions can be summarized as follows:\u2022 We expand the scope of previous feature compressionsystems to include the RAC trade-off.\u2022 We design a \ufb02exible neural network architecture forfeature coding, which is both ef\ufb01cient and achieves goodclassi\ufb01cation performance.\u2022 We conduct extensive experiments to verify the effec-tiveness of our approach, showing its advantages overprevious methods.arXiv:2211.09897v1  [eess.IV]  17 Nov 2022--- Page 2 ---II. R ELATED WORKA. Image CompressionTraditional image coders . Traditional image codecs typi-cally follow a linear transform coding paradigm, such as block-based discrete cosine transform (DCT) and discrete wavelettransform (DWT). Representative codecs include JPEG, WebP,and BPG (i.e., HEVC intra [5]). Their encoding complexityvaries depending on different factors such as the choice of theentropy coding algorithms, number of prediction modes, andhow much optimization is done during training. In general,more complex computation devoted at the encoder leads tobetter compression performance.Learned image coders . With the rise of deep learning,learned image coders (LICs) extend the traditional lineartransform coding paradigm to a non-linear fashion, which isachieved by transforming images using neural networks [6].Despite improved compression performance compared to tra-ditional codecs, LICs typically require more computationalresources due to highly parameterized neural network layersand autoregressive entropy models [7].The paradigm of image coding share a common disadvan-tage: they are designed for image reconstruction and may besuboptimal for vision tasks in most edge-cloud settings.B. Feature compressionVarious methods have been proposed to compress andtransmit features rather than images for machine vision.Hand-crafted feature coding. Earlier works have exploredcompressing deep features using hand-crafted algorithms. Choiand Baji \xb4c use a block-based intra and inter coding algorithmfor video feature compression in [8] and a HEVC-based fea-ture compression method for object detection in [9]. Followingtheir works, methods have been proposed for feature compres-sion using pruning [10], quantization [11], and bidirectionalprediction [10].Learned feature compression . Instead of manually de-signed methods, recent work has shifted attention to end-to-end learned feature compression and showed promisingresults. Singh et al. use a fully factorized entropy model toconstrain the entropy of features, which results in improvedrate-accuracy trade-off over previous methods [3]. Dubois etal. show that, given enough computational capacity, featurescan be extremely (around 1,000\xd7) compressed without affect-ing the vision task accuracy [12]. More recently, Matsubaraet al. train vision models with feature compressor jointly andshow state-of-the-art rate-accuracy performance [2].While existing methods mostly focus on improving therate-accuracy performance, their approach lacks scalabilityfor different encoder complexity. In this work, we extendthe scope to rate-accuracy-complexity (RAC) trade-off byproposing a easily scalable encoder architecture, which can beadapted to edge devices with different computation capacities.III. P RELIMINARIESThe current best method in terms of rate-accuracy perfor-mance, Entropic Student [2], is a good starting point for our\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43f\u0ddc\U0001d466\u0ddc\U0001d466teachLabel \U0001d466Conv5x5, s=2GDNConv5x5, s=2GDNConv2x2, s=1Conv2x2, s=1Inv. GDNConv2x2, s=1Inv. GDNConv2x2, s=1DecoderEncoder ClassifierEntropy modelPre-trained image classifierStage 1 Stage 2(a) Training overview. GDN refers to the generalized divisible nor-malization layer [13], the entropy model is fully factorized, and theclassi\ufb01er is ResNet-50 [14]. See text for description.Edge Cloud\u0ddc\U0001d466Enc. Dec.ClassifierEntropy modelrANS rANSBitsEntropy model(b) Testing (deployment). Entropy coding is done by the range-basedvariant of asymmetric numeral systems (rANS) [15].Fig. 2: Model training (a) and deployment (b) in the EntropicStudent framework [2] for feature compression.proposed work. In this section, we brie\ufb02y summarize it toprovide the background of our method, and we refer interestedreaders to the original paper [2] for more details.The framework for classi\ufb01cation is shown in Fig. 2. Forsplit computing, a feature encoder is designed to be deployedon the edge device, and a decoder together with a classi\ufb01er areexecuted on the cloud. Speci\ufb01cally, a ResNet-50 [14] classi\ufb01eris adopted for image classi\ufb01cation.Training is done in two stages. In the \ufb01rst stage, theclassi\ufb01er is initialized using a pre-trained model and \ufb01xed(i.e., no parameter updating), and the encoder and decoder arelearned to compress images and reconstruct desired features.In the second stage, the encoder is \ufb01xed, while the decoderand the classi\ufb01er are trained to optimize the task accuracy.The training loss functions are:Lstage1 = \u03bblR + lMSE,Lstage2 = \u03bblR + 0.5 \xb7(lKL + lCE), (1)where lR is the bit rate loss estimated by a factorized entropymodel [6], lMSE is the mean squared error (MSE) betweenthe classi\ufb01er\u2019s intermediate features and the ones from a pre-trained teacher model, lKL is the KL divergence loss forknowledge distillation [16], lCE is the standard cross-entropyloss for classi\ufb01cation, and \u03bb is a scalar hyperparameter totrade-off between rate and classi\ufb01cation accuracy.The deployment scenario is shown in Fig. 2b, where theentropy model is shared between the sender (edge) and thereceiver (cloud) for entropy coding, implemented by the range-based asymmetric numeral system (rANS) algorithm [15].Despite its strong rate-accuracy performance [2], the En-tropic Student model only operates at a single encoder com-plexity and is not feasible for RAC trade-off. In the nextsection, we introduce our method, which not only improves therate-accuracy performance, but also enables the RAC trade-off.--- Page 3 ---\u0ddc\U0001d466\u0ddc\U0001d466tLabel \U0001d466Enc. Dec. ClassifierPre-trained Classifier\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43fEntropy modelLearned or fixed(a) Training overview.Conv 8x8, s=8Res. BlockRes. Block\U0001d441 blocks\U0001d467 (b) Encoder architecture.GELUConv 3x3, s=1GELUConv 3x3, s=1 (c) Residual block.GELUConvT5x5Conv 3x3, s=1GELUConv 3x3, s=1GELUConv 1x1, s=1\U0001d467 (d) Decoder architecture.Fig. 3: Summary of proposed method. In (a), we show the training strategy, where the entropy model is fully factorized, andgray blocks indicate frozen models (i.e., no parameter update during training). The classi\ufb01er can either be trained jointly withthe system, or initialized with a pre-trained model and \ufb01xed throughout training. In (b) and (c), we show our encoder anddecoder architecture, respectively. Our encoder network complexity can be tuned by adjusting the number of residual blocks,N. For the classi\ufb01er, we use a truncated version of ResNet-50, following [2].IV. O UR APPROACHInspired by the success of knowledge distillation in the En-tropic Student model, we adopt a similar framework and lossfunctions, shown in Fig. 3a. However, we make modi\ufb01cationsincluding a new training strategy and network architecture, bywhich we are able to realize the RAC trade-off.A. Single-stage TrainingThe Entropic Student method adopts a two-stage trainingstrategy, where the encoder and decoder are trained in the\ufb01rst stage, and the decoder and classi\ufb01er are \ufb01ne-tuned in thesecond stage. We note that this decoupled training strategyrequires more hyperparameter searching, and it is also sub-optimal for rate-accuracy performance since network compo-nents are trained separately. Instead, we propose a simple \ufb01x,i.e., to train the encoder, decoder, and classi\ufb01er in an end-to-end fashion. Our single-stage loss function is as follows:Lsingle = \u03bblR + lMSE + 0.5 \xb7(lKL + lCE), (2)where the terms are de\ufb01ned in Sec. III. We keep the samescaling factor for each loss term as in the Entropic Studentwithout further tuning. Later in Sec. V-D, we show thatour single-stage strategy improves the rate-accuracy trade-offdespite being simpler to implement.B. Network ArchitectureWe introduce an encoder network architecture speci\ufb01callydesigned for ef\ufb01cient and \ufb02exible feature compression, shownin Fig. 3b. We now describe our design choices in details.Downsampling layer. We \ufb01rst noticed that the 5x5 con-volutions for downsampling in previous works (e.g., in theEntropic Student [2] and learned image coders [6], [17]) arecomputationally inef\ufb01cient due to the large overlap betweenconvolutional kernels. To remove this overlap, we adopt asingle patch embedding [18] layer for downsampling, whichcan be equivalently viewed as a convolution with kernel sizeequal to its strides.Network blocks. Instead of feedforward convolutions asin previous works, we adopt residual blocks in our featureencoder, which enables easy adjustment of the encoder com-plexity simply by setting the number of residual blocks, N. Alarger N (i.e., more residual blocks in the encoder) improvesthe rate-accuracy performance while results in higher encodercomplexity. Thus by tuning N, we can trade-off between therate-accuracy performance and the encoding complexity of oursystem. We use a standard design for the residual blocks asshown in Fig. 3c. For non-linearity layers, we use the GeLUactivation [19] instead of GDN [13], as we empirically foundthat the former leads to better performance with the samecomputational complexity.Downsampling ratio. Our \ufb01nal step is to use 8\xd7downsam-pling, instead of the 4\xd7downsampling in Entropic Student,during encoding. Our choice is motivated by the observationthat 8\xd7downsampling is a common choice for various imageprocessing tasks (e.g., in traditional codecs like JPEG) andcomputer vision tasks (e.g., in image object detection [20]and semantic segmentation [21]). Note that in our featuredecoder on the cloud (Fig. 3d), we upsample the feature by2\xd7to recover spatial details of the original image. With thiscon\ufb01guration of the encoder and the decoder, we are able toreduce the encoding latency and improve the average accuracyby a large margin, as discussed later in Sec. V-D.C. Feature Compression for a Fixed Classi\ufb01erSo far, we have considered the joint training setting, wherethe classi\ufb01er and our feature coder are trained jointly. Inaddition, we also examine the setting in which a pre-trainedclassi\ufb01er is available but \ufb01xed, illustrated in Fig. 3a. Thisscenario may happen in practice where there are different edgedevices sharing a single cloud vision model. The edge devicesmay use different feature encoders/decoders according to theircomputational capacity, but the vision model on the cloudshould be \ufb01xed. In this case, we use the same loss functionas in Eq. 2, but we train the encoder and decoder networkswithout updating the classi\ufb01er. We denote the methods in thissetting by cls. \ufb01xed to distinguish them from the joint trainingsetting, where the classi\ufb01er can be trained together with theencoder and the decoder.V. E XPERIMENTSWe compare our method to previous ones for edge-cloudimage classi\ufb01cation. For fair comparison, all methods use theResNet-50 [14] (for image coding methods) or a truncated--- Page 4 ---version of ResNet-50 (for feature coding methods) as theclassi\ufb01er.A. Dataset and MetricsWe use the 1,000-class ImageNet dataset [22] for trainingand testing. Standard data augmentations, including randomresizing and cropping, are applied during training. We use theImageNet val set for testing, in which we resize and center-crop all images such that they have a resolution of 224 \xd7224.All metrics are computed on the 224 \xd7224 val set. Bit rateis measured in terms of bits per pixel (bpp). For classi\ufb01cationaccuracy, we report the top-1 accuracy of the classi\ufb01er predic-tions. For encoding complexity, we focus on the CPU latencysince it well simulates real-world scenarios. All encodinglatencies are measured using an Intel 12700K CPU, averagedover the \ufb01rst 5,000 images in the val set.We also introduce a new metric, Delta-accuracy, whichcomputes the average accuracy improvement (w.r.t. a baselinemethod) over all bit rates. By using this metric, we cansummarize each rate-accuracy curve into a scalar, thus beingable to represent the three-way RAC trade-off in a 2-D plot.Our computation of Delta-accuracy follows the convention ofBD-PSNR and BD-rate [23] except one aspect. Unlike PSNR,the unit of accuracy is percentage (not decibel), so we computeaverage accuracy improvement over the normal bit rate space(instead of the logarithmic rate scale used in BD-PSNR).B. Experiment SettingsWe consider the following baseline methods.Image coding baselines. We compress images using animage codec at the sender side, and we reconstruct images atthe receiver side using the same codec. Then, we run ResNet-50 on the reconstructed images to infer class prediction. Ourselected codecs include handcrafted (e.g., WebP and BPG [5])and learning-based methods (e.g., the Factorized model [6],and the Hyperprior model [17]).Feature coding baseline. We use the existing best method,Entropic Student [2], as a powerful baseline.As we discussed in Sec. IV-C, we group methods into twogroups so that methods within each can be fairly compared.Classi\ufb01er \ufb01xed setting: The pre-trained ResNet-50 is usedand remains \ufb01xed (i.e., no \ufb01ne-tuning). The image codingbaselines all fall in this setting. We label the methods in thissetting as cls. \ufb01xed.Joint training setting: The pre-trained ResNet-50 is jointlytrained with the feature encoder and/or decoder. The EntropicStudent model falls in this setting. We label the methods inthis setting as joint.C. Main Results for RAC Trade-offTo benchmark our method against the baselines in terms ofRAC performance, we \ufb01rst measure the rate-accuracy curvesfor all baselines and our methods. Then, we compute the Delta-accuracy of each method w.r.t. the Entropic Student baseline(whose Delta-accuracy is 0 by de\ufb01nition). We show the Delta-accuracy as a function of encoding latency in Fig. 4. Note that3 4 7 10 14 20 30 40 50Encoding latency on CPU (ms)121086420Delta-accuracy (%)RAC performance on ImageNetFeature coding: ours (joint)Feature coding: Entropic Student [2] (joint)Feature coding: ours (cls. fixed)Image coding: BPG [5] (cls. fixed)Image coding: Hyperpiror model [17] (cls. fixed)Image coding: Factorized model [6] (cls. fixed)Image coding: WebP (cls. fixed)Fig. 4: RAC trade-off, visualized by Delta-accuracy vs en-coding complexity. The Delta-accuracy is w.r.t. the EntropicStudent baseline.the Entropic Student [2] corresponds to a single point on thecomplexity vs. Delta-accuracy plot since its complexity cannotbe tuned. Similarly, the Factorized model [6] and Hyperpriormodel [17] also produce a single point on the this plot. ForWebP and BPG [5] codec, we control the encoding latency bysetting the compression speed option provided by the codecimplementation. For our method, we control the latency bychoosing N \u2208{0,4,8}, where N is the number of residualblocks in the encoder.From Fig. 4, we observe that for classi\ufb01er \ufb01xed methods,our approach clearly outperforms the image coding baselinesby a large margin, demonstrating the effectiveness of featurecoding. This improvement is expected, because our featureencoder and decoder are explicitly trained to maximize classi-\ufb01cation accuracy, instead of pixel reconstruction as in imagecoding baselines. For the joint training setting, our methodachieves higher Delta-accuracy than the Entropic Student base-line with approximately only half of the encoding latency. Inaddition, the \ufb02exible design of our encoder enables encodingcomplexity adjustment, as opposed to the Entropic Student, bywhich only a single complexity can be achieved.D. Ablation StudyTo analyze how each component of our method contributesto the RAC improvement, we start from the Entropic Studentbaseline, and we include our proposed components one ata time. At each step, we show the change of rate-accuracyperformance as well as the encoding complexity.We \ufb01rst compare our single-stage strategy (referred to ascon\ufb01guration A) with the original two-stage one in Fig. 5. InTable I, we also show the encoding complexity and Delta-accuracy. We observe that the single-stage training does notimpair the performance; instead, the average accuracy slightlyincreases by 0.13% with encoding complexity unchanged.Then, we replace the Entropic Student encoder by ourpatch embedding layer and residual blocks, and we show the--- Page 5 ---0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2Bits per pixel (bpp)57.560.062.565.067.570.072.575.0T op-1 acc. (%)Rate-accuracy trade-off on ImageNetOurs C: B + 8x downsamplingOurs B: A + improved architectureOurs A: Baseline + single-stage trainingBaseline: Entropic StudentFig. 5: Ablation study of our approach compared to the En-tropic Student baseline. Trained and tested on ImageNet [22].TABLE I: Ablation study. FLOPs and encoding latency arefor 224x224 resolution inputs. All percentages are w.r.t. to thebaseline, Entropic Student.FLOPs\u2193 Enc. Latency\u2193 Delta-acc.\u2191Baseline: Entropic Student 0.466B 13.8ms 0.0%A: Baseline + single-stage 0.466B (-0.0%) 13.8ms (-0.0%) +0.13%B: A + improved architecture 0.457B (-0.02%) 9.10ms (-34.1%) +0.20%C: B + 8x downsampling 0.489B (+0.05%) 6.70ms (-51.4%) +0.93%resulting system as con\ufb01guration B in Fig. 5 and Table I. Notethat the downsampling ratio ( 4\xd7) is unchanged. We can seethat the improved architecture reduces the encoding latencyby more than 30% and improves the average accuracy by anadditional small percentage ( 0.07%).Finally, we increase the downsampling ratio to 8\xd7 aswe discussed in Sec. IV-B, and we refer to this version ascon\ufb01guration C . By increasing the downsampling ratio, weagain reduce the encoding latency and improve the averageaccuracy by an additional large margin, after which we achievean overall latency reduction of 50% and an average accuracyimprovement of 0.93%. From Fig. 5, we notice that theimprovement is most signi\ufb01cant at lower bit rates. We con-jecture that lower bit rates naturally call for a more compactcompressed representation, which can be achieved by moreaggressive downsampling. For higher bit rate (e.g., more than0.8 bpp), our approach remains on par with the baseline.VI. C ONCLUSIONIn this paper, we present a feature coding system for imageclassi\ufb01cation in edge-cloud systems. We show that by properdesign of the network architecture, existing feature codingmethods can be improved and extended to consider the RACtrade-off. Our system outperforms all previous methods interms of RAC trade-off, which we believe is valuable formany real-world applications deployment. A limitation of oursystem is the requirement of a separate neural network modelfor every rate-complexity pair, which we leave to our futurework to solve.REFERENCES[1] H. Choi and I. V . Baji \xb4c, \u201cDeep feature compression for collaborativeobject detection,\u201d IEEE International Conference on Image Processing ,pp. 3743\u20133747, Oct. 2018.[2] Y . Matsubara, R. Yang, M. Levorato, and S. Mandt, \u201cSupervised com-pression for resource-constrained edge computing systems,\u201d IEEE/CVFWinter Conference on Applications of Computer Vision , pp. 923\u2013933,Jan. 2022.[3] S. Singh, S. Abu-El-Haija, N. Johnston, J. Ball \xb4e, A. Shrivastava, andG. Toderici, \u201cEnd-to-end learning of compressible features,\u201d IEEEInternational Conference on Image Processing , pp. 3349\u20133353, Oct2020.[4] V . Kakkad, M. Patel, and M. Shah, \u201cBiometric authentication and imageencryption for image security in cloud framework,\u201d Multiscale andMultidisciplinary Modeling, Experiments and Design , vol. 2, no. 4, pp.233\u2013248, May 2019.[5] J. Lainema, F. Bossen, W. Han, J. Min, and K. Ugur, \u201cIntra coding of thehevc standard,\u201d IEEE Transactions on Circuits and Systems for VideoTechnology, vol. 22, no. 12, pp. 1792\u20131801, Dec. 2012.[6] J. Ball \xb4e, V . Laparra, and E. Simoncelli, \u201cEnd-to-end optimizationof nonlinear transform codes for perceptual quality,\u201d Picture CodingSymposium, pp. 1\u20135, Dec. 2016.[7] D. Minnen, J. Ball \xb4e, and G. Toderici, \u201cJoint autoregressive and hier-archical priors for learned image compression,\u201d Advances in NeuralInformation Processing Systems, vol. 31, pp. 10 794\u201310 803, Dec. 2018.[8] H. Choi and I. V . Baji \xb4c, \u201cNear-lossless deep feature compression forcollaborative intelligence,\u201d IEEE International Workshop on MultimediaSignal Processing, pp. 1\u20136, Aug. 2018.[9] \u2014\u2014, \u201cDeep feature compression for collaborative object detection,\u201dIEEE International Conference on Image Processing , pp. 3743\u20133747,Oct. 2018.[10] H. Choi, R. A. Cohen, and I. V . Baji \xb4c, \u201cBack-and-forth prediction fordeep tensor compression,\u201d IEEE International Conference on Acoustics,Speech and Signal Processing , pp. 4467\u20134471, May 2020.[11] R. A. Cohen, H. Choi, and I. V . Baji \xb4c, \u201cLightweight compressionof neural network feature tensors for collaborative intelligence,\u201d IEEEInternational Conference on Multimedia and Expo , pp. 1\u20136, Jul. 2020.[12] Y . Dubois, B. Bloem-Reddy, K. Ullrich, and C. J. Maddison, \u201cLossycompression for lossless prediction,\u201d Advances in Neural InformationProcessing Systems, vol. 34, pp. 14 014\u201314 028, Dec. 2021.[13] J. Ball \xb4e, V . Laparra, and E. P. Simoncelli, \u201cDensity modeling ofimages using a generalized normalization transformation,\u201d InternationalConference on Learning Representations , Jul. 2016.[14] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for imagerecognition,\u201d Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 770\u2013778, Jun. 2016.[15] J. Duda, \u201cAsymmetric numeral systems: entropy coding combiningspeed of huffman coding with compression rate of arithmetic coding,\u201darXiv preprint arXiv:1311.2540 , Dec. 2013.[16] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neuralnetwork,\u201d https://arxiv.org/abs/1503.02531, Mar. 2015.[17] J. Ball \xb4e, D. Minnen, S. Singh, S. Hwang, and N. Johnston, \u201cVariationalimage compression with a scale hyperprior,\u201d International Conferenceon Learning Representations , Apr. 2018.[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words:Transformers for image recognition at scale,\u201d International Conferenceon Learning Representations , May. 2021.[19] D. Hendrycks and K. Gimpel, \u201cGaussian error linear units (gelus),\u201d arXivpreprint arXiv:1606.08415, Jun. 2016.[20] J. Redmon and A. Farhadi, \u201cYolov3: An incremental improvement,\u201darXiv preprint arXiv:1804.02767 , Apr. 2018.[21] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsingnetwork,\u201d Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 6230\u20136239, Jul. 2017.[22] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, \u201cImagenet: A large-scale hierarchical image database,\u201d Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition , pp. 248\u2013255, Sep. 2009.[23] G. Bjontegaard, \u201cCalculation of average psnr differences between rd-curves,\u201d Video Coding Experts Group - M33 , Apr. 2001., Your response: ```ADD_PAPER\n2211.09897v1\nThis paper presents an efficient feature compression method for edge-cloud image classification systems, focusing on optimizing the three-way trade-off between bit rate, classification accuracy, and encoding complexity (RAC trade-off). The authors propose a flexible encoder architecture with residual blocks that can be scaled based on edge device capabilities, along with a single-stage training strategy. Key results show their method achieves 51.4% lower encoding latency and 0.93% higher accuracy compared to prior work (Entropic Student), while supporting adjustable complexity levels. The system demonstrates superior RAC performance across different bit rates, particularly excelling at lower bit rates (0.1-0.8 bpp) where it maintains accuracy while reducing computational overhead.\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\n~~~~~~~~~~\nCurrent Step #22, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2409.08943v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 24:
\n~~~~~~~~~~\nHistory: Step #5, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```ADD_PAPER\n2307.04443v1\nThis paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #22, Phase: literature review, Feedback: 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co, Your response: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach for joint image denoising and classification optimized for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, classification accuracy, and denoising performance. Key results show their NAS-designed models outperform manually designed alternatives by significant margins - achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency, while reducing computational overhead. The integrated design proves particularly effective for human-in-the-loop applications like medical imaging and surveillance systems where both classification and visual clarity are crucial.\n```\n~~~~~~~~~~\nCurrent Step #23, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2409.08943v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach for joint image denoising and classification optimized for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, classification accuracy, and denoising performance. Key results show their NAS-designed models outperform manually designed alternatives by significant margins - achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency, while reducing computational overhead. The integrated design proves particularly effective for human-in-the-loop applications like medical imaging and surveillance systems where both classification and visual clarity are crucial.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 25:
\n~~~~~~~~~~\nHistory: Step #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #22, Phase: literature review, Feedback: 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co, Your response: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach for joint image denoising and classification optimized for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, classification accuracy, and denoising performance. Key results show their NAS-designed models outperform manually designed alternatives by significant margins - achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency, while reducing computational overhead. The integrated design proves particularly effective for human-in-the-loop applications like medical imaging and surveillance systems where both classification and visual clarity are crucial.\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\n~~~~~~~~~~\nCurrent Step #24, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification hardware-aware NAS\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 26:
\n~~~~~~~~~~\nHistory: Step #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #22, Phase: literature review, Feedback: 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co, Your response: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach for joint image denoising and classification optimized for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, classification accuracy, and denoising performance. Key results show their NAS-designed models outperform manually designed alternatives by significant margins - achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency, while reducing computational overhead. The integrated design proves particularly effective for human-in-the-loop applications like medical imaging and surveillance systems where both classification and visual clarity are crucial.\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\n~~~~~~~~~~\nCurrent Step #25, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignDimitrios Stamoulis 1 * Ruizhou Ding 1 Di Wang2 Dimitrios Lymberopoulos2 Bodhi Priyantha 2 Jie Liu 3Diana Marculescu 1AbstractCan we automatically design a Convolutional Net-work (ConvNet) with the highest image classi\ufb01-cation accuracy under the latency constraint of amobile device? Neural Architecture Search (NAS)for ConvNet design is a challenging problem dueto the combinatorially large design space andsearch time (at least 200 GPU-hours). To alleviatethis complexity, we propose Single-Path NAS, anovel differentiable NAS method for designingdevice-ef\ufb01cient ConvNets in less than 4 hours .1. Novel NAS formulation : our method intro-duces a single-path, over-parameterized ConvNetto encode all architectural decisions with sharedconvolutional kernel parameters. 2. NAS ef\ufb01-ciency: Our method decreases the NAS searchcost down to 8 epochs (30 TPU-hours), i.e., up to5,000\xd7faster compared to prior work. 3. On-device image classi\ufb01cation : Single-Path NASachieves 74.96% top-1 accuracy on ImageNetwith 79ms inference latency on a Pixel 1 phone,which is state-of-the-art accuracy compared toNAS methods with similar latency (\u226480ms).1. Introduction\u201cIs it possible to reduce the NAS search cost down to onlyfew hours?\u201d NAS methods have revolutionized the designof ConvNets (Zoph et al., 2017), yielding state-of-the-art re-sults in deep learning applications (Real et al., 2018). NAShas a profound impact on the design of hardware-ef\ufb01cientConvNets for on-device computer vision, e.g., under infer-ence latency constraints on a mobile device (Tan et al., 2018).However, NAS remains an intrinsically costly problem with*Extended abstract of ODML-CDNNR 2019 presentation (re-quired non-archival arxiv.org version). Full paper can be foundin (Stamoulis et al., 2019). 1Department of ECE, Carnegie MellonUniversity, Pittsburgh, PA, USA2Microsoft, Redmond, W A, USA3Harbin Institute of Technology, Harbin, China. Correspondenceto: Dimitrios Stamoulis <dstamoul@andrew.cmu.edu>.Joint Workshop onOn-Device Machine Learning & Compact DeepNeural Network Representations (ODML-CDNNR 2019). ICML2019 Workshop. Copyright 2019 by the author(s).Figure 1.Single-Path NAS directly optimizes for the subset of con-volution weights of an over-parameterized \u201csuperkernel\u201d in eachConvNet layer (right). Our novel view of the design space elimi-nates the need for maintaining separate paths for each candidateoperation, as in previous multi-path approaches (left).a combinatorially large search space: e.g., searching for aConvNet with 22 layers and \ufb01ve candidate operations perlayer yields 522 \u22481015 possible networks.Inef\ufb01ciencies of multi-path NAS: Recent methods use one-shot formulations (Liu et al., 2018; Pham et al., 2018) byviewing the NAS problem as an operation/path selectionproblem: \ufb01rst, an over-parameterized, multi-path supernetis constructed, where, for each layer, every candidate oper-ation is added as a separate trainable path (Figure 1, left).Next, NAS searches for the paths of the multi-path supernetthat yield the optimal architecture. As expected, naivelybranching out all paths is inef\ufb01cient, since the number oftrainable parameters during the search grows linearly withrespect to the number of candidate operations per layer (Ben-der et al., 2018). To tame the memory explosion due to themulti-path supernet, current methods employ \u201cworkaround\u201dsolutions: e.g., searching on a proxy dataset (Wu et al.,2018), or employing a memory-wise scheme where onlyfew paths are updated during search (Cai et al., 2019). Nev-ertheless, these methods remain considerably costly, withtotal computational demand of at least 200 GPU-hours.In this paper, we propose Single-Path NAS, a novel NASmethod for designing hardware-ef\ufb01cient ConvNets in lessthan 4 hours . Our key insight is illustrated in Figure 1(right). Our key observation is that different candidateconvolutional operations can be viewed as subsets of theweights of an over-parameterizedsingle \u201csuperkernel\u201d. In-stead of choosing from different paths/operations as inmulti-arXiv:1905.04159v1  [cs.LG]  10 May 2019--- Page 2 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignFigure 2.Single-path NAS builds upon hierarchical MobileNetV2-like search spaces (Tan et al., 2018) to identify the mobile invertedbottleneck convolution (MBConv) per layer (left). Our one-shot supernet encapsulates all possible NAS architectures in the search space,i.e., different kernel size (middle) and expansion ratio (right) values, without the need for appending each candidate operation as a separatepath. Single-Path NAS directly searches over the weights of the per-layer searchable \u201csuperkernel\u201d that encodes all MBConv types.path methods, we view the NAS problem as \ufb01nding whichsubset of the per-layer \u201csuperkernel\u201d weights to use, hencesearching across a single-path one-shot NAS supernet.2. Proposed Method: Single-Path NASSearch space : We build upon the MobileNetV2-basedmacro-architecture (Figure 2, left), where layers are groupedinto blocks based on their \ufb01lter sizes (Sandler et al., 2018).Each layer is a mobile inverted bottleneck convolution(conv) MBConv-k\xd7k-e(Sandler et al., 2018), (i.e., a point-wise 1 \xd71 conv with expansion ratio e, a k\xd7kdepthwiseconv, and a linear 1 \xd71 conv; Figure 2, left). Our goal is toidentify the MBConv-k\xd7k-etype per layer.\u201cSuperkernel\u201d-based formulation: Our key insight is thatthe candidate operations can be viewed as subsets of the\u201csu-perkernel\u201d weights. Without loss of generality, we denotethe weights of two candidate operations, e.g., 3 \xd73 conv or5\xd75 conv, asw3\xd73 and w5\xd75, respectively. We observe thatw3\xd73 can be viewed as the inner core of the w5\xd75 kernel,while \u201czeroing\u201d out the \u201couter\u201d shell w5\xd75\\3\xd73 (Figure 2,middle). Thus, we can write the NAS decision as:wk = w3\xd73 + 1 (\ued79\ued79w5\xd75\\3\xd73\ued79\ued792>tk=5) \xb7w5\xd75\\3\xd73 (1)where 1 (\xb7) is the indicator function that encodes the archi-tectural NAS choice and tk=5 is a latent variable ( e.g., athreshold value) that controls the decision.Trainable NAS decisions: Drawing inspiration from quan-tization decisions (Ding et al., 2019), we use the groupLasso term in the 1 (\xb7) condition. Instead of picking thethresholds (e.g., tk=5) by hand, we seamlessly treat themas trainable parameters to learn via gradient descent. Tocompute the gradients for thresholds, we relax the indicatorfunction g(x,t) = 1 (x > t) to a sigmoid function \u03c3(\xb7),when computing gradients, i.e., \u02c6g(x,t) =\u03c3(x>t ).Searching for expansion ratio or skip-op : Since thekernel-based wk result (Equation 1) is a kernel itself, wecan in turn apply our formulation to encode expansion ratiodecisions, where e= 3or e= 6correspond to using half orall the channels of an MBConv-k\xd7k-6 layer, respectively(Figure 2, right). Finally, by \u201czeroing\u201d out all channels, weencode the NAS decision of dropping the entire layer:w = 1 (\u2225wk,3\u22252 >te=3) \xb7(wk,3+1 (\ued79\ued79wk,6\\3\ued79\ued792>te=6) \xb7wk,6\\3)(2)Hence, our searchable superkernelcan suf\ufb01ciently captureany MBConv type in the MobileNetV2-based design space(Figure 2). For input x, the output of the i-th MBConv layerof the network is oi(x) =conv(x,wi|tik=5,tie=6,tie=3).Differentiable NAS: To account for both the accuracy andinference latency of the searched ConvNet, we use a latency-aware formulation for the NAS problem (Wu et al., 2018):minwCE(w|tk,te) +\u03bb\xb7log(R(w|tk,te)) (3)where CE is the cross-entropy loss of the single-path modeland Ris the runtime in milliseconds (ms) of the searchedNAS model on the target device. The coef\ufb01cient \u03bbmodu-lates the trade-off between cross-entropy and runtime.Runtime model: Prior art has showed that the on-deviceConvNet runtime can be modeled as the sum of each i-thlayer\u2019s runtimeR(w|tk,te) =\u2211i Ri(wi|tik,tie) (Wu et al.,2018; Cai et al., 2017; Stamoulis et al., 2018a). To preservethe differentiability of the objective, we formulate the per-layer Ri as a function of the NAS decisions. We pro\ufb01le thetarget mobile device (Pixel 1 smartphone) and we recordthe runtime for each candidate kernel operation per layer i.As a function of the expansion ratio decisions, we write:Rie = 1 (\u2225wk,3\u22252 >te=3) \xb7(Ri5\xd75,3+1 (\ued79\ued79wk,6\\3\ued79\ued792>te=6) \xb7(Ri5\xd75,6 \u2212Ri5\xd75,3))(4)By incorporating the kernel size decision, the runtime is:Ri =Ri3\xd73,6Ri5\xd75,6\xb7Rie+Rie \xb7(1 \u2212Ri3\xd73,6Ri5\xd75,6) \xb71 (\ued79\ued79w5\xd75\\3\xd73\ued79\ued792>tk=5)(5)--- Page 3 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignTable 1.Single-Path NAS achieves state-of-the-art image classi\ufb01cation accuracy (%) on ImageNet for similar on-device latency settingcompared to previous NAS methods (\u226480ms on Pixel 1), with up to 5, 000\xd7reduced search cost in terms of number of epochs. *Thesearch cost in epochs is estimated based on the claim (Cai et al., 2019) that ProxylessNAS is 200\xd7faster than MnasNet. \u2021ChamNet doesnot detail the model derived under runtime constraints (Dai et al., 2018) so we cannot retrain or measure the latency.METHOD TOP-1 ACC (%) T OP-5 ACC (%) R UNTIME (MS) S EARCH COST (EPOCHS )MOBILE NETV2 (SANDLER ET AL ., 2018) 72.00 91.00 75.00 -MOBILE NETV2 (OUR IMPL .) 73.59 91.41 73.57RANDOM SEARCH 73.78 \xb10.85 91.42 \xb10.56 77.31 \xb10.9 MS -MNASNET (TAN ET AL ., 2018) 74.00 91.80 76.00 40,000MNASNET (OUR IMPL .) 74.61 91.95 74.65CHAMNET-B (DAI ET AL ., 2018) 73.80 \u2013 \u2013 240 \u2021PROXYLESS NAS-R (CAI ET AL ., 2019) 74.60 92.20 78.00 200*PROXYLESS NAS-R (OUR IMPL .) 74.65 92.18 77.48FBNET-B (WU ET AL ., 2018) 74.1 - - 90FBNET-B (OUR IMPL .) 73.70 91.51 78.33Single-PathNAS (PROPOSED ) 74.96 92.21 79.48 8 (3.75 HOURS)As in Equation 1, we relax the indicator function to a sig-moid function \u03c3(\xb7) when computing gradients. To evaluatethe prediction accuracy of the runtime model, we generate100 random ConvNets and we measure their runtime onthe device. Our model can accurately predict the actualruntimes: the Root Mean Squared Error (RMSE) is 1.32ms,which corresponds to an average 1.76% prediction error.3. ExperimentsExperimental Setup: We select Pixel 1 as the target devicesince it allows for a representative comparison with priorwork that optimizes for this platform. We run our frameworkusing TensorFlow (TF) on TPUs-v2 (Jouppi et al.,2017). We deploy the ConvNets on the device with TFTFLite. We pro\ufb01le on-device runtime using the FacebookAI Performance Evaluation Platform (FAI-PEP, 2018). Weimplement our trainable \u201csuperkernels\u201d on Keras.We apply our method to design ConvNets for image classi\ufb01-cation on ImageNet (Deng et al., 2009) running on Pixel 1with an overall target latency of 80ms. We train the derivedSingle-Path NAS model for 350 epochs. We summarize theresults in Table 1. To enable a representative comparison ofthe search cost per method, we directly report the numberof epochs per method, hence canceling out the effect ofdifferent hardware systems (GPU vs TPU hours).State-of-the-art on-device image classi\ufb01cation : Single-Path NAS achieves top-1 accuracy of 74.96%, which is thenew state-of-the-art ImageNet accuracy among hardware-ef\ufb01cient NAS methods. More speci\ufb01cally, our methodachieves better top-1 accuracy than ProxylessNAS by+0.31%, while maintaining on par target latency of\u226480mson the same target mobile phone. Single-Path NAS out-performs methods in this mobile latency range, i.e., bet-ter than MnasNet (+0.35%), FBNet-B (+0.86%), and Mo-bileNetV2 (+1.37%).Reduced search cost: Single-Path NAS has a total searchcost of 8 epochs, which is 5,000\xd7faster than MnasNet,25\xd7faster than ProxylessNAS, and 11\xd7faster than FBNet.Speci\ufb01cally, MnasNet reports a total of 40k train epochs. Inturn, ChamNet trains an accuracy predictor on 240 samples.ProxylessNAS reports 200\xd7search cost improvement overMnasNet, hence the overall cost is the TPU-equivalent of200 epochs. Finally, FBNet reports 90 epochs. Overall, wesearch for \u223c10ksteps (8 epochs with a batch size of1024),which corresponds to total wall-clock time of3.75 hours ona TPUv2. In particular, given than a TPUv2 has 2 chips with4 cores each, this corresponds to a total of 30 TPU-hours.4. Discussion & Future WorkNovel idea: The key insight behind our work is to revisitthe one-shot supernet NAS design space with a single-pathview, by formulating the NAS problem as\ufb01nding which sub-set of kernel weights to use in each ConvNet layer. Whileconcurrent works consider relaxed convolution formula-tions (Shin et al., 2018; Hundt et al., 2019; Guo et al., 2019),they either use design spaces and objectives that have beenshown to be hardware inef\ufb01cient ( e.g., cell-based space,FLOP count), or they do not intrinsically relax the kernelover both kernel-size and channels dimensions.Future work : The ef\ufb01ciency of our single-path designspace could enable future work beyond our differentiableNAS formulation and based on reinforcement learning orevolutionary methods. Moreover, our methodology can be\ufb02exibly extended to other hardware design goals,e.g., power,memory, energy, and communication constraints (Donget al., 2018; Stamoulis et al., 2018b;b). To this end andto foster reproducibility, unlike all recent hardware-ef\ufb01cientNAS methods which release pretrained models only, weopen-source and fully document our method at: https://github.com/dstamoulis/single-path-nas.--- Page 4 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignReferencesBender, G., Kindermans, P.-J., Zoph, B., Vasudevan, V ., andLe, Q. Understanding and simplifying one-shot archi-tecture search. In International Conference on MachineLearning, pp. 549\u2013558, 2018.Cai, E., Juan, D.-C., Stamoulis, D., and Marculescu, D. Neu-ralpower: Predict and deploy energy-ef\ufb01cient convolu-tional neural networks. In Asian Conference on MachineLearning, pp. 622\u2013637, 2017.Cai, H., Zhu, L., and Han, S. ProxylessNAS: Direct neu-ral architecture search on target task and hardware. InInternational Conference on Learning Representations,2019.Dai, X., Zhang, P., Wu, B., Yin, H., Sun, F., Wang, Y .,Dukhan, M., Hu, Y ., Wu, Y ., Jia, Y ., et al. Chamnet:Towards ef\ufb01cient network design through platform-awaremodel adaptation. arXiv preprint arXiv:1812.08934 ,2018.Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,L. Imagenet: A large-scale hierarchical image database.In 2009 IEEE conference on computer vision and patternrecognition, pp. 248\u2013255. Ieee, 2009.Ding, R., Liu, Z., Chin, T.-W., Marculescu, D., and Blan-ton, R. Flightnns: Lightweight quantized deep neuralnetworks for fast and accurate inference. In 2019 DesignAutomation Conference (DAC), 2019.Dong, J.-D., Cheng, A.-C., Juan, D.-C., Wei, W., andSun, M. Dpp-net: Device-aware progressive searchfor pareto-optimal neural architectures. arXiv preprintarXiv:1806.08198, 2018.FAI-PEP. Facebook AI Performance EvaluationPlatform. https://github.com/facebook/FAI-PEP, 2018.Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y ., andSun, J. Single path one-shot neural architecture searchwith uniform sampling. arXiv preprint arXiv:1904.00420,2019.Hundt, A., Jain, V ., and Hager, G. D. sharpdarts: Faster andmore accurate differentiable architecture search. arXivpreprint arXiv:1903.09900, 2019.Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,A., et al. In-datacenter performance analysis of a tensorprocessing unit. In 2017 ACM/IEEE 44th Annual Inter-national Symposium on Computer Architecture (ISCA),pp. 1\u201312. IEEE, 2017.Liu, H., Simonyan, K., and Yang, Y . Darts: Differentiablearchitecture search. arXiv preprint arXiv:1806.09055 ,2018.Pham, H., Guan, M. Y ., Zoph, B., Le, Q. V ., and Dean, J.Ef\ufb01cient neural architecture search via parameter sharing.arXiv preprint arXiv:1802.03268, 2018.Real, E., Aggarwal, A., Huang, Y ., and Le, Q. V . Regu-larized evolution for image classi\ufb01er architecture search.arXiv preprint arXiv:1802.01548, 2018.Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., andChen, L.-C. Mobilenetv2: Inverted residuals and linearbottlenecks. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pp. 4510\u20134520, 2018.Shin, R., Packer, C., and Song, D. Differentiable neuralnetwork architecture search. OpenReview, 2018.Stamoulis, D., Cai, E., Juan, D.-C., and Marculescu, D.Hyperpower: Power-and memory-constrained hyper-parameter optimization for neural networks. In 2018Design, Automation & Test in Europe Conference & Ex-hibition (DATE). IEEE, 2018a.Stamoulis, D., Chin, T.-W. R., Prakash, A. K., Fang, H.,Sajja, S., Bognar, M., and Marculescu, D. Designingadaptive neural networks for energy-constrained imageclassi\ufb01cation. In Proceedings of the International Con-ference on Computer-Aided Design. ACM, 2018b.Stamoulis, D., Ding, R., Wang, D., Lymberopoulos, D.,Priyantha, B., Liu, J., and Marculescu, D. Single-pathnas: Designing hardware-ef\ufb01cient convnets in less than 4hours. arXiv preprint arXiv:1904.02877, 2019.Tan, M., Chen, B., Pang, R., Vasudevan, V ., and Le, Q. V .Mnasnet: Platform-aware neural architecture search formobile. arXiv preprint arXiv:1807.11626, 2018.Wu, B., Dai, X., Zhang, P., Wang, Y ., Sun, F., Wu, Y ., Tian,Y ., Vajda, P., Jia, Y ., and Keutzer, K. Fbnet: Hardware-aware ef\ufb01cient convnet design via differentiable neuralarchitecture search. arXiv preprint arXiv:1812.03443 ,2018.Zoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . Learningtransferable architectures for scalable image recognition.arXiv preprint arXiv:1707.07012, 2(6), 2017.\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n1905.04159v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 27:
\n~~~~~~~~~~\nHistory: Step #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\nStep #25, Phase: literature review, Feedback: 3\n--- Page 1 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignDimitrios Stamoulis 1 * Ruizhou Ding 1 Di Wang2 Dimitrios Lymberopoulos2 Bodhi Priyantha 2 Jie Liu 3Diana Marculescu 1AbstractCan we automatically design a Convolutional Net-work (ConvNet) with the highest image classi\ufb01-cation accuracy under the latency constraint of amobile device? Neural Architecture Search (NAS)for ConvNet design is a challenging problem dueto the combinatorially large design space andsearch time (at least 200 GPU-hours). To alleviatethis complexity, we propose Single-Path NAS, anovel differentiable NAS method for designingdevice-ef\ufb01cient ConvNets in less than 4 hours .1. Novel NAS formulation : our method intro-duces a single-path, over-parameterized ConvNetto encode all architectural decisions with sharedconvolutional kernel parameters. 2. NAS ef\ufb01-ciency: Our method decreases the NAS searchcost down to 8 epochs (30 TPU-hours), i.e., up to5,000\xd7faster compared to prior work. 3. On-device image classi\ufb01cation : Single-Path NASachieves 74.96% top-1 accuracy on ImageNetwith 79ms inference latency on a Pixel 1 phone,which is state-of-the-art accuracy compared toNAS methods with similar latency (\u226480ms).1. Introduction\u201cIs it possible to reduce the NAS search cost down to onlyfew hours?\u201d NAS methods have revolutionized the designof ConvNets (Zoph et al., 2017), yielding state-of-the-art re-sults in deep learning applications (Real et al., 2018). NAShas a profound impact on the design of hardware-ef\ufb01cientConvNets for on-device computer vision, e.g., under infer-ence latency constraints on a mobile device (Tan et al., 2018).However, NAS remains an intrinsically costly problem with*Extended abstract of ODML-CDNNR 2019 presentation (re-quired non-archival arxiv.org version). Full paper can be foundin (Stamoulis et al., 2019). 1Department of ECE, Carnegie MellonUniversity, Pittsburgh, PA, USA2Microsoft, Redmond, W A, USA3Harbin Institute of Technology, Harbin, China. Correspondenceto: Dimitrios Stamoulis <dstamoul@andrew.cmu.edu>.Joint Workshop onOn-Device Machine Learning & Compact DeepNeural Network Representations (ODML-CDNNR 2019). ICML2019 Workshop. Copyright 2019 by the author(s).Figure 1.Single-Path NAS directly optimizes for the subset of con-volution weights of an over-parameterized \u201csuperkernel\u201d in eachConvNet layer (right). Our novel view of the design space elimi-nates the need for maintaining separate paths for each candidateoperation, as in previous multi-path approaches (left).a combinatorially large search space: e.g., searching for aConvNet with 22 layers and \ufb01ve candidate operations perlayer yields 522 \u22481015 possible networks.Inef\ufb01ciencies of multi-path NAS: Recent methods use one-shot formulations (Liu et al., 2018; Pham et al., 2018) byviewing the NAS problem as an operation/path selectionproblem: \ufb01rst, an over-parameterized, multi-path supernetis constructed, where, for each layer, every candidate oper-ation is added as a separate trainable path (Figure 1, left).Next, NAS searches for the paths of the multi-path supernetthat yield the optimal architecture. As expected, naivelybranching out all paths is inef\ufb01cient, since the number oftrainable parameters during the search grows linearly withrespect to the number of candidate operations per layer (Ben-der et al., 2018). To tame the memory explosion due to themulti-path supernet, current methods employ \u201cworkaround\u201dsolutions: e.g., searching on a proxy dataset (Wu et al.,2018), or employing a memory-wise scheme where onlyfew paths are updated during search (Cai et al., 2019). Nev-ertheless, these methods remain considerably costly, withtotal computational demand of at least 200 GPU-hours.In this paper, we propose Single-Path NAS, a novel NASmethod for designing hardware-ef\ufb01cient ConvNets in lessthan 4 hours . Our key insight is illustrated in Figure 1(right). Our key observation is that different candidateconvolutional operations can be viewed as subsets of theweights of an over-parameterizedsingle \u201csuperkernel\u201d. In-stead of choosing from different paths/operations as inmulti-arXiv:1905.04159v1  [cs.LG]  10 May 2019--- Page 2 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignFigure 2.Single-path NAS builds upon hierarchical MobileNetV2-like search spaces (Tan et al., 2018) to identify the mobile invertedbottleneck convolution (MBConv) per layer (left). Our one-shot supernet encapsulates all possible NAS architectures in the search space,i.e., different kernel size (middle) and expansion ratio (right) values, without the need for appending each candidate operation as a separatepath. Single-Path NAS directly searches over the weights of the per-layer searchable \u201csuperkernel\u201d that encodes all MBConv types.path methods, we view the NAS problem as \ufb01nding whichsubset of the per-layer \u201csuperkernel\u201d weights to use, hencesearching across a single-path one-shot NAS supernet.2. Proposed Method: Single-Path NASSearch space : We build upon the MobileNetV2-basedmacro-architecture (Figure 2, left), where layers are groupedinto blocks based on their \ufb01lter sizes (Sandler et al., 2018).Each layer is a mobile inverted bottleneck convolution(conv) MBConv-k\xd7k-e(Sandler et al., 2018), (i.e., a point-wise 1 \xd71 conv with expansion ratio e, a k\xd7kdepthwiseconv, and a linear 1 \xd71 conv; Figure 2, left). Our goal is toidentify the MBConv-k\xd7k-etype per layer.\u201cSuperkernel\u201d-based formulation: Our key insight is thatthe candidate operations can be viewed as subsets of the\u201csu-perkernel\u201d weights. Without loss of generality, we denotethe weights of two candidate operations, e.g., 3 \xd73 conv or5\xd75 conv, asw3\xd73 and w5\xd75, respectively. We observe thatw3\xd73 can be viewed as the inner core of the w5\xd75 kernel,while \u201czeroing\u201d out the \u201couter\u201d shell w5\xd75\\3\xd73 (Figure 2,middle). Thus, we can write the NAS decision as:wk = w3\xd73 + 1 (\ued79\ued79w5\xd75\\3\xd73\ued79\ued792>tk=5) \xb7w5\xd75\\3\xd73 (1)where 1 (\xb7) is the indicator function that encodes the archi-tectural NAS choice and tk=5 is a latent variable ( e.g., athreshold value) that controls the decision.Trainable NAS decisions: Drawing inspiration from quan-tization decisions (Ding et al., 2019), we use the groupLasso term in the 1 (\xb7) condition. Instead of picking thethresholds (e.g., tk=5) by hand, we seamlessly treat themas trainable parameters to learn via gradient descent. Tocompute the gradients for thresholds, we relax the indicatorfunction g(x,t) = 1 (x > t) to a sigmoid function \u03c3(\xb7),when computing gradients, i.e., \u02c6g(x,t) =\u03c3(x>t ).Searching for expansion ratio or skip-op : Since thekernel-based wk result (Equation 1) is a kernel itself, wecan in turn apply our formulation to encode expansion ratiodecisions, where e= 3or e= 6correspond to using half orall the channels of an MBConv-k\xd7k-6 layer, respectively(Figure 2, right). Finally, by \u201czeroing\u201d out all channels, weencode the NAS decision of dropping the entire layer:w = 1 (\u2225wk,3\u22252 >te=3) \xb7(wk,3+1 (\ued79\ued79wk,6\\3\ued79\ued792>te=6) \xb7wk,6\\3)(2)Hence, our searchable superkernelcan suf\ufb01ciently captureany MBConv type in the MobileNetV2-based design space(Figure 2). For input x, the output of the i-th MBConv layerof the network is oi(x) =conv(x,wi|tik=5,tie=6,tie=3).Differentiable NAS: To account for both the accuracy andinference latency of the searched ConvNet, we use a latency-aware formulation for the NAS problem (Wu et al., 2018):minwCE(w|tk,te) +\u03bb\xb7log(R(w|tk,te)) (3)where CE is the cross-entropy loss of the single-path modeland Ris the runtime in milliseconds (ms) of the searchedNAS model on the target device. The coef\ufb01cient \u03bbmodu-lates the trade-off between cross-entropy and runtime.Runtime model: Prior art has showed that the on-deviceConvNet runtime can be modeled as the sum of each i-thlayer\u2019s runtimeR(w|tk,te) =\u2211i Ri(wi|tik,tie) (Wu et al.,2018; Cai et al., 2017; Stamoulis et al., 2018a). To preservethe differentiability of the objective, we formulate the per-layer Ri as a function of the NAS decisions. We pro\ufb01le thetarget mobile device (Pixel 1 smartphone) and we recordthe runtime for each candidate kernel operation per layer i.As a function of the expansion ratio decisions, we write:Rie = 1 (\u2225wk,3\u22252 >te=3) \xb7(Ri5\xd75,3+1 (\ued79\ued79wk,6\\3\ued79\ued792>te=6) \xb7(Ri5\xd75,6 \u2212Ri5\xd75,3))(4)By incorporating the kernel size decision, the runtime is:Ri =Ri3\xd73,6Ri5\xd75,6\xb7Rie+Rie \xb7(1 \u2212Ri3\xd73,6Ri5\xd75,6) \xb71 (\ued79\ued79w5\xd75\\3\xd73\ued79\ued792>tk=5)(5)--- Page 3 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignTable 1.Single-Path NAS achieves state-of-the-art image classi\ufb01cation accuracy (%) on ImageNet for similar on-device latency settingcompared to previous NAS methods (\u226480ms on Pixel 1), with up to 5, 000\xd7reduced search cost in terms of number of epochs. *Thesearch cost in epochs is estimated based on the claim (Cai et al., 2019) that ProxylessNAS is 200\xd7faster than MnasNet. \u2021ChamNet doesnot detail the model derived under runtime constraints (Dai et al., 2018) so we cannot retrain or measure the latency.METHOD TOP-1 ACC (%) T OP-5 ACC (%) R UNTIME (MS) S EARCH COST (EPOCHS )MOBILE NETV2 (SANDLER ET AL ., 2018) 72.00 91.00 75.00 -MOBILE NETV2 (OUR IMPL .) 73.59 91.41 73.57RANDOM SEARCH 73.78 \xb10.85 91.42 \xb10.56 77.31 \xb10.9 MS -MNASNET (TAN ET AL ., 2018) 74.00 91.80 76.00 40,000MNASNET (OUR IMPL .) 74.61 91.95 74.65CHAMNET-B (DAI ET AL ., 2018) 73.80 \u2013 \u2013 240 \u2021PROXYLESS NAS-R (CAI ET AL ., 2019) 74.60 92.20 78.00 200*PROXYLESS NAS-R (OUR IMPL .) 74.65 92.18 77.48FBNET-B (WU ET AL ., 2018) 74.1 - - 90FBNET-B (OUR IMPL .) 73.70 91.51 78.33Single-PathNAS (PROPOSED ) 74.96 92.21 79.48 8 (3.75 HOURS)As in Equation 1, we relax the indicator function to a sig-moid function \u03c3(\xb7) when computing gradients. To evaluatethe prediction accuracy of the runtime model, we generate100 random ConvNets and we measure their runtime onthe device. Our model can accurately predict the actualruntimes: the Root Mean Squared Error (RMSE) is 1.32ms,which corresponds to an average 1.76% prediction error.3. ExperimentsExperimental Setup: We select Pixel 1 as the target devicesince it allows for a representative comparison with priorwork that optimizes for this platform. We run our frameworkusing TensorFlow (TF) on TPUs-v2 (Jouppi et al.,2017). We deploy the ConvNets on the device with TFTFLite. We pro\ufb01le on-device runtime using the FacebookAI Performance Evaluation Platform (FAI-PEP, 2018). Weimplement our trainable \u201csuperkernels\u201d on Keras.We apply our method to design ConvNets for image classi\ufb01-cation on ImageNet (Deng et al., 2009) running on Pixel 1with an overall target latency of 80ms. We train the derivedSingle-Path NAS model for 350 epochs. We summarize theresults in Table 1. To enable a representative comparison ofthe search cost per method, we directly report the numberof epochs per method, hence canceling out the effect ofdifferent hardware systems (GPU vs TPU hours).State-of-the-art on-device image classi\ufb01cation : Single-Path NAS achieves top-1 accuracy of 74.96%, which is thenew state-of-the-art ImageNet accuracy among hardware-ef\ufb01cient NAS methods. More speci\ufb01cally, our methodachieves better top-1 accuracy than ProxylessNAS by+0.31%, while maintaining on par target latency of\u226480mson the same target mobile phone. Single-Path NAS out-performs methods in this mobile latency range, i.e., bet-ter than MnasNet (+0.35%), FBNet-B (+0.86%), and Mo-bileNetV2 (+1.37%).Reduced search cost: Single-Path NAS has a total searchcost of 8 epochs, which is 5,000\xd7faster than MnasNet,25\xd7faster than ProxylessNAS, and 11\xd7faster than FBNet.Speci\ufb01cally, MnasNet reports a total of 40k train epochs. Inturn, ChamNet trains an accuracy predictor on 240 samples.ProxylessNAS reports 200\xd7search cost improvement overMnasNet, hence the overall cost is the TPU-equivalent of200 epochs. Finally, FBNet reports 90 epochs. Overall, wesearch for \u223c10ksteps (8 epochs with a batch size of1024),which corresponds to total wall-clock time of3.75 hours ona TPUv2. In particular, given than a TPUv2 has 2 chips with4 cores each, this corresponds to a total of 30 TPU-hours.4. Discussion & Future WorkNovel idea: The key insight behind our work is to revisitthe one-shot supernet NAS design space with a single-pathview, by formulating the NAS problem as\ufb01nding which sub-set of kernel weights to use in each ConvNet layer. Whileconcurrent works consider relaxed convolution formula-tions (Shin et al., 2018; Hundt et al., 2019; Guo et al., 2019),they either use design spaces and objectives that have beenshown to be hardware inef\ufb01cient ( e.g., cell-based space,FLOP count), or they do not intrinsically relax the kernelover both kernel-size and channels dimensions.Future work : The ef\ufb01ciency of our single-path designspace could enable future work beyond our differentiableNAS formulation and based on reinforcement learning orevolutionary methods. Moreover, our methodology can be\ufb02exibly extended to other hardware design goals,e.g., power,memory, energy, and communication constraints (Donget al., 2018; Stamoulis et al., 2018b;b). To this end andto foster reproducibility, unlike all recent hardware-ef\ufb01cientNAS methods which release pretrained models only, weopen-source and fully document our method at: https://github.com/dstamoulis/single-path-nas.--- Page 4 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignReferencesBender, G., Kindermans, P.-J., Zoph, B., Vasudevan, V ., andLe, Q. Understanding and simplifying one-shot archi-tecture search. In International Conference on MachineLearning, pp. 549\u2013558, 2018.Cai, E., Juan, D.-C., Stamoulis, D., and Marculescu, D. Neu-ralpower: Predict and deploy energy-ef\ufb01cient convolu-tional neural networks. In Asian Conference on MachineLearning, pp. 622\u2013637, 2017.Cai, H., Zhu, L., and Han, S. ProxylessNAS: Direct neu-ral architecture search on target task and hardware. InInternational Conference on Learning Representations,2019.Dai, X., Zhang, P., Wu, B., Yin, H., Sun, F., Wang, Y .,Dukhan, M., Hu, Y ., Wu, Y ., Jia, Y ., et al. Chamnet:Towards ef\ufb01cient network design through platform-awaremodel adaptation. arXiv preprint arXiv:1812.08934 ,2018.Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,L. Imagenet: A large-scale hierarchical image database.In 2009 IEEE conference on computer vision and patternrecognition, pp. 248\u2013255. Ieee, 2009.Ding, R., Liu, Z., Chin, T.-W., Marculescu, D., and Blan-ton, R. Flightnns: Lightweight quantized deep neuralnetworks for fast and accurate inference. In 2019 DesignAutomation Conference (DAC), 2019.Dong, J.-D., Cheng, A.-C., Juan, D.-C., Wei, W., andSun, M. Dpp-net: Device-aware progressive searchfor pareto-optimal neural architectures. arXiv preprintarXiv:1806.08198, 2018.FAI-PEP. Facebook AI Performance EvaluationPlatform. https://github.com/facebook/FAI-PEP, 2018.Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y ., andSun, J. Single path one-shot neural architecture searchwith uniform sampling. arXiv preprint arXiv:1904.00420,2019.Hundt, A., Jain, V ., and Hager, G. D. sharpdarts: Faster andmore accurate differentiable architecture search. arXivpreprint arXiv:1903.09900, 2019.Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,A., et al. In-datacenter performance analysis of a tensorprocessing unit. In 2017 ACM/IEEE 44th Annual Inter-national Symposium on Computer Architecture (ISCA),pp. 1\u201312. IEEE, 2017.Liu, H., Simonyan, K., and Yang, Y . Darts: Differentiablearchitecture search. arXiv preprint arXiv:1806.09055 ,2018.Pham, H., Guan, M. Y ., Zoph, B., Le, Q. V ., and Dean, J.Ef\ufb01cient neural architecture search via parameter sharing.arXiv preprint arXiv:1802.03268, 2018.Real, E., Aggarwal, A., Huang, Y ., and Le, Q. V . Regu-larized evolution for image classi\ufb01er architecture search.arXiv preprint arXiv:1802.01548, 2018.Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., andChen, L.-C. Mobilenetv2: Inverted residuals and linearbottlenecks. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pp. 4510\u20134520, 2018.Shin, R., Packer, C., and Song, D. Differentiable neuralnetwork architecture search. OpenReview, 2018.Stamoulis, D., Cai, E., Juan, D.-C., and Marculescu, D.Hyperpower: Power-and memory-constrained hyper-parameter optimization for neural networks. In 2018Design, Automation & Test in Europe Conference & Ex-hibition (DATE). IEEE, 2018a.Stamoulis, D., Chin, T.-W. R., Prakash, A. K., Fang, H.,Sajja, S., Bognar, M., and Marculescu, D. Designingadaptive neural networks for energy-constrained imageclassi\ufb01cation. In Proceedings of the International Con-ference on Computer-Aided Design. ACM, 2018b.Stamoulis, D., Ding, R., Wang, D., Lymberopoulos, D.,Priyantha, B., Liu, J., and Marculescu, D. Single-pathnas: Designing hardware-ef\ufb01cient convnets in less than 4hours. arXiv preprint arXiv:1904.02877, 2019.Tan, M., Chen, B., Pang, R., Vasudevan, V ., and Le, Q. V .Mnasnet: Platform-aware neural architecture search formobile. arXiv preprint arXiv:1807.11626, 2018.Wu, B., Dai, X., Zhang, P., Wang, Y ., Sun, F., Wu, Y ., Tian,Y ., Vajda, P., Jia, Y ., and Keutzer, K. Fbnet: Hardware-aware ef\ufb01cient convnet design via differentiable neuralarchitecture search. arXiv preprint arXiv:1812.03443 ,2018.Zoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . Learningtransferable architectures for scalable image recognition.arXiv preprint arXiv:1707.07012, 2(6), 2017., Your response: ```ADD_PAPER\n1905.04159v1\nThis paper introduces Single-Path NAS, a novel differentiable neural architecture search method that optimizes convolutional networks for edge devices by viewing NAS as selecting subsets of weights from an over-parameterized "superkernel" rather than choosing between separate paths. Key innovations include: (1) A single-path formulation that reduces search cost to 8 epochs (30 TPU-hours), achieving up to 5,000x speedup over prior work; (2) Direct optimization for target device latency (79ms on Pixel 1) through a differentiable runtime model; (3) State-of-the-art 74.96% ImageNet accuracy in the <80ms latency range. The method demonstrates how hardware-aware NAS can efficiently discover Pareto-optimal architectures by jointly optimizing accuracy and on-device performance.\n```\n~~~~~~~~~~\nCurrent Step #26, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 1905.04159v1\nNotes: \nYour previous command was: ```ADD_PAPER\n1905.04159v1\nThis paper introduces Single-Path NAS, a novel differentiable neural architecture search method that optimizes convolutional networks for edge devices by viewing NAS as selecting subsets of weights from an over-parameterized "superkernel" rather than choosing between separate paths. Key innovations include: (1) A single-path formulation that reduces search cost to 8 epochs (30 TPU-hours), achieving up to 5,000x speedup over prior work; (2) Direct optimization for target device latency (79ms on Pixel 1) through a differentiable runtime model; (3) State-of-the-art 74.96% ImageNet accuracy in the <80ms latency range. The method demonstrates how hardware-aware NAS can efficiently discover Pareto-optimal architectures by jointly optimizing accuracy and on-device performance.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 28:
\n~~~~~~~~~~\nHistory: Step #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge device image classification energy efficiency\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\nStep #25, Phase: literature review, Feedback: 3\n--- Page 1 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignDimitrios Stamoulis 1 * Ruizhou Ding 1 Di Wang2 Dimitrios Lymberopoulos2 Bodhi Priyantha 2 Jie Liu 3Diana Marculescu 1AbstractCan we automatically design a Convolutional Net-work (ConvNet) with the highest image classi\ufb01-cation accuracy under the latency constraint of amobile device? Neural Architecture Search (NAS)for ConvNet design is a challenging problem dueto the combinatorially large design space andsearch time (at least 200 GPU-hours). To alleviatethis complexity, we propose Single-Path NAS, anovel differentiable NAS method for designingdevice-ef\ufb01cient ConvNets in less than 4 hours .1. Novel NAS formulation : our method intro-duces a single-path, over-parameterized ConvNetto encode all architectural decisions with sharedconvolutional kernel parameters. 2. NAS ef\ufb01-ciency: Our method decreases the NAS searchcost down to 8 epochs (30 TPU-hours), i.e., up to5,000\xd7faster compared to prior work. 3. On-device image classi\ufb01cation : Single-Path NASachieves 74.96% top-1 accuracy on ImageNetwith 79ms inference latency on a Pixel 1 phone,which is state-of-the-art accuracy compared toNAS methods with similar latency (\u226480ms).1. Introduction\u201cIs it possible to reduce the NAS search cost down to onlyfew hours?\u201d NAS methods have revolutionized the designof ConvNets (Zoph et al., 2017), yielding state-of-the-art re-sults in deep learning applications (Real et al., 2018). NAShas a profound impact on the design of hardware-ef\ufb01cientConvNets for on-device computer vision, e.g., under infer-ence latency constraints on a mobile device (Tan et al., 2018).However, NAS remains an intrinsically costly problem with*Extended abstract of ODML-CDNNR 2019 presentation (re-quired non-archival arxiv.org version). Full paper can be foundin (Stamoulis et al., 2019). 1Department of ECE, Carnegie MellonUniversity, Pittsburgh, PA, USA2Microsoft, Redmond, W A, USA3Harbin Institute of Technology, Harbin, China. Correspondenceto: Dimitrios Stamoulis <dstamoul@andrew.cmu.edu>.Joint Workshop onOn-Device Machine Learning & Compact DeepNeural Network Representations (ODML-CDNNR 2019). ICML2019 Workshop. Copyright 2019 by the author(s).Figure 1.Single-Path NAS directly optimizes for the subset of con-volution weights of an over-parameterized \u201csuperkernel\u201d in eachConvNet layer (right). Our novel view of the design space elimi-nates the need for maintaining separate paths for each candidateoperation, as in previous multi-path approaches (left).a combinatorially large search space: e.g., searching for aConvNet with 22 layers and \ufb01ve candidate operations perlayer yields 522 \u22481015 possible networks.Inef\ufb01ciencies of multi-path NAS: Recent methods use one-shot formulations (Liu et al., 2018; Pham et al., 2018) byviewing the NAS problem as an operation/path selectionproblem: \ufb01rst, an over-parameterized, multi-path supernetis constructed, where, for each layer, every candidate oper-ation is added as a separate trainable path (Figure 1, left).Next, NAS searches for the paths of the multi-path supernetthat yield the optimal architecture. As expected, naivelybranching out all paths is inef\ufb01cient, since the number oftrainable parameters during the search grows linearly withrespect to the number of candidate operations per layer (Ben-der et al., 2018). To tame the memory explosion due to themulti-path supernet, current methods employ \u201cworkaround\u201dsolutions: e.g., searching on a proxy dataset (Wu et al.,2018), or employing a memory-wise scheme where onlyfew paths are updated during search (Cai et al., 2019). Nev-ertheless, these methods remain considerably costly, withtotal computational demand of at least 200 GPU-hours.In this paper, we propose Single-Path NAS, a novel NASmethod for designing hardware-ef\ufb01cient ConvNets in lessthan 4 hours . Our key insight is illustrated in Figure 1(right). Our key observation is that different candidateconvolutional operations can be viewed as subsets of theweights of an over-parameterizedsingle \u201csuperkernel\u201d. In-stead of choosing from different paths/operations as inmulti-arXiv:1905.04159v1  [cs.LG]  10 May 2019--- Page 2 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignFigure 2.Single-path NAS builds upon hierarchical MobileNetV2-like search spaces (Tan et al., 2018) to identify the mobile invertedbottleneck convolution (MBConv) per layer (left). Our one-shot supernet encapsulates all possible NAS architectures in the search space,i.e., different kernel size (middle) and expansion ratio (right) values, without the need for appending each candidate operation as a separatepath. Single-Path NAS directly searches over the weights of the per-layer searchable \u201csuperkernel\u201d that encodes all MBConv types.path methods, we view the NAS problem as \ufb01nding whichsubset of the per-layer \u201csuperkernel\u201d weights to use, hencesearching across a single-path one-shot NAS supernet.2. Proposed Method: Single-Path NASSearch space : We build upon the MobileNetV2-basedmacro-architecture (Figure 2, left), where layers are groupedinto blocks based on their \ufb01lter sizes (Sandler et al., 2018).Each layer is a mobile inverted bottleneck convolution(conv) MBConv-k\xd7k-e(Sandler et al., 2018), (i.e., a point-wise 1 \xd71 conv with expansion ratio e, a k\xd7kdepthwiseconv, and a linear 1 \xd71 conv; Figure 2, left). Our goal is toidentify the MBConv-k\xd7k-etype per layer.\u201cSuperkernel\u201d-based formulation: Our key insight is thatthe candidate operations can be viewed as subsets of the\u201csu-perkernel\u201d weights. Without loss of generality, we denotethe weights of two candidate operations, e.g., 3 \xd73 conv or5\xd75 conv, asw3\xd73 and w5\xd75, respectively. We observe thatw3\xd73 can be viewed as the inner core of the w5\xd75 kernel,while \u201czeroing\u201d out the \u201couter\u201d shell w5\xd75\\3\xd73 (Figure 2,middle). Thus, we can write the NAS decision as:wk = w3\xd73 + 1 (\ued79\ued79w5\xd75\\3\xd73\ued79\ued792>tk=5) \xb7w5\xd75\\3\xd73 (1)where 1 (\xb7) is the indicator function that encodes the archi-tectural NAS choice and tk=5 is a latent variable ( e.g., athreshold value) that controls the decision.Trainable NAS decisions: Drawing inspiration from quan-tization decisions (Ding et al., 2019), we use the groupLasso term in the 1 (\xb7) condition. Instead of picking thethresholds (e.g., tk=5) by hand, we seamlessly treat themas trainable parameters to learn via gradient descent. Tocompute the gradients for thresholds, we relax the indicatorfunction g(x,t) = 1 (x > t) to a sigmoid function \u03c3(\xb7),when computing gradients, i.e., \u02c6g(x,t) =\u03c3(x>t ).Searching for expansion ratio or skip-op : Since thekernel-based wk result (Equation 1) is a kernel itself, wecan in turn apply our formulation to encode expansion ratiodecisions, where e= 3or e= 6correspond to using half orall the channels of an MBConv-k\xd7k-6 layer, respectively(Figure 2, right). Finally, by \u201czeroing\u201d out all channels, weencode the NAS decision of dropping the entire layer:w = 1 (\u2225wk,3\u22252 >te=3) \xb7(wk,3+1 (\ued79\ued79wk,6\\3\ued79\ued792>te=6) \xb7wk,6\\3)(2)Hence, our searchable superkernelcan suf\ufb01ciently captureany MBConv type in the MobileNetV2-based design space(Figure 2). For input x, the output of the i-th MBConv layerof the network is oi(x) =conv(x,wi|tik=5,tie=6,tie=3).Differentiable NAS: To account for both the accuracy andinference latency of the searched ConvNet, we use a latency-aware formulation for the NAS problem (Wu et al., 2018):minwCE(w|tk,te) +\u03bb\xb7log(R(w|tk,te)) (3)where CE is the cross-entropy loss of the single-path modeland Ris the runtime in milliseconds (ms) of the searchedNAS model on the target device. The coef\ufb01cient \u03bbmodu-lates the trade-off between cross-entropy and runtime.Runtime model: Prior art has showed that the on-deviceConvNet runtime can be modeled as the sum of each i-thlayer\u2019s runtimeR(w|tk,te) =\u2211i Ri(wi|tik,tie) (Wu et al.,2018; Cai et al., 2017; Stamoulis et al., 2018a). To preservethe differentiability of the objective, we formulate the per-layer Ri as a function of the NAS decisions. We pro\ufb01le thetarget mobile device (Pixel 1 smartphone) and we recordthe runtime for each candidate kernel operation per layer i.As a function of the expansion ratio decisions, we write:Rie = 1 (\u2225wk,3\u22252 >te=3) \xb7(Ri5\xd75,3+1 (\ued79\ued79wk,6\\3\ued79\ued792>te=6) \xb7(Ri5\xd75,6 \u2212Ri5\xd75,3))(4)By incorporating the kernel size decision, the runtime is:Ri =Ri3\xd73,6Ri5\xd75,6\xb7Rie+Rie \xb7(1 \u2212Ri3\xd73,6Ri5\xd75,6) \xb71 (\ued79\ued79w5\xd75\\3\xd73\ued79\ued792>tk=5)(5)--- Page 3 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignTable 1.Single-Path NAS achieves state-of-the-art image classi\ufb01cation accuracy (%) on ImageNet for similar on-device latency settingcompared to previous NAS methods (\u226480ms on Pixel 1), with up to 5, 000\xd7reduced search cost in terms of number of epochs. *Thesearch cost in epochs is estimated based on the claim (Cai et al., 2019) that ProxylessNAS is 200\xd7faster than MnasNet. \u2021ChamNet doesnot detail the model derived under runtime constraints (Dai et al., 2018) so we cannot retrain or measure the latency.METHOD TOP-1 ACC (%) T OP-5 ACC (%) R UNTIME (MS) S EARCH COST (EPOCHS )MOBILE NETV2 (SANDLER ET AL ., 2018) 72.00 91.00 75.00 -MOBILE NETV2 (OUR IMPL .) 73.59 91.41 73.57RANDOM SEARCH 73.78 \xb10.85 91.42 \xb10.56 77.31 \xb10.9 MS -MNASNET (TAN ET AL ., 2018) 74.00 91.80 76.00 40,000MNASNET (OUR IMPL .) 74.61 91.95 74.65CHAMNET-B (DAI ET AL ., 2018) 73.80 \u2013 \u2013 240 \u2021PROXYLESS NAS-R (CAI ET AL ., 2019) 74.60 92.20 78.00 200*PROXYLESS NAS-R (OUR IMPL .) 74.65 92.18 77.48FBNET-B (WU ET AL ., 2018) 74.1 - - 90FBNET-B (OUR IMPL .) 73.70 91.51 78.33Single-PathNAS (PROPOSED ) 74.96 92.21 79.48 8 (3.75 HOURS)As in Equation 1, we relax the indicator function to a sig-moid function \u03c3(\xb7) when computing gradients. To evaluatethe prediction accuracy of the runtime model, we generate100 random ConvNets and we measure their runtime onthe device. Our model can accurately predict the actualruntimes: the Root Mean Squared Error (RMSE) is 1.32ms,which corresponds to an average 1.76% prediction error.3. ExperimentsExperimental Setup: We select Pixel 1 as the target devicesince it allows for a representative comparison with priorwork that optimizes for this platform. We run our frameworkusing TensorFlow (TF) on TPUs-v2 (Jouppi et al.,2017). We deploy the ConvNets on the device with TFTFLite. We pro\ufb01le on-device runtime using the FacebookAI Performance Evaluation Platform (FAI-PEP, 2018). Weimplement our trainable \u201csuperkernels\u201d on Keras.We apply our method to design ConvNets for image classi\ufb01-cation on ImageNet (Deng et al., 2009) running on Pixel 1with an overall target latency of 80ms. We train the derivedSingle-Path NAS model for 350 epochs. We summarize theresults in Table 1. To enable a representative comparison ofthe search cost per method, we directly report the numberof epochs per method, hence canceling out the effect ofdifferent hardware systems (GPU vs TPU hours).State-of-the-art on-device image classi\ufb01cation : Single-Path NAS achieves top-1 accuracy of 74.96%, which is thenew state-of-the-art ImageNet accuracy among hardware-ef\ufb01cient NAS methods. More speci\ufb01cally, our methodachieves better top-1 accuracy than ProxylessNAS by+0.31%, while maintaining on par target latency of\u226480mson the same target mobile phone. Single-Path NAS out-performs methods in this mobile latency range, i.e., bet-ter than MnasNet (+0.35%), FBNet-B (+0.86%), and Mo-bileNetV2 (+1.37%).Reduced search cost: Single-Path NAS has a total searchcost of 8 epochs, which is 5,000\xd7faster than MnasNet,25\xd7faster than ProxylessNAS, and 11\xd7faster than FBNet.Speci\ufb01cally, MnasNet reports a total of 40k train epochs. Inturn, ChamNet trains an accuracy predictor on 240 samples.ProxylessNAS reports 200\xd7search cost improvement overMnasNet, hence the overall cost is the TPU-equivalent of200 epochs. Finally, FBNet reports 90 epochs. Overall, wesearch for \u223c10ksteps (8 epochs with a batch size of1024),which corresponds to total wall-clock time of3.75 hours ona TPUv2. In particular, given than a TPUv2 has 2 chips with4 cores each, this corresponds to a total of 30 TPU-hours.4. Discussion & Future WorkNovel idea: The key insight behind our work is to revisitthe one-shot supernet NAS design space with a single-pathview, by formulating the NAS problem as\ufb01nding which sub-set of kernel weights to use in each ConvNet layer. Whileconcurrent works consider relaxed convolution formula-tions (Shin et al., 2018; Hundt et al., 2019; Guo et al., 2019),they either use design spaces and objectives that have beenshown to be hardware inef\ufb01cient ( e.g., cell-based space,FLOP count), or they do not intrinsically relax the kernelover both kernel-size and channels dimensions.Future work : The ef\ufb01ciency of our single-path designspace could enable future work beyond our differentiableNAS formulation and based on reinforcement learning orevolutionary methods. Moreover, our methodology can be\ufb02exibly extended to other hardware design goals,e.g., power,memory, energy, and communication constraints (Donget al., 2018; Stamoulis et al., 2018b;b). To this end andto foster reproducibility, unlike all recent hardware-ef\ufb01cientNAS methods which release pretrained models only, weopen-source and fully document our method at: https://github.com/dstamoulis/single-path-nas.--- Page 4 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignReferencesBender, G., Kindermans, P.-J., Zoph, B., Vasudevan, V ., andLe, Q. Understanding and simplifying one-shot archi-tecture search. In International Conference on MachineLearning, pp. 549\u2013558, 2018.Cai, E., Juan, D.-C., Stamoulis, D., and Marculescu, D. Neu-ralpower: Predict and deploy energy-ef\ufb01cient convolu-tional neural networks. In Asian Conference on MachineLearning, pp. 622\u2013637, 2017.Cai, H., Zhu, L., and Han, S. ProxylessNAS: Direct neu-ral architecture search on target task and hardware. InInternational Conference on Learning Representations,2019.Dai, X., Zhang, P., Wu, B., Yin, H., Sun, F., Wang, Y .,Dukhan, M., Hu, Y ., Wu, Y ., Jia, Y ., et al. Chamnet:Towards ef\ufb01cient network design through platform-awaremodel adaptation. arXiv preprint arXiv:1812.08934 ,2018.Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,L. Imagenet: A large-scale hierarchical image database.In 2009 IEEE conference on computer vision and patternrecognition, pp. 248\u2013255. Ieee, 2009.Ding, R., Liu, Z., Chin, T.-W., Marculescu, D., and Blan-ton, R. Flightnns: Lightweight quantized deep neuralnetworks for fast and accurate inference. In 2019 DesignAutomation Conference (DAC), 2019.Dong, J.-D., Cheng, A.-C., Juan, D.-C., Wei, W., andSun, M. Dpp-net: Device-aware progressive searchfor pareto-optimal neural architectures. arXiv preprintarXiv:1806.08198, 2018.FAI-PEP. Facebook AI Performance EvaluationPlatform. https://github.com/facebook/FAI-PEP, 2018.Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y ., andSun, J. Single path one-shot neural architecture searchwith uniform sampling. arXiv preprint arXiv:1904.00420,2019.Hundt, A., Jain, V ., and Hager, G. D. sharpdarts: Faster andmore accurate differentiable architecture search. arXivpreprint arXiv:1903.09900, 2019.Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,A., et al. In-datacenter performance analysis of a tensorprocessing unit. In 2017 ACM/IEEE 44th Annual Inter-national Symposium on Computer Architecture (ISCA),pp. 1\u201312. IEEE, 2017.Liu, H., Simonyan, K., and Yang, Y . Darts: Differentiablearchitecture search. arXiv preprint arXiv:1806.09055 ,2018.Pham, H., Guan, M. Y ., Zoph, B., Le, Q. V ., and Dean, J.Ef\ufb01cient neural architecture search via parameter sharing.arXiv preprint arXiv:1802.03268, 2018.Real, E., Aggarwal, A., Huang, Y ., and Le, Q. V . Regu-larized evolution for image classi\ufb01er architecture search.arXiv preprint arXiv:1802.01548, 2018.Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., andChen, L.-C. Mobilenetv2: Inverted residuals and linearbottlenecks. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pp. 4510\u20134520, 2018.Shin, R., Packer, C., and Song, D. Differentiable neuralnetwork architecture search. OpenReview, 2018.Stamoulis, D., Cai, E., Juan, D.-C., and Marculescu, D.Hyperpower: Power-and memory-constrained hyper-parameter optimization for neural networks. In 2018Design, Automation & Test in Europe Conference & Ex-hibition (DATE). IEEE, 2018a.Stamoulis, D., Chin, T.-W. R., Prakash, A. K., Fang, H.,Sajja, S., Bognar, M., and Marculescu, D. Designingadaptive neural networks for energy-constrained imageclassi\ufb01cation. In Proceedings of the International Con-ference on Computer-Aided Design. ACM, 2018b.Stamoulis, D., Ding, R., Wang, D., Lymberopoulos, D.,Priyantha, B., Liu, J., and Marculescu, D. Single-pathnas: Designing hardware-ef\ufb01cient convnets in less than 4hours. arXiv preprint arXiv:1904.02877, 2019.Tan, M., Chen, B., Pang, R., Vasudevan, V ., and Le, Q. V .Mnasnet: Platform-aware neural architecture search formobile. arXiv preprint arXiv:1807.11626, 2018.Wu, B., Dai, X., Zhang, P., Wang, Y ., Sun, F., Wu, Y ., Tian,Y ., Vajda, P., Jia, Y ., and Keutzer, K. Fbnet: Hardware-aware ef\ufb01cient convnet design via differentiable neuralarchitecture search. arXiv preprint arXiv:1812.03443 ,2018.Zoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . Learningtransferable architectures for scalable image recognition.arXiv preprint arXiv:1707.07012, 2(6), 2017., Your response: ```ADD_PAPER\n1905.04159v1\nThis paper introduces Single-Path NAS, a novel differentiable neural architecture search method that optimizes convolutional networks for edge devices by viewing NAS as selecting subsets of weights from an over-parameterized "superkernel" rather than choosing between separate paths. Key innovations include: (1) A single-path formulation that reduces search cost to 8 epochs (30 TPU-hours), achieving up to 5,000x speedup over prior work; (2) Direct optimization for target device latency (79ms on Pixel 1) through a differentiable runtime model; (3) State-of-the-art 74.96% ImageNet accuracy in the <80ms latency range. The method demonstrates how hardware-aware NAS can efficiently discover Pareto-optimal architectures by jointly optimizing accuracy and on-device performance.\n```\nStep #26, Phase: literature review, Feedback: Successfully added paper 1905.04159v1, Your response: ```SUMMARY\nedge device image classification dynamic inference\n```\n~~~~~~~~~~\nCurrent Step #27, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification dynamic inference, here was the response\nTitle: AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and Dynamic Model Placements on Edge Devices\nSummary: Many real-world applications are widely adopting the edge computing paradigm\ndue to its low latency and better privacy protection. With notable success in\nAI and deep learning (DL), edge devices and AI accelerators play a crucial role\nin deploying DL inference services at the edge of the Internet. While prior\nworks quantified various edge devices' efficiency, most studies focused on the\nperformance of edge devices with single DL tasks. Therefore, there is an urgent\nneed to investigate AI multi-tenancy on edge devices, required by many advanced\nDL applications for edge computing.\n  This work investigates two techniques - concurrent model executions and\ndynamic model placements - for AI multi-tenancy on edge devices. With image\nclassification as an example scenario, we empirically evaluate AI multi-tenancy\non various edge devices, AI accelerators, and DL frameworks to identify its\nbenefits and limitations. Our results show that multi-tenancy significantly\nimproves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI\nmulti-tenancy techniques also open up new opportunities for flexible deployment\nof multiple DL services on edge devices and AI accelerators.\nPublication Date: 2021-07-26\narXiv paper ID: 2107.12486v1\n\nTitle: Edge-PRUNE: Flexible Distributed Deep Learning Inference\nSummary: Collaborative deep learning inference between low-resource endpoint devices\nand edge servers has received significant research interest in the last few\nyears. Such computation partitioning can help reducing endpoint device energy\nconsumption and improve latency, but equally importantly also contributes to\nprivacy-preserving of sensitive data. This paper describes Edge-PRUNE, a\nflexible but light-weight computation framework for distributing machine\nlearning inference between edge servers and one or more client devices.\nCompared to previous approaches, Edge-PRUNE is based on a formal dataflow\ncomputing model, and is agnostic towards machine learning training frameworks,\noffering at the same time wide support for leveraging deep learning\naccelerators such as embedded GPUs. The experimental section of the paper\ndemonstrates the use and performance of Edge-PRUNE by image classification and\nobject tracking applications on two heterogeneous endpoint devices and an edge\nserver, over wireless and physical connections. Endpoint device inference time\nfor SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by\ncollaborative inference.\nPublication Date: 2022-04-27\narXiv paper ID: 2204.12947v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: AutoDiCE: Fully Automated Distributed CNN Inference at the Edge\nSummary: Deep Learning approaches based on Convolutional Neural Networks (CNNs) are\nextensively utilized and very successful in a wide range of application areas,\nincluding image classification and speech recognition. For the execution of\ntrained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud\nto the Edge. Unfortunately, deploying and inferring large, compute and memory\nintensive CNNs on edge devices is challenging because these devices typically\nhave limited power budgets and compute/memory resources. One approach to\naddress this challenge is to leverage all available resources across multiple\nedge devices to deploy and execute a large CNN by properly partitioning the CNN\nand running each CNN partition on a separate edge device. Although such\ndistribution, deployment, and execution of large CNNs on multiple edge devices\nis a desirable and beneficial approach, there currently does not exist a design\nand programming framework that takes a trained CNN model, together with a CNN\npartitioning specification, and fully automates the CNN model splitting and\ndeployment on multiple edge devices to facilitate distributed CNN inference at\nthe Edge. Therefore, in this paper, we propose a novel framework, called\nAutoDiCE, for automated splitting of a CNN model into a set of sub-models and\nautomated code generation for distributed and collaborative execution of these\nsub-models on multiple, possibly heterogeneous, edge devices, while supporting\nthe exploitation of parallelism among and within the edge devices. Our\nexperimental results show that AutoDiCE can deliver distributed CNN inference\nwith reduced energy consumption and memory usage per edge device, and improved\noverall system throughput at the same time.\nPublication Date: 2022-07-20\narXiv paper ID: 2207.12113v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification dynamic inference\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 29:
\n~~~~~~~~~~\nHistory: Step #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\nStep #25, Phase: literature review, Feedback: 3\n--- Page 1 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignDimitrios Stamoulis 1 * Ruizhou Ding 1 Di Wang2 Dimitrios Lymberopoulos2 Bodhi Priyantha 2 Jie Liu 3Diana Marculescu 1AbstractCan we automatically design a Convolutional Net-work (ConvNet) with the highest image classi\ufb01-cation accuracy under the latency constraint of amobile device? Neural Architecture Search (NAS)for ConvNet design is a challenging problem dueto the combinatorially large design space andsearch time (at least 200 GPU-hours). To alleviatethis complexity, we propose Single-Path NAS, anovel differentiable NAS method for designingdevice-ef\ufb01cient ConvNets in less than 4 hours .1. Novel NAS formulation : our method intro-duces a single-path, over-parameterized ConvNetto encode all architectural decisions with sharedconvolutional kernel parameters. 2. NAS ef\ufb01-ciency: Our method decreases the NAS searchcost down to 8 epochs (30 TPU-hours), i.e., up to5,000\xd7faster compared to prior work. 3. On-device image classi\ufb01cation : Single-Path NASachieves 74.96% top-1 accuracy on ImageNetwith 79ms inference latency on a Pixel 1 phone,which is state-of-the-art accuracy compared toNAS methods with similar latency (\u226480ms).1. Introduction\u201cIs it possible to reduce the NAS search cost down to onlyfew hours?\u201d NAS methods have revolutionized the designof ConvNets (Zoph et al., 2017), yielding state-of-the-art re-sults in deep learning applications (Real et al., 2018). NAShas a profound impact on the design of hardware-ef\ufb01cientConvNets for on-device computer vision, e.g., under infer-ence latency constraints on a mobile device (Tan et al., 2018).However, NAS remains an intrinsically costly problem with*Extended abstract of ODML-CDNNR 2019 presentation (re-quired non-archival arxiv.org version). Full paper can be foundin (Stamoulis et al., 2019). 1Department of ECE, Carnegie MellonUniversity, Pittsburgh, PA, USA2Microsoft, Redmond, W A, USA3Harbin Institute of Technology, Harbin, China. Correspondenceto: Dimitrios Stamoulis <dstamoul@andrew.cmu.edu>.Joint Workshop onOn-Device Machine Learning & Compact DeepNeural Network Representations (ODML-CDNNR 2019). ICML2019 Workshop. Copyright 2019 by the author(s).Figure 1.Single-Path NAS directly optimizes for the subset of con-volution weights of an over-parameterized \u201csuperkernel\u201d in eachConvNet layer (right). Our novel view of the design space elimi-nates the need for maintaining separate paths for each candidateoperation, as in previous multi-path approaches (left).a combinatorially large search space: e.g., searching for aConvNet with 22 layers and \ufb01ve candidate operations perlayer yields 522 \u22481015 possible networks.Inef\ufb01ciencies of multi-path NAS: Recent methods use one-shot formulations (Liu et al., 2018; Pham et al., 2018) byviewing the NAS problem as an operation/path selectionproblem: \ufb01rst, an over-parameterized, multi-path supernetis constructed, where, for each layer, every candidate oper-ation is added as a separate trainable path (Figure 1, left).Next, NAS searches for the paths of the multi-path supernetthat yield the optimal architecture. As expected, naivelybranching out all paths is inef\ufb01cient, since the number oftrainable parameters during the search grows linearly withrespect to the number of candidate operations per layer (Ben-der et al., 2018). To tame the memory explosion due to themulti-path supernet, current methods employ \u201cworkaround\u201dsolutions: e.g., searching on a proxy dataset (Wu et al.,2018), or employing a memory-wise scheme where onlyfew paths are updated during search (Cai et al., 2019). Nev-ertheless, these methods remain considerably costly, withtotal computational demand of at least 200 GPU-hours.In this paper, we propose Single-Path NAS, a novel NASmethod for designing hardware-ef\ufb01cient ConvNets in lessthan 4 hours . Our key insight is illustrated in Figure 1(right). Our key observation is that different candidateconvolutional operations can be viewed as subsets of theweights of an over-parameterizedsingle \u201csuperkernel\u201d. In-stead of choosing from different paths/operations as inmulti-arXiv:1905.04159v1  [cs.LG]  10 May 2019--- Page 2 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignFigure 2.Single-path NAS builds upon hierarchical MobileNetV2-like search spaces (Tan et al., 2018) to identify the mobile invertedbottleneck convolution (MBConv) per layer (left). Our one-shot supernet encapsulates all possible NAS architectures in the search space,i.e., different kernel size (middle) and expansion ratio (right) values, without the need for appending each candidate operation as a separatepath. Single-Path NAS directly searches over the weights of the per-layer searchable \u201csuperkernel\u201d that encodes all MBConv types.path methods, we view the NAS problem as \ufb01nding whichsubset of the per-layer \u201csuperkernel\u201d weights to use, hencesearching across a single-path one-shot NAS supernet.2. Proposed Method: Single-Path NASSearch space : We build upon the MobileNetV2-basedmacro-architecture (Figure 2, left), where layers are groupedinto blocks based on their \ufb01lter sizes (Sandler et al., 2018).Each layer is a mobile inverted bottleneck convolution(conv) MBConv-k\xd7k-e(Sandler et al., 2018), (i.e., a point-wise 1 \xd71 conv with expansion ratio e, a k\xd7kdepthwiseconv, and a linear 1 \xd71 conv; Figure 2, left). Our goal is toidentify the MBConv-k\xd7k-etype per layer.\u201cSuperkernel\u201d-based formulation: Our key insight is thatthe candidate operations can be viewed as subsets of the\u201csu-perkernel\u201d weights. Without loss of generality, we denotethe weights of two candidate operations, e.g., 3 \xd73 conv or5\xd75 conv, asw3\xd73 and w5\xd75, respectively. We observe thatw3\xd73 can be viewed as the inner core of the w5\xd75 kernel,while \u201czeroing\u201d out the \u201couter\u201d shell w5\xd75\\3\xd73 (Figure 2,middle). Thus, we can write the NAS decision as:wk = w3\xd73 + 1 (\ued79\ued79w5\xd75\\3\xd73\ued79\ued792>tk=5) \xb7w5\xd75\\3\xd73 (1)where 1 (\xb7) is the indicator function that encodes the archi-tectural NAS choice and tk=5 is a latent variable ( e.g., athreshold value) that controls the decision.Trainable NAS decisions: Drawing inspiration from quan-tization decisions (Ding et al., 2019), we use the groupLasso term in the 1 (\xb7) condition. Instead of picking thethresholds (e.g., tk=5) by hand, we seamlessly treat themas trainable parameters to learn via gradient descent. Tocompute the gradients for thresholds, we relax the indicatorfunction g(x,t) = 1 (x > t) to a sigmoid function \u03c3(\xb7),when computing gradients, i.e., \u02c6g(x,t) =\u03c3(x>t ).Searching for expansion ratio or skip-op : Since thekernel-based wk result (Equation 1) is a kernel itself, wecan in turn apply our formulation to encode expansion ratiodecisions, where e= 3or e= 6correspond to using half orall the channels of an MBConv-k\xd7k-6 layer, respectively(Figure 2, right). Finally, by \u201czeroing\u201d out all channels, weencode the NAS decision of dropping the entire layer:w = 1 (\u2225wk,3\u22252 >te=3) \xb7(wk,3+1 (\ued79\ued79wk,6\\3\ued79\ued792>te=6) \xb7wk,6\\3)(2)Hence, our searchable superkernelcan suf\ufb01ciently captureany MBConv type in the MobileNetV2-based design space(Figure 2). For input x, the output of the i-th MBConv layerof the network is oi(x) =conv(x,wi|tik=5,tie=6,tie=3).Differentiable NAS: To account for both the accuracy andinference latency of the searched ConvNet, we use a latency-aware formulation for the NAS problem (Wu et al., 2018):minwCE(w|tk,te) +\u03bb\xb7log(R(w|tk,te)) (3)where CE is the cross-entropy loss of the single-path modeland Ris the runtime in milliseconds (ms) of the searchedNAS model on the target device. The coef\ufb01cient \u03bbmodu-lates the trade-off between cross-entropy and runtime.Runtime model: Prior art has showed that the on-deviceConvNet runtime can be modeled as the sum of each i-thlayer\u2019s runtimeR(w|tk,te) =\u2211i Ri(wi|tik,tie) (Wu et al.,2018; Cai et al., 2017; Stamoulis et al., 2018a). To preservethe differentiability of the objective, we formulate the per-layer Ri as a function of the NAS decisions. We pro\ufb01le thetarget mobile device (Pixel 1 smartphone) and we recordthe runtime for each candidate kernel operation per layer i.As a function of the expansion ratio decisions, we write:Rie = 1 (\u2225wk,3\u22252 >te=3) \xb7(Ri5\xd75,3+1 (\ued79\ued79wk,6\\3\ued79\ued792>te=6) \xb7(Ri5\xd75,6 \u2212Ri5\xd75,3))(4)By incorporating the kernel size decision, the runtime is:Ri =Ri3\xd73,6Ri5\xd75,6\xb7Rie+Rie \xb7(1 \u2212Ri3\xd73,6Ri5\xd75,6) \xb71 (\ued79\ued79w5\xd75\\3\xd73\ued79\ued792>tk=5)(5)--- Page 3 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignTable 1.Single-Path NAS achieves state-of-the-art image classi\ufb01cation accuracy (%) on ImageNet for similar on-device latency settingcompared to previous NAS methods (\u226480ms on Pixel 1), with up to 5, 000\xd7reduced search cost in terms of number of epochs. *Thesearch cost in epochs is estimated based on the claim (Cai et al., 2019) that ProxylessNAS is 200\xd7faster than MnasNet. \u2021ChamNet doesnot detail the model derived under runtime constraints (Dai et al., 2018) so we cannot retrain or measure the latency.METHOD TOP-1 ACC (%) T OP-5 ACC (%) R UNTIME (MS) S EARCH COST (EPOCHS )MOBILE NETV2 (SANDLER ET AL ., 2018) 72.00 91.00 75.00 -MOBILE NETV2 (OUR IMPL .) 73.59 91.41 73.57RANDOM SEARCH 73.78 \xb10.85 91.42 \xb10.56 77.31 \xb10.9 MS -MNASNET (TAN ET AL ., 2018) 74.00 91.80 76.00 40,000MNASNET (OUR IMPL .) 74.61 91.95 74.65CHAMNET-B (DAI ET AL ., 2018) 73.80 \u2013 \u2013 240 \u2021PROXYLESS NAS-R (CAI ET AL ., 2019) 74.60 92.20 78.00 200*PROXYLESS NAS-R (OUR IMPL .) 74.65 92.18 77.48FBNET-B (WU ET AL ., 2018) 74.1 - - 90FBNET-B (OUR IMPL .) 73.70 91.51 78.33Single-PathNAS (PROPOSED ) 74.96 92.21 79.48 8 (3.75 HOURS)As in Equation 1, we relax the indicator function to a sig-moid function \u03c3(\xb7) when computing gradients. To evaluatethe prediction accuracy of the runtime model, we generate100 random ConvNets and we measure their runtime onthe device. Our model can accurately predict the actualruntimes: the Root Mean Squared Error (RMSE) is 1.32ms,which corresponds to an average 1.76% prediction error.3. ExperimentsExperimental Setup: We select Pixel 1 as the target devicesince it allows for a representative comparison with priorwork that optimizes for this platform. We run our frameworkusing TensorFlow (TF) on TPUs-v2 (Jouppi et al.,2017). We deploy the ConvNets on the device with TFTFLite. We pro\ufb01le on-device runtime using the FacebookAI Performance Evaluation Platform (FAI-PEP, 2018). Weimplement our trainable \u201csuperkernels\u201d on Keras.We apply our method to design ConvNets for image classi\ufb01-cation on ImageNet (Deng et al., 2009) running on Pixel 1with an overall target latency of 80ms. We train the derivedSingle-Path NAS model for 350 epochs. We summarize theresults in Table 1. To enable a representative comparison ofthe search cost per method, we directly report the numberof epochs per method, hence canceling out the effect ofdifferent hardware systems (GPU vs TPU hours).State-of-the-art on-device image classi\ufb01cation : Single-Path NAS achieves top-1 accuracy of 74.96%, which is thenew state-of-the-art ImageNet accuracy among hardware-ef\ufb01cient NAS methods. More speci\ufb01cally, our methodachieves better top-1 accuracy than ProxylessNAS by+0.31%, while maintaining on par target latency of\u226480mson the same target mobile phone. Single-Path NAS out-performs methods in this mobile latency range, i.e., bet-ter than MnasNet (+0.35%), FBNet-B (+0.86%), and Mo-bileNetV2 (+1.37%).Reduced search cost: Single-Path NAS has a total searchcost of 8 epochs, which is 5,000\xd7faster than MnasNet,25\xd7faster than ProxylessNAS, and 11\xd7faster than FBNet.Speci\ufb01cally, MnasNet reports a total of 40k train epochs. Inturn, ChamNet trains an accuracy predictor on 240 samples.ProxylessNAS reports 200\xd7search cost improvement overMnasNet, hence the overall cost is the TPU-equivalent of200 epochs. Finally, FBNet reports 90 epochs. Overall, wesearch for \u223c10ksteps (8 epochs with a batch size of1024),which corresponds to total wall-clock time of3.75 hours ona TPUv2. In particular, given than a TPUv2 has 2 chips with4 cores each, this corresponds to a total of 30 TPU-hours.4. Discussion & Future WorkNovel idea: The key insight behind our work is to revisitthe one-shot supernet NAS design space with a single-pathview, by formulating the NAS problem as\ufb01nding which sub-set of kernel weights to use in each ConvNet layer. Whileconcurrent works consider relaxed convolution formula-tions (Shin et al., 2018; Hundt et al., 2019; Guo et al., 2019),they either use design spaces and objectives that have beenshown to be hardware inef\ufb01cient ( e.g., cell-based space,FLOP count), or they do not intrinsically relax the kernelover both kernel-size and channels dimensions.Future work : The ef\ufb01ciency of our single-path designspace could enable future work beyond our differentiableNAS formulation and based on reinforcement learning orevolutionary methods. Moreover, our methodology can be\ufb02exibly extended to other hardware design goals,e.g., power,memory, energy, and communication constraints (Donget al., 2018; Stamoulis et al., 2018b;b). To this end andto foster reproducibility, unlike all recent hardware-ef\ufb01cientNAS methods which release pretrained models only, weopen-source and fully document our method at: https://github.com/dstamoulis/single-path-nas.--- Page 4 ---Single-Path NAS: Device-Aware Ef\ufb01cient ConvNet DesignReferencesBender, G., Kindermans, P.-J., Zoph, B., Vasudevan, V ., andLe, Q. Understanding and simplifying one-shot archi-tecture search. In International Conference on MachineLearning, pp. 549\u2013558, 2018.Cai, E., Juan, D.-C., Stamoulis, D., and Marculescu, D. Neu-ralpower: Predict and deploy energy-ef\ufb01cient convolu-tional neural networks. In Asian Conference on MachineLearning, pp. 622\u2013637, 2017.Cai, H., Zhu, L., and Han, S. ProxylessNAS: Direct neu-ral architecture search on target task and hardware. InInternational Conference on Learning Representations,2019.Dai, X., Zhang, P., Wu, B., Yin, H., Sun, F., Wang, Y .,Dukhan, M., Hu, Y ., Wu, Y ., Jia, Y ., et al. Chamnet:Towards ef\ufb01cient network design through platform-awaremodel adaptation. arXiv preprint arXiv:1812.08934 ,2018.Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,L. Imagenet: A large-scale hierarchical image database.In 2009 IEEE conference on computer vision and patternrecognition, pp. 248\u2013255. Ieee, 2009.Ding, R., Liu, Z., Chin, T.-W., Marculescu, D., and Blan-ton, R. Flightnns: Lightweight quantized deep neuralnetworks for fast and accurate inference. In 2019 DesignAutomation Conference (DAC), 2019.Dong, J.-D., Cheng, A.-C., Juan, D.-C., Wei, W., andSun, M. Dpp-net: Device-aware progressive searchfor pareto-optimal neural architectures. arXiv preprintarXiv:1806.08198, 2018.FAI-PEP. Facebook AI Performance EvaluationPlatform. https://github.com/facebook/FAI-PEP, 2018.Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y ., andSun, J. Single path one-shot neural architecture searchwith uniform sampling. arXiv preprint arXiv:1904.00420,2019.Hundt, A., Jain, V ., and Hager, G. D. sharpdarts: Faster andmore accurate differentiable architecture search. arXivpreprint arXiv:1903.09900, 2019.Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,A., et al. In-datacenter performance analysis of a tensorprocessing unit. In 2017 ACM/IEEE 44th Annual Inter-national Symposium on Computer Architecture (ISCA),pp. 1\u201312. IEEE, 2017.Liu, H., Simonyan, K., and Yang, Y . Darts: Differentiablearchitecture search. arXiv preprint arXiv:1806.09055 ,2018.Pham, H., Guan, M. Y ., Zoph, B., Le, Q. V ., and Dean, J.Ef\ufb01cient neural architecture search via parameter sharing.arXiv preprint arXiv:1802.03268, 2018.Real, E., Aggarwal, A., Huang, Y ., and Le, Q. V . Regu-larized evolution for image classi\ufb01er architecture search.arXiv preprint arXiv:1802.01548, 2018.Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., andChen, L.-C. Mobilenetv2: Inverted residuals and linearbottlenecks. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pp. 4510\u20134520, 2018.Shin, R., Packer, C., and Song, D. Differentiable neuralnetwork architecture search. OpenReview, 2018.Stamoulis, D., Cai, E., Juan, D.-C., and Marculescu, D.Hyperpower: Power-and memory-constrained hyper-parameter optimization for neural networks. In 2018Design, Automation & Test in Europe Conference & Ex-hibition (DATE). IEEE, 2018a.Stamoulis, D., Chin, T.-W. R., Prakash, A. K., Fang, H.,Sajja, S., Bognar, M., and Marculescu, D. Designingadaptive neural networks for energy-constrained imageclassi\ufb01cation. In Proceedings of the International Con-ference on Computer-Aided Design. ACM, 2018b.Stamoulis, D., Ding, R., Wang, D., Lymberopoulos, D.,Priyantha, B., Liu, J., and Marculescu, D. Single-pathnas: Designing hardware-ef\ufb01cient convnets in less than 4hours. arXiv preprint arXiv:1904.02877, 2019.Tan, M., Chen, B., Pang, R., Vasudevan, V ., and Le, Q. V .Mnasnet: Platform-aware neural architecture search formobile. arXiv preprint arXiv:1807.11626, 2018.Wu, B., Dai, X., Zhang, P., Wang, Y ., Sun, F., Wu, Y ., Tian,Y ., Vajda, P., Jia, Y ., and Keutzer, K. Fbnet: Hardware-aware ef\ufb01cient convnet design via differentiable neuralarchitecture search. arXiv preprint arXiv:1812.03443 ,2018.Zoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . Learningtransferable architectures for scalable image recognition.arXiv preprint arXiv:1707.07012, 2(6), 2017., Your response: ```ADD_PAPER\n1905.04159v1\nThis paper introduces Single-Path NAS, a novel differentiable neural architecture search method that optimizes convolutional networks for edge devices by viewing NAS as selecting subsets of weights from an over-parameterized "superkernel" rather than choosing between separate paths. Key innovations include: (1) A single-path formulation that reduces search cost to 8 epochs (30 TPU-hours), achieving up to 5,000x speedup over prior work; (2) Direct optimization for target device latency (79ms on Pixel 1) through a differentiable runtime model; (3) State-of-the-art 74.96% ImageNet accuracy in the <80ms latency range. The method demonstrates how hardware-aware NAS can efficiently discover Pareto-optimal architectures by jointly optimizing accuracy and on-device performance.\n```\nStep #26, Phase: literature review, Feedback: Successfully added paper 1905.04159v1, Your response: ```SUMMARY\nedge device image classification dynamic inference\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification dynamic inference, here was the response\nTitle: AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and Dynamic Model Placements on Edge Devices\nSummary: Many real-world applications are widely adopting the edge computing paradigm\ndue to its low latency and better privacy protection. With notable success in\nAI and deep learning (DL), edge devices and AI accelerators play a crucial role\nin deploying DL inference services at the edge of the Internet. While prior\nworks quantified various edge devices' efficiency, most studies focused on the\nperformance of edge devices with single DL tasks. Therefore, there is an urgent\nneed to investigate AI multi-tenancy on edge devices, required by many advanced\nDL applications for edge computing.\n  This work investigates two techniques - concurrent model executions and\ndynamic model placements - for AI multi-tenancy on edge devices. With image\nclassification as an example scenario, we empirically evaluate AI multi-tenancy\non various edge devices, AI accelerators, and DL frameworks to identify its\nbenefits and limitations. Our results show that multi-tenancy significantly\nimproves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI\nmulti-tenancy techniques also open up new opportunities for flexible deployment\nof multiple DL services on edge devices and AI accelerators.\nPublication Date: 2021-07-26\narXiv paper ID: 2107.12486v1\n\nTitle: Edge-PRUNE: Flexible Distributed Deep Learning Inference\nSummary: Collaborative deep learning inference between low-resource endpoint devices\nand edge servers has received significant research interest in the last few\nyears. Such computation partitioning can help reducing endpoint device energy\nconsumption and improve latency, but equally importantly also contributes to\nprivacy-preserving of sensitive data. This paper describes Edge-PRUNE, a\nflexible but light-weight computation framework for distributing machine\nlearning inference between edge servers and one or more client devices.\nCompared to previous approaches, Edge-PRUNE is based on a formal dataflow\ncomputing model, and is agnostic towards machine learning training frameworks,\noffering at the same time wide support for leveraging deep learning\naccelerators such as embedded GPUs. The experimental section of the paper\ndemonstrates the use and performance of Edge-PRUNE by image classification and\nobject tracking applications on two heterogeneous endpoint devices and an edge\nserver, over wireless and physical connections. Endpoint device inference time\nfor SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by\ncollaborative inference.\nPublication Date: 2022-04-27\narXiv paper ID: 2204.12947v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: AutoDiCE: Fully Automated Distributed CNN Inference at the Edge\nSummary: Deep Learning approaches based on Convolutional Neural Networks (CNNs) are\nextensively utilized and very successful in a wide range of application areas,\nincluding image classification and speech recognition. For the execution of\ntrained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud\nto the Edge. Unfortunately, deploying and inferring large, compute and memory\nintensive CNNs on edge devices is challenging because these devices typically\nhave limited power budgets and compute/memory resources. One approach to\naddress this challenge is to leverage all available resources across multiple\nedge devices to deploy and execute a large CNN by properly partitioning the CNN\nand running each CNN partition on a separate edge device. Although such\ndistribution, deployment, and execution of large CNNs on multiple edge devices\nis a desirable and beneficial approach, there currently does not exist a design\nand programming framework that takes a trained CNN model, together with a CNN\npartitioning specification, and fully automates the CNN model splitting and\ndeployment on multiple edge devices to facilitate distributed CNN inference at\nthe Edge. Therefore, in this paper, we propose a novel framework, called\nAutoDiCE, for automated splitting of a CNN model into a set of sub-models and\nautomated code generation for distributed and collaborative execution of these\nsub-models on multiple, possibly heterogeneous, edge devices, while supporting\nthe exploitation of parallelism among and within the edge devices. Our\nexperimental results show that AutoDiCE can deliver distributed CNN inference\nwith reduced energy consumption and memory usage per edge device, and improved\noverall system throughput at the same time.\nPublication Date: 2022-07-20\narXiv paper ID: 2207.12113v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n, Your response: ```FULL_TEXT\n2404.07217v2\n```\n~~~~~~~~~~\nCurrent Step #28, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---1Attention-aware Semantic Communications forCollaborative InferenceJiwoong Im, Nayoung Kwon, Taewoo Park, Jiheon Woo, Jaeho Lee, and Yongjune KimAbstract\u2014We propose a communication-efficient collaborativeinference framework in the domain of edge inference, focusingon the efficient use of vision transformer (ViT) models. Thepartitioning strategy of conventional collaborative inference failsto reduce communication cost because of the inherent archi-tecture of ViTs maintaining consistent layer dimensions acrossthe entire transformer encoder. Therefore, instead of employingthe partitioning strategy, our framework utilizes a lightweightViT model on the edge device, with the server deploying acomplicated ViT model. To enhance communication efficiencyand achieve the classification accuracy of the server model,we propose two strategies: 1) attention-aware patch selectionand 2) entropy-aware image transmission. Attention-aware patchselection leverages the attention scores generated by the edgedevice\u2019s transformer encoder to identify and select the imagepatches critical for classification. This strategy enables the edgedevice to transmit only the essential patches to the server,significantly improving communication efficiency. Entropy-awareimage transmission uses min-entropy as a metric to accuratelydetermine whether to depend on the lightweight model on theedge device or to request the inference from the server model.In our framework, the lightweight ViT model on the edge deviceacts as a semantic encoder, efficiently identifying and selectingthe crucial image information required for the classification task.Our experiments demonstrate that the proposed collaborativeinference framework can reduce communication overhead by68 %with only a minimal loss in accuracy compared to the servermodel on the ImageNet dataset.Index Terms\u2014Collaborative inference, edge computing, edgeinference, Internet of Things (IoT), semantic communications,split inference, vision transformer.I. I NTRODUCTIONThe rapid advancement of computational resources, coupledwith the proliferation of massive datasets, has significantlyenhanced the practicality of artificial intelligence (AI) ser-vices. Integrating AI techniques with edge devices, includingsmartphones, wearable devices, and Internet of things (IoT)devices, seeks to seamlessly incorporate AI services into awide range of daily life. This effort to advance AI technologiesin the domain of edge computing is commonly known as edgeAI [1]\u2013[3].An important research theme in edge AI is edge inference,focused on efficiently executing inference tasks within theedge network [2]\u2013[5]. Traditionally, raw data is sent fromedge devices (clients) to a server, where a complicated modelJ. Im, N. Kwon, T. Park, J. Woo, J. Lee, and Y . Kim are with theDepartment of Electrical Engineering, Pohang University of Science andTechnology (POSTECH), Pohang 37673, South Korea (e-mail: {jw3562,kwonna, parktaewoo, jhwoo1997, jaeho.lee, yongjune }@postech.ac.kr). J. Imand N. Kwon contributed equally.conducts the inference task, i.e., server-based inference. How-ever, this method incurs significant communication overhead,particularly in scenarios dealing with large volumes of rawdata [3], [4]. An alternative is on-device inference , whichexecutes the inference task directly on resource-constraineddevices, thereby minimizing communication costs. However,this approach often leads to lower performance due to thelimited computational capabilities of edge devices [3], [4].To address the dual challenges of excessive communicationoverhead and limited computational resources, the conceptof collaborative inference has been introduced [2]\u2013[6]. Thisstrategy involves dividing a deep neural network (DNN) modelinto separate parts for the edge device and the server. It lever-ages the model architecture of DNNs, where the dimensionsof intermediate layers can be significantly smaller than theinput dimensions. Within this framework, the edge devicefirst uses its component to extract features from the raw dataand then transmits them to the server. As these extractedfeatures are typically more compressed than the raw data, thecommunication cost can be aggressively reduced. The serverthen utilizes these features and its portion of the model todetermine the final inference result, which is sent back tothe device [5]. The selection of the split point is critical asit significantly impacts the computational load on the edgedevice and the communication overhead [3]. This approachis also known as split inference [5], [7] and device-edgeserver co-inference [3], [6]. Notably, collaborative inferenceis closely connected to semantic communications [8]\u2013[14],considering that the extracted features are essentially semanticinformation tailored for the inference task.Transformers, originally developed for natural languageprocessing (NLP) [15], have been widely adopted across mul-tiple domains. Particularly, the vision transformer (ViT) [16],[17] has demonstrated superior performance and efficiencyin image classification tasks. However, the deployment ofViTs on resource-constrained edge devices is challenging dueto their substantial model size and intensive computationalrequirements [18].In collaborative inference scenarios, the strategy of parti-tioning ViT models fails to effectively reduce communicationoverhead. This limitation stems from the inherent architec-ture of ViTs, which maintains consistent layer dimensionsacross the entire transformer encoder [16], in contrast toDNN models whose intermediate layer dimensions can besignificantly smaller than the raw data dimensions. Hence,partitioning ViT models for collaborative inference cannotreduce communication overhead.In this paper, we propose a communication-efficient collab-arXiv:2404.07217v2  [eess.SP]  31 May 2024--- Page 2 ---2TABLE ITHE COMPARISON OF DEIT MODEL COMPLEXITY AND CLASSIFICATIONACCURACY ON THE IMAGE NET DATASET [17]Model Parameters Memory FLOPs Classification(million) (MB) (G) Accuracy (%)DeiT-Tiny 5 21.22 1.26 72.2DeiT-Small 22 83.21 4.61 79.8DeiT-Base 86 329.55 17.58 81.8orative inference framework utilizing pre-trained ViT models.Note that the collaborative inference in our work correspondsto the collaboration between an edge device and a server,rather than collaboration among multiple edge devices. Insteadof partitioning a single model as in prior work [2]\u2013[6], ourapproach involves the edge device operating a lightweightViT model (e.g., DeiT-Tiny), while the server employs amore complex ViT model (e.g., DeiT-Base). As shown inTable I, DeiT-Tiny (DeiT-Ti) is notably lightweight, makingit suitable for edge deployment. However, its classificationaccuracy is approximately 10 %lower than that of DeiT-Base(DeiT-B) [17].Our objective is to develop a collaborative inference strategythat achieves classification accuracy comparable to the servermodel while minimizing communication overhead betweenthe edge device and the server. This strategy is designed toleverage the strengths of both models: the efficiency and lowresource demand of the tiny model on the edge device, and thehigher classification accuracy of the base model on the server.In our proposed framework, the edge device utilizes its tinymodel to conduct an initial inference without transmitting theimage to the server. Subsequently, the edge device assesseswhether to accept this initial inference or to send the image tothe server for a more accurate inference using the base model.By doing so, we aim to achieve an optimal trade-off betweenclassification accuracy and communication cost in edge-servercollaborative systems.To enhance communication efficiency in our framework,we propose two primary strategies: 1) attention-aware patchselection, which involves selectively transmitting only the mostrelevant patches of the image, and 2) entropy-aware imagetransmission, where the decision to transmit the image to theserver is determined by the level of uncertainty or confidencein the edge device\u2019s initial inference.\u2022 Attention-aware patch selection: In cases where the edgedevice needs to transmit the image to the server, ourstrategy is to transmit only the essential patches thatare crucial for classification, rather than the entire im-age. This selective transmission approach is guided bythe attention scores from the class token to the imagepatches, as processed by the tiny model. These atten-tion scores indicate the relevance of each patch to theclassification task. We investigate several techniques toeffectively select these important image patches usingattention scores. A crucial finding is that the tiny modelis capable of accurately identifying the essential patchesfor the inference task, even when the client classifiesthe image incorrectly. Our experimental results validatethat this approach enables the server model to maintainits classification accuracy, although it processes only se-lectively transmitted patches. Hence, our attention-awarepatch selection can effectively reduce the communicationcost while achieving the classification accuracy of theserver model.\u2022 Entropy-aware image transmission : It is critical for theedge device to make an accurate decision between relyingon its tiny model and requesting a more accurate infer-ence from the base model on the server. This decisionsignificantly affects both the communication overheadand the classification accuracy. Ideally, the edge deviceshould transmit only those images incorrectly classifiedby the tiny model while avoiding the transmission ofimages where the initial inference is correct. To facilitatethis decision, we utilize the min-entropy derived from thesoftmax output values of the classification head in thetiny ViT model. Our experimental findings reveal that thedecisions based on the min-entropy yield higher accuracycompared to those based on the Shannon entropy.In the context of semantic communications, our frameworkis aptly characterized as attention-aware semantic communi-cations. The tiny model\u2019s transformer encoder on the edgedevice acts as a semantic encoder , particularly when thedecision is made to transmit the image to the server. The edgedevice leverages attention scores generated by the transformerencoder to identify the most essential image patches, which arecritical for accurate classification. Interestingly, our findingsreveal that the tiny model functions effectively as a semanticencoder, in spite of its lower classification accuracy comparedto the server model.Moreover, the proposed collaborative inference frameworkoffers the advantage of reducing the computational complexityon the server model, as the server\u2019s inference is conductedsolely on the selected patches. While our primary goal focuseson minimizing communication overhead between the edgedevice and the server, this framework also yields the ancillarybenefit of server-side computational efficiency.The rest of this paper is organized as follows. Section IIoffers a brief overview of the ViT and related work. Section IIIdetails our collaborative inference framework. Section IV andV present our main contributions, which include attention-aware patch selection and entropy-aware image transmission,respectively. Section VI provides experimental results, fol-lowed by conclusions in Section VII.II. B ACKGROUNDSA. Vision TransformerThe ViT [16] is a transformer-based model for computervision tasks, setting a standard in vision models. A simplifiedoverview of the ViT model is shown in Fig. 1. An inputimage x \u2208 RH\xd7W\xd7C is reshaped into a sequence of flattened2D patches xp \u2208 RN\xd7(P2\xb7C), where (H, W), C, and (P, P)denote the resolution of the original image, the number ofchannels, and the resolution of each image patch, respectively.Note that N = HWP2 is the resulting number of patches. Thesepatches are then linearly projected to a consistent dimension D--- Page 3 ---3Fig. 1. Overview of the ViT model [16].across the transformer layers via E \u2208 R(P2\xb7C)\xd7D. The inputembedding of the ViT\u2019s transformer encoder z0 \u2208 R(N+1)\xd7Dis given byz0 =xcls; x1pE; . . .; xNp E+ Epos, (1)where Epos denotes the standard learnable position embedding.The class token z00 = xcls \u2208 R1\xd7D is particularly prependedto the sequence of embedded patches [16]. This class tokenis crucial in classification tasks, serving as a key elementin aggregating the information from the entire sequence ofpatches for the final classification output.The transformer encoder is composed of alternating layersof multi-head self-attention (MSA) and multi-layer perceptron(MLP) blocks as follows:z\u2032l = MSA (LN (zl\u22121)) +zl\u22121, l = 1, . . . , L; (2)zl = MLP (LN (z\u2032l)) +z\u2032l, l = 1, . . . , L; (3)y = LN(z0L), (4)where LN represents the layer normalization. In particular, theimage representation y is the encoder output of the class tokenz00. This image representation y then serves as the input forthe MLP head as shown in Fig. 1.In the MSA block for an input sequence z \u2208 R(N+1)\xd7D, aweighted sum of all values v is computed using query q, keyk, and value v, where q, k, v \u2208 R(N+1)\xd7Dh. The standardself-attention (SA) is formalized as follows [15], [16]:[q, k, v] =zUqkv, (5)A = softmax qk\u22a4\u221aDh, (6)SA(z) =Av, (7)where Uqkv \u2208 RD\xd73Dh and A \u2208 R(N+1)\xd7(N+1) denote theprojection matrix and the attention weight matrix, respectively.The MSA extends the standard self-attention (SA) by perform-ing H parallel SA operations (i.e., heads) and concatenatingtheir outputs [16]:MSA(z) = [SA1(z), \xb7\xb7\xb7 , SAH(z)] UMSA, (8)where UMSA is the projection matrix for the MSA output.B. Related WorkPrior work on collaborative inference has primarily focusedon convolutional neural network (CNN) architectures [2]\u2013[6], [19]. The exploration of collaborative inference withtransformer models has been limited because of the consistentdimension of the MSA blocks in the transformer encoder.Recent studies have delved into on-device inference utilizinglightweight ViT models [18], [20], [21], yet achieving theaccuracy of server models is challenging. To improve the clas-sification accuracy of the edge device, an ensemble strategyemploying multiple edge devices was proposed in [22]. Inthis approach, a central edge device disseminates raw datato nearby edge devices, which then conduct inference usingcompact ViT models and return their intermediate inferenceresults to the central edge device. The central edge deviceobtains the final inference outcome by aggregating the receivedintermediate results. However, this approach leads to increasedcommunication overhead among the edge devices.To reduce communication overhead, recent studies haveintroduced collaborative inference schemes utilizing ViT mod-els on the server, namely masked autoencoder (MAE)-basedoffloading for transformer inference (MOT) [23] and adaptiveMOT (A-MOT) [24]. In these approaches, the edge deviceselects image patches randomly for transmission to the server.The server then reconstructs the entire image using the decoderof MAE and performs classification on this reconstructed im-age. In spite of the benefit of reducing computational demandson edge devices, the classification accuracy is compromised bythe random selection of image patches.Transformers have been utilized in semantic communi-cations for the transmission of text [25] and images [26].Nonetheless, these works do not pertain to classification tasks.Our approach distinctively employs attention scores, particu-larly for the class token, setting our work apart from existingresearch in semantic communications.Recent studies have considered the importance of trainingdata samples to enhance training performance in edge learning.In [27], [28], the authors attempt to identify important trainingdata samples and allocate more communication resourcesto these samples for improved communication efficiency.The authors of [29] select important features of trainingdata samples to enhance communication efficiency duringthe training phase. In contrast, we focus on communication-efficient collaborative inference, which specifically identifiesimportant patches of test data samples and transmits onlythese essential patches. Unlike these works that aim to enhancetraining performance, our framework is designed to improvecommunication efficiency during the inference phase.III. C OLLABORATIVE INFERENCE FRAMEWORK BASED ONTRANSFORMER MODELSWe propose a collaborative inference framework that utilizespre-trained ViT models. This framework is designed to achieveserver-level classification accuracy with minimized communi-cation overhead between the edge device and the server.Due to the consistent layer dimensions of ViTs, conven-tional methods of collaborative inference [2]\u2013[6], [19], which--- Page 4 ---4Fig. 2. The proposed collaborative inference framework utilizing pre-trained ViT models: A lightweight model (e.g., DeiT-Tiny) on the edge device and acomplicated model (DeiT-Base) on the server. The edge device (client) evaluates the uncertainty of its inference results by computing the entropy. If thisentropy exceeds a predetermined threshold, the edge device selects the most important patches of the image based on the attention scores and transmits theseto the server.typically partition a single DNN model, are ineffective atreducing communication costs for ViT models. As a solution,we employ a lightweight ViT model (e.g., DeiT-Tiny) at theedge device, instead of splitting a complex ViT model (e.g.,DeiT-Base), as depicted in Fig. 2. The proposed inferenceframework establishes an efficient collaborative protocol be-tween the edge device and the server, aiming to achieve highclassification accuracy of DeiT-Base model while significantlyreducing communication overhead.In our collaborative inference framework, the edge device(client) first performs inference with its tiny model. The edgedevice then evaluates the entropy level of this initial inference.High entropy (or low confidence) necessitates transmitting theimage to the server since it indicates that the tiny model\u2019sinference would be unreliable. In such instances, only essentialpatches for classification are transmitted instead of the entireimage patches to minimize communication costs. The server,utilizing its complex ViT model, conducts inference based onthese selected patches and sends its classification results backto the edge device, as shown in Fig. 2. This process of selectingcritical patches is governed by the proposed attention-awarepatch selection rule, elaborated in Section IV.If the initial inference\u2019s entropy is low, the edge deviceconfirms its classification result without further interactionwith the server, as shown in Fig. 2. Reducing reliance onthe server to reduce communication costs is achieved throughentropy-aware image transmission rule, detailed in Section V.By integrating these rules, our framework significantly lowerscommunication costs while maintaining classification accuracycomparable to the server model.The steps of the proposed collaborative inference are out-lined in Algorithm 1. Here, Step 2 and Step 3 involvecomputing the initial inference result fc(x(i)) and its entropyg(x(i)), respectively. If the entropy is below a given threshold\u03b7, then fc(x(i)) is deemed the final classification outcome.In cases of higher entropy, as identified in Step 4, the clientAlgorithm 1 Proposed Collaborative Inference FrameworkInput: Images {x(1), x(2), . . . ,x(n)}.Output: Classification results {y(1), y(2), . . . , y(n)}.1: for i = 1 :n do2: y(i)c \u2190 fc(x(i)) \u25b7 Inference on edge device3: Client computes entropy g(x(i))4: if g(x(i)) \u2265 \u03b7 then5: ex(i) \u2190 patch-selection(x(i))6: Client transmits ex(i) to server7: y(i)s \u2190 fs(ex(i)) \u25b7 Inference on server8: Server transmits y(i)s to client9: y(i)c \u2190 y(i)s10: end if11: end forselects and transmits only essential patches to the serverat Step 5 and 6, effectively lowering communication costsby ensuring dim(ex(i)) < dim(x(i)). At Step 7, the serverconducts inference on these selected patches, producing theresult fs(ex(i)), which is then sent back to the client at Step 8.The proposed collaborative inference framework can reducethe computational complexity for the server model by limitingthe inference process to only the selected patches. The com-putational complexity of DeiT-Base, according to [30], [31],is given by144ND2 + 24N2D, (9)where N is the number of patches. Assuming the numberof patches of ex(i) is represented by eN such that eN < N,our framework not only reduces communication overhead butalso enhances computational efficiency on the server side. Thisresults in a significant secondary benefit of our collaborativeinference framework.--- Page 5 ---5Fig. 3. The visualization of attention-aware patch selection. The left is animage of the ImageNet dataset that the client model (DeiT-Tiny) inaccuratelyclassifies as \u2018Hammerhead Shark\u2019. The right shows the selected patches byattention-aware patch selection. These selected patches allow the server model(DeiT-Base) to correctly classify the image as \u2018White Shark\u2019.Fig. 4. The visualization of images and attention scores. In the left column, wehave the original images of the ImageNet dataset. The middle column displaysthe attention score maps generated by DeiT-Tiny. The right column shows thepatches selected by the attention-sum threshold selection. The images arelabeled as \u2018Brambling\u2019, \u2018Irish Setter\u2019, and \u2018Snow Leopard\u2019, respectively.IV. A TTENTION -AWARE PATCH SELECTIONThis section introduces our attention-aware patch selectionmethod, motivated by an intriguing observation: the tiny ViTmodel is capable of identifying the essential patches forclassification, even when its classification is incorrect (seeFig. 3). Consequently, the tiny model on the edge deviceacts as a semantic encoder , effectively extracting essentialinformation for the classification task.To enhance communication efficiency and classificationaccuracy, we address two key questions: 1) how to accuratelyquantify the importance of each patch and 2) how to determinethe optimal number of selected patches.A. Quantifying Patch ImportanceTo quantify the importance of each patch for classification,we utilize the attention scores generated by the SA mecha-nism. The attention score for the class token in a single-headattention is calculated as follows:a = softmax qclsk\u22a4p\u221aDh!, (10)Fig. 5. The comparison of image patches selected by DeiT-Tiny (middlecolumn) and DeiT-Base (right column). The same number of patches areselected according to the mean attention scores. The left column displays theoriginal images of the ImageNet dataset. The images are labeled as \u2018MiniatureSchnauzer\u2019, \u2018Great Gray Owl\u2019, and \u2018Magpie\u2019, respectively.where qcls \u2208 R1\xd7Dh represents the query for the classtoken of the last layer and kp \u2208 RN\xd7Dh denotes the keyscorresponding to the image patches in the last layer. The meanattention score is then obtained by averaging the attentionscores from all multi-heads.Our experimental findings indicate that the mean attentionscores, as computed by the tiny model, effectively assess thesignificance of each patch in contributing to the classificationtask. Fig. 4 presents a side-by-side comparison of ImageNetdataset images (left column) and their corresponding attentionscore maps (middle column). These maps clearly reveal thatpatches crucial for classification are distinguished by higherattention scores, setting them apart from less critical areas,such as background patches, which receive lower attentionscores.This observation supports that the tiny model on the edgedevice is adept at identifying and selecting the most in-formative patches for classification. Within ViT models, theclass token aggregates information from other tokens (imagepatches) via the attention mechanism. For the final classifica-tion, the ViT relies on the MLP head, which considers onlythe information associated with the class token from the lastlayer, disregarding any other inputs. Thus, the attention scoreas defined in (10) serves as a key metric for quantifying thecontributions of individual image patches to the class token. Inthe context of semantic communications, the tiny model of theedge device acts as a semantic encoder, tasked with extractingessential information for the classification task. This rolealigns with the broader objectives of semantic communicationsto emphasize meaning and relevance in the transmission ofinformation [9]\u2013[11]An interesting finding is that DeiT-Tiny can act moreeffectively as a semantic encoder than DeiT-Base in spite--- Page 6 ---6Fig. 6. The comparison of attention score maps computed by DeiT-Tiny(middle column) and DeiT-Base (right column). The left column displays theoriginal images of the ImageNet dataset. The images are labeled as \u2018MiniatureSchnauzer\u2019, \u2018Great Gray Owl\u2019, and \u2018Magpie\u2019, respectively.of its inferior classification accuracy. Fig. 5 compares thepatches selected by DeiT-Tiny (middle column) and thoseby DeiT-Base (right column), showing DeiT-Tiny\u2019s superiorability to discard irrelevant image patches. This seeminglycontradictory finding can be elucidated by the insights fromrecent work [32], which shows that large ViT models tendto allocate high attention scores to less informative back-ground areas. It is because the large models adeptly identifypatches with minimal information, such as background areas,repurposing the corresponding patches to assimilate globalimage information while neglecting their spatial information.High attention scores are allocated to these repurposed patchescontaining global information, particularly in the backgroundareas, as shown in Fig. 6. Although this strategy enhances theclassification accuracy of larger ViT models, it compromisestheir effectiveness as semantic encoders. The experimentalresults on how model complexity affects the efficacy ofsemantic encoders are presented in Section VI-E.The mean attention score from the last layer has previouslybeen leveraged for purposes such as model interpretation [33],[34] and knowledge distillation [31]. Our approach distinc-tively utilizes the mean attention score to reduce commu-nication costs within our collaborative inference framework,differentiating our methodology from previous applications.Additionally, while attention rollout [35] is an establishedtechnique for interpreting transformer models, we opt for themean attention score. This decision is based on the observationthat attention rollout tends to produce more uniform attentionscores, which do not align well with our objective of attention-aware patch selection. The experimental evidence supportingthis decision is provided in Section VI-C.B. Patch Selection RuleIn this subsection, we investigate the patch selection ruleutilizing mean attention scores. Selecting an appropriate num-0 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.01Attention Score of Patch00.010.020.030.040.050.06Normalized Number of PatchesFig. 7. The normalized histogram of the mean attention scores obtained byDeiT-Tiny on the ImageNet dataset.ber of patches for transmission to the server is crucial, as thisdirectly impacts both communication costs and classificationaccuracy. Our goal is to transmit the fewest possible patchesto the server to minimize communication overhead. However,this approach poses a trade-off, as reducing the number oftransmitted patches can limit the information available to theserver model, potentially lowering classification accuracy.The distribution of attention scores reveals that most patchesare assigned low values, as shown in Fig. 7. This propertyenables a significant reduction in the number of transmittedpatches without affecting classification accuracy. By takingadvantage of this beneficial property, we explore the followingpatch selection rules:\u2022 Top-k selection: Selecting the top-k patches that have thehighest attention scores.\u2022 Attention threshold selection: Selecting the patches whoseattention scores exceed a predefined threshold \u03b4.\u2022 Attention-sum threshold selection : Selecting the patcheswith the highest attention scores until their cumulativeattention sum reaches a predetermined threshold \u03b4sum.The top- k selection method selects a fixed number ofpatches based on the highest attention scores, resulting inconsistent communication costs for all input images. Nonethe-less, it overlooks the variation in attention score distributionamong different images. Fig. 4 shows that the quantity ofpatches essential for classification can vary across images,with important information typically concentrated on the classobject, where the highest attention scores are located. Conse-quently, to achieve optimal classification accuracy, the numberof selected patches should be tailored to the size of the objectwithin each image.Both the attention threshold selection and attention-sumthreshold selection methods provide the capability to adjustthe number of selected patches for transmission, making themmore adaptable than the top- k selection method. For instance,as shown in Fig. 4, the numbers of selected patches for \u2018Bram-bling\u2019 and \u2018Snow Leopard\u2019 are 35 and 124, respectively, byusing the attention-sum threshold selection with \u03b4sum = 0.94.Adopting this method ensures the transmission of a consistentsum of attention scores, effectively lowering the risk of omit-ting crucial information. The experimental results show that--- Page 7 ---7the attention threshold selection and attention-sum thresholdselection methods outperform the top- k selection method inachieving an optimal trade-off between classification accuracyand communication efficiency, as detailed in Section VI-C.V. E NTROPY -AWARE IMAGE TRANSMISSIONThis section delves into entropy-aware image transmission,a strategy aimed at reducing communication overhead by con-sidering the varied classification difficulty inherent to differentimages. For less complex images, the edge device\u2019s initialinference may be accurate enough, eliminating the need forfurther interaction with the server. In contrast, more intricateimages necessitate more accurate classification from the servermodel, leading to increased communication overhead. It iscritical for the edge device to make an accurate decisionbetween relying on its initial inference and requesting moreaccurate classification from the server model.Even though the edge device cannot ascertain the correct-ness of its initial inference, it can estimate the inference\u2019sconfidence through the softmax output values of the MLPclassification head. This softmax output can be interpreted asthe posterior probability p\u03b8(y|x), where y denotes the classlabel and \u03b8 denotes the tiny model. Then, we set an entropyfunction g : RL \u2192 R, where L denotes the number of classlabels. The client requests more accurate inference results fromthe server if:g(x) \u2265 \u03b7, (11)where \u03b7 denotes a predetermined threshold.To assess the confidence of the client\u2019s inference, weconsider two exemplary entropy measures: 1) Shannon entropyand 2) min-entropy, with their respective thresholds.The Shannon entropy, a widely used metric for quantifyinguncertainty [36], is calculated bygs(x) =\u2212Xy\u2208Yp\u03b8(y|x) log2 p\u03b8(y|x), (12)where Y denotes the set of all possible class labels. High Shan-non entropy indicates that the given image x is challenging forthe tiny model to classify accurately. Therefore, if gs(x) \u2265 \u03b7s,the edge device transmits the selected patches to the server foran inference from the base model.Another key metric, min-entropy, evaluates uncertainty inthe most conservative manner [37]. The min-entropy is definedasgm(x) =\u2212log2 maxy\u2208Yp\u03b8(y|x), (13)which is directly associated with the confidence level of theinitial inference. If gm(x) \u2265 \u03b7m, the edge device transmits theselected patches to the server for an inference from the basemodel.Our experimental results in Section VI-D show that themin-entropy serves as a better metric within our collabora-tive inference framework. Entropy-aware image transmissionutilizing the min-entropy improves communication efficiencyfor a given level of classification accuracy when compared tousing the Shannon entropy.The entropy has been utilized in diverse applications, suchas prioritizing unlabeled data inputs in active learning [38] andoptimizing wireless data acquisition for edge learning [28].These works typically enhance training procedures using theentropy values calculated by complicated server models. Incontrast, our approach utilizes the min-entropy to assess theuncertainty of initial inferences made by a tiny model onthe edge device instead of a complex server model. This useof uncertainty metrics aims to minimize unnecessary imagetransmissions, thereby reducing communication overhead.VI. E XPERIMENTAL RESULTSA. Experiment SettingsOur experiments employ the ImageNet validation datasetand resize each image to a resolution of 224 \xd7 224 pixels bycenter cropping. An image is flattened to N = 196 patchesbefore the inference.We deploy DeiT-Tiny on the edge device and DeiT-Baseon the server since resource-constrained edge devices havechallenges in employing complicated models such as DeiT-Base. The model complexity of these models is comparedin Table I. DeiT-Tiny can be viable for deployment onedge devices such as NVIDIA Jetson Nano [22], RaspberryPi 4B [39], and iPhone 12 [18]. In contrast, DeiT-Baseis generally considered inappropriate due to its substantialmemory consumption and prolonged inference latency. Forinstance, the authors of [22] explicitly state that DeiT-Baseis inadequate for deployment on NVIDIA Jetson Nano dueto its substantial memory consumption and computationalcomplexity. Similarly, the authors of [18] emphasize memoryconsumption as a critical factor for edge device deployment,considering only lightweight models such as DeiT-Tiny andPiT [40] with approximately 3\u20136 million parameters.We evaluate the impact of varying the number of transmittedpatches on communication cost and classification accuracy.The communication cost in our collaborative inference systemis quantified by the ratio of the number of transmitted patchesto the total number of patches. Consequently, if the edge de-vice sends all image patches to the server, the communicationcost is quantified as 1.When the edge device transmits the selected patches, it isrequired to transmit the position information of these selectedpatches. For instance, assigning one bit per patch as a markerof its selection status is a practical solution. The overhead ofthese additional bits for position information is negligible incomparison to the size of the image itself, given that only onebit is appended for each image patch containing 6, 144 bits.B. Communication Cost vs. Classification AccuracyIn our collaborative inference framework, we assess thetrade-off between communication cost and classification ac-curacy. We utilize the attention-sum threshold selection forattention-aware patch selection and the min-entropy forentropy-aware image transmission.Fig. 8 shows the trade-off between communication cost andclassification accuracy, employing DeiT-Tiny on the edge de-vice and DeiT-Base on the server. We achieve a68 %reduction--- Page 8 ---80.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost777879808182Accuracy (%)Server AccuracyAchieved Accuracy = 0.1 = 0.2 = 0.3 = 0.4 = 0.6 = 0.8 = 1.0Fig. 8. Trade-off between communication cost and classification accuracy,with the edge device employing DeiT-Tiny and the server employing DeiT-Base. The black line represents the achievable curve by the proposed collab-orative inference. We utilize the attention-sum threshold selection method forattention-aware patch selection. For entropy-aware image transmission, themin-entropy serves as the entropy metric, using a threshold value \u03b7 = \u03b7m.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost727374757677787980Accuracy (%)Server AccuracyAchieved Accuracy = 0.1 = 0.2 = 0.3 = 0.4 = 0.6 = 0.8 = 1.0Fig. 9. Trade-off between communication cost and classification accuracy,with the edge device employing DeiT-Tiny and the server employing DeiT-Small. The black line represents the achievable curve by the proposed col-laborative inference. We utilize the attention-sum threshold selection methodfor attention-aware patch selection. For entropy-aware image transmission, themin-entropy serves as the entropy metric, using a threshold value \u03b7 = \u03b7m.in communication cost while attaining a classification accuracyof 80.84 %, with only a minimal loss in accuracy comparedto DeiT-Base\u2019s accuracy of 81.8 %. This communication costreduction is achieved by the attention-sum threshold selectionmethod with a threshold \u03b4sum = 0.97 and the min-entropy withthe threshold \u03b7m = 0.8. The black line indicates the optimizedtrade-off curve achieved by selecting the optimized thresholdvalues of \u03b4sum and \u03b7m. Table II and Table III detail the resultingcommunication costs depending on threshold values.By controlling the threshold values of \u03b7 and \u03b4, our proposedcollaborative inference framework can effectively managecommunication resources. In scenarios with reliable commu-nication channels, lower values of \u03b7 and \u03b4 can be selectedto increase data transmissions and maximize classificationaccuracy. When the channel quality is worse and communi-cation resources are limited, higher values of \u03b7 and \u03b4 canbe set to balance classification accuracy with the availablecommunication resource budget. This strategy offers flexibilityin adapting to varying communication channel conditions,TABLE IIATTENTION -SUM PATCH SELECTION THRESHOLD VALUES ANDEXPECTED NUMBER OF TRANSMITTED PATCHES\u03b4sum Expected Number of Transmitted Patches0.87 88.400.9 100.840.92 110.820.93 116.510.94 122.800.95 129.860.96 137.890.97 147.240.98 158.470.99 172.76TABLE IIIMIN-ENTROPY THRESHOLD VALUES AND EXPECTED RATIO OFTRANSMITTED IMAGES\u03b7m Expected Ratio of Transmitted Images1 0.35670.8 0.42900.6 0.51160.4 0.62460.3 0.71090.2 0.84450.1 0.9714thereby enhancing the efficiency of communication resourcemanagement.Additionally, we investigate a case where the server employsDeiT-Small instead of DeiT-Base. Fig. 9 shows the trade-offbetween communication cost and classification accuracy. Inthis case, we achieve a 71 %reduction in communication costwhile attaining a classification accuracy of 78.8 %, with only aminimal loss in accuracy compared to DeiT-Small\u2019s accuracyof 79.8 %. It is observed that greater communication costreductions can be realized when the accuracy gap betweenthe edge device and the server is reduced.Fig. 10 compares our collaborative inference frameworkwith several existing methods, including server-driven trans-mission (SDT) [41], image super-resolution (ISR) [42],masked autoencoder (MAE)-based offloading for transformerinference (MOT) [23], and adaptive MOT (A-MOT) [24].SDT transmits low-quality images initially and then of-floads high-quality content from the target area based on theserver\u2019s feedback [41]. ISR transmits a low-quality image andthen reconstructs a high-resolution image by super-resolutionmethod [42]. The server model performs inference on thisreconstructed high-resolution image. In MOT and A-MOT, theedge device randomly selects image patches for transmissionto the server. The server then reconstructs the entire imageusing the decoder of MAE [43] and performs classification onthis reconstructed image. The experimental results show thatour proposed collaborative inference framework significantlyimproves performance by employing the tiny model on theedge device, which adeptly transmits essential image patchesfor classification.--- Page 9 ---90.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost68707274767880Accuracy (%)Server AccuracyProposedSDTISRMOTA-MOTFig. 10. Comparison of previous methods and our collaborative inferenceframework, utilizing DeiT-Tiny on the edge device and DeiT-Small on theserver. The experiment is conducted on the ImageNet dataset.C. Attention-aware Patch SelectionIn this subsection, we delve into identifying the effectiveattention metrics for determining patch importance and the op-timal rules for patch selection. First, we examine and comparethe efficacy of mean attention scores versus attention rollout.Next, we evaluate different patch selection methodologies,including top- k selection, attention threshold selection, andattention-sum threshold selection. We maintain a consistentenvironment with DeiT-Tiny on the edge device and DeiT-Base on the server. For this analysis, we specifically focuson attention-aware patch selection, excluding considerationsof entropy-aware image transmission.Fig. 11(a) reveals that both the mean attention score andattention rollout have distinct advantages. In situations de-manding substantial reductions in communication cost, atten-tion rollout is better than the mean attention score. However,as shown in Fig. 11(b), the mean attention score is a bettermetric for attaining accuracy comparable to that of the server.The theoretical advantage of the mean attention score overattention rollout is shown in Fig. 11(c), which compares thenormalized histograms of attention scores for both metrics.Unlike attention rollout, which considers attention scoresfrom multiple layers resulting in a more uniform distributionof scores, the mean attention scores concentrate on fewerpatches with significantly higher relevance to the objects to beclassified. This concentration of attention scores is preferredbecause it implies that fewer but more relevant patches can betransmitted without compromising the classification accuracy,thus aligning with our objective to minimize communicationoverhead while maintaining classification accuracy.Hence, we adopt the mean attention score for our primaryexperiments.Fig. 12 shows that both the attention threshold selectionand attention-sum threshold selection outperform the top- kselection. These methods offer the capability to adjust thenumber of selected patches for transmission, facilitating themaintaining classification accuracy while minimizing commu-nication costs.The variability in informational content across patches,influenced by factors such as object size, type, and thepresence of background, directly impacts their importance0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Communication Cost5055606570758085Accuracy (%)Server AccuracyMean Attention ScoreAttention Rollout(a)0.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost7878.57979.58080.58181.582Accuracy (%)Server AccuracyMean Attention ScoreAttention Rollout(b)0 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.01Attention Score of Patch00.0050.010.0150.020.0250.030.0350.040.0450.05Normalized Number of PatchesMean Attention ScoreAttention Rollout(c)Fig. 11. Comparison of mean attention score and attention rollout, withthe edge device employing DeiT-Tiny and the server employing DeiT-Baseon the ImageNet dataset. The patch selection rule is the attention-sumthreshold selection. (a) shows the overall trade-off between communicationcost and classification accuracy, while (b) zooms in on the region of interest,focusing on areas near the server model\u2019s classification accuracy. (c) shows thenormalized histograms of mean attention score and attention rollout, obtainedby DeiT-Tiny.for accurate classification. Theoretically, the attention scoresreflect each patch\u2019s contribution to the model prediction.By setting thresholds based on cumulative attention scores(attention-sum threshold) or individual patch attention scores(attention threshold), we can dynamically control the numberof transmitted patches based on their estimated relevance.--- Page 10 ---100.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost7878.57979.58080.58181.582Accuracy (%)Server AccuracyTop-kAttention ThresholdAttention-Sum ThresholdFig. 12. Comparison of the patch selection rules: Top- k selection, attentionthreshold selection, and attention-sum threshold selection. The edge deviceand the server employ DeiT-Tiny and DeiT-Base, respectively. The patchimportance is quantified by the mean attention score.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Communication Cost7273747576777879808182Accuracy (%)Server AccuracyEdge Device AccuracyMin-EntropyShannon EntropyFig. 13. Comparison of min-entropy and Shannon entropy with the edgedevice employing DeiT-Tiny and the server employing DeiT-Base.D. Entropy-aware Image TransmissionTo determine the most effective entropy measure, we com-pare the min-entropy and the Shannon entropy within our col-laborative inference framework. For this analysis, we specifi-cally focus on entropy-aware image transmission, deliberatelysetting aside the aspect of attention-aware patch selection. Theentropy values are derived from the softmax output of theMLP classification head in the DeiT-Tiny model used on theedge device. As shown in Fig. 13, the min-entropy is a bettermetric for our collaborative inference, demonstrating a moreeffective distinction between correctly and incorrectly inferredimages compared to the Shannon entropy. Consequently, min-entropy is chosen as the preferred metric for entropy-awareimage transmission.E. Comparison of Models as Semantic EncoderAs discussed in Section IV-A, DeiT-Tiny can act moreeffectively as a semantic encoder than DeiT-Base in spiteof its inferior classification accuracy. In our experiments,we concentrate solely on attention-aware patch selection toevaluate and compare DeiT-Tiny and DeiT-Base. Both modelsselect only important patches based on the mean attention0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost01020304050607080Accuracy (%)Server AccuracyDeiT-TinyDeiT-BaseFig. 14. Comparison of DeiT-Tiny and DeiT-Base as semantic encoders toselect important patches. The classification accuracy is obtained by DeiT-Base.scores, disregarding initial inference outcomes. The classifi-cation accuracy is obtained by DeiT-Base, which processesonly these selected image patches. Fig. 14 shows that DeiT-Tiny more effectively identifies the essential image patches forclassification better than DeiT-Base.In particular, Fig. 5 and Fig. 6 in Section IV-A show that theattention scores obtained by DeiT-Tiny concentrate more onrelevant objects than those from DeiT-Base. Recent work [32]supports this observation, showing that large ViT models oftenallocate high attention scores to less informative backgroundareas. It is because the large ViT models adeptly identifypatches containing minimal information, such as backgroundareas, repurposing the corresponding patches to assimilateglobal image information while neglecting spatial information.While these high attention score patches may enhance theclassification accuracy, they diminish the effectiveness of basemodels as semantic encoders.F . Inference Latency AnalysisIn this subsection, we evaluate the end-to-end inference la-tency of both our collaborative inference and the conventionalserver-based inference.The end-to-end inference latency consists of client infer-ence latency, server inference latency, and communicationlatency. Client inference latency, set as 10.99 milliseconds(ms), reflects the inference latency of DeiT-Tiny on the iPhone12 neural engine [18]. Server inference latency is set at8.32 ms, corresponding to the inference latency of DeiT-Baseon an NVIDIA RTX 3090 GPU [22]. This latency can beeliminated if the image is not transmitted to the server via thestrategy of entropy-aware image transmission. Additionally,server inference latency can be reduced in proportion to thedecreased computational complexity (FLOPs) if only selectedpatches are transmitted via the strategy of attention-awarepatch selection. The FLOPs of DeiT-Base are calculated by(9). Communication latency is calculated using the transmitteddata size divided by the upload data rate. We estimate the datasize of a typical image to be 147 KB, based on an imagecropped to 224 \xd7 224 \xd7 3 bytes. The upload data rates of 1Mbps, 8 Mbps, and 20 Mbps are considered as in [44].--- Page 11 ---1118 2 0Upload Data Rate (Mbps)020040060080010001200Latency (ms)1184.3155.367.1394.561.3 32.7Server-based InferenceProposed Collaborative InferenceFig. 15. Comparison of inference latency for different upload data rates, 1Mbps, 8 Mbps, and 20 Mbps.18 2 0Upload Data Rate (Mbps)050100150200250300350400Latency (ms)Client Inference LatencyCommunication LatencyServer Inference LatencyFig. 16. Breakdown of end-to-end inference latency of the proposed collab-orative inference.Fig. 15 compares the inference latency of the server-basedinference and our proposed inference for each upload data rate.Across these rates, our proposed inference framework consis-tently exhibits better inference latency. We set the thresholds\u03b4sum and \u03b7m at values that result in a minimal accuracyloss of 1 %, as described in Section VI-B. Fig. 16 showsthe proportions of client inference latency, server inferencelatency, and communication latency. Notably, communicationlatency occupies a significant portion at lower upload datarates.VII. C ONCLUSIONWe presented a novel attention-aware \n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2404.07217v2\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 30:
\n~~~~~~~~~~\nHistory: Step #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification energy efficiency, here was the response\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nSummary: Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on "converting" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a "converting" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\nPublication Date: 2024-03-11\narXiv paper ID: 2403.07036v1\n\nTitle: Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels\nSummary: Today's intelligent applications can achieve high performance accuracy using\nmachine learning (ML) techniques, such as deep neural networks (DNNs).\nTraditionally, in a remote DNN inference problem, an edge device transmits raw\ndata to a remote node that performs the inference task. However, this may incur\nhigh transmission energy costs and puts data privacy at risk. In this paper, we\npropose a technique to reduce the total energy bill at the edge device by\nutilizing model compression and time-varying model split between the edge and\nremote nodes. The time-varying representation accounts for time-varying\nchannels and can significantly reduce the total energy at the edge device while\nmaintaining high accuracy (low loss). We implement our approach in an image\nclassification task using the MNIST dataset, and the system environment is\nsimulated as a trajectory navigation scenario to emulate different channel\nconditions. Numerical simulations show that our proposed solution results in\nminimal energy consumption and $CO_2$ emission compared to the considered\nbaselines while exhibiting robust performance across different channel\nconditions and bandwidth regime choices.\nPublication Date: 2021-06-02\narXiv paper ID: 2106.00995v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2005.10851v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\nStep #26, Phase: literature review, Feedback: Successfully added paper 1905.04159v1, Your response: ```SUMMARY\nedge device image classification dynamic inference\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification dynamic inference, here was the response\nTitle: AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and Dynamic Model Placements on Edge Devices\nSummary: Many real-world applications are widely adopting the edge computing paradigm\ndue to its low latency and better privacy protection. With notable success in\nAI and deep learning (DL), edge devices and AI accelerators play a crucial role\nin deploying DL inference services at the edge of the Internet. While prior\nworks quantified various edge devices' efficiency, most studies focused on the\nperformance of edge devices with single DL tasks. Therefore, there is an urgent\nneed to investigate AI multi-tenancy on edge devices, required by many advanced\nDL applications for edge computing.\n  This work investigates two techniques - concurrent model executions and\ndynamic model placements - for AI multi-tenancy on edge devices. With image\nclassification as an example scenario, we empirically evaluate AI multi-tenancy\non various edge devices, AI accelerators, and DL frameworks to identify its\nbenefits and limitations. Our results show that multi-tenancy significantly\nimproves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI\nmulti-tenancy techniques also open up new opportunities for flexible deployment\nof multiple DL services on edge devices and AI accelerators.\nPublication Date: 2021-07-26\narXiv paper ID: 2107.12486v1\n\nTitle: Edge-PRUNE: Flexible Distributed Deep Learning Inference\nSummary: Collaborative deep learning inference between low-resource endpoint devices\nand edge servers has received significant research interest in the last few\nyears. Such computation partitioning can help reducing endpoint device energy\nconsumption and improve latency, but equally importantly also contributes to\nprivacy-preserving of sensitive data. This paper describes Edge-PRUNE, a\nflexible but light-weight computation framework for distributing machine\nlearning inference between edge servers and one or more client devices.\nCompared to previous approaches, Edge-PRUNE is based on a formal dataflow\ncomputing model, and is agnostic towards machine learning training frameworks,\noffering at the same time wide support for leveraging deep learning\naccelerators such as embedded GPUs. The experimental section of the paper\ndemonstrates the use and performance of Edge-PRUNE by image classification and\nobject tracking applications on two heterogeneous endpoint devices and an edge\nserver, over wireless and physical connections. Endpoint device inference time\nfor SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by\ncollaborative inference.\nPublication Date: 2022-04-27\narXiv paper ID: 2204.12947v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: AutoDiCE: Fully Automated Distributed CNN Inference at the Edge\nSummary: Deep Learning approaches based on Convolutional Neural Networks (CNNs) are\nextensively utilized and very successful in a wide range of application areas,\nincluding image classification and speech recognition. For the execution of\ntrained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud\nto the Edge. Unfortunately, deploying and inferring large, compute and memory\nintensive CNNs on edge devices is challenging because these devices typically\nhave limited power budgets and compute/memory resources. One approach to\naddress this challenge is to leverage all available resources across multiple\nedge devices to deploy and execute a large CNN by properly partitioning the CNN\nand running each CNN partition on a separate edge device. Although such\ndistribution, deployment, and execution of large CNNs on multiple edge devices\nis a desirable and beneficial approach, there currently does not exist a design\nand programming framework that takes a trained CNN model, together with a CNN\npartitioning specification, and fully automates the CNN model splitting and\ndeployment on multiple edge devices to facilitate distributed CNN inference at\nthe Edge. Therefore, in this paper, we propose a novel framework, called\nAutoDiCE, for automated splitting of a CNN model into a set of sub-models and\nautomated code generation for distributed and collaborative execution of these\nsub-models on multiple, possibly heterogeneous, edge devices, while supporting\nthe exploitation of parallelism among and within the edge devices. Our\nexperimental results show that AutoDiCE can deliver distributed CNN inference\nwith reduced energy consumption and memory usage per edge device, and improved\noverall system throughput at the same time.\nPublication Date: 2022-07-20\narXiv paper ID: 2207.12113v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n, Your response: ```FULL_TEXT\n2404.07217v2\n```\nStep #28, Phase: literature review, Feedback: 3\n--- Page 1 ---1Attention-aware Semantic Communications forCollaborative InferenceJiwoong Im, Nayoung Kwon, Taewoo Park, Jiheon Woo, Jaeho Lee, and Yongjune KimAbstract\u2014We propose a communication-efficient collaborativeinference framework in the domain of edge inference, focusingon the efficient use of vision transformer (ViT) models. Thepartitioning strategy of conventional collaborative inference failsto reduce communication cost because of the inherent archi-tecture of ViTs maintaining consistent layer dimensions acrossthe entire transformer encoder. Therefore, instead of employingthe partitioning strategy, our framework utilizes a lightweightViT model on the edge device, with the server deploying acomplicated ViT model. To enhance communication efficiencyand achieve the classification accuracy of the server model,we propose two strategies: 1) attention-aware patch selectionand 2) entropy-aware image transmission. Attention-aware patchselection leverages the attention scores generated by the edgedevice\u2019s transformer encoder to identify and select the imagepatches critical for classification. This strategy enables the edgedevice to transmit only the essential patches to the server,significantly improving communication efficiency. Entropy-awareimage transmission uses min-entropy as a metric to accuratelydetermine whether to depend on the lightweight model on theedge device or to request the inference from the server model.In our framework, the lightweight ViT model on the edge deviceacts as a semantic encoder, efficiently identifying and selectingthe crucial image information required for the classification task.Our experiments demonstrate that the proposed collaborativeinference framework can reduce communication overhead by68 %with only a minimal loss in accuracy compared to the servermodel on the ImageNet dataset.Index Terms\u2014Collaborative inference, edge computing, edgeinference, Internet of Things (IoT), semantic communications,split inference, vision transformer.I. I NTRODUCTIONThe rapid advancement of computational resources, coupledwith the proliferation of massive datasets, has significantlyenhanced the practicality of artificial intelligence (AI) ser-vices. Integrating AI techniques with edge devices, includingsmartphones, wearable devices, and Internet of things (IoT)devices, seeks to seamlessly incorporate AI services into awide range of daily life. This effort to advance AI technologiesin the domain of edge computing is commonly known as edgeAI [1]\u2013[3].An important research theme in edge AI is edge inference,focused on efficiently executing inference tasks within theedge network [2]\u2013[5]. Traditionally, raw data is sent fromedge devices (clients) to a server, where a complicated modelJ. Im, N. Kwon, T. Park, J. Woo, J. Lee, and Y . Kim are with theDepartment of Electrical Engineering, Pohang University of Science andTechnology (POSTECH), Pohang 37673, South Korea (e-mail: {jw3562,kwonna, parktaewoo, jhwoo1997, jaeho.lee, yongjune }@postech.ac.kr). J. Imand N. Kwon contributed equally.conducts the inference task, i.e., server-based inference. How-ever, this method incurs significant communication overhead,particularly in scenarios dealing with large volumes of rawdata [3], [4]. An alternative is on-device inference , whichexecutes the inference task directly on resource-constraineddevices, thereby minimizing communication costs. However,this approach often leads to lower performance due to thelimited computational capabilities of edge devices [3], [4].To address the dual challenges of excessive communicationoverhead and limited computational resources, the conceptof collaborative inference has been introduced [2]\u2013[6]. Thisstrategy involves dividing a deep neural network (DNN) modelinto separate parts for the edge device and the server. It lever-ages the model architecture of DNNs, where the dimensionsof intermediate layers can be significantly smaller than theinput dimensions. Within this framework, the edge devicefirst uses its component to extract features from the raw dataand then transmits them to the server. As these extractedfeatures are typically more compressed than the raw data, thecommunication cost can be aggressively reduced. The serverthen utilizes these features and its portion of the model todetermine the final inference result, which is sent back tothe device [5]. The selection of the split point is critical asit significantly impacts the computational load on the edgedevice and the communication overhead [3]. This approachis also known as split inference [5], [7] and device-edgeserver co-inference [3], [6]. Notably, collaborative inferenceis closely connected to semantic communications [8]\u2013[14],considering that the extracted features are essentially semanticinformation tailored for the inference task.Transformers, originally developed for natural languageprocessing (NLP) [15], have been widely adopted across mul-tiple domains. Particularly, the vision transformer (ViT) [16],[17] has demonstrated superior performance and efficiencyin image classification tasks. However, the deployment ofViTs on resource-constrained edge devices is challenging dueto their substantial model size and intensive computationalrequirements [18].In collaborative inference scenarios, the strategy of parti-tioning ViT models fails to effectively reduce communicationoverhead. This limitation stems from the inherent architec-ture of ViTs, which maintains consistent layer dimensionsacross the entire transformer encoder [16], in contrast toDNN models whose intermediate layer dimensions can besignificantly smaller than the raw data dimensions. Hence,partitioning ViT models for collaborative inference cannotreduce communication overhead.In this paper, we propose a communication-efficient collab-arXiv:2404.07217v2  [eess.SP]  31 May 2024--- Page 2 ---2TABLE ITHE COMPARISON OF DEIT MODEL COMPLEXITY AND CLASSIFICATIONACCURACY ON THE IMAGE NET DATASET [17]Model Parameters Memory FLOPs Classification(million) (MB) (G) Accuracy (%)DeiT-Tiny 5 21.22 1.26 72.2DeiT-Small 22 83.21 4.61 79.8DeiT-Base 86 329.55 17.58 81.8orative inference framework utilizing pre-trained ViT models.Note that the collaborative inference in our work correspondsto the collaboration between an edge device and a server,rather than collaboration among multiple edge devices. Insteadof partitioning a single model as in prior work [2]\u2013[6], ourapproach involves the edge device operating a lightweightViT model (e.g., DeiT-Tiny), while the server employs amore complex ViT model (e.g., DeiT-Base). As shown inTable I, DeiT-Tiny (DeiT-Ti) is notably lightweight, makingit suitable for edge deployment. However, its classificationaccuracy is approximately 10 %lower than that of DeiT-Base(DeiT-B) [17].Our objective is to develop a collaborative inference strategythat achieves classification accuracy comparable to the servermodel while minimizing communication overhead betweenthe edge device and the server. This strategy is designed toleverage the strengths of both models: the efficiency and lowresource demand of the tiny model on the edge device, and thehigher classification accuracy of the base model on the server.In our proposed framework, the edge device utilizes its tinymodel to conduct an initial inference without transmitting theimage to the server. Subsequently, the edge device assesseswhether to accept this initial inference or to send the image tothe server for a more accurate inference using the base model.By doing so, we aim to achieve an optimal trade-off betweenclassification accuracy and communication cost in edge-servercollaborative systems.To enhance communication efficiency in our framework,we propose two primary strategies: 1) attention-aware patchselection, which involves selectively transmitting only the mostrelevant patches of the image, and 2) entropy-aware imagetransmission, where the decision to transmit the image to theserver is determined by the level of uncertainty or confidencein the edge device\u2019s initial inference.\u2022 Attention-aware patch selection: In cases where the edgedevice needs to transmit the image to the server, ourstrategy is to transmit only the essential patches thatare crucial for classification, rather than the entire im-age. This selective transmission approach is guided bythe attention scores from the class token to the imagepatches, as processed by the tiny model. These atten-tion scores indicate the relevance of each patch to theclassification task. We investigate several techniques toeffectively select these important image patches usingattention scores. A crucial finding is that the tiny modelis capable of accurately identifying the essential patchesfor the inference task, even when the client classifiesthe image incorrectly. Our experimental results validatethat this approach enables the server model to maintainits classification accuracy, although it processes only se-lectively transmitted patches. Hence, our attention-awarepatch selection can effectively reduce the communicationcost while achieving the classification accuracy of theserver model.\u2022 Entropy-aware image transmission : It is critical for theedge device to make an accurate decision between relyingon its tiny model and requesting a more accurate infer-ence from the base model on the server. This decisionsignificantly affects both the communication overheadand the classification accuracy. Ideally, the edge deviceshould transmit only those images incorrectly classifiedby the tiny model while avoiding the transmission ofimages where the initial inference is correct. To facilitatethis decision, we utilize the min-entropy derived from thesoftmax output values of the classification head in thetiny ViT model. Our experimental findings reveal that thedecisions based on the min-entropy yield higher accuracycompared to those based on the Shannon entropy.In the context of semantic communications, our frameworkis aptly characterized as attention-aware semantic communi-cations. The tiny model\u2019s transformer encoder on the edgedevice acts as a semantic encoder , particularly when thedecision is made to transmit the image to the server. The edgedevice leverages attention scores generated by the transformerencoder to identify the most essential image patches, which arecritical for accurate classification. Interestingly, our findingsreveal that the tiny model functions effectively as a semanticencoder, in spite of its lower classification accuracy comparedto the server model.Moreover, the proposed collaborative inference frameworkoffers the advantage of reducing the computational complexityon the server model, as the server\u2019s inference is conductedsolely on the selected patches. While our primary goal focuseson minimizing communication overhead between the edgedevice and the server, this framework also yields the ancillarybenefit of server-side computational efficiency.The rest of this paper is organized as follows. Section IIoffers a brief overview of the ViT and related work. Section IIIdetails our collaborative inference framework. Section IV andV present our main contributions, which include attention-aware patch selection and entropy-aware image transmission,respectively. Section VI provides experimental results, fol-lowed by conclusions in Section VII.II. B ACKGROUNDSA. Vision TransformerThe ViT [16] is a transformer-based model for computervision tasks, setting a standard in vision models. A simplifiedoverview of the ViT model is shown in Fig. 1. An inputimage x \u2208 RH\xd7W\xd7C is reshaped into a sequence of flattened2D patches xp \u2208 RN\xd7(P2\xb7C), where (H, W), C, and (P, P)denote the resolution of the original image, the number ofchannels, and the resolution of each image patch, respectively.Note that N = HWP2 is the resulting number of patches. Thesepatches are then linearly projected to a consistent dimension D--- Page 3 ---3Fig. 1. Overview of the ViT model [16].across the transformer layers via E \u2208 R(P2\xb7C)\xd7D. The inputembedding of the ViT\u2019s transformer encoder z0 \u2208 R(N+1)\xd7Dis given byz0 =xcls; x1pE; . . .; xNp E+ Epos, (1)where Epos denotes the standard learnable position embedding.The class token z00 = xcls \u2208 R1\xd7D is particularly prependedto the sequence of embedded patches [16]. This class tokenis crucial in classification tasks, serving as a key elementin aggregating the information from the entire sequence ofpatches for the final classification output.The transformer encoder is composed of alternating layersof multi-head self-attention (MSA) and multi-layer perceptron(MLP) blocks as follows:z\u2032l = MSA (LN (zl\u22121)) +zl\u22121, l = 1, . . . , L; (2)zl = MLP (LN (z\u2032l)) +z\u2032l, l = 1, . . . , L; (3)y = LN(z0L), (4)where LN represents the layer normalization. In particular, theimage representation y is the encoder output of the class tokenz00. This image representation y then serves as the input forthe MLP head as shown in Fig. 1.In the MSA block for an input sequence z \u2208 R(N+1)\xd7D, aweighted sum of all values v is computed using query q, keyk, and value v, where q, k, v \u2208 R(N+1)\xd7Dh. The standardself-attention (SA) is formalized as follows [15], [16]:[q, k, v] =zUqkv, (5)A = softmax qk\u22a4\u221aDh, (6)SA(z) =Av, (7)where Uqkv \u2208 RD\xd73Dh and A \u2208 R(N+1)\xd7(N+1) denote theprojection matrix and the attention weight matrix, respectively.The MSA extends the standard self-attention (SA) by perform-ing H parallel SA operations (i.e., heads) and concatenatingtheir outputs [16]:MSA(z) = [SA1(z), \xb7\xb7\xb7 , SAH(z)] UMSA, (8)where UMSA is the projection matrix for the MSA output.B. Related WorkPrior work on collaborative inference has primarily focusedon convolutional neural network (CNN) architectures [2]\u2013[6], [19]. The exploration of collaborative inference withtransformer models has been limited because of the consistentdimension of the MSA blocks in the transformer encoder.Recent studies have delved into on-device inference utilizinglightweight ViT models [18], [20], [21], yet achieving theaccuracy of server models is challenging. To improve the clas-sification accuracy of the edge device, an ensemble strategyemploying multiple edge devices was proposed in [22]. Inthis approach, a central edge device disseminates raw datato nearby edge devices, which then conduct inference usingcompact ViT models and return their intermediate inferenceresults to the central edge device. The central edge deviceobtains the final inference outcome by aggregating the receivedintermediate results. However, this approach leads to increasedcommunication overhead among the edge devices.To reduce communication overhead, recent studies haveintroduced collaborative inference schemes utilizing ViT mod-els on the server, namely masked autoencoder (MAE)-basedoffloading for transformer inference (MOT) [23] and adaptiveMOT (A-MOT) [24]. In these approaches, the edge deviceselects image patches randomly for transmission to the server.The server then reconstructs the entire image using the decoderof MAE and performs classification on this reconstructed im-age. In spite of the benefit of reducing computational demandson edge devices, the classification accuracy is compromised bythe random selection of image patches.Transformers have been utilized in semantic communi-cations for the transmission of text [25] and images [26].Nonetheless, these works do not pertain to classification tasks.Our approach distinctively employs attention scores, particu-larly for the class token, setting our work apart from existingresearch in semantic communications.Recent studies have considered the importance of trainingdata samples to enhance training performance in edge learning.In [27], [28], the authors attempt to identify important trainingdata samples and allocate more communication resourcesto these samples for improved communication efficiency.The authors of [29] select important features of trainingdata samples to enhance communication efficiency duringthe training phase. In contrast, we focus on communication-efficient collaborative inference, which specifically identifiesimportant patches of test data samples and transmits onlythese essential patches. Unlike these works that aim to enhancetraining performance, our framework is designed to improvecommunication efficiency during the inference phase.III. C OLLABORATIVE INFERENCE FRAMEWORK BASED ONTRANSFORMER MODELSWe propose a collaborative inference framework that utilizespre-trained ViT models. This framework is designed to achieveserver-level classification accuracy with minimized communi-cation overhead between the edge device and the server.Due to the consistent layer dimensions of ViTs, conven-tional methods of collaborative inference [2]\u2013[6], [19], which--- Page 4 ---4Fig. 2. The proposed collaborative inference framework utilizing pre-trained ViT models: A lightweight model (e.g., DeiT-Tiny) on the edge device and acomplicated model (DeiT-Base) on the server. The edge device (client) evaluates the uncertainty of its inference results by computing the entropy. If thisentropy exceeds a predetermined threshold, the edge device selects the most important patches of the image based on the attention scores and transmits theseto the server.typically partition a single DNN model, are ineffective atreducing communication costs for ViT models. As a solution,we employ a lightweight ViT model (e.g., DeiT-Tiny) at theedge device, instead of splitting a complex ViT model (e.g.,DeiT-Base), as depicted in Fig. 2. The proposed inferenceframework establishes an efficient collaborative protocol be-tween the edge device and the server, aiming to achieve highclassification accuracy of DeiT-Base model while significantlyreducing communication overhead.In our collaborative inference framework, the edge device(client) first performs inference with its tiny model. The edgedevice then evaluates the entropy level of this initial inference.High entropy (or low confidence) necessitates transmitting theimage to the server since it indicates that the tiny model\u2019sinference would be unreliable. In such instances, only essentialpatches for classification are transmitted instead of the entireimage patches to minimize communication costs. The server,utilizing its complex ViT model, conducts inference based onthese selected patches and sends its classification results backto the edge device, as shown in Fig. 2. This process of selectingcritical patches is governed by the proposed attention-awarepatch selection rule, elaborated in Section IV.If the initial inference\u2019s entropy is low, the edge deviceconfirms its classification result without further interactionwith the server, as shown in Fig. 2. Reducing reliance onthe server to reduce communication costs is achieved throughentropy-aware image transmission rule, detailed in Section V.By integrating these rules, our framework significantly lowerscommunication costs while maintaining classification accuracycomparable to the server model.The steps of the proposed collaborative inference are out-lined in Algorithm 1. Here, Step 2 and Step 3 involvecomputing the initial inference result fc(x(i)) and its entropyg(x(i)), respectively. If the entropy is below a given threshold\u03b7, then fc(x(i)) is deemed the final classification outcome.In cases of higher entropy, as identified in Step 4, the clientAlgorithm 1 Proposed Collaborative Inference FrameworkInput: Images {x(1), x(2), . . . ,x(n)}.Output: Classification results {y(1), y(2), . . . , y(n)}.1: for i = 1 :n do2: y(i)c \u2190 fc(x(i)) \u25b7 Inference on edge device3: Client computes entropy g(x(i))4: if g(x(i)) \u2265 \u03b7 then5: ex(i) \u2190 patch-selection(x(i))6: Client transmits ex(i) to server7: y(i)s \u2190 fs(ex(i)) \u25b7 Inference on server8: Server transmits y(i)s to client9: y(i)c \u2190 y(i)s10: end if11: end forselects and transmits only essential patches to the serverat Step 5 and 6, effectively lowering communication costsby ensuring dim(ex(i)) < dim(x(i)). At Step 7, the serverconducts inference on these selected patches, producing theresult fs(ex(i)), which is then sent back to the client at Step 8.The proposed collaborative inference framework can reducethe computational complexity for the server model by limitingthe inference process to only the selected patches. The com-putational complexity of DeiT-Base, according to [30], [31],is given by144ND2 + 24N2D, (9)where N is the number of patches. Assuming the numberof patches of ex(i) is represented by eN such that eN < N,our framework not only reduces communication overhead butalso enhances computational efficiency on the server side. Thisresults in a significant secondary benefit of our collaborativeinference framework.--- Page 5 ---5Fig. 3. The visualization of attention-aware patch selection. The left is animage of the ImageNet dataset that the client model (DeiT-Tiny) inaccuratelyclassifies as \u2018Hammerhead Shark\u2019. The right shows the selected patches byattention-aware patch selection. These selected patches allow the server model(DeiT-Base) to correctly classify the image as \u2018White Shark\u2019.Fig. 4. The visualization of images and attention scores. In the left column, wehave the original images of the ImageNet dataset. The middle column displaysthe attention score maps generated by DeiT-Tiny. The right column shows thepatches selected by the attention-sum threshold selection. The images arelabeled as \u2018Brambling\u2019, \u2018Irish Setter\u2019, and \u2018Snow Leopard\u2019, respectively.IV. A TTENTION -AWARE PATCH SELECTIONThis section introduces our attention-aware patch selectionmethod, motivated by an intriguing observation: the tiny ViTmodel is capable of identifying the essential patches forclassification, even when its classification is incorrect (seeFig. 3). Consequently, the tiny model on the edge deviceacts as a semantic encoder , effectively extracting essentialinformation for the classification task.To enhance communication efficiency and classificationaccuracy, we address two key questions: 1) how to accuratelyquantify the importance of each patch and 2) how to determinethe optimal number of selected patches.A. Quantifying Patch ImportanceTo quantify the importance of each patch for classification,we utilize the attention scores generated by the SA mecha-nism. The attention score for the class token in a single-headattention is calculated as follows:a = softmax qclsk\u22a4p\u221aDh!, (10)Fig. 5. The comparison of image patches selected by DeiT-Tiny (middlecolumn) and DeiT-Base (right column). The same number of patches areselected according to the mean attention scores. The left column displays theoriginal images of the ImageNet dataset. The images are labeled as \u2018MiniatureSchnauzer\u2019, \u2018Great Gray Owl\u2019, and \u2018Magpie\u2019, respectively.where qcls \u2208 R1\xd7Dh represents the query for the classtoken of the last layer and kp \u2208 RN\xd7Dh denotes the keyscorresponding to the image patches in the last layer. The meanattention score is then obtained by averaging the attentionscores from all multi-heads.Our experimental findings indicate that the mean attentionscores, as computed by the tiny model, effectively assess thesignificance of each patch in contributing to the classificationtask. Fig. 4 presents a side-by-side comparison of ImageNetdataset images (left column) and their corresponding attentionscore maps (middle column). These maps clearly reveal thatpatches crucial for classification are distinguished by higherattention scores, setting them apart from less critical areas,such as background patches, which receive lower attentionscores.This observation supports that the tiny model on the edgedevice is adept at identifying and selecting the most in-formative patches for classification. Within ViT models, theclass token aggregates information from other tokens (imagepatches) via the attention mechanism. For the final classifica-tion, the ViT relies on the MLP head, which considers onlythe information associated with the class token from the lastlayer, disregarding any other inputs. Thus, the attention scoreas defined in (10) serves as a key metric for quantifying thecontributions of individual image patches to the class token. Inthe context of semantic communications, the tiny model of theedge device acts as a semantic encoder, tasked with extractingessential information for the classification task. This rolealigns with the broader objectives of semantic communicationsto emphasize meaning and relevance in the transmission ofinformation [9]\u2013[11]An interesting finding is that DeiT-Tiny can act moreeffectively as a semantic encoder than DeiT-Base in spite--- Page 6 ---6Fig. 6. The comparison of attention score maps computed by DeiT-Tiny(middle column) and DeiT-Base (right column). The left column displays theoriginal images of the ImageNet dataset. The images are labeled as \u2018MiniatureSchnauzer\u2019, \u2018Great Gray Owl\u2019, and \u2018Magpie\u2019, respectively.of its inferior classification accuracy. Fig. 5 compares thepatches selected by DeiT-Tiny (middle column) and thoseby DeiT-Base (right column), showing DeiT-Tiny\u2019s superiorability to discard irrelevant image patches. This seeminglycontradictory finding can be elucidated by the insights fromrecent work [32], which shows that large ViT models tendto allocate high attention scores to less informative back-ground areas. It is because the large models adeptly identifypatches with minimal information, such as background areas,repurposing the corresponding patches to assimilate globalimage information while neglecting their spatial information.High attention scores are allocated to these repurposed patchescontaining global information, particularly in the backgroundareas, as shown in Fig. 6. Although this strategy enhances theclassification accuracy of larger ViT models, it compromisestheir effectiveness as semantic encoders. The experimentalresults on how model complexity affects the efficacy ofsemantic encoders are presented in Section VI-E.The mean attention score from the last layer has previouslybeen leveraged for purposes such as model interpretation [33],[34] and knowledge distillation [31]. Our approach distinc-tively utilizes the mean attention score to reduce commu-nication costs within our collaborative inference framework,differentiating our methodology from previous applications.Additionally, while attention rollout [35] is an establishedtechnique for interpreting transformer models, we opt for themean attention score. This decision is based on the observationthat attention rollout tends to produce more uniform attentionscores, which do not align well with our objective of attention-aware patch selection. The experimental evidence supportingthis decision is provided in Section VI-C.B. Patch Selection RuleIn this subsection, we investigate the patch selection ruleutilizing mean attention scores. Selecting an appropriate num-0 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.01Attention Score of Patch00.010.020.030.040.050.06Normalized Number of PatchesFig. 7. The normalized histogram of the mean attention scores obtained byDeiT-Tiny on the ImageNet dataset.ber of patches for transmission to the server is crucial, as thisdirectly impacts both communication costs and classificationaccuracy. Our goal is to transmit the fewest possible patchesto the server to minimize communication overhead. However,this approach poses a trade-off, as reducing the number oftransmitted patches can limit the information available to theserver model, potentially lowering classification accuracy.The distribution of attention scores reveals that most patchesare assigned low values, as shown in Fig. 7. This propertyenables a significant reduction in the number of transmittedpatches without affecting classification accuracy. By takingadvantage of this beneficial property, we explore the followingpatch selection rules:\u2022 Top-k selection: Selecting the top-k patches that have thehighest attention scores.\u2022 Attention threshold selection: Selecting the patches whoseattention scores exceed a predefined threshold \u03b4.\u2022 Attention-sum threshold selection : Selecting the patcheswith the highest attention scores until their cumulativeattention sum reaches a predetermined threshold \u03b4sum.The top- k selection method selects a fixed number ofpatches based on the highest attention scores, resulting inconsistent communication costs for all input images. Nonethe-less, it overlooks the variation in attention score distributionamong different images. Fig. 4 shows that the quantity ofpatches essential for classification can vary across images,with important information typically concentrated on the classobject, where the highest attention scores are located. Conse-quently, to achieve optimal classification accuracy, the numberof selected patches should be tailored to the size of the objectwithin each image.Both the attention threshold selection and attention-sumthreshold selection methods provide the capability to adjustthe number of selected patches for transmission, making themmore adaptable than the top- k selection method. For instance,as shown in Fig. 4, the numbers of selected patches for \u2018Bram-bling\u2019 and \u2018Snow Leopard\u2019 are 35 and 124, respectively, byusing the attention-sum threshold selection with \u03b4sum = 0.94.Adopting this method ensures the transmission of a consistentsum of attention scores, effectively lowering the risk of omit-ting crucial information. The experimental results show that--- Page 7 ---7the attention threshold selection and attention-sum thresholdselection methods outperform the top- k selection method inachieving an optimal trade-off between classification accuracyand communication efficiency, as detailed in Section VI-C.V. E NTROPY -AWARE IMAGE TRANSMISSIONThis section delves into entropy-aware image transmission,a strategy aimed at reducing communication overhead by con-sidering the varied classification difficulty inherent to differentimages. For less complex images, the edge device\u2019s initialinference may be accurate enough, eliminating the need forfurther interaction with the server. In contrast, more intricateimages necessitate more accurate classification from the servermodel, leading to increased communication overhead. It iscritical for the edge device to make an accurate decisionbetween relying on its initial inference and requesting moreaccurate classification from the server model.Even though the edge device cannot ascertain the correct-ness of its initial inference, it can estimate the inference\u2019sconfidence through the softmax output values of the MLPclassification head. This softmax output can be interpreted asthe posterior probability p\u03b8(y|x), where y denotes the classlabel and \u03b8 denotes the tiny model. Then, we set an entropyfunction g : RL \u2192 R, where L denotes the number of classlabels. The client requests more accurate inference results fromthe server if:g(x) \u2265 \u03b7, (11)where \u03b7 denotes a predetermined threshold.To assess the confidence of the client\u2019s inference, weconsider two exemplary entropy measures: 1) Shannon entropyand 2) min-entropy, with their respective thresholds.The Shannon entropy, a widely used metric for quantifyinguncertainty [36], is calculated bygs(x) =\u2212Xy\u2208Yp\u03b8(y|x) log2 p\u03b8(y|x), (12)where Y denotes the set of all possible class labels. High Shan-non entropy indicates that the given image x is challenging forthe tiny model to classify accurately. Therefore, if gs(x) \u2265 \u03b7s,the edge device transmits the selected patches to the server foran inference from the base model.Another key metric, min-entropy, evaluates uncertainty inthe most conservative manner [37]. The min-entropy is definedasgm(x) =\u2212log2 maxy\u2208Yp\u03b8(y|x), (13)which is directly associated with the confidence level of theinitial inference. If gm(x) \u2265 \u03b7m, the edge device transmits theselected patches to the server for an inference from the basemodel.Our experimental results in Section VI-D show that themin-entropy serves as a better metric within our collabora-tive inference framework. Entropy-aware image transmissionutilizing the min-entropy improves communication efficiencyfor a given level of classification accuracy when compared tousing the Shannon entropy.The entropy has been utilized in diverse applications, suchas prioritizing unlabeled data inputs in active learning [38] andoptimizing wireless data acquisition for edge learning [28].These works typically enhance training procedures using theentropy values calculated by complicated server models. Incontrast, our approach utilizes the min-entropy to assess theuncertainty of initial inferences made by a tiny model onthe edge device instead of a complex server model. This useof uncertainty metrics aims to minimize unnecessary imagetransmissions, thereby reducing communication overhead.VI. E XPERIMENTAL RESULTSA. Experiment SettingsOur experiments employ the ImageNet validation datasetand resize each image to a resolution of 224 \xd7 224 pixels bycenter cropping. An image is flattened to N = 196 patchesbefore the inference.We deploy DeiT-Tiny on the edge device and DeiT-Baseon the server since resource-constrained edge devices havechallenges in employing complicated models such as DeiT-Base. The model complexity of these models is comparedin Table I. DeiT-Tiny can be viable for deployment onedge devices such as NVIDIA Jetson Nano [22], RaspberryPi 4B [39], and iPhone 12 [18]. In contrast, DeiT-Baseis generally considered inappropriate due to its substantialmemory consumption and prolonged inference latency. Forinstance, the authors of [22] explicitly state that DeiT-Baseis inadequate for deployment on NVIDIA Jetson Nano dueto its substantial memory consumption and computationalcomplexity. Similarly, the authors of [18] emphasize memoryconsumption as a critical factor for edge device deployment,considering only lightweight models such as DeiT-Tiny andPiT [40] with approximately 3\u20136 million parameters.We evaluate the impact of varying the number of transmittedpatches on communication cost and classification accuracy.The communication cost in our collaborative inference systemis quantified by the ratio of the number of transmitted patchesto the total number of patches. Consequently, if the edge de-vice sends all image patches to the server, the communicationcost is quantified as 1.When the edge device transmits the selected patches, it isrequired to transmit the position information of these selectedpatches. For instance, assigning one bit per patch as a markerof its selection status is a practical solution. The overhead ofthese additional bits for position information is negligible incomparison to the size of the image itself, given that only onebit is appended for each image patch containing 6, 144 bits.B. Communication Cost vs. Classification AccuracyIn our collaborative inference framework, we assess thetrade-off between communication cost and classification ac-curacy. We utilize the attention-sum threshold selection forattention-aware patch selection and the min-entropy forentropy-aware image transmission.Fig. 8 shows the trade-off between communication cost andclassification accuracy, employing DeiT-Tiny on the edge de-vice and DeiT-Base on the server. We achieve a68 %reduction--- Page 8 ---80.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost777879808182Accuracy (%)Server AccuracyAchieved Accuracy = 0.1 = 0.2 = 0.3 = 0.4 = 0.6 = 0.8 = 1.0Fig. 8. Trade-off between communication cost and classification accuracy,with the edge device employing DeiT-Tiny and the server employing DeiT-Base. The black line represents the achievable curve by the proposed collab-orative inference. We utilize the attention-sum threshold selection method forattention-aware patch selection. For entropy-aware image transmission, themin-entropy serves as the entropy metric, using a threshold value \u03b7 = \u03b7m.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost727374757677787980Accuracy (%)Server AccuracyAchieved Accuracy = 0.1 = 0.2 = 0.3 = 0.4 = 0.6 = 0.8 = 1.0Fig. 9. Trade-off between communication cost and classification accuracy,with the edge device employing DeiT-Tiny and the server employing DeiT-Small. The black line represents the achievable curve by the proposed col-laborative inference. We utilize the attention-sum threshold selection methodfor attention-aware patch selection. For entropy-aware image transmission, themin-entropy serves as the entropy metric, using a threshold value \u03b7 = \u03b7m.in communication cost while attaining a classification accuracyof 80.84 %, with only a minimal loss in accuracy comparedto DeiT-Base\u2019s accuracy of 81.8 %. This communication costreduction is achieved by the attention-sum threshold selectionmethod with a threshold \u03b4sum = 0.97 and the min-entropy withthe threshold \u03b7m = 0.8. The black line indicates the optimizedtrade-off curve achieved by selecting the optimized thresholdvalues of \u03b4sum and \u03b7m. Table II and Table III detail the resultingcommunication costs depending on threshold values.By controlling the threshold values of \u03b7 and \u03b4, our proposedcollaborative inference framework can effectively managecommunication resources. In scenarios with reliable commu-nication channels, lower values of \u03b7 and \u03b4 can be selectedto increase data transmissions and maximize classificationaccuracy. When the channel quality is worse and communi-cation resources are limited, higher values of \u03b7 and \u03b4 canbe set to balance classification accuracy with the availablecommunication resource budget. This strategy offers flexibilityin adapting to varying communication channel conditions,TABLE IIATTENTION -SUM PATCH SELECTION THRESHOLD VALUES ANDEXPECTED NUMBER OF TRANSMITTED PATCHES\u03b4sum Expected Number of Transmitted Patches0.87 88.400.9 100.840.92 110.820.93 116.510.94 122.800.95 129.860.96 137.890.97 147.240.98 158.470.99 172.76TABLE IIIMIN-ENTROPY THRESHOLD VALUES AND EXPECTED RATIO OFTRANSMITTED IMAGES\u03b7m Expected Ratio of Transmitted Images1 0.35670.8 0.42900.6 0.51160.4 0.62460.3 0.71090.2 0.84450.1 0.9714thereby enhancing the efficiency of communication resourcemanagement.Additionally, we investigate a case where the server employsDeiT-Small instead of DeiT-Base. Fig. 9 shows the trade-offbetween communication cost and classification accuracy. Inthis case, we achieve a 71 %reduction in communication costwhile attaining a classification accuracy of 78.8 %, with only aminimal loss in accuracy compared to DeiT-Small\u2019s accuracyof 79.8 %. It is observed that greater communication costreductions can be realized when the accuracy gap betweenthe edge device and the server is reduced.Fig. 10 compares our collaborative inference frameworkwith several existing methods, including server-driven trans-mission (SDT) [41], image super-resolution (ISR) [42],masked autoencoder (MAE)-based offloading for transformerinference (MOT) [23], and adaptive MOT (A-MOT) [24].SDT transmits low-quality images initially and then of-floads high-quality content from the target area based on theserver\u2019s feedback [41]. ISR transmits a low-quality image andthen reconstructs a high-resolution image by super-resolutionmethod [42]. The server model performs inference on thisreconstructed high-resolution image. In MOT and A-MOT, theedge device randomly selects image patches for transmissionto the server. The server then reconstructs the entire imageusing the decoder of MAE [43] and performs classification onthis reconstructed image. The experimental results show thatour proposed collaborative inference framework significantlyimproves performance by employing the tiny model on theedge device, which adeptly transmits essential image patchesfor classification.--- Page 9 ---90.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost68707274767880Accuracy (%)Server AccuracyProposedSDTISRMOTA-MOTFig. 10. Comparison of previous methods and our collaborative inferenceframework, utilizing DeiT-Tiny on the edge device and DeiT-Small on theserver. The experiment is conducted on the ImageNet dataset.C. Attention-aware Patch SelectionIn this subsection, we delve into identifying the effectiveattention metrics for determining patch importance and the op-timal rules for patch selection. First, we examine and comparethe efficacy of mean attention scores versus attention rollout.Next, we evaluate different patch selection methodologies,including top- k selection, attention threshold selection, andattention-sum threshold selection. We maintain a consistentenvironment with DeiT-Tiny on the edge device and DeiT-Base on the server. For this analysis, we specifically focuson attention-aware patch selection, excluding considerationsof entropy-aware image transmission.Fig. 11(a) reveals that both the mean attention score andattention rollout have distinct advantages. In situations de-manding substantial reductions in communication cost, atten-tion rollout is better than the mean attention score. However,as shown in Fig. 11(b), the mean attention score is a bettermetric for attaining accuracy comparable to that of the server.The theoretical advantage of the mean attention score overattention rollout is shown in Fig. 11(c), which compares thenormalized histograms of attention scores for both metrics.Unlike attention rollout, which considers attention scoresfrom multiple layers resulting in a more uniform distributionof scores, the mean attention scores concentrate on fewerpatches with significantly higher relevance to the objects to beclassified. This concentration of attention scores is preferredbecause it implies that fewer but more relevant patches can betransmitted without compromising the classification accuracy,thus aligning with our objective to minimize communicationoverhead while maintaining classification accuracy.Hence, we adopt the mean attention score for our primaryexperiments.Fig. 12 shows that both the attention threshold selectionand attention-sum threshold selection outperform the top- kselection. These methods offer the capability to adjust thenumber of selected patches for transmission, facilitating themaintaining classification accuracy while minimizing commu-nication costs.The variability in informational content across patches,influenced by factors such as object size, type, and thepresence of background, directly impacts their importance0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Communication Cost5055606570758085Accuracy (%)Server AccuracyMean Attention ScoreAttention Rollout(a)0.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost7878.57979.58080.58181.582Accuracy (%)Server AccuracyMean Attention ScoreAttention Rollout(b)0 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.01Attention Score of Patch00.0050.010.0150.020.0250.030.0350.040.0450.05Normalized Number of PatchesMean Attention ScoreAttention Rollout(c)Fig. 11. Comparison of mean attention score and attention rollout, withthe edge device employing DeiT-Tiny and the server employing DeiT-Baseon the ImageNet dataset. The patch selection rule is the attention-sumthreshold selection. (a) shows the overall trade-off between communicationcost and classification accuracy, while (b) zooms in on the region of interest,focusing on areas near the server model\u2019s classification accuracy. (c) shows thenormalized histograms of mean attention score and attention rollout, obtainedby DeiT-Tiny.for accurate classification. Theoretically, the attention scoresreflect each patch\u2019s contribution to the model prediction.By setting thresholds based on cumulative attention scores(attention-sum threshold) or individual patch attention scores(attention threshold), we can dynamically control the numberof transmitted patches based on their estimated relevance.--- Page 10 ---100.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost7878.57979.58080.58181.582Accuracy (%)Server AccuracyTop-kAttention ThresholdAttention-Sum ThresholdFig. 12. Comparison of the patch selection rules: Top- k selection, attentionthreshold selection, and attention-sum threshold selection. The edge deviceand the server employ DeiT-Tiny and DeiT-Base, respectively. The patchimportance is quantified by the mean attention score.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Communication Cost7273747576777879808182Accuracy (%)Server AccuracyEdge Device AccuracyMin-EntropyShannon EntropyFig. 13. Comparison of min-entropy and Shannon entropy with the edgedevice employing DeiT-Tiny and the server employing DeiT-Base.D. Entropy-aware Image TransmissionTo determine the most effective entropy measure, we com-pare the min-entropy and the Shannon entropy within our col-laborative inference framework. For this analysis, we specifi-cally focus on entropy-aware image transmission, deliberatelysetting aside the aspect of attention-aware patch selection. Theentropy values are derived from the softmax output of theMLP classification head in the DeiT-Tiny model used on theedge device. As shown in Fig. 13, the min-entropy is a bettermetric for our collaborative inference, demonstrating a moreeffective distinction between correctly and incorrectly inferredimages compared to the Shannon entropy. Consequently, min-entropy is chosen as the preferred metric for entropy-awareimage transmission.E. Comparison of Models as Semantic EncoderAs discussed in Section IV-A, DeiT-Tiny can act moreeffectively as a semantic encoder than DeiT-Base in spiteof its inferior classification accuracy. In our experiments,we concentrate solely on attention-aware patch selection toevaluate and compare DeiT-Tiny and DeiT-Base. Both modelsselect only important patches based on the mean attention0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost01020304050607080Accuracy (%)Server AccuracyDeiT-TinyDeiT-BaseFig. 14. Comparison of DeiT-Tiny and DeiT-Base as semantic encoders toselect important patches. The classification accuracy is obtained by DeiT-Base.scores, disregarding initial inference outcomes. The classifi-cation accuracy is obtained by DeiT-Base, which processesonly these selected image patches. Fig. 14 shows that DeiT-Tiny more effectively identifies the essential image patches forclassification better than DeiT-Base.In particular, Fig. 5 and Fig. 6 in Section IV-A show that theattention scores obtained by DeiT-Tiny concentrate more onrelevant objects than those from DeiT-Base. Recent work [32]supports this observation, showing that large ViT models oftenallocate high attention scores to less informative backgroundareas. It is because the large ViT models adeptly identifypatches containing minimal information, such as backgroundareas, repurposing the corresponding patches to assimilateglobal image information while neglecting spatial information.While these high attention score patches may enhance theclassification accuracy, they diminish the effectiveness of basemodels as semantic encoders.F . Inference Latency AnalysisIn this subsection, we evaluate the end-to-end inference la-tency of both our collaborative inference and the conventionalserver-based inference.The end-to-end inference latency consists of client infer-ence latency, server inference latency, and communicationlatency. Client inference latency, set as 10.99 milliseconds(ms), reflects the inference latency of DeiT-Tiny on the iPhone12 neural engine [18]. Server inference latency is set at8.32 ms, corresponding to the inference latency of DeiT-Baseon an NVIDIA RTX 3090 GPU [22]. This latency can beeliminated if the image is not transmitted to the server via thestrategy of entropy-aware image transmission. Additionally,server inference latency can be reduced in proportion to thedecreased computational complexity (FLOPs) if only selectedpatches are transmitted via the strategy of attention-awarepatch selection. The FLOPs of DeiT-Base are calculated by(9). Communication latency is calculated using the transmitteddata size divided by the upload data rate. We estimate the datasize of a typical image to be 147 KB, based on an imagecropped to 224 \xd7 224 \xd7 3 bytes. The upload data rates of 1Mbps, 8 Mbps, and 20 Mbps are considered as in [44].--- Page 11 ---1118 2 0Upload Data Rate (Mbps)020040060080010001200Latency (ms)1184.3155.367.1394.561.3 32.7Server-based InferenceProposed Collaborative InferenceFig. 15. Comparison of inference latency for different upload data rates, 1Mbps, 8 Mbps, and 20 Mbps.18 2 0Upload Data Rate (Mbps)050100150200250300350400Latency (ms)Client Inference LatencyCommunication LatencyServer Inference LatencyFig. 16. Breakdown of end-to-end inference latency of the proposed collab-orative inference.Fig. 15 compares the inference latency of the server-basedinference and our proposed inference for each upload data rate.Across these rates, our proposed inference framework consis-tently exhibits better inference latency. We set the thresholds\u03b4sum and \u03b7m at values that result in a minimal accuracyloss of 1 %, as described in Section VI-B. Fig. 16 showsthe proportions of client inference latency, server inferencelatency, and communication latency. Notably, communicationlatency occupies a significant portion at lower upload datarates.VII. C ONCLUSIONWe presented a novel attention-aware, Your response: ```ADD_PAPER\n2404.07217v2\nThis paper presents an attention-aware semantic communication framework for collaborative inference between edge devices and cloud servers using Vision Transformers (ViTs). Key innovations include: (1) Attention-aware patch selection that leverages edge device attention scores to identify and transmit only critical image patches (reducing communication overhead by 68% on ImageNet); (2) Entropy-aware transmission that uses min-entropy to determine when to offload computation to the server; (3) A novel finding that lightweight edge models (DeiT-Tiny) can better identify semantically important patches than larger server models despite lower classification accuracy. The framework achieves server-level accuracy (80.84% vs DeiT-Base's 81.8%) while significantly reducing both communication costs and server computation through selective patch processing. Experimental results demonstrate superior performance compared to prior methods like MAE-based offloading, with particular advantages in low-bandwidth scenarios.\n```\n~~~~~~~~~~\nCurrent Step #29, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2404.07217v2\nNotes: \nYour previous command was: ```ADD_PAPER\n2404.07217v2\nThis paper presents an attention-aware semantic communication framework for collaborative inference between edge devices and cloud servers using Vision Transformers (ViTs). Key innovations include: (1) Attention-aware patch selection that leverages edge device attention scores to identify and transmit only critical image patches (reducing communication overhead by 68% on ImageNet); (2) Entropy-aware transmission that uses min-entropy to determine when to offload computation to the server; (3) A novel finding that lightweight edge models (DeiT-Tiny) can better identify semantically important patches than larger server models despite lower classification accuracy. The framework achieves server-level accuracy (80.84% vs DeiT-Base's 81.8%) while significantly reducing both communication costs and server computation through selective patch processing. Experimental results demonstrate superior performance compared to prior methods like MAE-based offloading, with particular advantages in low-bandwidth scenarios.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 31:
\n~~~~~~~~~~\nHistory: Step #12, Phase: literature review, Feedback: Successfully added paper 2005.10851v1, Your response: ```SUMMARY\nedge device image classification model compression\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\nStep #26, Phase: literature review, Feedback: Successfully added paper 1905.04159v1, Your response: ```SUMMARY\nedge device image classification dynamic inference\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification dynamic inference, here was the response\nTitle: AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and Dynamic Model Placements on Edge Devices\nSummary: Many real-world applications are widely adopting the edge computing paradigm\ndue to its low latency and better privacy protection. With notable success in\nAI and deep learning (DL), edge devices and AI accelerators play a crucial role\nin deploying DL inference services at the edge of the Internet. While prior\nworks quantified various edge devices' efficiency, most studies focused on the\nperformance of edge devices with single DL tasks. Therefore, there is an urgent\nneed to investigate AI multi-tenancy on edge devices, required by many advanced\nDL applications for edge computing.\n  This work investigates two techniques - concurrent model executions and\ndynamic model placements - for AI multi-tenancy on edge devices. With image\nclassification as an example scenario, we empirically evaluate AI multi-tenancy\non various edge devices, AI accelerators, and DL frameworks to identify its\nbenefits and limitations. Our results show that multi-tenancy significantly\nimproves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI\nmulti-tenancy techniques also open up new opportunities for flexible deployment\nof multiple DL services on edge devices and AI accelerators.\nPublication Date: 2021-07-26\narXiv paper ID: 2107.12486v1\n\nTitle: Edge-PRUNE: Flexible Distributed Deep Learning Inference\nSummary: Collaborative deep learning inference between low-resource endpoint devices\nand edge servers has received significant research interest in the last few\nyears. Such computation partitioning can help reducing endpoint device energy\nconsumption and improve latency, but equally importantly also contributes to\nprivacy-preserving of sensitive data. This paper describes Edge-PRUNE, a\nflexible but light-weight computation framework for distributing machine\nlearning inference between edge servers and one or more client devices.\nCompared to previous approaches, Edge-PRUNE is based on a formal dataflow\ncomputing model, and is agnostic towards machine learning training frameworks,\noffering at the same time wide support for leveraging deep learning\naccelerators such as embedded GPUs. The experimental section of the paper\ndemonstrates the use and performance of Edge-PRUNE by image classification and\nobject tracking applications on two heterogeneous endpoint devices and an edge\nserver, over wireless and physical connections. Endpoint device inference time\nfor SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by\ncollaborative inference.\nPublication Date: 2022-04-27\narXiv paper ID: 2204.12947v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: AutoDiCE: Fully Automated Distributed CNN Inference at the Edge\nSummary: Deep Learning approaches based on Convolutional Neural Networks (CNNs) are\nextensively utilized and very successful in a wide range of application areas,\nincluding image classification and speech recognition. For the execution of\ntrained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud\nto the Edge. Unfortunately, deploying and inferring large, compute and memory\nintensive CNNs on edge devices is challenging because these devices typically\nhave limited power budgets and compute/memory resources. One approach to\naddress this challenge is to leverage all available resources across multiple\nedge devices to deploy and execute a large CNN by properly partitioning the CNN\nand running each CNN partition on a separate edge device. Although such\ndistribution, deployment, and execution of large CNNs on multiple edge devices\nis a desirable and beneficial approach, there currently does not exist a design\nand programming framework that takes a trained CNN model, together with a CNN\npartitioning specification, and fully automates the CNN model splitting and\ndeployment on multiple edge devices to facilitate distributed CNN inference at\nthe Edge. Therefore, in this paper, we propose a novel framework, called\nAutoDiCE, for automated splitting of a CNN model into a set of sub-models and\nautomated code generation for distributed and collaborative execution of these\nsub-models on multiple, possibly heterogeneous, edge devices, while supporting\nthe exploitation of parallelism among and within the edge devices. Our\nexperimental results show that AutoDiCE can deliver distributed CNN inference\nwith reduced energy consumption and memory usage per edge device, and improved\noverall system throughput at the same time.\nPublication Date: 2022-07-20\narXiv paper ID: 2207.12113v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n, Your response: ```FULL_TEXT\n2404.07217v2\n```\nStep #28, Phase: literature review, Feedback: 3\n--- Page 1 ---1Attention-aware Semantic Communications forCollaborative InferenceJiwoong Im, Nayoung Kwon, Taewoo Park, Jiheon Woo, Jaeho Lee, and Yongjune KimAbstract\u2014We propose a communication-efficient collaborativeinference framework in the domain of edge inference, focusingon the efficient use of vision transformer (ViT) models. Thepartitioning strategy of conventional collaborative inference failsto reduce communication cost because of the inherent archi-tecture of ViTs maintaining consistent layer dimensions acrossthe entire transformer encoder. Therefore, instead of employingthe partitioning strategy, our framework utilizes a lightweightViT model on the edge device, with the server deploying acomplicated ViT model. To enhance communication efficiencyand achieve the classification accuracy of the server model,we propose two strategies: 1) attention-aware patch selectionand 2) entropy-aware image transmission. Attention-aware patchselection leverages the attention scores generated by the edgedevice\u2019s transformer encoder to identify and select the imagepatches critical for classification. This strategy enables the edgedevice to transmit only the essential patches to the server,significantly improving communication efficiency. Entropy-awareimage transmission uses min-entropy as a metric to accuratelydetermine whether to depend on the lightweight model on theedge device or to request the inference from the server model.In our framework, the lightweight ViT model on the edge deviceacts as a semantic encoder, efficiently identifying and selectingthe crucial image information required for the classification task.Our experiments demonstrate that the proposed collaborativeinference framework can reduce communication overhead by68 %with only a minimal loss in accuracy compared to the servermodel on the ImageNet dataset.Index Terms\u2014Collaborative inference, edge computing, edgeinference, Internet of Things (IoT), semantic communications,split inference, vision transformer.I. I NTRODUCTIONThe rapid advancement of computational resources, coupledwith the proliferation of massive datasets, has significantlyenhanced the practicality of artificial intelligence (AI) ser-vices. Integrating AI techniques with edge devices, includingsmartphones, wearable devices, and Internet of things (IoT)devices, seeks to seamlessly incorporate AI services into awide range of daily life. This effort to advance AI technologiesin the domain of edge computing is commonly known as edgeAI [1]\u2013[3].An important research theme in edge AI is edge inference,focused on efficiently executing inference tasks within theedge network [2]\u2013[5]. Traditionally, raw data is sent fromedge devices (clients) to a server, where a complicated modelJ. Im, N. Kwon, T. Park, J. Woo, J. Lee, and Y . Kim are with theDepartment of Electrical Engineering, Pohang University of Science andTechnology (POSTECH), Pohang 37673, South Korea (e-mail: {jw3562,kwonna, parktaewoo, jhwoo1997, jaeho.lee, yongjune }@postech.ac.kr). J. Imand N. Kwon contributed equally.conducts the inference task, i.e., server-based inference. How-ever, this method incurs significant communication overhead,particularly in scenarios dealing with large volumes of rawdata [3], [4]. An alternative is on-device inference , whichexecutes the inference task directly on resource-constraineddevices, thereby minimizing communication costs. However,this approach often leads to lower performance due to thelimited computational capabilities of edge devices [3], [4].To address the dual challenges of excessive communicationoverhead and limited computational resources, the conceptof collaborative inference has been introduced [2]\u2013[6]. Thisstrategy involves dividing a deep neural network (DNN) modelinto separate parts for the edge device and the server. It lever-ages the model architecture of DNNs, where the dimensionsof intermediate layers can be significantly smaller than theinput dimensions. Within this framework, the edge devicefirst uses its component to extract features from the raw dataand then transmits them to the server. As these extractedfeatures are typically more compressed than the raw data, thecommunication cost can be aggressively reduced. The serverthen utilizes these features and its portion of the model todetermine the final inference result, which is sent back tothe device [5]. The selection of the split point is critical asit significantly impacts the computational load on the edgedevice and the communication overhead [3]. This approachis also known as split inference [5], [7] and device-edgeserver co-inference [3], [6]. Notably, collaborative inferenceis closely connected to semantic communications [8]\u2013[14],considering that the extracted features are essentially semanticinformation tailored for the inference task.Transformers, originally developed for natural languageprocessing (NLP) [15], have been widely adopted across mul-tiple domains. Particularly, the vision transformer (ViT) [16],[17] has demonstrated superior performance and efficiencyin image classification tasks. However, the deployment ofViTs on resource-constrained edge devices is challenging dueto their substantial model size and intensive computationalrequirements [18].In collaborative inference scenarios, the strategy of parti-tioning ViT models fails to effectively reduce communicationoverhead. This limitation stems from the inherent architec-ture of ViTs, which maintains consistent layer dimensionsacross the entire transformer encoder [16], in contrast toDNN models whose intermediate layer dimensions can besignificantly smaller than the raw data dimensions. Hence,partitioning ViT models for collaborative inference cannotreduce communication overhead.In this paper, we propose a communication-efficient collab-arXiv:2404.07217v2  [eess.SP]  31 May 2024--- Page 2 ---2TABLE ITHE COMPARISON OF DEIT MODEL COMPLEXITY AND CLASSIFICATIONACCURACY ON THE IMAGE NET DATASET [17]Model Parameters Memory FLOPs Classification(million) (MB) (G) Accuracy (%)DeiT-Tiny 5 21.22 1.26 72.2DeiT-Small 22 83.21 4.61 79.8DeiT-Base 86 329.55 17.58 81.8orative inference framework utilizing pre-trained ViT models.Note that the collaborative inference in our work correspondsto the collaboration between an edge device and a server,rather than collaboration among multiple edge devices. Insteadof partitioning a single model as in prior work [2]\u2013[6], ourapproach involves the edge device operating a lightweightViT model (e.g., DeiT-Tiny), while the server employs amore complex ViT model (e.g., DeiT-Base). As shown inTable I, DeiT-Tiny (DeiT-Ti) is notably lightweight, makingit suitable for edge deployment. However, its classificationaccuracy is approximately 10 %lower than that of DeiT-Base(DeiT-B) [17].Our objective is to develop a collaborative inference strategythat achieves classification accuracy comparable to the servermodel while minimizing communication overhead betweenthe edge device and the server. This strategy is designed toleverage the strengths of both models: the efficiency and lowresource demand of the tiny model on the edge device, and thehigher classification accuracy of the base model on the server.In our proposed framework, the edge device utilizes its tinymodel to conduct an initial inference without transmitting theimage to the server. Subsequently, the edge device assesseswhether to accept this initial inference or to send the image tothe server for a more accurate inference using the base model.By doing so, we aim to achieve an optimal trade-off betweenclassification accuracy and communication cost in edge-servercollaborative systems.To enhance communication efficiency in our framework,we propose two primary strategies: 1) attention-aware patchselection, which involves selectively transmitting only the mostrelevant patches of the image, and 2) entropy-aware imagetransmission, where the decision to transmit the image to theserver is determined by the level of uncertainty or confidencein the edge device\u2019s initial inference.\u2022 Attention-aware patch selection: In cases where the edgedevice needs to transmit the image to the server, ourstrategy is to transmit only the essential patches thatare crucial for classification, rather than the entire im-age. This selective transmission approach is guided bythe attention scores from the class token to the imagepatches, as processed by the tiny model. These atten-tion scores indicate the relevance of each patch to theclassification task. We investigate several techniques toeffectively select these important image patches usingattention scores. A crucial finding is that the tiny modelis capable of accurately identifying the essential patchesfor the inference task, even when the client classifiesthe image incorrectly. Our experimental results validatethat this approach enables the server model to maintainits classification accuracy, although it processes only se-lectively transmitted patches. Hence, our attention-awarepatch selection can effectively reduce the communicationcost while achieving the classification accuracy of theserver model.\u2022 Entropy-aware image transmission : It is critical for theedge device to make an accurate decision between relyingon its tiny model and requesting a more accurate infer-ence from the base model on the server. This decisionsignificantly affects both the communication overheadand the classification accuracy. Ideally, the edge deviceshould transmit only those images incorrectly classifiedby the tiny model while avoiding the transmission ofimages where the initial inference is correct. To facilitatethis decision, we utilize the min-entropy derived from thesoftmax output values of the classification head in thetiny ViT model. Our experimental findings reveal that thedecisions based on the min-entropy yield higher accuracycompared to those based on the Shannon entropy.In the context of semantic communications, our frameworkis aptly characterized as attention-aware semantic communi-cations. The tiny model\u2019s transformer encoder on the edgedevice acts as a semantic encoder , particularly when thedecision is made to transmit the image to the server. The edgedevice leverages attention scores generated by the transformerencoder to identify the most essential image patches, which arecritical for accurate classification. Interestingly, our findingsreveal that the tiny model functions effectively as a semanticencoder, in spite of its lower classification accuracy comparedto the server model.Moreover, the proposed collaborative inference frameworkoffers the advantage of reducing the computational complexityon the server model, as the server\u2019s inference is conductedsolely on the selected patches. While our primary goal focuseson minimizing communication overhead between the edgedevice and the server, this framework also yields the ancillarybenefit of server-side computational efficiency.The rest of this paper is organized as follows. Section IIoffers a brief overview of the ViT and related work. Section IIIdetails our collaborative inference framework. Section IV andV present our main contributions, which include attention-aware patch selection and entropy-aware image transmission,respectively. Section VI provides experimental results, fol-lowed by conclusions in Section VII.II. B ACKGROUNDSA. Vision TransformerThe ViT [16] is a transformer-based model for computervision tasks, setting a standard in vision models. A simplifiedoverview of the ViT model is shown in Fig. 1. An inputimage x \u2208 RH\xd7W\xd7C is reshaped into a sequence of flattened2D patches xp \u2208 RN\xd7(P2\xb7C), where (H, W), C, and (P, P)denote the resolution of the original image, the number ofchannels, and the resolution of each image patch, respectively.Note that N = HWP2 is the resulting number of patches. Thesepatches are then linearly projected to a consistent dimension D--- Page 3 ---3Fig. 1. Overview of the ViT model [16].across the transformer layers via E \u2208 R(P2\xb7C)\xd7D. The inputembedding of the ViT\u2019s transformer encoder z0 \u2208 R(N+1)\xd7Dis given byz0 =xcls; x1pE; . . .; xNp E+ Epos, (1)where Epos denotes the standard learnable position embedding.The class token z00 = xcls \u2208 R1\xd7D is particularly prependedto the sequence of embedded patches [16]. This class tokenis crucial in classification tasks, serving as a key elementin aggregating the information from the entire sequence ofpatches for the final classification output.The transformer encoder is composed of alternating layersof multi-head self-attention (MSA) and multi-layer perceptron(MLP) blocks as follows:z\u2032l = MSA (LN (zl\u22121)) +zl\u22121, l = 1, . . . , L; (2)zl = MLP (LN (z\u2032l)) +z\u2032l, l = 1, . . . , L; (3)y = LN(z0L), (4)where LN represents the layer normalization. In particular, theimage representation y is the encoder output of the class tokenz00. This image representation y then serves as the input forthe MLP head as shown in Fig. 1.In the MSA block for an input sequence z \u2208 R(N+1)\xd7D, aweighted sum of all values v is computed using query q, keyk, and value v, where q, k, v \u2208 R(N+1)\xd7Dh. The standardself-attention (SA) is formalized as follows [15], [16]:[q, k, v] =zUqkv, (5)A = softmax qk\u22a4\u221aDh, (6)SA(z) =Av, (7)where Uqkv \u2208 RD\xd73Dh and A \u2208 R(N+1)\xd7(N+1) denote theprojection matrix and the attention weight matrix, respectively.The MSA extends the standard self-attention (SA) by perform-ing H parallel SA operations (i.e., heads) and concatenatingtheir outputs [16]:MSA(z) = [SA1(z), \xb7\xb7\xb7 , SAH(z)] UMSA, (8)where UMSA is the projection matrix for the MSA output.B. Related WorkPrior work on collaborative inference has primarily focusedon convolutional neural network (CNN) architectures [2]\u2013[6], [19]. The exploration of collaborative inference withtransformer models has been limited because of the consistentdimension of the MSA blocks in the transformer encoder.Recent studies have delved into on-device inference utilizinglightweight ViT models [18], [20], [21], yet achieving theaccuracy of server models is challenging. To improve the clas-sification accuracy of the edge device, an ensemble strategyemploying multiple edge devices was proposed in [22]. Inthis approach, a central edge device disseminates raw datato nearby edge devices, which then conduct inference usingcompact ViT models and return their intermediate inferenceresults to the central edge device. The central edge deviceobtains the final inference outcome by aggregating the receivedintermediate results. However, this approach leads to increasedcommunication overhead among the edge devices.To reduce communication overhead, recent studies haveintroduced collaborative inference schemes utilizing ViT mod-els on the server, namely masked autoencoder (MAE)-basedoffloading for transformer inference (MOT) [23] and adaptiveMOT (A-MOT) [24]. In these approaches, the edge deviceselects image patches randomly for transmission to the server.The server then reconstructs the entire image using the decoderof MAE and performs classification on this reconstructed im-age. In spite of the benefit of reducing computational demandson edge devices, the classification accuracy is compromised bythe random selection of image patches.Transformers have been utilized in semantic communi-cations for the transmission of text [25] and images [26].Nonetheless, these works do not pertain to classification tasks.Our approach distinctively employs attention scores, particu-larly for the class token, setting our work apart from existingresearch in semantic communications.Recent studies have considered the importance of trainingdata samples to enhance training performance in edge learning.In [27], [28], the authors attempt to identify important trainingdata samples and allocate more communication resourcesto these samples for improved communication efficiency.The authors of [29] select important features of trainingdata samples to enhance communication efficiency duringthe training phase. In contrast, we focus on communication-efficient collaborative inference, which specifically identifiesimportant patches of test data samples and transmits onlythese essential patches. Unlike these works that aim to enhancetraining performance, our framework is designed to improvecommunication efficiency during the inference phase.III. C OLLABORATIVE INFERENCE FRAMEWORK BASED ONTRANSFORMER MODELSWe propose a collaborative inference framework that utilizespre-trained ViT models. This framework is designed to achieveserver-level classification accuracy with minimized communi-cation overhead between the edge device and the server.Due to the consistent layer dimensions of ViTs, conven-tional methods of collaborative inference [2]\u2013[6], [19], which--- Page 4 ---4Fig. 2. The proposed collaborative inference framework utilizing pre-trained ViT models: A lightweight model (e.g., DeiT-Tiny) on the edge device and acomplicated model (DeiT-Base) on the server. The edge device (client) evaluates the uncertainty of its inference results by computing the entropy. If thisentropy exceeds a predetermined threshold, the edge device selects the most important patches of the image based on the attention scores and transmits theseto the server.typically partition a single DNN model, are ineffective atreducing communication costs for ViT models. As a solution,we employ a lightweight ViT model (e.g., DeiT-Tiny) at theedge device, instead of splitting a complex ViT model (e.g.,DeiT-Base), as depicted in Fig. 2. The proposed inferenceframework establishes an efficient collaborative protocol be-tween the edge device and the server, aiming to achieve highclassification accuracy of DeiT-Base model while significantlyreducing communication overhead.In our collaborative inference framework, the edge device(client) first performs inference with its tiny model. The edgedevice then evaluates the entropy level of this initial inference.High entropy (or low confidence) necessitates transmitting theimage to the server since it indicates that the tiny model\u2019sinference would be unreliable. In such instances, only essentialpatches for classification are transmitted instead of the entireimage patches to minimize communication costs. The server,utilizing its complex ViT model, conducts inference based onthese selected patches and sends its classification results backto the edge device, as shown in Fig. 2. This process of selectingcritical patches is governed by the proposed attention-awarepatch selection rule, elaborated in Section IV.If the initial inference\u2019s entropy is low, the edge deviceconfirms its classification result without further interactionwith the server, as shown in Fig. 2. Reducing reliance onthe server to reduce communication costs is achieved throughentropy-aware image transmission rule, detailed in Section V.By integrating these rules, our framework significantly lowerscommunication costs while maintaining classification accuracycomparable to the server model.The steps of the proposed collaborative inference are out-lined in Algorithm 1. Here, Step 2 and Step 3 involvecomputing the initial inference result fc(x(i)) and its entropyg(x(i)), respectively. If the entropy is below a given threshold\u03b7, then fc(x(i)) is deemed the final classification outcome.In cases of higher entropy, as identified in Step 4, the clientAlgorithm 1 Proposed Collaborative Inference FrameworkInput: Images {x(1), x(2), . . . ,x(n)}.Output: Classification results {y(1), y(2), . . . , y(n)}.1: for i = 1 :n do2: y(i)c \u2190 fc(x(i)) \u25b7 Inference on edge device3: Client computes entropy g(x(i))4: if g(x(i)) \u2265 \u03b7 then5: ex(i) \u2190 patch-selection(x(i))6: Client transmits ex(i) to server7: y(i)s \u2190 fs(ex(i)) \u25b7 Inference on server8: Server transmits y(i)s to client9: y(i)c \u2190 y(i)s10: end if11: end forselects and transmits only essential patches to the serverat Step 5 and 6, effectively lowering communication costsby ensuring dim(ex(i)) < dim(x(i)). At Step 7, the serverconducts inference on these selected patches, producing theresult fs(ex(i)), which is then sent back to the client at Step 8.The proposed collaborative inference framework can reducethe computational complexity for the server model by limitingthe inference process to only the selected patches. The com-putational complexity of DeiT-Base, according to [30], [31],is given by144ND2 + 24N2D, (9)where N is the number of patches. Assuming the numberof patches of ex(i) is represented by eN such that eN < N,our framework not only reduces communication overhead butalso enhances computational efficiency on the server side. Thisresults in a significant secondary benefit of our collaborativeinference framework.--- Page 5 ---5Fig. 3. The visualization of attention-aware patch selection. The left is animage of the ImageNet dataset that the client model (DeiT-Tiny) inaccuratelyclassifies as \u2018Hammerhead Shark\u2019. The right shows the selected patches byattention-aware patch selection. These selected patches allow the server model(DeiT-Base) to correctly classify the image as \u2018White Shark\u2019.Fig. 4. The visualization of images and attention scores. In the left column, wehave the original images of the ImageNet dataset. The middle column displaysthe attention score maps generated by DeiT-Tiny. The right column shows thepatches selected by the attention-sum threshold selection. The images arelabeled as \u2018Brambling\u2019, \u2018Irish Setter\u2019, and \u2018Snow Leopard\u2019, respectively.IV. A TTENTION -AWARE PATCH SELECTIONThis section introduces our attention-aware patch selectionmethod, motivated by an intriguing observation: the tiny ViTmodel is capable of identifying the essential patches forclassification, even when its classification is incorrect (seeFig. 3). Consequently, the tiny model on the edge deviceacts as a semantic encoder , effectively extracting essentialinformation for the classification task.To enhance communication efficiency and classificationaccuracy, we address two key questions: 1) how to accuratelyquantify the importance of each patch and 2) how to determinethe optimal number of selected patches.A. Quantifying Patch ImportanceTo quantify the importance of each patch for classification,we utilize the attention scores generated by the SA mecha-nism. The attention score for the class token in a single-headattention is calculated as follows:a = softmax qclsk\u22a4p\u221aDh!, (10)Fig. 5. The comparison of image patches selected by DeiT-Tiny (middlecolumn) and DeiT-Base (right column). The same number of patches areselected according to the mean attention scores. The left column displays theoriginal images of the ImageNet dataset. The images are labeled as \u2018MiniatureSchnauzer\u2019, \u2018Great Gray Owl\u2019, and \u2018Magpie\u2019, respectively.where qcls \u2208 R1\xd7Dh represents the query for the classtoken of the last layer and kp \u2208 RN\xd7Dh denotes the keyscorresponding to the image patches in the last layer. The meanattention score is then obtained by averaging the attentionscores from all multi-heads.Our experimental findings indicate that the mean attentionscores, as computed by the tiny model, effectively assess thesignificance of each patch in contributing to the classificationtask. Fig. 4 presents a side-by-side comparison of ImageNetdataset images (left column) and their corresponding attentionscore maps (middle column). These maps clearly reveal thatpatches crucial for classification are distinguished by higherattention scores, setting them apart from less critical areas,such as background patches, which receive lower attentionscores.This observation supports that the tiny model on the edgedevice is adept at identifying and selecting the most in-formative patches for classification. Within ViT models, theclass token aggregates information from other tokens (imagepatches) via the attention mechanism. For the final classifica-tion, the ViT relies on the MLP head, which considers onlythe information associated with the class token from the lastlayer, disregarding any other inputs. Thus, the attention scoreas defined in (10) serves as a key metric for quantifying thecontributions of individual image patches to the class token. Inthe context of semantic communications, the tiny model of theedge device acts as a semantic encoder, tasked with extractingessential information for the classification task. This rolealigns with the broader objectives of semantic communicationsto emphasize meaning and relevance in the transmission ofinformation [9]\u2013[11]An interesting finding is that DeiT-Tiny can act moreeffectively as a semantic encoder than DeiT-Base in spite--- Page 6 ---6Fig. 6. The comparison of attention score maps computed by DeiT-Tiny(middle column) and DeiT-Base (right column). The left column displays theoriginal images of the ImageNet dataset. The images are labeled as \u2018MiniatureSchnauzer\u2019, \u2018Great Gray Owl\u2019, and \u2018Magpie\u2019, respectively.of its inferior classification accuracy. Fig. 5 compares thepatches selected by DeiT-Tiny (middle column) and thoseby DeiT-Base (right column), showing DeiT-Tiny\u2019s superiorability to discard irrelevant image patches. This seeminglycontradictory finding can be elucidated by the insights fromrecent work [32], which shows that large ViT models tendto allocate high attention scores to less informative back-ground areas. It is because the large models adeptly identifypatches with minimal information, such as background areas,repurposing the corresponding patches to assimilate globalimage information while neglecting their spatial information.High attention scores are allocated to these repurposed patchescontaining global information, particularly in the backgroundareas, as shown in Fig. 6. Although this strategy enhances theclassification accuracy of larger ViT models, it compromisestheir effectiveness as semantic encoders. The experimentalresults on how model complexity affects the efficacy ofsemantic encoders are presented in Section VI-E.The mean attention score from the last layer has previouslybeen leveraged for purposes such as model interpretation [33],[34] and knowledge distillation [31]. Our approach distinc-tively utilizes the mean attention score to reduce commu-nication costs within our collaborative inference framework,differentiating our methodology from previous applications.Additionally, while attention rollout [35] is an establishedtechnique for interpreting transformer models, we opt for themean attention score. This decision is based on the observationthat attention rollout tends to produce more uniform attentionscores, which do not align well with our objective of attention-aware patch selection. The experimental evidence supportingthis decision is provided in Section VI-C.B. Patch Selection RuleIn this subsection, we investigate the patch selection ruleutilizing mean attention scores. Selecting an appropriate num-0 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.01Attention Score of Patch00.010.020.030.040.050.06Normalized Number of PatchesFig. 7. The normalized histogram of the mean attention scores obtained byDeiT-Tiny on the ImageNet dataset.ber of patches for transmission to the server is crucial, as thisdirectly impacts both communication costs and classificationaccuracy. Our goal is to transmit the fewest possible patchesto the server to minimize communication overhead. However,this approach poses a trade-off, as reducing the number oftransmitted patches can limit the information available to theserver model, potentially lowering classification accuracy.The distribution of attention scores reveals that most patchesare assigned low values, as shown in Fig. 7. This propertyenables a significant reduction in the number of transmittedpatches without affecting classification accuracy. By takingadvantage of this beneficial property, we explore the followingpatch selection rules:\u2022 Top-k selection: Selecting the top-k patches that have thehighest attention scores.\u2022 Attention threshold selection: Selecting the patches whoseattention scores exceed a predefined threshold \u03b4.\u2022 Attention-sum threshold selection : Selecting the patcheswith the highest attention scores until their cumulativeattention sum reaches a predetermined threshold \u03b4sum.The top- k selection method selects a fixed number ofpatches based on the highest attention scores, resulting inconsistent communication costs for all input images. Nonethe-less, it overlooks the variation in attention score distributionamong different images. Fig. 4 shows that the quantity ofpatches essential for classification can vary across images,with important information typically concentrated on the classobject, where the highest attention scores are located. Conse-quently, to achieve optimal classification accuracy, the numberof selected patches should be tailored to the size of the objectwithin each image.Both the attention threshold selection and attention-sumthreshold selection methods provide the capability to adjustthe number of selected patches for transmission, making themmore adaptable than the top- k selection method. For instance,as shown in Fig. 4, the numbers of selected patches for \u2018Bram-bling\u2019 and \u2018Snow Leopard\u2019 are 35 and 124, respectively, byusing the attention-sum threshold selection with \u03b4sum = 0.94.Adopting this method ensures the transmission of a consistentsum of attention scores, effectively lowering the risk of omit-ting crucial information. The experimental results show that--- Page 7 ---7the attention threshold selection and attention-sum thresholdselection methods outperform the top- k selection method inachieving an optimal trade-off between classification accuracyand communication efficiency, as detailed in Section VI-C.V. E NTROPY -AWARE IMAGE TRANSMISSIONThis section delves into entropy-aware image transmission,a strategy aimed at reducing communication overhead by con-sidering the varied classification difficulty inherent to differentimages. For less complex images, the edge device\u2019s initialinference may be accurate enough, eliminating the need forfurther interaction with the server. In contrast, more intricateimages necessitate more accurate classification from the servermodel, leading to increased communication overhead. It iscritical for the edge device to make an accurate decisionbetween relying on its initial inference and requesting moreaccurate classification from the server model.Even though the edge device cannot ascertain the correct-ness of its initial inference, it can estimate the inference\u2019sconfidence through the softmax output values of the MLPclassification head. This softmax output can be interpreted asthe posterior probability p\u03b8(y|x), where y denotes the classlabel and \u03b8 denotes the tiny model. Then, we set an entropyfunction g : RL \u2192 R, where L denotes the number of classlabels. The client requests more accurate inference results fromthe server if:g(x) \u2265 \u03b7, (11)where \u03b7 denotes a predetermined threshold.To assess the confidence of the client\u2019s inference, weconsider two exemplary entropy measures: 1) Shannon entropyand 2) min-entropy, with their respective thresholds.The Shannon entropy, a widely used metric for quantifyinguncertainty [36], is calculated bygs(x) =\u2212Xy\u2208Yp\u03b8(y|x) log2 p\u03b8(y|x), (12)where Y denotes the set of all possible class labels. High Shan-non entropy indicates that the given image x is challenging forthe tiny model to classify accurately. Therefore, if gs(x) \u2265 \u03b7s,the edge device transmits the selected patches to the server foran inference from the base model.Another key metric, min-entropy, evaluates uncertainty inthe most conservative manner [37]. The min-entropy is definedasgm(x) =\u2212log2 maxy\u2208Yp\u03b8(y|x), (13)which is directly associated with the confidence level of theinitial inference. If gm(x) \u2265 \u03b7m, the edge device transmits theselected patches to the server for an inference from the basemodel.Our experimental results in Section VI-D show that themin-entropy serves as a better metric within our collabora-tive inference framework. Entropy-aware image transmissionutilizing the min-entropy improves communication efficiencyfor a given level of classification accuracy when compared tousing the Shannon entropy.The entropy has been utilized in diverse applications, suchas prioritizing unlabeled data inputs in active learning [38] andoptimizing wireless data acquisition for edge learning [28].These works typically enhance training procedures using theentropy values calculated by complicated server models. Incontrast, our approach utilizes the min-entropy to assess theuncertainty of initial inferences made by a tiny model onthe edge device instead of a complex server model. This useof uncertainty metrics aims to minimize unnecessary imagetransmissions, thereby reducing communication overhead.VI. E XPERIMENTAL RESULTSA. Experiment SettingsOur experiments employ the ImageNet validation datasetand resize each image to a resolution of 224 \xd7 224 pixels bycenter cropping. An image is flattened to N = 196 patchesbefore the inference.We deploy DeiT-Tiny on the edge device and DeiT-Baseon the server since resource-constrained edge devices havechallenges in employing complicated models such as DeiT-Base. The model complexity of these models is comparedin Table I. DeiT-Tiny can be viable for deployment onedge devices such as NVIDIA Jetson Nano [22], RaspberryPi 4B [39], and iPhone 12 [18]. In contrast, DeiT-Baseis generally considered inappropriate due to its substantialmemory consumption and prolonged inference latency. Forinstance, the authors of [22] explicitly state that DeiT-Baseis inadequate for deployment on NVIDIA Jetson Nano dueto its substantial memory consumption and computationalcomplexity. Similarly, the authors of [18] emphasize memoryconsumption as a critical factor for edge device deployment,considering only lightweight models such as DeiT-Tiny andPiT [40] with approximately 3\u20136 million parameters.We evaluate the impact of varying the number of transmittedpatches on communication cost and classification accuracy.The communication cost in our collaborative inference systemis quantified by the ratio of the number of transmitted patchesto the total number of patches. Consequently, if the edge de-vice sends all image patches to the server, the communicationcost is quantified as 1.When the edge device transmits the selected patches, it isrequired to transmit the position information of these selectedpatches. For instance, assigning one bit per patch as a markerof its selection status is a practical solution. The overhead ofthese additional bits for position information is negligible incomparison to the size of the image itself, given that only onebit is appended for each image patch containing 6, 144 bits.B. Communication Cost vs. Classification AccuracyIn our collaborative inference framework, we assess thetrade-off between communication cost and classification ac-curacy. We utilize the attention-sum threshold selection forattention-aware patch selection and the min-entropy forentropy-aware image transmission.Fig. 8 shows the trade-off between communication cost andclassification accuracy, employing DeiT-Tiny on the edge de-vice and DeiT-Base on the server. We achieve a68 %reduction--- Page 8 ---80.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost777879808182Accuracy (%)Server AccuracyAchieved Accuracy = 0.1 = 0.2 = 0.3 = 0.4 = 0.6 = 0.8 = 1.0Fig. 8. Trade-off between communication cost and classification accuracy,with the edge device employing DeiT-Tiny and the server employing DeiT-Base. The black line represents the achievable curve by the proposed collab-orative inference. We utilize the attention-sum threshold selection method forattention-aware patch selection. For entropy-aware image transmission, themin-entropy serves as the entropy metric, using a threshold value \u03b7 = \u03b7m.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost727374757677787980Accuracy (%)Server AccuracyAchieved Accuracy = 0.1 = 0.2 = 0.3 = 0.4 = 0.6 = 0.8 = 1.0Fig. 9. Trade-off between communication cost and classification accuracy,with the edge device employing DeiT-Tiny and the server employing DeiT-Small. The black line represents the achievable curve by the proposed col-laborative inference. We utilize the attention-sum threshold selection methodfor attention-aware patch selection. For entropy-aware image transmission, themin-entropy serves as the entropy metric, using a threshold value \u03b7 = \u03b7m.in communication cost while attaining a classification accuracyof 80.84 %, with only a minimal loss in accuracy comparedto DeiT-Base\u2019s accuracy of 81.8 %. This communication costreduction is achieved by the attention-sum threshold selectionmethod with a threshold \u03b4sum = 0.97 and the min-entropy withthe threshold \u03b7m = 0.8. The black line indicates the optimizedtrade-off curve achieved by selecting the optimized thresholdvalues of \u03b4sum and \u03b7m. Table II and Table III detail the resultingcommunication costs depending on threshold values.By controlling the threshold values of \u03b7 and \u03b4, our proposedcollaborative inference framework can effectively managecommunication resources. In scenarios with reliable commu-nication channels, lower values of \u03b7 and \u03b4 can be selectedto increase data transmissions and maximize classificationaccuracy. When the channel quality is worse and communi-cation resources are limited, higher values of \u03b7 and \u03b4 canbe set to balance classification accuracy with the availablecommunication resource budget. This strategy offers flexibilityin adapting to varying communication channel conditions,TABLE IIATTENTION -SUM PATCH SELECTION THRESHOLD VALUES ANDEXPECTED NUMBER OF TRANSMITTED PATCHES\u03b4sum Expected Number of Transmitted Patches0.87 88.400.9 100.840.92 110.820.93 116.510.94 122.800.95 129.860.96 137.890.97 147.240.98 158.470.99 172.76TABLE IIIMIN-ENTROPY THRESHOLD VALUES AND EXPECTED RATIO OFTRANSMITTED IMAGES\u03b7m Expected Ratio of Transmitted Images1 0.35670.8 0.42900.6 0.51160.4 0.62460.3 0.71090.2 0.84450.1 0.9714thereby enhancing the efficiency of communication resourcemanagement.Additionally, we investigate a case where the server employsDeiT-Small instead of DeiT-Base. Fig. 9 shows the trade-offbetween communication cost and classification accuracy. Inthis case, we achieve a 71 %reduction in communication costwhile attaining a classification accuracy of 78.8 %, with only aminimal loss in accuracy compared to DeiT-Small\u2019s accuracyof 79.8 %. It is observed that greater communication costreductions can be realized when the accuracy gap betweenthe edge device and the server is reduced.Fig. 10 compares our collaborative inference frameworkwith several existing methods, including server-driven trans-mission (SDT) [41], image super-resolution (ISR) [42],masked autoencoder (MAE)-based offloading for transformerinference (MOT) [23], and adaptive MOT (A-MOT) [24].SDT transmits low-quality images initially and then of-floads high-quality content from the target area based on theserver\u2019s feedback [41]. ISR transmits a low-quality image andthen reconstructs a high-resolution image by super-resolutionmethod [42]. The server model performs inference on thisreconstructed high-resolution image. In MOT and A-MOT, theedge device randomly selects image patches for transmissionto the server. The server then reconstructs the entire imageusing the decoder of MAE [43] and performs classification onthis reconstructed image. The experimental results show thatour proposed collaborative inference framework significantlyimproves performance by employing the tiny model on theedge device, which adeptly transmits essential image patchesfor classification.--- Page 9 ---90.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost68707274767880Accuracy (%)Server AccuracyProposedSDTISRMOTA-MOTFig. 10. Comparison of previous methods and our collaborative inferenceframework, utilizing DeiT-Tiny on the edge device and DeiT-Small on theserver. The experiment is conducted on the ImageNet dataset.C. Attention-aware Patch SelectionIn this subsection, we delve into identifying the effectiveattention metrics for determining patch importance and the op-timal rules for patch selection. First, we examine and comparethe efficacy of mean attention scores versus attention rollout.Next, we evaluate different patch selection methodologies,including top- k selection, attention threshold selection, andattention-sum threshold selection. We maintain a consistentenvironment with DeiT-Tiny on the edge device and DeiT-Base on the server. For this analysis, we specifically focuson attention-aware patch selection, excluding considerationsof entropy-aware image transmission.Fig. 11(a) reveals that both the mean attention score andattention rollout have distinct advantages. In situations de-manding substantial reductions in communication cost, atten-tion rollout is better than the mean attention score. However,as shown in Fig. 11(b), the mean attention score is a bettermetric for attaining accuracy comparable to that of the server.The theoretical advantage of the mean attention score overattention rollout is shown in Fig. 11(c), which compares thenormalized histograms of attention scores for both metrics.Unlike attention rollout, which considers attention scoresfrom multiple layers resulting in a more uniform distributionof scores, the mean attention scores concentrate on fewerpatches with significantly higher relevance to the objects to beclassified. This concentration of attention scores is preferredbecause it implies that fewer but more relevant patches can betransmitted without compromising the classification accuracy,thus aligning with our objective to minimize communicationoverhead while maintaining classification accuracy.Hence, we adopt the mean attention score for our primaryexperiments.Fig. 12 shows that both the attention threshold selectionand attention-sum threshold selection outperform the top- kselection. These methods offer the capability to adjust thenumber of selected patches for transmission, facilitating themaintaining classification accuracy while minimizing commu-nication costs.The variability in informational content across patches,influenced by factors such as object size, type, and thepresence of background, directly impacts their importance0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Communication Cost5055606570758085Accuracy (%)Server AccuracyMean Attention ScoreAttention Rollout(a)0.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost7878.57979.58080.58181.582Accuracy (%)Server AccuracyMean Attention ScoreAttention Rollout(b)0 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.01Attention Score of Patch00.0050.010.0150.020.0250.030.0350.040.0450.05Normalized Number of PatchesMean Attention ScoreAttention Rollout(c)Fig. 11. Comparison of mean attention score and attention rollout, withthe edge device employing DeiT-Tiny and the server employing DeiT-Baseon the ImageNet dataset. The patch selection rule is the attention-sumthreshold selection. (a) shows the overall trade-off between communicationcost and classification accuracy, while (b) zooms in on the region of interest,focusing on areas near the server model\u2019s classification accuracy. (c) shows thenormalized histograms of mean attention score and attention rollout, obtainedby DeiT-Tiny.for accurate classification. Theoretically, the attention scoresreflect each patch\u2019s contribution to the model prediction.By setting thresholds based on cumulative attention scores(attention-sum threshold) or individual patch attention scores(attention threshold), we can dynamically control the numberof transmitted patches based on their estimated relevance.--- Page 10 ---100.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost7878.57979.58080.58181.582Accuracy (%)Server AccuracyTop-kAttention ThresholdAttention-Sum ThresholdFig. 12. Comparison of the patch selection rules: Top- k selection, attentionthreshold selection, and attention-sum threshold selection. The edge deviceand the server employ DeiT-Tiny and DeiT-Base, respectively. The patchimportance is quantified by the mean attention score.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Communication Cost7273747576777879808182Accuracy (%)Server AccuracyEdge Device AccuracyMin-EntropyShannon EntropyFig. 13. Comparison of min-entropy and Shannon entropy with the edgedevice employing DeiT-Tiny and the server employing DeiT-Base.D. Entropy-aware Image TransmissionTo determine the most effective entropy measure, we com-pare the min-entropy and the Shannon entropy within our col-laborative inference framework. For this analysis, we specifi-cally focus on entropy-aware image transmission, deliberatelysetting aside the aspect of attention-aware patch selection. Theentropy values are derived from the softmax output of theMLP classification head in the DeiT-Tiny model used on theedge device. As shown in Fig. 13, the min-entropy is a bettermetric for our collaborative inference, demonstrating a moreeffective distinction between correctly and incorrectly inferredimages compared to the Shannon entropy. Consequently, min-entropy is chosen as the preferred metric for entropy-awareimage transmission.E. Comparison of Models as Semantic EncoderAs discussed in Section IV-A, DeiT-Tiny can act moreeffectively as a semantic encoder than DeiT-Base in spiteof its inferior classification accuracy. In our experiments,we concentrate solely on attention-aware patch selection toevaluate and compare DeiT-Tiny and DeiT-Base. Both modelsselect only important patches based on the mean attention0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost01020304050607080Accuracy (%)Server AccuracyDeiT-TinyDeiT-BaseFig. 14. Comparison of DeiT-Tiny and DeiT-Base as semantic encoders toselect important patches. The classification accuracy is obtained by DeiT-Base.scores, disregarding initial inference outcomes. The classifi-cation accuracy is obtained by DeiT-Base, which processesonly these selected image patches. Fig. 14 shows that DeiT-Tiny more effectively identifies the essential image patches forclassification better than DeiT-Base.In particular, Fig. 5 and Fig. 6 in Section IV-A show that theattention scores obtained by DeiT-Tiny concentrate more onrelevant objects than those from DeiT-Base. Recent work [32]supports this observation, showing that large ViT models oftenallocate high attention scores to less informative backgroundareas. It is because the large ViT models adeptly identifypatches containing minimal information, such as backgroundareas, repurposing the corresponding patches to assimilateglobal image information while neglecting spatial information.While these high attention score patches may enhance theclassification accuracy, they diminish the effectiveness of basemodels as semantic encoders.F . Inference Latency AnalysisIn this subsection, we evaluate the end-to-end inference la-tency of both our collaborative inference and the conventionalserver-based inference.The end-to-end inference latency consists of client infer-ence latency, server inference latency, and communicationlatency. Client inference latency, set as 10.99 milliseconds(ms), reflects the inference latency of DeiT-Tiny on the iPhone12 neural engine [18]. Server inference latency is set at8.32 ms, corresponding to the inference latency of DeiT-Baseon an NVIDIA RTX 3090 GPU [22]. This latency can beeliminated if the image is not transmitted to the server via thestrategy of entropy-aware image transmission. Additionally,server inference latency can be reduced in proportion to thedecreased computational complexity (FLOPs) if only selectedpatches are transmitted via the strategy of attention-awarepatch selection. The FLOPs of DeiT-Base are calculated by(9). Communication latency is calculated using the transmitteddata size divided by the upload data rate. We estimate the datasize of a typical image to be 147 KB, based on an imagecropped to 224 \xd7 224 \xd7 3 bytes. The upload data rates of 1Mbps, 8 Mbps, and 20 Mbps are considered as in [44].--- Page 11 ---1118 2 0Upload Data Rate (Mbps)020040060080010001200Latency (ms)1184.3155.367.1394.561.3 32.7Server-based InferenceProposed Collaborative InferenceFig. 15. Comparison of inference latency for different upload data rates, 1Mbps, 8 Mbps, and 20 Mbps.18 2 0Upload Data Rate (Mbps)050100150200250300350400Latency (ms)Client Inference LatencyCommunication LatencyServer Inference LatencyFig. 16. Breakdown of end-to-end inference latency of the proposed collab-orative inference.Fig. 15 compares the inference latency of the server-basedinference and our proposed inference for each upload data rate.Across these rates, our proposed inference framework consis-tently exhibits better inference latency. We set the thresholds\u03b4sum and \u03b7m at values that result in a minimal accuracyloss of 1 %, as described in Section VI-B. Fig. 16 showsthe proportions of client inference latency, server inferencelatency, and communication latency. Notably, communicationlatency occupies a significant portion at lower upload datarates.VII. C ONCLUSIONWe presented a novel attention-aware, Your response: ```ADD_PAPER\n2404.07217v2\nThis paper presents an attention-aware semantic communication framework for collaborative inference between edge devices and cloud servers using Vision Transformers (ViTs). Key innovations include: (1) Attention-aware patch selection that leverages edge device attention scores to identify and transmit only critical image patches (reducing communication overhead by 68% on ImageNet); (2) Entropy-aware transmission that uses min-entropy to determine when to offload computation to the server; (3) A novel finding that lightweight edge models (DeiT-Tiny) can better identify semantically important patches than larger server models despite lower classification accuracy. The framework achieves server-level accuracy (80.84% vs DeiT-Base's 81.8%) while significantly reducing both communication costs and server computation through selective patch processing. Experimental results demonstrate superior performance compared to prior methods like MAE-based offloading, with particular advantages in low-bandwidth scenarios.\n```\nStep #29, Phase: literature review, Feedback: Successfully added paper 2404.07217v2, Your response: ```SUMMARY\nedge device image classification early exit networks\n```\n~~~~~~~~~~\nCurrent Step #30, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification early exit networks, here was the response\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Resource-Constrained Edge AI with Early Exit Prediction\nSummary: By leveraging the data sample diversity, the early-exit network recently\nemerges as a prominent neural network architecture to accelerate the deep\nlearning inference process. However, intermediate classifiers of the early\nexits introduce additional computation overhead, which is unfavorable for\nresource-constrained edge artificial intelligence (AI). In this paper, we\npropose an early exit prediction mechanism to reduce the on-device computation\noverhead in a device-edge co-inference system supported by early-exit networks.\nSpecifically, we design a low-complexity module, namely the Exit Predictor, to\nguide some distinctly "hard" samples to bypass the computation of the early\nexits. Besides, considering the varying communication bandwidth, we extend the\nearly exit prediction mechanism for latency-aware edge inference, which adapts\nthe prediction thresholds of the Exit Predictor and the confidence thresholds\nof the early-exit network via a few simple regression models. Extensive\nexperiment results demonstrate the effectiveness of the Exit Predictor in\nachieving a better tradeoff between accuracy and on-device computation overhead\nfor early-exit networks. Besides, compared with the baseline methods, the\nproposed method for latency-aware edge inference attains higher inference\naccuracy under different bandwidth conditions.\nPublication Date: 2022-06-15\narXiv paper ID: 2206.07269v2\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification early exit networks\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 32:
\n~~~~~~~~~~\nHistory: Step #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\nStep #26, Phase: literature review, Feedback: Successfully added paper 1905.04159v1, Your response: ```SUMMARY\nedge device image classification dynamic inference\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification dynamic inference, here was the response\nTitle: AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and Dynamic Model Placements on Edge Devices\nSummary: Many real-world applications are widely adopting the edge computing paradigm\ndue to its low latency and better privacy protection. With notable success in\nAI and deep learning (DL), edge devices and AI accelerators play a crucial role\nin deploying DL inference services at the edge of the Internet. While prior\nworks quantified various edge devices' efficiency, most studies focused on the\nperformance of edge devices with single DL tasks. Therefore, there is an urgent\nneed to investigate AI multi-tenancy on edge devices, required by many advanced\nDL applications for edge computing.\n  This work investigates two techniques - concurrent model executions and\ndynamic model placements - for AI multi-tenancy on edge devices. With image\nclassification as an example scenario, we empirically evaluate AI multi-tenancy\non various edge devices, AI accelerators, and DL frameworks to identify its\nbenefits and limitations. Our results show that multi-tenancy significantly\nimproves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI\nmulti-tenancy techniques also open up new opportunities for flexible deployment\nof multiple DL services on edge devices and AI accelerators.\nPublication Date: 2021-07-26\narXiv paper ID: 2107.12486v1\n\nTitle: Edge-PRUNE: Flexible Distributed Deep Learning Inference\nSummary: Collaborative deep learning inference between low-resource endpoint devices\nand edge servers has received significant research interest in the last few\nyears. Such computation partitioning can help reducing endpoint device energy\nconsumption and improve latency, but equally importantly also contributes to\nprivacy-preserving of sensitive data. This paper describes Edge-PRUNE, a\nflexible but light-weight computation framework for distributing machine\nlearning inference between edge servers and one or more client devices.\nCompared to previous approaches, Edge-PRUNE is based on a formal dataflow\ncomputing model, and is agnostic towards machine learning training frameworks,\noffering at the same time wide support for leveraging deep learning\naccelerators such as embedded GPUs. The experimental section of the paper\ndemonstrates the use and performance of Edge-PRUNE by image classification and\nobject tracking applications on two heterogeneous endpoint devices and an edge\nserver, over wireless and physical connections. Endpoint device inference time\nfor SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by\ncollaborative inference.\nPublication Date: 2022-04-27\narXiv paper ID: 2204.12947v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: AutoDiCE: Fully Automated Distributed CNN Inference at the Edge\nSummary: Deep Learning approaches based on Convolutional Neural Networks (CNNs) are\nextensively utilized and very successful in a wide range of application areas,\nincluding image classification and speech recognition. For the execution of\ntrained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud\nto the Edge. Unfortunately, deploying and inferring large, compute and memory\nintensive CNNs on edge devices is challenging because these devices typically\nhave limited power budgets and compute/memory resources. One approach to\naddress this challenge is to leverage all available resources across multiple\nedge devices to deploy and execute a large CNN by properly partitioning the CNN\nand running each CNN partition on a separate edge device. Although such\ndistribution, deployment, and execution of large CNNs on multiple edge devices\nis a desirable and beneficial approach, there currently does not exist a design\nand programming framework that takes a trained CNN model, together with a CNN\npartitioning specification, and fully automates the CNN model splitting and\ndeployment on multiple edge devices to facilitate distributed CNN inference at\nthe Edge. Therefore, in this paper, we propose a novel framework, called\nAutoDiCE, for automated splitting of a CNN model into a set of sub-models and\nautomated code generation for distributed and collaborative execution of these\nsub-models on multiple, possibly heterogeneous, edge devices, while supporting\nthe exploitation of parallelism among and within the edge devices. Our\nexperimental results show that AutoDiCE can deliver distributed CNN inference\nwith reduced energy consumption and memory usage per edge device, and improved\noverall system throughput at the same time.\nPublication Date: 2022-07-20\narXiv paper ID: 2207.12113v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n, Your response: ```FULL_TEXT\n2404.07217v2\n```\nStep #28, Phase: literature review, Feedback: 3\n--- Page 1 ---1Attention-aware Semantic Communications forCollaborative InferenceJiwoong Im, Nayoung Kwon, Taewoo Park, Jiheon Woo, Jaeho Lee, and Yongjune KimAbstract\u2014We propose a communication-efficient collaborativeinference framework in the domain of edge inference, focusingon the efficient use of vision transformer (ViT) models. Thepartitioning strategy of conventional collaborative inference failsto reduce communication cost because of the inherent archi-tecture of ViTs maintaining consistent layer dimensions acrossthe entire transformer encoder. Therefore, instead of employingthe partitioning strategy, our framework utilizes a lightweightViT model on the edge device, with the server deploying acomplicated ViT model. To enhance communication efficiencyand achieve the classification accuracy of the server model,we propose two strategies: 1) attention-aware patch selectionand 2) entropy-aware image transmission. Attention-aware patchselection leverages the attention scores generated by the edgedevice\u2019s transformer encoder to identify and select the imagepatches critical for classification. This strategy enables the edgedevice to transmit only the essential patches to the server,significantly improving communication efficiency. Entropy-awareimage transmission uses min-entropy as a metric to accuratelydetermine whether to depend on the lightweight model on theedge device or to request the inference from the server model.In our framework, the lightweight ViT model on the edge deviceacts as a semantic encoder, efficiently identifying and selectingthe crucial image information required for the classification task.Our experiments demonstrate that the proposed collaborativeinference framework can reduce communication overhead by68 %with only a minimal loss in accuracy compared to the servermodel on the ImageNet dataset.Index Terms\u2014Collaborative inference, edge computing, edgeinference, Internet of Things (IoT), semantic communications,split inference, vision transformer.I. I NTRODUCTIONThe rapid advancement of computational resources, coupledwith the proliferation of massive datasets, has significantlyenhanced the practicality of artificial intelligence (AI) ser-vices. Integrating AI techniques with edge devices, includingsmartphones, wearable devices, and Internet of things (IoT)devices, seeks to seamlessly incorporate AI services into awide range of daily life. This effort to advance AI technologiesin the domain of edge computing is commonly known as edgeAI [1]\u2013[3].An important research theme in edge AI is edge inference,focused on efficiently executing inference tasks within theedge network [2]\u2013[5]. Traditionally, raw data is sent fromedge devices (clients) to a server, where a complicated modelJ. Im, N. Kwon, T. Park, J. Woo, J. Lee, and Y . Kim are with theDepartment of Electrical Engineering, Pohang University of Science andTechnology (POSTECH), Pohang 37673, South Korea (e-mail: {jw3562,kwonna, parktaewoo, jhwoo1997, jaeho.lee, yongjune }@postech.ac.kr). J. Imand N. Kwon contributed equally.conducts the inference task, i.e., server-based inference. How-ever, this method incurs significant communication overhead,particularly in scenarios dealing with large volumes of rawdata [3], [4]. An alternative is on-device inference , whichexecutes the inference task directly on resource-constraineddevices, thereby minimizing communication costs. However,this approach often leads to lower performance due to thelimited computational capabilities of edge devices [3], [4].To address the dual challenges of excessive communicationoverhead and limited computational resources, the conceptof collaborative inference has been introduced [2]\u2013[6]. Thisstrategy involves dividing a deep neural network (DNN) modelinto separate parts for the edge device and the server. It lever-ages the model architecture of DNNs, where the dimensionsof intermediate layers can be significantly smaller than theinput dimensions. Within this framework, the edge devicefirst uses its component to extract features from the raw dataand then transmits them to the server. As these extractedfeatures are typically more compressed than the raw data, thecommunication cost can be aggressively reduced. The serverthen utilizes these features and its portion of the model todetermine the final inference result, which is sent back tothe device [5]. The selection of the split point is critical asit significantly impacts the computational load on the edgedevice and the communication overhead [3]. This approachis also known as split inference [5], [7] and device-edgeserver co-inference [3], [6]. Notably, collaborative inferenceis closely connected to semantic communications [8]\u2013[14],considering that the extracted features are essentially semanticinformation tailored for the inference task.Transformers, originally developed for natural languageprocessing (NLP) [15], have been widely adopted across mul-tiple domains. Particularly, the vision transformer (ViT) [16],[17] has demonstrated superior performance and efficiencyin image classification tasks. However, the deployment ofViTs on resource-constrained edge devices is challenging dueto their substantial model size and intensive computationalrequirements [18].In collaborative inference scenarios, the strategy of parti-tioning ViT models fails to effectively reduce communicationoverhead. This limitation stems from the inherent architec-ture of ViTs, which maintains consistent layer dimensionsacross the entire transformer encoder [16], in contrast toDNN models whose intermediate layer dimensions can besignificantly smaller than the raw data dimensions. Hence,partitioning ViT models for collaborative inference cannotreduce communication overhead.In this paper, we propose a communication-efficient collab-arXiv:2404.07217v2  [eess.SP]  31 May 2024--- Page 2 ---2TABLE ITHE COMPARISON OF DEIT MODEL COMPLEXITY AND CLASSIFICATIONACCURACY ON THE IMAGE NET DATASET [17]Model Parameters Memory FLOPs Classification(million) (MB) (G) Accuracy (%)DeiT-Tiny 5 21.22 1.26 72.2DeiT-Small 22 83.21 4.61 79.8DeiT-Base 86 329.55 17.58 81.8orative inference framework utilizing pre-trained ViT models.Note that the collaborative inference in our work correspondsto the collaboration between an edge device and a server,rather than collaboration among multiple edge devices. Insteadof partitioning a single model as in prior work [2]\u2013[6], ourapproach involves the edge device operating a lightweightViT model (e.g., DeiT-Tiny), while the server employs amore complex ViT model (e.g., DeiT-Base). As shown inTable I, DeiT-Tiny (DeiT-Ti) is notably lightweight, makingit suitable for edge deployment. However, its classificationaccuracy is approximately 10 %lower than that of DeiT-Base(DeiT-B) [17].Our objective is to develop a collaborative inference strategythat achieves classification accuracy comparable to the servermodel while minimizing communication overhead betweenthe edge device and the server. This strategy is designed toleverage the strengths of both models: the efficiency and lowresource demand of the tiny model on the edge device, and thehigher classification accuracy of the base model on the server.In our proposed framework, the edge device utilizes its tinymodel to conduct an initial inference without transmitting theimage to the server. Subsequently, the edge device assesseswhether to accept this initial inference or to send the image tothe server for a more accurate inference using the base model.By doing so, we aim to achieve an optimal trade-off betweenclassification accuracy and communication cost in edge-servercollaborative systems.To enhance communication efficiency in our framework,we propose two primary strategies: 1) attention-aware patchselection, which involves selectively transmitting only the mostrelevant patches of the image, and 2) entropy-aware imagetransmission, where the decision to transmit the image to theserver is determined by the level of uncertainty or confidencein the edge device\u2019s initial inference.\u2022 Attention-aware patch selection: In cases where the edgedevice needs to transmit the image to the server, ourstrategy is to transmit only the essential patches thatare crucial for classification, rather than the entire im-age. This selective transmission approach is guided bythe attention scores from the class token to the imagepatches, as processed by the tiny model. These atten-tion scores indicate the relevance of each patch to theclassification task. We investigate several techniques toeffectively select these important image patches usingattention scores. A crucial finding is that the tiny modelis capable of accurately identifying the essential patchesfor the inference task, even when the client classifiesthe image incorrectly. Our experimental results validatethat this approach enables the server model to maintainits classification accuracy, although it processes only se-lectively transmitted patches. Hence, our attention-awarepatch selection can effectively reduce the communicationcost while achieving the classification accuracy of theserver model.\u2022 Entropy-aware image transmission : It is critical for theedge device to make an accurate decision between relyingon its tiny model and requesting a more accurate infer-ence from the base model on the server. This decisionsignificantly affects both the communication overheadand the classification accuracy. Ideally, the edge deviceshould transmit only those images incorrectly classifiedby the tiny model while avoiding the transmission ofimages where the initial inference is correct. To facilitatethis decision, we utilize the min-entropy derived from thesoftmax output values of the classification head in thetiny ViT model. Our experimental findings reveal that thedecisions based on the min-entropy yield higher accuracycompared to those based on the Shannon entropy.In the context of semantic communications, our frameworkis aptly characterized as attention-aware semantic communi-cations. The tiny model\u2019s transformer encoder on the edgedevice acts as a semantic encoder , particularly when thedecision is made to transmit the image to the server. The edgedevice leverages attention scores generated by the transformerencoder to identify the most essential image patches, which arecritical for accurate classification. Interestingly, our findingsreveal that the tiny model functions effectively as a semanticencoder, in spite of its lower classification accuracy comparedto the server model.Moreover, the proposed collaborative inference frameworkoffers the advantage of reducing the computational complexityon the server model, as the server\u2019s inference is conductedsolely on the selected patches. While our primary goal focuseson minimizing communication overhead between the edgedevice and the server, this framework also yields the ancillarybenefit of server-side computational efficiency.The rest of this paper is organized as follows. Section IIoffers a brief overview of the ViT and related work. Section IIIdetails our collaborative inference framework. Section IV andV present our main contributions, which include attention-aware patch selection and entropy-aware image transmission,respectively. Section VI provides experimental results, fol-lowed by conclusions in Section VII.II. B ACKGROUNDSA. Vision TransformerThe ViT [16] is a transformer-based model for computervision tasks, setting a standard in vision models. A simplifiedoverview of the ViT model is shown in Fig. 1. An inputimage x \u2208 RH\xd7W\xd7C is reshaped into a sequence of flattened2D patches xp \u2208 RN\xd7(P2\xb7C), where (H, W), C, and (P, P)denote the resolution of the original image, the number ofchannels, and the resolution of each image patch, respectively.Note that N = HWP2 is the resulting number of patches. Thesepatches are then linearly projected to a consistent dimension D--- Page 3 ---3Fig. 1. Overview of the ViT model [16].across the transformer layers via E \u2208 R(P2\xb7C)\xd7D. The inputembedding of the ViT\u2019s transformer encoder z0 \u2208 R(N+1)\xd7Dis given byz0 =xcls; x1pE; . . .; xNp E+ Epos, (1)where Epos denotes the standard learnable position embedding.The class token z00 = xcls \u2208 R1\xd7D is particularly prependedto the sequence of embedded patches [16]. This class tokenis crucial in classification tasks, serving as a key elementin aggregating the information from the entire sequence ofpatches for the final classification output.The transformer encoder is composed of alternating layersof multi-head self-attention (MSA) and multi-layer perceptron(MLP) blocks as follows:z\u2032l = MSA (LN (zl\u22121)) +zl\u22121, l = 1, . . . , L; (2)zl = MLP (LN (z\u2032l)) +z\u2032l, l = 1, . . . , L; (3)y = LN(z0L), (4)where LN represents the layer normalization. In particular, theimage representation y is the encoder output of the class tokenz00. This image representation y then serves as the input forthe MLP head as shown in Fig. 1.In the MSA block for an input sequence z \u2208 R(N+1)\xd7D, aweighted sum of all values v is computed using query q, keyk, and value v, where q, k, v \u2208 R(N+1)\xd7Dh. The standardself-attention (SA) is formalized as follows [15], [16]:[q, k, v] =zUqkv, (5)A = softmax qk\u22a4\u221aDh, (6)SA(z) =Av, (7)where Uqkv \u2208 RD\xd73Dh and A \u2208 R(N+1)\xd7(N+1) denote theprojection matrix and the attention weight matrix, respectively.The MSA extends the standard self-attention (SA) by perform-ing H parallel SA operations (i.e., heads) and concatenatingtheir outputs [16]:MSA(z) = [SA1(z), \xb7\xb7\xb7 , SAH(z)] UMSA, (8)where UMSA is the projection matrix for the MSA output.B. Related WorkPrior work on collaborative inference has primarily focusedon convolutional neural network (CNN) architectures [2]\u2013[6], [19]. The exploration of collaborative inference withtransformer models has been limited because of the consistentdimension of the MSA blocks in the transformer encoder.Recent studies have delved into on-device inference utilizinglightweight ViT models [18], [20], [21], yet achieving theaccuracy of server models is challenging. To improve the clas-sification accuracy of the edge device, an ensemble strategyemploying multiple edge devices was proposed in [22]. Inthis approach, a central edge device disseminates raw datato nearby edge devices, which then conduct inference usingcompact ViT models and return their intermediate inferenceresults to the central edge device. The central edge deviceobtains the final inference outcome by aggregating the receivedintermediate results. However, this approach leads to increasedcommunication overhead among the edge devices.To reduce communication overhead, recent studies haveintroduced collaborative inference schemes utilizing ViT mod-els on the server, namely masked autoencoder (MAE)-basedoffloading for transformer inference (MOT) [23] and adaptiveMOT (A-MOT) [24]. In these approaches, the edge deviceselects image patches randomly for transmission to the server.The server then reconstructs the entire image using the decoderof MAE and performs classification on this reconstructed im-age. In spite of the benefit of reducing computational demandson edge devices, the classification accuracy is compromised bythe random selection of image patches.Transformers have been utilized in semantic communi-cations for the transmission of text [25] and images [26].Nonetheless, these works do not pertain to classification tasks.Our approach distinctively employs attention scores, particu-larly for the class token, setting our work apart from existingresearch in semantic communications.Recent studies have considered the importance of trainingdata samples to enhance training performance in edge learning.In [27], [28], the authors attempt to identify important trainingdata samples and allocate more communication resourcesto these samples for improved communication efficiency.The authors of [29] select important features of trainingdata samples to enhance communication efficiency duringthe training phase. In contrast, we focus on communication-efficient collaborative inference, which specifically identifiesimportant patches of test data samples and transmits onlythese essential patches. Unlike these works that aim to enhancetraining performance, our framework is designed to improvecommunication efficiency during the inference phase.III. C OLLABORATIVE INFERENCE FRAMEWORK BASED ONTRANSFORMER MODELSWe propose a collaborative inference framework that utilizespre-trained ViT models. This framework is designed to achieveserver-level classification accuracy with minimized communi-cation overhead between the edge device and the server.Due to the consistent layer dimensions of ViTs, conven-tional methods of collaborative inference [2]\u2013[6], [19], which--- Page 4 ---4Fig. 2. The proposed collaborative inference framework utilizing pre-trained ViT models: A lightweight model (e.g., DeiT-Tiny) on the edge device and acomplicated model (DeiT-Base) on the server. The edge device (client) evaluates the uncertainty of its inference results by computing the entropy. If thisentropy exceeds a predetermined threshold, the edge device selects the most important patches of the image based on the attention scores and transmits theseto the server.typically partition a single DNN model, are ineffective atreducing communication costs for ViT models. As a solution,we employ a lightweight ViT model (e.g., DeiT-Tiny) at theedge device, instead of splitting a complex ViT model (e.g.,DeiT-Base), as depicted in Fig. 2. The proposed inferenceframework establishes an efficient collaborative protocol be-tween the edge device and the server, aiming to achieve highclassification accuracy of DeiT-Base model while significantlyreducing communication overhead.In our collaborative inference framework, the edge device(client) first performs inference with its tiny model. The edgedevice then evaluates the entropy level of this initial inference.High entropy (or low confidence) necessitates transmitting theimage to the server since it indicates that the tiny model\u2019sinference would be unreliable. In such instances, only essentialpatches for classification are transmitted instead of the entireimage patches to minimize communication costs. The server,utilizing its complex ViT model, conducts inference based onthese selected patches and sends its classification results backto the edge device, as shown in Fig. 2. This process of selectingcritical patches is governed by the proposed attention-awarepatch selection rule, elaborated in Section IV.If the initial inference\u2019s entropy is low, the edge deviceconfirms its classification result without further interactionwith the server, as shown in Fig. 2. Reducing reliance onthe server to reduce communication costs is achieved throughentropy-aware image transmission rule, detailed in Section V.By integrating these rules, our framework significantly lowerscommunication costs while maintaining classification accuracycomparable to the server model.The steps of the proposed collaborative inference are out-lined in Algorithm 1. Here, Step 2 and Step 3 involvecomputing the initial inference result fc(x(i)) and its entropyg(x(i)), respectively. If the entropy is below a given threshold\u03b7, then fc(x(i)) is deemed the final classification outcome.In cases of higher entropy, as identified in Step 4, the clientAlgorithm 1 Proposed Collaborative Inference FrameworkInput: Images {x(1), x(2), . . . ,x(n)}.Output: Classification results {y(1), y(2), . . . , y(n)}.1: for i = 1 :n do2: y(i)c \u2190 fc(x(i)) \u25b7 Inference on edge device3: Client computes entropy g(x(i))4: if g(x(i)) \u2265 \u03b7 then5: ex(i) \u2190 patch-selection(x(i))6: Client transmits ex(i) to server7: y(i)s \u2190 fs(ex(i)) \u25b7 Inference on server8: Server transmits y(i)s to client9: y(i)c \u2190 y(i)s10: end if11: end forselects and transmits only essential patches to the serverat Step 5 and 6, effectively lowering communication costsby ensuring dim(ex(i)) < dim(x(i)). At Step 7, the serverconducts inference on these selected patches, producing theresult fs(ex(i)), which is then sent back to the client at Step 8.The proposed collaborative inference framework can reducethe computational complexity for the server model by limitingthe inference process to only the selected patches. The com-putational complexity of DeiT-Base, according to [30], [31],is given by144ND2 + 24N2D, (9)where N is the number of patches. Assuming the numberof patches of ex(i) is represented by eN such that eN < N,our framework not only reduces communication overhead butalso enhances computational efficiency on the server side. Thisresults in a significant secondary benefit of our collaborativeinference framework.--- Page 5 ---5Fig. 3. The visualization of attention-aware patch selection. The left is animage of the ImageNet dataset that the client model (DeiT-Tiny) inaccuratelyclassifies as \u2018Hammerhead Shark\u2019. The right shows the selected patches byattention-aware patch selection. These selected patches allow the server model(DeiT-Base) to correctly classify the image as \u2018White Shark\u2019.Fig. 4. The visualization of images and attention scores. In the left column, wehave the original images of the ImageNet dataset. The middle column displaysthe attention score maps generated by DeiT-Tiny. The right column shows thepatches selected by the attention-sum threshold selection. The images arelabeled as \u2018Brambling\u2019, \u2018Irish Setter\u2019, and \u2018Snow Leopard\u2019, respectively.IV. A TTENTION -AWARE PATCH SELECTIONThis section introduces our attention-aware patch selectionmethod, motivated by an intriguing observation: the tiny ViTmodel is capable of identifying the essential patches forclassification, even when its classification is incorrect (seeFig. 3). Consequently, the tiny model on the edge deviceacts as a semantic encoder , effectively extracting essentialinformation for the classification task.To enhance communication efficiency and classificationaccuracy, we address two key questions: 1) how to accuratelyquantify the importance of each patch and 2) how to determinethe optimal number of selected patches.A. Quantifying Patch ImportanceTo quantify the importance of each patch for classification,we utilize the attention scores generated by the SA mecha-nism. The attention score for the class token in a single-headattention is calculated as follows:a = softmax qclsk\u22a4p\u221aDh!, (10)Fig. 5. The comparison of image patches selected by DeiT-Tiny (middlecolumn) and DeiT-Base (right column). The same number of patches areselected according to the mean attention scores. The left column displays theoriginal images of the ImageNet dataset. The images are labeled as \u2018MiniatureSchnauzer\u2019, \u2018Great Gray Owl\u2019, and \u2018Magpie\u2019, respectively.where qcls \u2208 R1\xd7Dh represents the query for the classtoken of the last layer and kp \u2208 RN\xd7Dh denotes the keyscorresponding to the image patches in the last layer. The meanattention score is then obtained by averaging the attentionscores from all multi-heads.Our experimental findings indicate that the mean attentionscores, as computed by the tiny model, effectively assess thesignificance of each patch in contributing to the classificationtask. Fig. 4 presents a side-by-side comparison of ImageNetdataset images (left column) and their corresponding attentionscore maps (middle column). These maps clearly reveal thatpatches crucial for classification are distinguished by higherattention scores, setting them apart from less critical areas,such as background patches, which receive lower attentionscores.This observation supports that the tiny model on the edgedevice is adept at identifying and selecting the most in-formative patches for classification. Within ViT models, theclass token aggregates information from other tokens (imagepatches) via the attention mechanism. For the final classifica-tion, the ViT relies on the MLP head, which considers onlythe information associated with the class token from the lastlayer, disregarding any other inputs. Thus, the attention scoreas defined in (10) serves as a key metric for quantifying thecontributions of individual image patches to the class token. Inthe context of semantic communications, the tiny model of theedge device acts as a semantic encoder, tasked with extractingessential information for the classification task. This rolealigns with the broader objectives of semantic communicationsto emphasize meaning and relevance in the transmission ofinformation [9]\u2013[11]An interesting finding is that DeiT-Tiny can act moreeffectively as a semantic encoder than DeiT-Base in spite--- Page 6 ---6Fig. 6. The comparison of attention score maps computed by DeiT-Tiny(middle column) and DeiT-Base (right column). The left column displays theoriginal images of the ImageNet dataset. The images are labeled as \u2018MiniatureSchnauzer\u2019, \u2018Great Gray Owl\u2019, and \u2018Magpie\u2019, respectively.of its inferior classification accuracy. Fig. 5 compares thepatches selected by DeiT-Tiny (middle column) and thoseby DeiT-Base (right column), showing DeiT-Tiny\u2019s superiorability to discard irrelevant image patches. This seeminglycontradictory finding can be elucidated by the insights fromrecent work [32], which shows that large ViT models tendto allocate high attention scores to less informative back-ground areas. It is because the large models adeptly identifypatches with minimal information, such as background areas,repurposing the corresponding patches to assimilate globalimage information while neglecting their spatial information.High attention scores are allocated to these repurposed patchescontaining global information, particularly in the backgroundareas, as shown in Fig. 6. Although this strategy enhances theclassification accuracy of larger ViT models, it compromisestheir effectiveness as semantic encoders. The experimentalresults on how model complexity affects the efficacy ofsemantic encoders are presented in Section VI-E.The mean attention score from the last layer has previouslybeen leveraged for purposes such as model interpretation [33],[34] and knowledge distillation [31]. Our approach distinc-tively utilizes the mean attention score to reduce commu-nication costs within our collaborative inference framework,differentiating our methodology from previous applications.Additionally, while attention rollout [35] is an establishedtechnique for interpreting transformer models, we opt for themean attention score. This decision is based on the observationthat attention rollout tends to produce more uniform attentionscores, which do not align well with our objective of attention-aware patch selection. The experimental evidence supportingthis decision is provided in Section VI-C.B. Patch Selection RuleIn this subsection, we investigate the patch selection ruleutilizing mean attention scores. Selecting an appropriate num-0 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.01Attention Score of Patch00.010.020.030.040.050.06Normalized Number of PatchesFig. 7. The normalized histogram of the mean attention scores obtained byDeiT-Tiny on the ImageNet dataset.ber of patches for transmission to the server is crucial, as thisdirectly impacts both communication costs and classificationaccuracy. Our goal is to transmit the fewest possible patchesto the server to minimize communication overhead. However,this approach poses a trade-off, as reducing the number oftransmitted patches can limit the information available to theserver model, potentially lowering classification accuracy.The distribution of attention scores reveals that most patchesare assigned low values, as shown in Fig. 7. This propertyenables a significant reduction in the number of transmittedpatches without affecting classification accuracy. By takingadvantage of this beneficial property, we explore the followingpatch selection rules:\u2022 Top-k selection: Selecting the top-k patches that have thehighest attention scores.\u2022 Attention threshold selection: Selecting the patches whoseattention scores exceed a predefined threshold \u03b4.\u2022 Attention-sum threshold selection : Selecting the patcheswith the highest attention scores until their cumulativeattention sum reaches a predetermined threshold \u03b4sum.The top- k selection method selects a fixed number ofpatches based on the highest attention scores, resulting inconsistent communication costs for all input images. Nonethe-less, it overlooks the variation in attention score distributionamong different images. Fig. 4 shows that the quantity ofpatches essential for classification can vary across images,with important information typically concentrated on the classobject, where the highest attention scores are located. Conse-quently, to achieve optimal classification accuracy, the numberof selected patches should be tailored to the size of the objectwithin each image.Both the attention threshold selection and attention-sumthreshold selection methods provide the capability to adjustthe number of selected patches for transmission, making themmore adaptable than the top- k selection method. For instance,as shown in Fig. 4, the numbers of selected patches for \u2018Bram-bling\u2019 and \u2018Snow Leopard\u2019 are 35 and 124, respectively, byusing the attention-sum threshold selection with \u03b4sum = 0.94.Adopting this method ensures the transmission of a consistentsum of attention scores, effectively lowering the risk of omit-ting crucial information. The experimental results show that--- Page 7 ---7the attention threshold selection and attention-sum thresholdselection methods outperform the top- k selection method inachieving an optimal trade-off between classification accuracyand communication efficiency, as detailed in Section VI-C.V. E NTROPY -AWARE IMAGE TRANSMISSIONThis section delves into entropy-aware image transmission,a strategy aimed at reducing communication overhead by con-sidering the varied classification difficulty inherent to differentimages. For less complex images, the edge device\u2019s initialinference may be accurate enough, eliminating the need forfurther interaction with the server. In contrast, more intricateimages necessitate more accurate classification from the servermodel, leading to increased communication overhead. It iscritical for the edge device to make an accurate decisionbetween relying on its initial inference and requesting moreaccurate classification from the server model.Even though the edge device cannot ascertain the correct-ness of its initial inference, it can estimate the inference\u2019sconfidence through the softmax output values of the MLPclassification head. This softmax output can be interpreted asthe posterior probability p\u03b8(y|x), where y denotes the classlabel and \u03b8 denotes the tiny model. Then, we set an entropyfunction g : RL \u2192 R, where L denotes the number of classlabels. The client requests more accurate inference results fromthe server if:g(x) \u2265 \u03b7, (11)where \u03b7 denotes a predetermined threshold.To assess the confidence of the client\u2019s inference, weconsider two exemplary entropy measures: 1) Shannon entropyand 2) min-entropy, with their respective thresholds.The Shannon entropy, a widely used metric for quantifyinguncertainty [36], is calculated bygs(x) =\u2212Xy\u2208Yp\u03b8(y|x) log2 p\u03b8(y|x), (12)where Y denotes the set of all possible class labels. High Shan-non entropy indicates that the given image x is challenging forthe tiny model to classify accurately. Therefore, if gs(x) \u2265 \u03b7s,the edge device transmits the selected patches to the server foran inference from the base model.Another key metric, min-entropy, evaluates uncertainty inthe most conservative manner [37]. The min-entropy is definedasgm(x) =\u2212log2 maxy\u2208Yp\u03b8(y|x), (13)which is directly associated with the confidence level of theinitial inference. If gm(x) \u2265 \u03b7m, the edge device transmits theselected patches to the server for an inference from the basemodel.Our experimental results in Section VI-D show that themin-entropy serves as a better metric within our collabora-tive inference framework. Entropy-aware image transmissionutilizing the min-entropy improves communication efficiencyfor a given level of classification accuracy when compared tousing the Shannon entropy.The entropy has been utilized in diverse applications, suchas prioritizing unlabeled data inputs in active learning [38] andoptimizing wireless data acquisition for edge learning [28].These works typically enhance training procedures using theentropy values calculated by complicated server models. Incontrast, our approach utilizes the min-entropy to assess theuncertainty of initial inferences made by a tiny model onthe edge device instead of a complex server model. This useof uncertainty metrics aims to minimize unnecessary imagetransmissions, thereby reducing communication overhead.VI. E XPERIMENTAL RESULTSA. Experiment SettingsOur experiments employ the ImageNet validation datasetand resize each image to a resolution of 224 \xd7 224 pixels bycenter cropping. An image is flattened to N = 196 patchesbefore the inference.We deploy DeiT-Tiny on the edge device and DeiT-Baseon the server since resource-constrained edge devices havechallenges in employing complicated models such as DeiT-Base. The model complexity of these models is comparedin Table I. DeiT-Tiny can be viable for deployment onedge devices such as NVIDIA Jetson Nano [22], RaspberryPi 4B [39], and iPhone 12 [18]. In contrast, DeiT-Baseis generally considered inappropriate due to its substantialmemory consumption and prolonged inference latency. Forinstance, the authors of [22] explicitly state that DeiT-Baseis inadequate for deployment on NVIDIA Jetson Nano dueto its substantial memory consumption and computationalcomplexity. Similarly, the authors of [18] emphasize memoryconsumption as a critical factor for edge device deployment,considering only lightweight models such as DeiT-Tiny andPiT [40] with approximately 3\u20136 million parameters.We evaluate the impact of varying the number of transmittedpatches on communication cost and classification accuracy.The communication cost in our collaborative inference systemis quantified by the ratio of the number of transmitted patchesto the total number of patches. Consequently, if the edge de-vice sends all image patches to the server, the communicationcost is quantified as 1.When the edge device transmits the selected patches, it isrequired to transmit the position information of these selectedpatches. For instance, assigning one bit per patch as a markerof its selection status is a practical solution. The overhead ofthese additional bits for position information is negligible incomparison to the size of the image itself, given that only onebit is appended for each image patch containing 6, 144 bits.B. Communication Cost vs. Classification AccuracyIn our collaborative inference framework, we assess thetrade-off between communication cost and classification ac-curacy. We utilize the attention-sum threshold selection forattention-aware patch selection and the min-entropy forentropy-aware image transmission.Fig. 8 shows the trade-off between communication cost andclassification accuracy, employing DeiT-Tiny on the edge de-vice and DeiT-Base on the server. We achieve a68 %reduction--- Page 8 ---80.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost777879808182Accuracy (%)Server AccuracyAchieved Accuracy = 0.1 = 0.2 = 0.3 = 0.4 = 0.6 = 0.8 = 1.0Fig. 8. Trade-off between communication cost and classification accuracy,with the edge device employing DeiT-Tiny and the server employing DeiT-Base. The black line represents the achievable curve by the proposed collab-orative inference. We utilize the attention-sum threshold selection method forattention-aware patch selection. For entropy-aware image transmission, themin-entropy serves as the entropy metric, using a threshold value \u03b7 = \u03b7m.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost727374757677787980Accuracy (%)Server AccuracyAchieved Accuracy = 0.1 = 0.2 = 0.3 = 0.4 = 0.6 = 0.8 = 1.0Fig. 9. Trade-off between communication cost and classification accuracy,with the edge device employing DeiT-Tiny and the server employing DeiT-Small. The black line represents the achievable curve by the proposed col-laborative inference. We utilize the attention-sum threshold selection methodfor attention-aware patch selection. For entropy-aware image transmission, themin-entropy serves as the entropy metric, using a threshold value \u03b7 = \u03b7m.in communication cost while attaining a classification accuracyof 80.84 %, with only a minimal loss in accuracy comparedto DeiT-Base\u2019s accuracy of 81.8 %. This communication costreduction is achieved by the attention-sum threshold selectionmethod with a threshold \u03b4sum = 0.97 and the min-entropy withthe threshold \u03b7m = 0.8. The black line indicates the optimizedtrade-off curve achieved by selecting the optimized thresholdvalues of \u03b4sum and \u03b7m. Table II and Table III detail the resultingcommunication costs depending on threshold values.By controlling the threshold values of \u03b7 and \u03b4, our proposedcollaborative inference framework can effectively managecommunication resources. In scenarios with reliable commu-nication channels, lower values of \u03b7 and \u03b4 can be selectedto increase data transmissions and maximize classificationaccuracy. When the channel quality is worse and communi-cation resources are limited, higher values of \u03b7 and \u03b4 canbe set to balance classification accuracy with the availablecommunication resource budget. This strategy offers flexibilityin adapting to varying communication channel conditions,TABLE IIATTENTION -SUM PATCH SELECTION THRESHOLD VALUES ANDEXPECTED NUMBER OF TRANSMITTED PATCHES\u03b4sum Expected Number of Transmitted Patches0.87 88.400.9 100.840.92 110.820.93 116.510.94 122.800.95 129.860.96 137.890.97 147.240.98 158.470.99 172.76TABLE IIIMIN-ENTROPY THRESHOLD VALUES AND EXPECTED RATIO OFTRANSMITTED IMAGES\u03b7m Expected Ratio of Transmitted Images1 0.35670.8 0.42900.6 0.51160.4 0.62460.3 0.71090.2 0.84450.1 0.9714thereby enhancing the efficiency of communication resourcemanagement.Additionally, we investigate a case where the server employsDeiT-Small instead of DeiT-Base. Fig. 9 shows the trade-offbetween communication cost and classification accuracy. Inthis case, we achieve a 71 %reduction in communication costwhile attaining a classification accuracy of 78.8 %, with only aminimal loss in accuracy compared to DeiT-Small\u2019s accuracyof 79.8 %. It is observed that greater communication costreductions can be realized when the accuracy gap betweenthe edge device and the server is reduced.Fig. 10 compares our collaborative inference frameworkwith several existing methods, including server-driven trans-mission (SDT) [41], image super-resolution (ISR) [42],masked autoencoder (MAE)-based offloading for transformerinference (MOT) [23], and adaptive MOT (A-MOT) [24].SDT transmits low-quality images initially and then of-floads high-quality content from the target area based on theserver\u2019s feedback [41]. ISR transmits a low-quality image andthen reconstructs a high-resolution image by super-resolutionmethod [42]. The server model performs inference on thisreconstructed high-resolution image. In MOT and A-MOT, theedge device randomly selects image patches for transmissionto the server. The server then reconstructs the entire imageusing the decoder of MAE [43] and performs classification onthis reconstructed image. The experimental results show thatour proposed collaborative inference framework significantlyimproves performance by employing the tiny model on theedge device, which adeptly transmits essential image patchesfor classification.--- Page 9 ---90.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost68707274767880Accuracy (%)Server AccuracyProposedSDTISRMOTA-MOTFig. 10. Comparison of previous methods and our collaborative inferenceframework, utilizing DeiT-Tiny on the edge device and DeiT-Small on theserver. The experiment is conducted on the ImageNet dataset.C. Attention-aware Patch SelectionIn this subsection, we delve into identifying the effectiveattention metrics for determining patch importance and the op-timal rules for patch selection. First, we examine and comparethe efficacy of mean attention scores versus attention rollout.Next, we evaluate different patch selection methodologies,including top- k selection, attention threshold selection, andattention-sum threshold selection. We maintain a consistentenvironment with DeiT-Tiny on the edge device and DeiT-Base on the server. For this analysis, we specifically focuson attention-aware patch selection, excluding considerationsof entropy-aware image transmission.Fig. 11(a) reveals that both the mean attention score andattention rollout have distinct advantages. In situations de-manding substantial reductions in communication cost, atten-tion rollout is better than the mean attention score. However,as shown in Fig. 11(b), the mean attention score is a bettermetric for attaining accuracy comparable to that of the server.The theoretical advantage of the mean attention score overattention rollout is shown in Fig. 11(c), which compares thenormalized histograms of attention scores for both metrics.Unlike attention rollout, which considers attention scoresfrom multiple layers resulting in a more uniform distributionof scores, the mean attention scores concentrate on fewerpatches with significantly higher relevance to the objects to beclassified. This concentration of attention scores is preferredbecause it implies that fewer but more relevant patches can betransmitted without compromising the classification accuracy,thus aligning with our objective to minimize communicationoverhead while maintaining classification accuracy.Hence, we adopt the mean attention score for our primaryexperiments.Fig. 12 shows that both the attention threshold selectionand attention-sum threshold selection outperform the top- kselection. These methods offer the capability to adjust thenumber of selected patches for transmission, facilitating themaintaining classification accuracy while minimizing commu-nication costs.The variability in informational content across patches,influenced by factors such as object size, type, and thepresence of background, directly impacts their importance0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Communication Cost5055606570758085Accuracy (%)Server AccuracyMean Attention ScoreAttention Rollout(a)0.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost7878.57979.58080.58181.582Accuracy (%)Server AccuracyMean Attention ScoreAttention Rollout(b)0 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.01Attention Score of Patch00.0050.010.0150.020.0250.030.0350.040.0450.05Normalized Number of PatchesMean Attention ScoreAttention Rollout(c)Fig. 11. Comparison of mean attention score and attention rollout, withthe edge device employing DeiT-Tiny and the server employing DeiT-Baseon the ImageNet dataset. The patch selection rule is the attention-sumthreshold selection. (a) shows the overall trade-off between communicationcost and classification accuracy, while (b) zooms in on the region of interest,focusing on areas near the server model\u2019s classification accuracy. (c) shows thenormalized histograms of mean attention score and attention rollout, obtainedby DeiT-Tiny.for accurate classification. Theoretically, the attention scoresreflect each patch\u2019s contribution to the model prediction.By setting thresholds based on cumulative attention scores(attention-sum threshold) or individual patch attention scores(attention threshold), we can dynamically control the numberof transmitted patches based on their estimated relevance.--- Page 10 ---100.5 0.55 0.6 0.65 0.7 0.75 0.8Communication Cost7878.57979.58080.58181.582Accuracy (%)Server AccuracyTop-kAttention ThresholdAttention-Sum ThresholdFig. 12. Comparison of the patch selection rules: Top- k selection, attentionthreshold selection, and attention-sum threshold selection. The edge deviceand the server employ DeiT-Tiny and DeiT-Base, respectively. The patchimportance is quantified by the mean attention score.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Communication Cost7273747576777879808182Accuracy (%)Server AccuracyEdge Device AccuracyMin-EntropyShannon EntropyFig. 13. Comparison of min-entropy and Shannon entropy with the edgedevice employing DeiT-Tiny and the server employing DeiT-Base.D. Entropy-aware Image TransmissionTo determine the most effective entropy measure, we com-pare the min-entropy and the Shannon entropy within our col-laborative inference framework. For this analysis, we specifi-cally focus on entropy-aware image transmission, deliberatelysetting aside the aspect of attention-aware patch selection. Theentropy values are derived from the softmax output of theMLP classification head in the DeiT-Tiny model used on theedge device. As shown in Fig. 13, the min-entropy is a bettermetric for our collaborative inference, demonstrating a moreeffective distinction between correctly and incorrectly inferredimages compared to the Shannon entropy. Consequently, min-entropy is chosen as the preferred metric for entropy-awareimage transmission.E. Comparison of Models as Semantic EncoderAs discussed in Section IV-A, DeiT-Tiny can act moreeffectively as a semantic encoder than DeiT-Base in spiteof its inferior classification accuracy. In our experiments,we concentrate solely on attention-aware patch selection toevaluate and compare DeiT-Tiny and DeiT-Base. Both modelsselect only important patches based on the mean attention0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Communication Cost01020304050607080Accuracy (%)Server AccuracyDeiT-TinyDeiT-BaseFig. 14. Comparison of DeiT-Tiny and DeiT-Base as semantic encoders toselect important patches. The classification accuracy is obtained by DeiT-Base.scores, disregarding initial inference outcomes. The classifi-cation accuracy is obtained by DeiT-Base, which processesonly these selected image patches. Fig. 14 shows that DeiT-Tiny more effectively identifies the essential image patches forclassification better than DeiT-Base.In particular, Fig. 5 and Fig. 6 in Section IV-A show that theattention scores obtained by DeiT-Tiny concentrate more onrelevant objects than those from DeiT-Base. Recent work [32]supports this observation, showing that large ViT models oftenallocate high attention scores to less informative backgroundareas. It is because the large ViT models adeptly identifypatches containing minimal information, such as backgroundareas, repurposing the corresponding patches to assimilateglobal image information while neglecting spatial information.While these high attention score patches may enhance theclassification accuracy, they diminish the effectiveness of basemodels as semantic encoders.F . Inference Latency AnalysisIn this subsection, we evaluate the end-to-end inference la-tency of both our collaborative inference and the conventionalserver-based inference.The end-to-end inference latency consists of client infer-ence latency, server inference latency, and communicationlatency. Client inference latency, set as 10.99 milliseconds(ms), reflects the inference latency of DeiT-Tiny on the iPhone12 neural engine [18]. Server inference latency is set at8.32 ms, corresponding to the inference latency of DeiT-Baseon an NVIDIA RTX 3090 GPU [22]. This latency can beeliminated if the image is not transmitted to the server via thestrategy of entropy-aware image transmission. Additionally,server inference latency can be reduced in proportion to thedecreased computational complexity (FLOPs) if only selectedpatches are transmitted via the strategy of attention-awarepatch selection. The FLOPs of DeiT-Base are calculated by(9). Communication latency is calculated using the transmitteddata size divided by the upload data rate. We estimate the datasize of a typical image to be 147 KB, based on an imagecropped to 224 \xd7 224 \xd7 3 bytes. The upload data rates of 1Mbps, 8 Mbps, and 20 Mbps are considered as in [44].--- Page 11 ---1118 2 0Upload Data Rate (Mbps)020040060080010001200Latency (ms)1184.3155.367.1394.561.3 32.7Server-based InferenceProposed Collaborative InferenceFig. 15. Comparison of inference latency for different upload data rates, 1Mbps, 8 Mbps, and 20 Mbps.18 2 0Upload Data Rate (Mbps)050100150200250300350400Latency (ms)Client Inference LatencyCommunication LatencyServer Inference LatencyFig. 16. Breakdown of end-to-end inference latency of the proposed collab-orative inference.Fig. 15 compares the inference latency of the server-basedinference and our proposed inference for each upload data rate.Across these rates, our proposed inference framework consis-tently exhibits better inference latency. We set the thresholds\u03b4sum and \u03b7m at values that result in a minimal accuracyloss of 1 %, as described in Section VI-B. Fig. 16 showsthe proportions of client inference latency, server inferencelatency, and communication latency. Notably, communicationlatency occupies a significant portion at lower upload datarates.VII. C ONCLUSIONWe presented a novel attention-aware, Your response: ```ADD_PAPER\n2404.07217v2\nThis paper presents an attention-aware semantic communication framework for collaborative inference between edge devices and cloud servers using Vision Transformers (ViTs). Key innovations include: (1) Attention-aware patch selection that leverages edge device attention scores to identify and transmit only critical image patches (reducing communication overhead by 68% on ImageNet); (2) Entropy-aware transmission that uses min-entropy to determine when to offload computation to the server; (3) A novel finding that lightweight edge models (DeiT-Tiny) can better identify semantically important patches than larger server models despite lower classification accuracy. The framework achieves server-level accuracy (80.84% vs DeiT-Base's 81.8%) while significantly reducing both communication costs and server computation through selective patch processing. Experimental results demonstrate superior performance compared to prior methods like MAE-based offloading, with particular advantages in low-bandwidth scenarios.\n```\nStep #29, Phase: literature review, Feedback: Successfully added paper 2404.07217v2, Your response: ```SUMMARY\nedge device image classification early exit networks\n```\nStep #30, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification early exit networks, here was the response\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Resource-Constrained Edge AI with Early Exit Prediction\nSummary: By leveraging the data sample diversity, the early-exit network recently\nemerges as a prominent neural network architecture to accelerate the deep\nlearning inference process. However, intermediate classifiers of the early\nexits introduce additional computation overhead, which is unfavorable for\nresource-constrained edge artificial intelligence (AI). In this paper, we\npropose an early exit prediction mechanism to reduce the on-device computation\noverhead in a device-edge co-inference system supported by early-exit networks.\nSpecifically, we design a low-complexity module, namely the Exit Predictor, to\nguide some distinctly "hard" samples to bypass the computation of the early\nexits. Besides, considering the varying communication bandwidth, we extend the\nearly exit prediction mechanism for latency-aware edge inference, which adapts\nthe prediction thresholds of the Exit Predictor and the confidence thresholds\nof the early-exit network via a few simple regression models. Extensive\nexperiment results demonstrate the effectiveness of the Exit Predictor in\nachieving a better tradeoff between accuracy and on-device computation overhead\nfor early-exit networks. Besides, compared with the baseline methods, the\nproposed method for latency-aware edge inference attains higher inference\naccuracy under different bandwidth conditions.\nPublication Date: 2022-06-15\narXiv paper ID: 2206.07269v2\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n, Your response: ```FULL_TEXT\\n2311.18098v1\n```\n~~~~~~~~~~\nCurrent Step #31, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Error retrieving paper: Page request resulted in HTTP 400 (https://export.arxiv.org/api/query?search_query=&id_list=%5Cn2311.18098v1&sortBy=relevance&sortOrder=descending&start=0&max_results=100)\nNotes: \nYour previous command was: ```FULL_TEXT\\n2311.18098v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 33:
\n~~~~~~~~~~\nHistory: Step #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: HiRISE: High-Resolution Image Scaling for Edge ML via In-Sensor Compression and Selective ROI\nSummary: With the rise of tiny IoT devices powered by machine learning (ML), many\nresearchers have directed their focus toward compressing models to fit on tiny\nedge devices. Recent works have achieved remarkable success in compressing ML\nmodels for object detection and image classification on microcontrollers with\nsmall memory, e.g., 512kB SRAM. However, there remain many challenges\nprohibiting the deployment of ML systems that require high-resolution images.\nDue to fundamental limits in memory capacity for tiny IoT devices, it may be\nphysically impossible to store large images without external hardware. To this\nend, we propose a high-resolution image scaling system for edge ML, called\nHiRISE, which is equipped with selective region-of-interest (ROI) capability\nleveraging analog in-sensor image scaling. Our methodology not only\nsignificantly reduces the peak memory requirements, but also achieves up to\n17.7x reduction in data transfer and energy consumption.\nPublication Date: 2024-07-23\narXiv paper ID: 2408.03956v1\n\nTitle: A Multi-task Supervised Compression Model for Split Computing\nSummary: Split computing ($\\neq$ split learning) is a promising approach to deep\nlearning models for resource-constrained edge computing systems, where weak\nsensor (mobile) devices are wirelessly connected to stronger edge servers\nthrough channels with limited communication capacity. State-of-theart work on\nsplit computing presents methods for single tasks such as image classification,\nobject detection, or semantic segmentation. The application of existing methods\nto multitask problems degrades model accuracy and/or significantly increase\nruntime latency. In this study, we propose Ladon, the first multi-task-head\nsupervised compression model for multi-task split computing. Experimental\nresults show that the multi-task supervised compression model either\noutperformed or rivaled strong lightweight baseline models in terms of\npredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets\nwhile learning compressed representations at its early layers. Furthermore, our\nmodels reduced end-to-end latency (by up to 95.4%) and energy consumption of\nmobile devices (by up to 88.2%) in multi-task split computing scenarios.\nPublication Date: 2025-01-02\narXiv paper ID: 2501.01420v2\n\nTitle: Efficient Privacy Preserving Edge Computing Framework for Image Classification\nSummary: In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\nPublication Date: 2020-05-10\narXiv paper ID: 2005.04563v2\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\nStep #26, Phase: literature review, Feedback: Successfully added paper 1905.04159v1, Your response: ```SUMMARY\nedge device image classification dynamic inference\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification dynamic inference, here was the response\nTitle: AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and Dynamic Model Placements on Edge Devices\nSummary: Many real-world applications are widely adopting the edge computing paradigm\ndue to its low latency and better privacy protection. With notable success in\nAI and deep learning (DL), edge devices and AI accelerators play a crucial role\nin deploying DL inference services at the edge of the Internet. While prior\nworks quantified various edge devices' efficiency, most studies focused on the\nperformance of edge devices with single DL tasks. Therefore, there is an urgent\nneed to investigate AI multi-tenancy on edge devices, required by many advanced\nDL applications for edge computing.\n  This work investigates two techniques - concurrent model executions and\ndynamic model placements - for AI multi-tenancy on edge devices. With image\nclassification as an example scenario, we empirically evaluate AI multi-tenancy\non various edge devices, AI accelerators, and DL frameworks to identify its\nbenefits and limitations. Our results show that multi-tenancy significantly\nimproves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI\nmulti-tenancy techniques also open up new opportunities for flexible deployment\nof multiple DL services on edge devices and AI accelerators.\nPublication Date: 2021-07-26\narXiv paper ID: 2107.12486v1\n\nTitle: Edge-PRUNE: Flexible Distributed Deep Learning Inference\nSummary: Collaborative deep learning inference between low-resource endpoint devices\nand edge servers has received significant research interest in the last few\nyears. Such computation partitioning can help reducing endpoint device energy\nconsumption and improve latency, but equally importantly also contributes to\nprivacy-preserving of sensitive data. This paper describes Edge-PRUNE, a\nflexible but light-weight computation framework for distributing machine\nlearning inference between edge servers and one or more client devices.\nCompared to previous approaches, Edge-PRUNE is based on a formal dataflow\ncomputing model, and is agnostic towards machine learning training frameworks,\noffering at the same time wide support for leveraging deep learning\naccelerators such as embedded GPUs. The experimental section of the paper\ndemonstrates the use and performance of Edge-PRUNE by image classification and\nobject tracking applications on two heterogeneous endpoint devices and an edge\nserver, over wireless and physical connections. Endpoint device inference time\nfor SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by\ncollaborative inference.\nPublication Date: 2022-04-27\narXiv paper ID: 2204.12947v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: AutoDiCE: Fully Automated Distributed CNN Inference at the Edge\nSummary: Deep Learning approaches based on Convolutional Neural Networks (CNNs) are\nextensively utilized and very successful in a wide range of application areas,\nincluding image classification and speech recognition. For the execution of\ntrained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud\nto the Edge. Unfortunately, deploying and inferring large, compute and memory\nintensive CNNs on edge devices is challenging because these devices typically\nhave limited power budgets and compute/memory resources. One approach to\naddress this challenge is to leverage all available resources across multiple\nedge devices to deploy and execute a large CNN by properly partitioning the CNN\nand running each CNN partition on a separate edge device. Although such\ndistribution, deployment, and execution of large CNNs on multiple edge devices\nis a desirable and beneficial approach, there currently does not exist a design\nand programming framework that takes a trained CNN model, together with a CNN\npartitioning specification, and fully automates the CNN model splitting and\ndeployment on multiple edge devices to facilitate distributed CNN inference at\nthe Edge. Therefore, in this paper, we propose a novel framework, called\nAutoDiCE, for automated splitting of a CNN model into a set of sub-models and\nautomated code generation for distributed and collaborative execution of these\nsub-models on multiple, possibly heterogeneous, edge devices, while supporting\nthe exploitation of parallelism among and within the edge devices. Our\nexperimental results show that AutoDiCE can deliver distributed CNN inference\nwith reduced energy consumption and memory usage per edge device, and improved\noverall system throughput at the same time.\nPublication Date: 2022-07-20\narXiv paper ID: 2207.12113v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n, Your response: ```FULL_TEXT\n2404.07217v2\n```\nStep #29, Phase: literature review, Feedback: Successfully added paper 2404.07217v2, Your response: ```SUMMARY\nedge device image classification early exit networks\n```\nStep #30, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification early exit networks, here was the response\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Resource-Constrained Edge AI with Early Exit Prediction\nSummary: By leveraging the data sample diversity, the early-exit network recently\nemerges as a prominent neural network architecture to accelerate the deep\nlearning inference process. However, intermediate classifiers of the early\nexits introduce additional computation overhead, which is unfavorable for\nresource-constrained edge artificial intelligence (AI). In this paper, we\npropose an early exit prediction mechanism to reduce the on-device computation\noverhead in a device-edge co-inference system supported by early-exit networks.\nSpecifically, we design a low-complexity module, namely the Exit Predictor, to\nguide some distinctly "hard" samples to bypass the computation of the early\nexits. Besides, considering the varying communication bandwidth, we extend the\nearly exit prediction mechanism for latency-aware edge inference, which adapts\nthe prediction thresholds of the Exit Predictor and the confidence thresholds\nof the early-exit network via a few simple regression models. Extensive\nexperiment results demonstrate the effectiveness of the Exit Predictor in\nachieving a better tradeoff between accuracy and on-device computation overhead\nfor early-exit networks. Besides, compared with the baseline methods, the\nproposed method for latency-aware edge inference attains higher inference\naccuracy under different bandwidth conditions.\nPublication Date: 2022-06-15\narXiv paper ID: 2206.07269v2\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n, Your response: ```FULL_TEXT\\n2311.18098v1\n```\nStep #31, Phase: literature review, Feedback: Error retrieving paper: Page request resulted in HTTP 400 (https://export.arxiv.org/api/query?search_query=&id_list=%5Cn2311.18098v1&sortBy=relevance&sortOrder=descending&start=0&max_results=100), Your response: ```SUMMARY\nedge image classification early exit dynamic inference\n```\n~~~~~~~~~~\nCurrent Step #32, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification early exit dynamic inference, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification early exit dynamic inference\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 34:
\n~~~~~~~~~~\nHistory: Step #15, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification model compression quantization, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence\nSummary: In collaborative intelligence applications, part of a deep neural network\n(DNN) is deployed on a lightweight device such as a mobile phone or edge\ndevice, and the remaining portion of the DNN is processed where more computing\nresources are available, such as in the cloud. This paper presents a novel\nlightweight compression technique designed specifically to quantize and\ncompress the features output by the intermediate layer of a split DNN, without\nrequiring any retraining of the network weights. Mathematical models for\nestimating the clipping and quantization error of ReLU and leaky-ReLU\nactivations at this intermediate layer are developed and used to compute\noptimal clipping ranges for coarse quantization. We also present a modified\nentropy-constrained design algorithm for quantizing clipped activations. When\napplied to popular object-detection and classification DNNs, we were able to\ncompress the 32-bit floating point intermediate activations down to 0.6 to 0.8\nbits, while keeping the loss in accuracy to less than 1%. When compared to\nHEVC, we found that the lightweight codec consistently provided better\ninference accuracy, by up to 1.3%. The performance and simplicity of this\nlightweight compression technique makes it an attractive option for coding an\nintermediate layer of a split neural network for edge/cloud applications.\nPublication Date: 2021-05-15\narXiv paper ID: 2105.07102v1\n\nTitle: Physics Inspired Criterion for Pruning-Quantization Joint Learning\nSummary: Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.\nPublication Date: 2023-12-01\narXiv paper ID: 2312.00851v2\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge image classification quantization efficiency tradeoffs\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\nStep #26, Phase: literature review, Feedback: Successfully added paper 1905.04159v1, Your response: ```SUMMARY\nedge device image classification dynamic inference\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification dynamic inference, here was the response\nTitle: AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and Dynamic Model Placements on Edge Devices\nSummary: Many real-world applications are widely adopting the edge computing paradigm\ndue to its low latency and better privacy protection. With notable success in\nAI and deep learning (DL), edge devices and AI accelerators play a crucial role\nin deploying DL inference services at the edge of the Internet. While prior\nworks quantified various edge devices' efficiency, most studies focused on the\nperformance of edge devices with single DL tasks. Therefore, there is an urgent\nneed to investigate AI multi-tenancy on edge devices, required by many advanced\nDL applications for edge computing.\n  This work investigates two techniques - concurrent model executions and\ndynamic model placements - for AI multi-tenancy on edge devices. With image\nclassification as an example scenario, we empirically evaluate AI multi-tenancy\non various edge devices, AI accelerators, and DL frameworks to identify its\nbenefits and limitations. Our results show that multi-tenancy significantly\nimproves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI\nmulti-tenancy techniques also open up new opportunities for flexible deployment\nof multiple DL services on edge devices and AI accelerators.\nPublication Date: 2021-07-26\narXiv paper ID: 2107.12486v1\n\nTitle: Edge-PRUNE: Flexible Distributed Deep Learning Inference\nSummary: Collaborative deep learning inference between low-resource endpoint devices\nand edge servers has received significant research interest in the last few\nyears. Such computation partitioning can help reducing endpoint device energy\nconsumption and improve latency, but equally importantly also contributes to\nprivacy-preserving of sensitive data. This paper describes Edge-PRUNE, a\nflexible but light-weight computation framework for distributing machine\nlearning inference between edge servers and one or more client devices.\nCompared to previous approaches, Edge-PRUNE is based on a formal dataflow\ncomputing model, and is agnostic towards machine learning training frameworks,\noffering at the same time wide support for leveraging deep learning\naccelerators such as embedded GPUs. The experimental section of the paper\ndemonstrates the use and performance of Edge-PRUNE by image classification and\nobject tracking applications on two heterogeneous endpoint devices and an edge\nserver, over wireless and physical connections. Endpoint device inference time\nfor SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by\ncollaborative inference.\nPublication Date: 2022-04-27\narXiv paper ID: 2204.12947v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: AutoDiCE: Fully Automated Distributed CNN Inference at the Edge\nSummary: Deep Learning approaches based on Convolutional Neural Networks (CNNs) are\nextensively utilized and very successful in a wide range of application areas,\nincluding image classification and speech recognition. For the execution of\ntrained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud\nto the Edge. Unfortunately, deploying and inferring large, compute and memory\nintensive CNNs on edge devices is challenging because these devices typically\nhave limited power budgets and compute/memory resources. One approach to\naddress this challenge is to leverage all available resources across multiple\nedge devices to deploy and execute a large CNN by properly partitioning the CNN\nand running each CNN partition on a separate edge device. Although such\ndistribution, deployment, and execution of large CNNs on multiple edge devices\nis a desirable and beneficial approach, there currently does not exist a design\nand programming framework that takes a trained CNN model, together with a CNN\npartitioning specification, and fully automates the CNN model splitting and\ndeployment on multiple edge devices to facilitate distributed CNN inference at\nthe Edge. Therefore, in this paper, we propose a novel framework, called\nAutoDiCE, for automated splitting of a CNN model into a set of sub-models and\nautomated code generation for distributed and collaborative execution of these\nsub-models on multiple, possibly heterogeneous, edge devices, while supporting\nthe exploitation of parallelism among and within the edge devices. Our\nexperimental results show that AutoDiCE can deliver distributed CNN inference\nwith reduced energy consumption and memory usage per edge device, and improved\noverall system throughput at the same time.\nPublication Date: 2022-07-20\narXiv paper ID: 2207.12113v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n, Your response: ```FULL_TEXT\n2404.07217v2\n```\nStep #29, Phase: literature review, Feedback: Successfully added paper 2404.07217v2, Your response: ```SUMMARY\nedge device image classification early exit networks\n```\nStep #30, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification early exit networks, here was the response\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Resource-Constrained Edge AI with Early Exit Prediction\nSummary: By leveraging the data sample diversity, the early-exit network recently\nemerges as a prominent neural network architecture to accelerate the deep\nlearning inference process. However, intermediate classifiers of the early\nexits introduce additional computation overhead, which is unfavorable for\nresource-constrained edge artificial intelligence (AI). In this paper, we\npropose an early exit prediction mechanism to reduce the on-device computation\noverhead in a device-edge co-inference system supported by early-exit networks.\nSpecifically, we design a low-complexity module, namely the Exit Predictor, to\nguide some distinctly "hard" samples to bypass the computation of the early\nexits. Besides, considering the varying communication bandwidth, we extend the\nearly exit prediction mechanism for latency-aware edge inference, which adapts\nthe prediction thresholds of the Exit Predictor and the confidence thresholds\nof the early-exit network via a few simple regression models. Extensive\nexperiment results demonstrate the effectiveness of the Exit Predictor in\nachieving a better tradeoff between accuracy and on-device computation overhead\nfor early-exit networks. Besides, compared with the baseline methods, the\nproposed method for latency-aware edge inference attains higher inference\naccuracy under different bandwidth conditions.\nPublication Date: 2022-06-15\narXiv paper ID: 2206.07269v2\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n, Your response: ```FULL_TEXT\\n2311.18098v1\n```\nStep #31, Phase: literature review, Feedback: Error retrieving paper: Page request resulted in HTTP 400 (https://export.arxiv.org/api/query?search_query=&id_list=%5Cn2311.18098v1&sortBy=relevance&sortOrder=descending&start=0&max_results=100), Your response: ```SUMMARY\nedge image classification early exit dynamic inference\n```\nStep #32, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit dynamic inference, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\n~~~~~~~~~~\nCurrent Step #33, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Single-Layer Vision Transformers for More Accurate Early Exits with Less OverheadArian Bakhtiarniaa,\u2217, Qi Zhanga, Alexandros Iosi\ufb01disaaDIGIT, Department of Electrical and Computer Engineering, Aarhus University, Finlandsgade 22, Aarhus, 8200, Midtjylland, DenmarkAbstractDeploying deep learning models in time-critical applications with limited computational resources, for instance in edgecomputing systems and IoT networks, is a challenging task that often relies on dynamic inference methods such as earlyexiting. In this paper, we introduce a novel architecture for early exiting based on the vision transformer architecture,as well as a \ufb01ne-tuning strategy that signi\ufb01cantly increase the accuracy of early exit branches compared to conventionalapproaches while introducing less overhead. Through extensive experiments on image and audio classi\ufb01cation as well asaudiovisual crowd counting, we show that our method works for both classi\ufb01cation and regression problems, and in bothsingle- and multi-modal settings. Additionally, we introduce a novel method for integrating audio and visual modalitieswithin early exits in audiovisual data analysis, that can lead to a more \ufb01ne-grained dynamic inference.Keywords: dynamic inference, early exiting, multi-exit architecture, vision transformer, multi-modal, deep learning1. IntroductionOver the past decade, deep learning has shown tremen-dous success across various \ufb01elds, such as computer vi-sion and natural language processing [1]. However, deeplearning models are by de\ufb01nition composed of many layersof interconnected neurons, even reaching billions of pa-rameters, which makes them computationally expensive.This has sparked a great deal of research in order to makedeep learning models more lightweight, for which manyapproaches have been proposed, for instance, model com-pression methods [2] such as quantization [3], pruning [4],low-rank approximation [5] and knowledge distillation [6].More and more emerging internet of things (IoT) appli-cations are integrating deep learning models, such as videosurveillance, voice assistants, augmented reality and coop-erative autonomous driving, which are often time-sensitiveand require inputs to be processed within speci\ufb01c deadlines[7, 8]. The heavy computational burden of deep learningbecomes problematic for these time-critical IoT applica-tions, due to resource-constrained IoT devices. Edge com-puting is a promising computing paradigm for addressingthis issue, in which the deep learning task is o\ufb04oaded toedge servers in the proximity of IoT devices.Since edge computing systems introduce computationo\ufb04oading over a communication network and involve mul-tiple nodes working collaboratively in order to completethe task in a timely manner, transmission time has tobe taken into account in addition to the deep learningcomputation time. However, transmission time may vary\u2217Corresponding authorEmail addresses: arianbakh@ece.au.dk (Arian Bakhtiarnia),qz@ece.au.dk (Qi Zhang), ai@ece.au.dk (Alexandros Iosi\ufb01dis)greatly over time and across di\ufb00erent channels. Conse-quently, deep learning models running on edge computingsystems and IoT networks should be capable of anytimeprediction, meaning they should be able to provide a validresponse even if they are interrupted before traversing theentire neural network, although the model is expected toprovide a better answer if it is allowed to run for longertime.Dynamic inference approaches [9] modify the computa-tion graph based on each input during the inference phasein order to \ufb01t the time constraints. A dynamic inferenceapproach that particularly suits anytime prediction isearlyexiting [10], also referred to as multi-exit architectures orauxiliary classi\ufb01ers in the literature. In multi-exit archi-tectures, one or more early exit branches are placed aftersome of the intermediate hidden layers of thebackbone net-work. The goal of each of these branches is to provide anearly result similar to the \ufb01nal result of the neural net-work using only the features extracted up to that particu-lar branch location. These early results are inevitably lessaccurate than the \ufb01nal result of the network. In order toachieve anytime prediction using early exiting, the latestearly result can be used whenever the execution is inter-rupted, for instance, whenever a hard deadline is reached.Computation time can be further decreased by applyingmodel compression techniques on the backbone of multi-exit architectures. Besides anytime prediction, early exit-ing can also be used in budgeted batch classi\ufb01cation wherea \ufb01xed amount of time is available in order to classify aset of input samples. In such a setting, the result of ear-lier branches can be used for \u201ceasier\u201d samples whereas theresult of later branches or the \ufb01nal result can be used for\u201charder\u201d ones. The di\ufb03culty of each sample can be de-termined based on the con\ufb01dence of the network about itsPreprint submitted to Neural Networks June 30, 2022arXiv:2105.09121v3  [cs.LG]  29 Jun 2022--- Page 2 ---output [11], although other approaches exist in the litera-ture [10].Early exit branches are expected to have a low over-head in terms of the extra computation they introduce,since a high overhead would defeat the purpose. There-fore, they often contain only a handful of layers. Ideally,we want the accuracy of the early results to be close to thatof the \ufb01nal result, since a higher accuracy for early exitbranches means that the overall reliability of the systemincreases. However, the low-overhead constraint makes itquite challenging to achieve a high accuracy since the earlyexit branches have signi\ufb01cantly less trainable parameterscompared to the rest of the network. Several approachesfor increasing the accuracy of early exits such as knowledgedistillation [12], curriculum learning [13] and architecturesdesigned speci\ufb01cally for early exit branches [14] have beensuggested. In this paper, we propose a novel architecturein order to obtain more accurate early exits for convolu-tional neural network (CNN) backbones.A neural architecture called vision transformer (ViT )[15] has been recently introduced for image classi\ufb01cationwhich is radically di\ufb00erent from convolutional neural net-works. The building blocks of Vision Transformer havebeen used for early exits placed on Vision Transformerbackbones [14], however, using Transformer-based earlyexit branches on CNN backbones is not intuitive and re-quires additional steps and architectural modi\ufb01cations. Weuse a modi\ufb01ed version of this architecture instead of theusual convolution and pooling layers in early exit branchesand show that our method can signi\ufb01cantly increase theaccuracy of early exits compared to conventional archi-tectures by fusing local and global receptive \ufb01elds 1. Thecontributions of this paper can be summarized as follows:\u2022 We propose a novel architecture for early exit branchesin multi-exit architectures based on vision transform-ers, called single-layer vision transformer (SL-ViT ).We compare our method with conventional CNN-based early exit architectures across 27 scenarios in-volving di\ufb00erent datasets, branch locations and back-bone networks and show that our method is signi\ufb01-cantly more accurate in 26 of these scenarios, whilehaving less overhead in terms of number of param-eters and \ufb02oating point operators (FLOPS). To thebest of our knowledge the fusion of global and localscope in early exits has never been used in multi-exitarchitectures before.\u2022 We show that our method is a general purpose ap-proach that works across di\ufb00erent modalities as wellas multi-modal settings by investigating image clas-si\ufb01cation, audio classi\ufb01cation and audiovisual crowdcounting scenarios. We also show that our methodworks for both classi\ufb01cation and regression prob-lems.1Our code will be available athttps://gitlab.au.dk/maleci/sl_vit.\u2022 We introduce a novel way of integrating audio andvisual features in early exits using vision transform-ers. To the best of our knowledge, this is the \ufb01rsttime early exits have been studied in multi-modalsettings.\u2022 We provide insight into why our method achievesbetter results compared to conventional CNN-basedarchitectures by investigating the role of attentionand receptive \ufb01eld.\u2022 We introduce a \ufb01ne-tuning strategy for SL-ViT calledcopycat single-layer vision transformer (CC-SL-ViT )which is based on the copycat strategy developed forCNNs [16] and show that this method can furtherincrease the accuracy of SL-ViT early exits. To thebest of our knowledge this is the \ufb01rst time the copy-cat strategy is used for vision transformers or earlyexits.The rest of this paper is organized as follows: Section2 provides an overview of the relevant literature; Section 3describes our proposed method in detail; Section 4 explainsthe details of our experiments; Section 5 showcases theexperiment results; and, \ufb01nally, Section 6 brie\ufb02y discussesthe results and concludes the paper.2. Related WorkThis section provides the necessary prerequisites forunderstanding our method and experiments. We startby describing the particulars of multi-exit architectures.Subsequently, we provide the details of the vision trans-former architecture, which is the foundation of the pro-posed method. Then, we brie\ufb02y touch on how audio clas-si\ufb01cation is normally carried out, which is included in sev-eral scenarios in our experiments. Finally, we explain an-other scenario investigated in our experiments, i.e. crowdcounting, and how it can be approached in a multi-modalmanner.2.1. Multi-Exit ArchitecturesIn order to describe multi-exit architectures, we use thesame notation as Scardapane et al. [10] where a neural net-work is formulated as a functionf(X) = fL(fL\u22121(...f1(X))).In this formulation Lsigni\ufb01es the total number of layers inthe network andfi is the operator corresponding to layeri,which can be a convolutional layer, a fully-connected layer,a normalization layer, or any other di\ufb00erentiable operator.hi = fi(hi\u22121) denotes the output of layeri, where h0 is theinput X. Finally, \u03b8i symbolizes the trainable parametersof layer i.Equation (1) formulates the training process for theneural network which is achieved by tuning its parametersusing an optimization algorithm on the landscape de\ufb01nedby a loss function. In this equation, the parameters of theneural network are denoted by \u03b8 = \u22c3Li=1 \u03b8i, the training2--- Page 3 ---samples are signi\ufb01ed by {(Xn,yn)}Nn=1, and l(\xb7,\xb7) is theloss function.f\u2217= arg min\u03b8N\u2211n=1l(yn,f(Xn)) (1)Extending this notation to multi-exit architectures,B \u2286{1,..,L }signi\ufb01es the set of selected branch locations afterwhich early exit branches will be placed. cb(hb) = yb is theclassi\ufb01er or regressor representing the early exit branch ateach branch location b, where yb denotes the early resultat that location. The schematic illustration of a multi-exitarchitecture is presented in Figure 1. However, since thereare multiple outputs, and thus multiple loss signals in amulti-exit architecture, its training is not as straightfor-ward.Figure 1: Schematic illustration of a multi-exit architecture with twoearly exits.Three di\ufb00erent approaches for training multi-exit ar-chitectures exist in the literature [10, 17, 13]. In the \ufb01rstapproach, called end-to-end training, the loss signals ofall exits are combined and backpropagated through thenetwork at the same time. With end-to-end training, thecontribution of each loss signal to the total loss is expressedwith weight values, which are therefore hyper-parametersof the model.The second approach, called layer-wise training, \ufb01rsttrains the network up to and including the \ufb01rst exit branch.Subsequently, the part of the network that has been trainedso far is frozen, meaning its parameters are not modi\ufb01edany further, and the remainder of the network up to andincluding the second exit branch is trained. This processcontinues until the entire network is trained. Note thatwith this approach, there is no guarantee that the accu-racy of the \ufb01nal exit remains unchanged.In the \ufb01nal approach, called classi\ufb01er-wise training,the backbone network is completely frozen and each branchis trained independent of the rest of the network and otherbranches, meaning the parameters \u03b8 are not modi\ufb01ed andonly the parameters of the classifers/regressors {cb},b \u2208Bare trained. With this approach, no new hyper-parametersare introduced and the backbone remains unchanged. How-ever, the early exit branches a\ufb00ect a lower number of train-able parameters compared to the other approaches.In this paper, we choose to follow the classi\ufb01er-wisetraining approach due to its practical importance. Thisis because with classi\ufb01er-wise training, early exit branchescan be easily added on top of existing backbone networkswithout the need for re-training and hyper-parameter op-timization, which can be computationally expensive andtime consuming. Furthermore, with end-to-end and layer-wise training strategies, the number of branches and theirplacement can lead to further trade-o\ufb00s and a\ufb00ect theoverall performance of the model. Since branches are inde-pendently trained in the classi\ufb01er-wise strategy, any num-ber of branches can exist and a branch can be placed atany location without a\ufb00ecting the performance of otherbranches or the backbone.It is important to mention that branches placed laterin the backbone network do not necessarily result in ahigher accuracy compared to branches placed earlier. Theusage of such branches would therefore not be sensiblesince earlier branches exist that require less computationand provide more accurate results. We hereby use the termimpractical to refer to such branches.As previously mentioned, there are several methodsthat try to improve the accuracy of early exits. The methodin [12] uses the combination of the distillation loss fromthe \ufb01nal exit and the loss signal from ground truth labelsto train more accurate early exits using in the end-to-endtraining setting. The method in [18] expands on this ideaby adding a third loss signal based on the di\ufb00erence be-tween features of the latest early exit with earlier exits.The method in [19] proposes a technique called gradientequilibrium to combat the problem of gradient imbalancethat surfaces when using the end-to-end strategy, whichis when the variance of the gradients becomes very largewhen loss signals from multiple exits are combined, lead-ing to unstable training. Moreover, this paper introducesforward and backward knowledge transfer that aims to en-courage collaboration among di\ufb00erent exits. The methodin [20] improves the accuracy of later exits by reusing pre-dictions from earlier exits. The method in [21] circumventsthe problem of impractical branches by adaptively select-ing the exit location based on time budget and the speci\ufb01cinput. The method in [22] simpli\ufb01es the design of multi-exit architectures by removing the hyper-parameters of theend-to-end training strategy that specify the contributionof each loss signal.Besides e\ufb03cient inference, early exits can prove use-ful in several other applications, for instance, the methodin [23] allows for parallel training of the segments of theDNN that exist between early exits, by training each seg-ment based on the loss signal of the next segment obtainedin the previous training stage. Moreover, early exits canbe added to the network during the training in order toincrease the accuracy of the backbone network and dis-carded after the training phase, for instance, the widelyused Inception model [24] was trained in this way.Besides early exiting, several other approaches existfor dynamic inference, for instance, layer skipping [25, 26,27, 28] where the execution of some of the layers of theDNN are skipped, and channel skipping [29] where lessimpactful channels of convolutional neural networks areignored and their computation is skipped during the infer-3--- Page 4 ---ence phase. However, unlike early exits, these approachescannot provide an output if the execution is interrupteddue to a strict deadline, as these methods need to performthe computations until the very last layer.2.2. Vision TransformerThe transformer architecture was \ufb01rst introduced byVaswani et al. [30] for natural language processing, andit has recently been adapted for solving computer visionproblems by Dosovitskiy et al. [15]. Vision transformerwas originally developed for the problem of image clas-si\ufb01cation, however, variations of vision transformer havesince been applied to many computer vision problems, suchas object detection, depth estimation, semantic segmenta-tion, image generation and action recognition, as well asmulti-modal data analysis tasks such as text-to-image syn-thesis and visual question answering [31, 32, 33].In order to describe the vision transformer architecture,we \ufb01rst explain the self-attention layer. The input of thislayer is in the form of a sequence X = (x1,...,x n) whereX \u2208Rn\xd7d and dis the embedding dimension to representeach entity. Its output is in the form of Z = (z1,...,z n)where Z \u2208Rn\xd7dv . The goal of self-attention is to capturethe interaction between the entities in the sequence. Forthis purpose, each vector xi in the sequence is transformedinto three separate vectors: the query vector qi \u2208Rdq ,the key vector ki \u2208Rdk and the value vector vi \u2208Rdv ,where dq = dk. To construct the output vector zi thatcorresponds to the input xi, for each vector xj in X (in-cluding xi itself), the scalar aij is calculated by the innerproduct of qi and kj. Output vector zi is then calculatedby summing the value vectors v1,...,v n weighted by theircorresponding scalars, that is, zi = \u2211nj=1 aijvj. The scalaraij basically speci\ufb01es how much attention the i-th entityshould pay to the j-th entity, since aij determines the con-tribution of vj to the combined output zi. In practice, thescalars are normalized by \u221adk and converted into proba-bilities using the softmax function.If the key, query and value vectors are packed into ma-trices Q= XWQ, K = XWK and V = XWV, where WQ,WK and WV are learnable weight matrices, the above op-eration can be rephrased as follows:Z = softmax(QKT\u221adk)V (2)In order to enable the model to capture more than onetype of relationship between the entities in the sequence,self-attention is extended to multi-head attention by con-catenating the output of h di\ufb00erent self-attention blocksZ1,...,Z h each with its own set of learnable weight ma-trices, into a single matrix Z\u2032 = [Z0,...,Z h] \u2208Rn\xd7h.dv ,which is then projected using a weight matrixW\u2032\u2208Rh.dv\xd7d.A transformer encoder is constructed by passing theinput sequence into a normalization layer, a multi-headattention layer, a second normalization layer and a multi-layer perceptron (MLP), respectively. Two residual con-nections are added, one by adding the input sequence tothe output of the multi-head attention, and the other byadding the output of the multi-head attention to the out-put of the MLP.Putting it all together, a vision transformer is createdby \ufb01rst splitting the input image into patches. Subse-quently, the sequence of patches is projected into a se-quence of vectors and a positional embedding is addedto the corresponding vector of each patch. An additionallearnable embedding called classi\ufb01cation token is added tothe beginning of the sequence. The sequence then passesthrough L transformer encoders. Finally, the \ufb01rst vectorin the output of the last transformer encoder, which cor-responds to the classi\ufb01cation token, is passed to a MLPwhich outputs the \ufb01nal classi\ufb01cation result. The architec-ture of vision transformer is depicted in Figure 2.Figure 2: The vision transformer (ViT) architecture for image clas-si\ufb01cation.ViT-EE is a method which uses transformer encodersfor early exits placed on ViT backbones [14]. ViT-EE usesthe exact same layer as the ViT backbone. Using the build-ing blocks of the backbone network for early exit branchesis simple and intuitive, and it is the reason why so far,mostly convolutional layers have been used for early exit-ing CNN backbones. However, as we show in this work,carefully designing the architecture of early exit branchescan lead to signi\ufb01cant improvements. Using Transformer-based early exit branches on CNN backbones is not intu-itive, and requires additional steps such as converting ten-sors to patches, dealing with the classi\ufb01cation token and\ufb01ne-tuning the architecture parameters including patchsize, attention heads, embedding representation, the sizeand number of layers for MLP, and dropout. Moreover,we show that removing the last residual connection in thetransformer encoder can improve the performance in somecases.Furthermore, ViT backbones have a global receptive\ufb01eld in every layer, this means that ViT-EE is not neces-sarily ideal for early exits at all layers, as it adds too muchoverhead without providing improvements in terms of re-4--- Page 5 ---ceptive \ufb01eld. On the other hand, CNN backbones have alimited receptive \ufb01eld particularly in earlier layers, there-fore fusing this receptive \ufb01eld with a global one leads toimprovements.2.3. Audio Classi\ufb01cationSimilar to image classi\ufb01cation, audio classi\ufb01cation isthe problem of categorizing a given audio waveform intoone of several predetermined classes. For instance, thegiven audio waveform could be a musical recording, andthe goal could be to specify which genre of music it belongsto. To represent the input features, spectrogramsobtainedby applying short-time Fourier transform (STFT) andMelspectrograms are commonly used [34], although raw audiowaveforms can been used as well [35]. Mel spectrogramsare spectrograms that are constructed using the Mel scalewhich is a nonlinear transformation of the frequency scaledesigned based on domain knowledge about the humanauditory system. Various deep learning models for au-dio classi\ufb01cation exist in the literature, including modelsthat are commonly used for image classi\ufb01cation, namelyResNet [36], DenseNet [37] and Inception [38], which havebeen shown to be quite e\ufb00ective for audio classi\ufb01cationas well [39]. Conveniently, the same three networks havepreviously been used as backbone networks when investi-gating early exiting for image classi\ufb01cation [13]. Thereforewe use these backbone networks for both image and audioclassi\ufb01cation in our experiments.2.4. Audiovisual Crowd CountingCrowd counting refers to the problem of identifying thetotal number of people present in a given image. Crowdcounting has many applications such as safety monitoring,disaster management, design of public spaces, intelligencegathering and analysis, creation of virtual environmentsand forensic search [40]. With many of these applications,it is vital for the model to perform in near real-time. How-ever, the input images in these scenarios often have highresolutions, such as HD or Full HD. Moreover, many ofthe available methods contain an immense number of pa-rameters [41]. This means that crowd counting modelsare often very computationally expensive, therefore, dy-namic inference methods such as early exiting and otherlightweight deep learning methods become essential in realworld applications.Although the main objective of this task is to obtain asingle count from an image, many methods treat this prob-lem as dense prediction where the output is a density mapdepicting the density of the crowd across the input image,and the total count is calculated by the sum of all valuesin the density map. Therefore, in most crowd countingdatasets, such as Shanghai Tech [42] and World Expo \u201910[43], the locations of the heads of individuals in the imageare annotated and provided as targets. A ground truthdensity map can then be obtained from these head anno-tations using Gaussian kernels or more complicated andFigure 3: An example image from the Shanghai Tech dataset and itscorresponding ground truth density map.specialized methods [41]. Figure 3 shows an image fromthe Shanghai Tech dataset and the ground truth densitymap that was generated from the provided head annota-tions using the method presented in Zhang et al [42]. Incrowd counting, Mean Absolute Error (MAE) is usuallyused as a measure of accuracy whereas Mean Squared Er-ror (MSE) is used as a measure of robustness [44].Many crowd counting methods exist in the literature[41], however, most of these methods are applied in asingle-modal fashion where the input is an image or a videoframe. In contrast, AudioCSRNet [45], a multi-modal ex-tension of the widely-used CSRNet model for crowd count-ing [46], takes as input the ambient audio of a scene inaddition to its image. The authors show that the ambi-ent audio improves the result in situations where the im-age quality is not ideal, for instance, low image resolution,presence of noise, occlusion and low illumination.In CSRNet, the features extracted from the input im-age by the \ufb01rst 10 layers of a VGG-16 [47] network pre-trained on the ImageNet dataset [48] are passed through6 dilated convolution layers and a 1 \xd71 convolution layerin order to obtain the density map. AudioCSRNet ex-tends this architecture by converting each of the dilatedconvolution layers into a fusion block. The architecture ofAudioCSRNet is depicted in Figure 4. First, a Mel spec-trogram is obtained from the raw audio waveform. Subse-quently, in each fusion block, the features extracted fromthe input Mel spectrogram by the \ufb01rst 6 layers of a VG-Gish [49] network pre-trained on the AudioSet dataset [49]are projected to two vectors called \u03b3 and \u03b2 which repre-sent the multiplicative and additive aspects of the audiofeatures. The \u03b3 and \u03b2 vectors are then tiled in order tomatch the size of the visual features. Finally, the outputof the dilated convolution is element-wise multiplied by \u03b3and added to \u03b2.The fusion operation can be summarized asvl+1 = Fl(\u03b3l \u2299Dl(vl) + \u03b2l), (3)where vl \u2208 RCl\xd7Wl\xd7Hl is the output of the l-th fusionblock, Fl denotes an activation function, \u03b3l and \u03b2l are thetiled vectors and Dl represents the l-th dilated convolu-tion.In practice, a batch normalization layer [50] is addedimmediately after each dilated convolution. Furthermore,the height and width of the intermediate features remain5--- Page 6 ---Figure 4: Architecture of AudioCSRNet.unchanged by using padding in the convolution operations,meaning Hl = Hl+1 and Wl = Wl+1. Additionally, sincethe \ufb01rst 10 layers of VGG-16 decrease both height andwidth by a factor of 8 via several pooling operations, the\ufb01nal result of the network needs to be upsampled by afactor of 8 in order to match the resolution of the inputimage. It is important to preserve the total sum of thedensity map during this upsampling operation, since itrepresents the total count.3. Single-Layer Vision Transformers for Early Ex-itsWe assume a pre-trained and high performing back-bone network is already available. Due to time constraintsarising from the particular application, it is desirable thatthe network provides a result within the speci\ufb01c deadlinerather than not providing a result at all, even though thisresult may be less accurate than it would be if time con-straints did not exist. Therefore, the backbone needs tobe augmented with early exit branches to allow for dy-namic inference and anytime prediction. As previouslymentioned, we use the classi\ufb01er-wise approach for trainingthe early exit branches since it results in \u201cplug-and-play\u201dbranches that can easily be added to the backbone networkwithout any re-training or hyper-parameter tuning.3.1. SL-ViTTypically, the architecture of early exit branches startswith one or more convolution layers, although some mayhave no convolutions at all. Afterwards, they may have apooling layer, which may be global pooling, and one MLP[51, 11]. Here, as a baseline, we choose to utilize the ar-chitecture depicted in Figure 5 with one 3 \xd73 convolution,followed by a 2 \xd72 max pooling layer and \ufb01nally a MLP.The size of the max pooling layer is increased to 4 \xd74 forcrowd counting since the input images have a very highresolution. Additionally, we use dropout [52] inside theMLP to avoid over\ufb01tting. We use a single convolutionsince early exits with two or more convolution layers havea high overhead and may even lead to lower accuracy [11].Early exits without convolutions are sometimes used verylate in the network, however, since they are straightfor-ward and leave no room for modi\ufb01cations, we do not applyour method for such cases. The resulting architecture is acommon setup within the literature, and is e\ufb00ectively thesame architecture used for earlier exits by Hu et al. [51].Figure 5: Architecture of CNN early exit branches. Size of the \ufb02at-tened feature vector depends on the dimensions of the features at thespeci\ufb01c branch location. For branches placed on the AudioCSRNetbackbone, max pooling size is increased to 4x4 since the input im-ages have a high resolution. Figure created using the NN-SVG tool[53].Our method called single-layer vision transformer orSL-ViT for short, is an alternative architecture for earlyexit branches that can achieve a higher accuracy comparedto the aforementioned baseline, while having less overheadin terms of the number of parameters as well as \ufb02oatingpoint operations per second (FLOPS). Our proposed ar-chitecture is based on the vision transformer architectureintroduced in section 2.2, where instead of the input im-age, we split the intermediate features at the branch loca-tion into patches (sub-tensors) and pass them to a visiontransformer.The choice of vision transformer architecture is pri-marily due to its global receptive \ufb01eld. Receptive \ufb01eld6--- Page 7 ---is crucial in many deep learning problems, including onesstudied in this work. The receptive \ufb01eld of state-of-the-art CNNs developed for image classi\ufb01cation has steadilyincreased over time and is correlated with increased classi-\ufb01cation accuracy [54]. Additionally, in audio classi\ufb01cationusing spectrograms, each location relates to a di\ufb00erent fre-quency band in a di\ufb00erent window of time. It is reasonableto assume that processing combinations of frequencies andwindows that are not necessarily adjacent could be of im-portance. Moreover, many crowd counting methods havemade use of global information through visual attentionmechanisms and dilated convolutions [41]. Since the re-ceptive \ufb01eld is particularly limited in early layers of CNNbackbones, choosing an architecture for early exit brancheswith a global receptive \ufb01eld could be bene\ufb01cial.Many other designs strive to increase the receptive \ufb01eldin their building blocks, for instance, the pyramid pool-ing module (PPM) in PSPNet [55] or atrous spatial pyra-mid pooling (ASPP) in DeepLab [56]. However, they allfall short in comparison with the global receptive \ufb01eld oftransformers. PPM increases the receptive \ufb01eld throughaggregating di\ufb00erent levels of pooling, which means far lo-cations have only access to coarse representations of eachother, and ASPP has holes in its receptive \ufb01eld.It is important to mention that the local receptive \ufb01eldof convolutional layers is not fundamentally bad. On thecontrary, it plays a key role in representation learning andextracting local information, especially in the early layersof the network where the receptive \ufb01eld of the convolu-tional \ufb01lters is small. Filters in successive convolutionallayers have increasingly larger receptive \ufb01elds, therefore,\ufb01nal layers in a CNN architecture have \ufb01lters of largeenough receptive \ufb01elds that can e\ufb00ectively aggregate in-formation from the entire input image to provide a properresponse. However, this process of cascading local recep-tive \ufb01elds of increasing size requires the number of layersin the CNN to be large, or at least all the layers in thenetwork to be traversed in order to provide the network\u2019sresponse. When an early exit is added at an early layer,this chain of increasingly larger receptive \ufb01elds is broken,and an early exit that has a local receptive \ufb01eld may notbe able to e\ufb00ectively aggregate all required informationin the image to provide a suitable response. This situa-tion is the motivation behind the proposed branch archi-tecture, which fuses the local receptive \ufb01eld of the layerin the network where the early exit branch is attached,with the global receptive \ufb01eld of the early exit, in orderto e\ufb00ectively aggregate information from the entire inputand provide a more accurate response. Indeed, the orig-inal Vision Transformer paper [15] attributes the successof their model to the combination of local and global re-ceptive \ufb01elds and shows that even in very early layers, thisability to integrate information globally is indeed used bythe model.There are some crucial di\ufb00erences between the origi-nal vision transformer and the architecture in our method.First, in order to introduce a low overhead for early exitbranches, we only use a single transformer encoder layerinstead of the original 12 to 36 layers, meaning that L= 1in our case. Secondly, we do not utilize a separate clas-si\ufb01cation token and instead pass the entire output of thetransformer encoder layer to the MLP head. This is pos-sible because the width and height of tensors are gener-ally reduced throughout CNN backbones by pooling oper-ations, and thus the number of patches in our architectureis lower than that of the original vision transformer. Inaddition to the number of patches, the size of the embed-ding dimension ( d) is also reduced in our proposed archi-tecture, introducing far less parameters when passing theentire output of the last transformer encoder layer to theMLP head, even with high-resolution inputs such as inour crowd counting experiments. Variations of our archi-tecture have 5 \xd75, 7 \xd77 or 16 \xd79 patches and embeddingdimensions of 32 or 36, whereas di\ufb00erent versions of theoriginal vision transformer have 14 \xd714 or 16 \xd716 patchesand embedding dimensions of 768, 1024 or 1280. We em-pirically found that using the entire transformer encoderoutput instead of just one classi\ufb01cation token can increasethe accuracy, perhaps because in a single-layer version,there are not enough layers for the classi\ufb01cation token tolearn to properly summarize other patches. Our proposedarchitecture is shown in Figure 6. It is also important tonote that the MLP head used in our architecture is exactlythe same as the MLP in the CNN early exit architecture.Figure 6: Architecture of SL-ViT early exit branches. Unlike typicalvision transformers, only a single transformer encoder layer is used,extra learnable classi\ufb01cation token is not added to the sequence andthe entire output of the transformer encoder is passed on to the MLPhead. The MLP head is the same as CNN early exit branches.Our model has several hyper-parameters, namely thesize of each patch, the embedding dimension d and thenumber of attention heads hin multi-head attention. Thepatch size creates a trade-o\ufb00 where smaller patches resultin a more \ufb01ne-grained attention mechanism while increas-ing the total number of parameters in a bi-quadratic fash-ion. Therefore, similar to the original vision transformer,we choose the size of the patch to be close to the square7--- Page 8 ---root of the height and width of the input features. We alsomake sure that the size of the patch can divide the size ofthe input features to avoid padding, for instance, a patchsize of 4 \xd74 for input features of size 28 \xd728. We performa grid search to \ufb01nd the values of d and h that result inthe highest accuracy, while keeping the total number ofparameters less than or equal to that of the CNN earlyexit counterpart.At a \ufb01rst glance, it might seem like the SL-ViT archi-tecture introduces more hyper-parameters than the con-ventional CNN architecture, however, the CNN architec-ture includes many design choices as well, such as the num-ber of \ufb01lters, \ufb01lter size, padding, dilation, stride, poolingtype and pooling size. The design choices for CNN archi-tectures might seem simpler since they have been studiedmore extensively compared to vision transformers whichwere introduced more recently.3.2. Audiovisual SL-ViTWith audiovisual backbones such as the AudioCSR-Net model for audiovisual crowd counting, described insection 2.4, it is desirable to have audiovisual early exitsthat use both visual and audio features in order to achievea higher accuracy. The simplest way to have such branchesis to add the branches after the blocks where the fusionof visual and audio features take place. However, withour proposed SL-ViT architecture, it is also possible toinclude audio features as one or more patches alongsideother patches, and directly fuse the features in the earlyexit. The advantage of this approach is that since in vi-sion transformers, any of the patches can pay attention toany other patch, the visual features can be fused with theaudio features without being directly impacted and mod-i\ufb01ed. In contrast, since convolutional \ufb01lters only take theimmediate vicinity into account, the audio features mustbe present in every location. One option is to concatenatethe visual features and the tiled audio features along thedepth. However, that would greatly increase the amount ofcomputation for each fusion operation, therefore intrusiveoperations such as element-wise multiplication and addi-tion are used instead.3.3. Copycat SL-ViTFinally, we introduce a \ufb01ne-tuning strategy for SL-ViTbranches that can further increase their accuracy. Correia-Silva et al. [16] developed a method called copycat CNNwhere they create a \u201cfake\u201d dataset by taking images fromanother domain, giving them as input to a network trainedon the target domain, and recording the output of thenetwork as labels for these images. For instance, imagesfrom the ImageNet dataset [48] can be given to a networktrained on the CIFAR-10 dataset [57], where the imageof a camel may be labelled as a \u201cdog\u201d since there are nolabels for \u201ccamel\u201d in CIFAR-10. This fake dataset is thencombined with a dataset for the target domain and usedto train a new network. We use this strategy to \ufb01ne-tunean already trained SL-ViT branch and obtain a copycatsingle-layer vision transformer (CC-SL-ViT ). Note thatthe ratio of the fake data mixed with the available datasetis a hyper-parameter of this \ufb01ne-tuning strategy.4. Experimental SetupIn this section, we provide the details of our experi-ments. We begin by giving a short summary of the datasetsas well as the training details for the backbone networks.We then lay out the details of the branch architectures,their training procedure and their placement on the back-bone networks, and \ufb01nally explain how the copycat strat-egy was used to \ufb01ne-tune the branches.A total of 27 di\ufb00erent scenarios were tested in our ex-periments. For both image and audio classi\ufb01cation, twodatasets, three backbone networks and two di\ufb00erent branchlocations on each backbone were tested. In addition, threedi\ufb00erent branch locations for the audiovisual crowd count-ing backbone network were covered. All experiments wererepeated 5 times and the average accuracy as well as thestandard deviation were recorded. 4 \xd7 Nvidia 2080TiGPUs were used for the training of our models.4.1. Datasets4.1.1. CIFAR-10 and CIFAR-100These are widely-used datasets for image classi\ufb01cation[57]. Both datasets consist of 60,000 color images of size32 \xd732 pixels and their corresponding class labels. Theimages in CIFAR-10 and CIFAR-100 are categorized into10 and 100 di\ufb00erent classes, respectively. We use 40,000examples for training, 10,000 for validation and another10,000 for testing. Since our backbone networks are pre-trained on ImageNet which consists of 224 \xd7224 pixelimages, we resize each image to these dimensions beforepassing them into the network.4.1.2. Speech Commands (SC)A well-known audio dataset of spoken words [58]. Itconsists of 100,503 1-second audio clips with a samplingrate of 16kHz, each labelled as one of 12 classes: 10 dif-ferent spoken words such as \u201cYes\u201d, \u201cNo\u201d, \u201cDown\u201d and\u201cStop\u201d as well as one class for background noise and an-other for unknown words. We use 85,511 examples fortraining, 10,102 for validation and 4,890 for testing. Weconvert the raw audio waveforms into spectrograms usingshort-time Fourier transform (STFT) with a window sizeof 255 samples and step size of 128 samples, and resize theresulting spectrograms to 224 \xd7224 before passing theminto the network.4.1.3. GTZANIt is the most widely-used dataset for music genre recog-nition [59]. The original dataset consists of 10 genres suchas \u201cPop\u201d and \u201cRock\u201d and 100 30-second audio clips pergenre with a sampling rate of 22,050Hz. We follow the8--- Page 9 ---common approach to split each audio clip into 10 separate3-second clips in order to increase the size of the datasetto 10,000. We use 8,000 examples for training, 1,000 forvalidation and another 1,000 for testing. Following the ap-proach of Palanisamy et al. [39] where di\ufb00erent spectro-grams with di\ufb00erent parameters are placed in each channelof the input image, we use one spectrogram obtained fromSTFT with window size of 512 samples and step size of 256samples as well as two Mel spectrograms with 128 bins andwindow sizes of 100 and 50 milliseconds, and step sizes of50 and 25 milliseconds, respectively.4.1.4. DISCOAn audiovisual dataset for crowd counting which con-tains 1,935 images of Full HD resolution (1920 \xd71080)[45]. For each image, a corresponding 1-second audio clipof ambient sounds with a sampling rate of 48kHz, starting0.5 seconds before the image was taken and ending 0.5 sec-onds afterwards, exists as well. The labels are provided inthe form of head annotations in the image. At the time ofthis writing, DISCO is the only publicly available datasetfor audiovisual crowd counting. We use 1435 examples fortraining, 200 for validation and 300 for testing. The inputimage is resized to 1024 \xd7576 pixels to reduce memoryand computation requirements. Similar to Hershey et al.[49], the input audio waveform is transformed into a Melspectrogram with 64 bins, window size of 25 millisecondsand step size of 10 milliseconds. Following Hu et al. [45]the ground truth density maps are obtained by convolv-ing the head annotations with a 15 \xd715 Gaussian kernelK\u223cN (0,4.0).4.2. Backbone networksTransfer learning is used to train the ResNet152,DenseNet201 and InceptionV3 backbone networks for bothimage and audio classi\ufb01cation. The backbone networks areall pre-trained on the ImageNet dataset and the top layeris replaced. We found that instead of adding just one denselayer at the top, as is common in transfer learning, usingtwo dense layers and a dropout layer in between leads to ahigher accuracy in our case. The resulting network is thentrained using the Adam optimizer [60] with a learning rateof 10\u22124 and categorical cross-entropy loss function. Thelearning rate is reduced by a factor of 0.6 on plateau witha tolerance of 2 epochs, and an early stopping mechanismwith a tolerance of 5 epochs is used.The audiovisual crowd counting backbone is trained intwo stages. We \ufb01rst train a network with the AudioCSR-Net architecture described in Section 2.4 for 100 epochs.L2 norm is used as loss function and AdamW [61] with alearning rate of 10 \u22125 and weight decay of 10 \u22124 is used asoptimizer, where the learning rate is multiplied by a factorof 0.99 each epoch. This is the same training procedureused in the original paper [45]. Subsequently, in order toconvert the problem from dense prediction to regression,a dense layer with an output size of one is added after thelast layer of the trained AudioCSRNet. This layer is ini-tialized as a sum, meaning the initial weights are all equalto one and no bias is used. Then the entire network isre-trained for another 100 epochs using MAE as loss func-tion instead of the previous L2 loss, a learning rate of 10\u22126and weight decay of 10 \u22125. The learning rate is similarlymultiplied by a factor of 0.99 every epoch. The resultingmodel achieves a MAE of 13.63 which is even lower thanthe MAE of 14.27 reported in the original paper. However,the output of the network is just a single number repre-senting the total count instead of a density map. The \ufb01nalaccuracy of all trained backbones can be seen in Table 1.When training the backbone networks, in order to \ufb01tthe limitations of our available computational resources,the batch sizes are adjusted and some layers of the back-bone networks are frozen. All backbone networks weretrained with a batch size of 32 except AudioCSRNet whichhas a batch size of 4 as well as InceptionV3 when trainedon CIFAR-10 and CIFAR-100 which has a batch size of 64.All layers of the backbone networks were trained, exceptin the case of ResNet152 and DenseNet201 when trainedon CIFAR-10 and CIFAR-100 where only the batch nor-malization layers were trained. We found that trainingonly the batch normalization layers is su\ufb03cient to achievea high-performing backbone network in these cases [62].Table 1: Performance of backbone networks on each datasetBackboneCIFAR-10 Acc. CIFAR-100 Acc. SC Acc. GTZAN Acc. DISCO MAEResNet152 95.36% 82.25% 95.85% 91.29% -DenseNet20196.48% 82.53% 96.36% 92.09% -InceptionV396.56% 83.80% 94.93% 87.79% -AudioCSRNet- - - - 13.634.3. BranchesAll branches were trained from scratch using the Heinitialization method [63] and the Adam optimizer with alearning rate of 10 \u22124 where the learning rate is reducedby a factor of 0.6 on plateau with a tolerance of 2 epochs,and an early stopping mechanism with a tolerance of 5epochs is utilized. The branches on classi\ufb01cation back-bones use a categorical cross-entropy loss function whereasthe branches on the audiovisual crowd counting backboneuse mean absolute error loss. The training batch size forbranches were 64 in scenarios involving CIFAR-10, CIFAR-100 and Speech Commands, 32 in scenarios involvingGTZAN and 4 in scenarios involving DISCO.Table 2 shows the location of the branches placed oneach backbone network. For the AudioCSRNet backbonenetwork, branch V1 uses only the output of the VGG-16layers, therefore, it only has access to the visual features.Branch AV1 uses the outputs of both VGG-16 and VG-Gish, therefore it has access to both audio and visual fea-tures. In this branch location, the fusion of audio andvisual features is performed as described in Section 3 forthe SL-ViT architecture, and similar to the fusion blocks9--- Page 10 ---in AudioCSRNet for the CNN architecture, however, with-out dilation. Finally, branch AV2 is placed after the \ufb01rstfusion block in AudioCSRNet, therefore audio and visualfeatures have already been fused and thus fusion operationis not required within the branches. Adding branches afterthe second fusion block or later would not be reasonablesince more than 85% of the computation of the backboneis carried out before that point, and thus the accelerationresulting from early exits would be negligible.Table 2: Placement of branches for each backbone betworkBackbone BN \u2217 Branch Placed AfterDenseNet201 1 Transition Layer 12 Transition Layer 2ResNet152 1 12th Convolution2 36th ConvolutionInceptionV3 1 First Filter Concat2 Second Filter ConcatAudioCSRNet V1 Last Layer of VGGAV1 Last Layers of VGG and VGGishAV2 First Fusion Block\u2217Branch Number4.4. SL-ViT and CC-SL-ViT ParametersTable 3 summarizes the hyper-parameters used for theSL-ViT branches in each scenario. \u201cPatch Size\u201d shows thewidth and height of each image patch, \u201cPatches\u201d denotesthe resulting number of patches across width and heightof the input image, d is the size of embedding dimensionand h is the number of heads in multi-head attention.For copycat SL-ViT, images from the Tiny ImageNetdataset, which are the images from ImageNet down-sampledto 32\xd732, were given to the InceptionV3 backbone trainedon CIFAR-10, and the outputs were used to create the fakedataset. Then the fake dataset was mixed with CIFAR-10with a 2-to-1 ratio and used for re-training.Table 3: Hyper-parameters of SL-ViT for di\ufb00erent backbone net-works and branchesBackbone Dataset BN \u2217 Patch Size Patchesd hDenseNet201 all all 4x4 7x7 32 12ResNet152 SC 2 4x4 7x7 32 24GTZAN 2 4x4 7x7 32 24Other 4x4 7x7 32 12InceptionV3 CIFAR-100 all 5x5 5x5 36 8Other 5x5 5x5 32 12AudioCSRNet DISCO all 8x8 16x9 32 12\u2217Branch Number5. ResultsThe results of our experiments are presented in Tables4 to 8. In these Tables, the \ufb01nal accuracy, the total FLOPSof the model up to and including the branch and the num-ber of parameters of just the early exit branch are com-pared between the CNN architecture and the SL-ViT ar-chitecture. Higher accuracies, lower errors, lower numberof parameters and lower total FLOPS are highlighted inthese tables. Furthermore, the acceleration caused by SL-ViT early exits, de\ufb01ned as the total FLOPS of the back-bone network divided by the total FLOPS of the modelup to and including the SL-ViT branch, is also provided.Several observations can be made about these results.First, in all scenarios except one, SL-ViT early exits achievea signi\ufb01cantly higher accuracy. Even in the one excep-tional scenario, namely branch 2 of ResNet152 in Table6, the accuracy of SL-ViT is very close to its CNN coun-terpart. Secondly, while in some cases SL-ViT brancheshave an equal number of parameters compared to CNNbranches, in all scenarios, the total FLOPS of SL-ViTbranches is \n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2105.09121v3\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 35:
\n~~~~~~~~~~\nHistory: Step #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization efficiency tradeoffs, here was the response\nTitle: CDC: Classification Driven Compression for Bandwidth Efficient Edge-Cloud Collaborative Deep Learning\nSummary: The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\nPublication Date: 2020-05-04\narXiv paper ID: 2005.02177v1\n\nTitle: Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems\nSummary: The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\nPublication Date: 2021-12-21\narXiv paper ID: 2112.10948v1\n\nTitle: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study\nSummary: Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\nPublication Date: 2020-08-03\narXiv paper ID: 2008.00605v1\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n\nTitle: A Quantization-Friendly Separable Convolution for MobileNets\nSummary: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\nPublication Date: 2018-03-22\narXiv paper ID: 1803.08607v3\n, Your response: ```SUMMARY\nedge device image classification accuracy-latency tradeoffs\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\nStep #26, Phase: literature review, Feedback: Successfully added paper 1905.04159v1, Your response: ```SUMMARY\nedge device image classification dynamic inference\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification dynamic inference, here was the response\nTitle: AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and Dynamic Model Placements on Edge Devices\nSummary: Many real-world applications are widely adopting the edge computing paradigm\ndue to its low latency and better privacy protection. With notable success in\nAI and deep learning (DL), edge devices and AI accelerators play a crucial role\nin deploying DL inference services at the edge of the Internet. While prior\nworks quantified various edge devices' efficiency, most studies focused on the\nperformance of edge devices with single DL tasks. Therefore, there is an urgent\nneed to investigate AI multi-tenancy on edge devices, required by many advanced\nDL applications for edge computing.\n  This work investigates two techniques - concurrent model executions and\ndynamic model placements - for AI multi-tenancy on edge devices. With image\nclassification as an example scenario, we empirically evaluate AI multi-tenancy\non various edge devices, AI accelerators, and DL frameworks to identify its\nbenefits and limitations. Our results show that multi-tenancy significantly\nimproves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI\nmulti-tenancy techniques also open up new opportunities for flexible deployment\nof multiple DL services on edge devices and AI accelerators.\nPublication Date: 2021-07-26\narXiv paper ID: 2107.12486v1\n\nTitle: Edge-PRUNE: Flexible Distributed Deep Learning Inference\nSummary: Collaborative deep learning inference between low-resource endpoint devices\nand edge servers has received significant research interest in the last few\nyears. Such computation partitioning can help reducing endpoint device energy\nconsumption and improve latency, but equally importantly also contributes to\nprivacy-preserving of sensitive data. This paper describes Edge-PRUNE, a\nflexible but light-weight computation framework for distributing machine\nlearning inference between edge servers and one or more client devices.\nCompared to previous approaches, Edge-PRUNE is based on a formal dataflow\ncomputing model, and is agnostic towards machine learning training frameworks,\noffering at the same time wide support for leveraging deep learning\naccelerators such as embedded GPUs. The experimental section of the paper\ndemonstrates the use and performance of Edge-PRUNE by image classification and\nobject tracking applications on two heterogeneous endpoint devices and an edge\nserver, over wireless and physical connections. Endpoint device inference time\nfor SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by\ncollaborative inference.\nPublication Date: 2022-04-27\narXiv paper ID: 2204.12947v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: AutoDiCE: Fully Automated Distributed CNN Inference at the Edge\nSummary: Deep Learning approaches based on Convolutional Neural Networks (CNNs) are\nextensively utilized and very successful in a wide range of application areas,\nincluding image classification and speech recognition. For the execution of\ntrained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud\nto the Edge. Unfortunately, deploying and inferring large, compute and memory\nintensive CNNs on edge devices is challenging because these devices typically\nhave limited power budgets and compute/memory resources. One approach to\naddress this challenge is to leverage all available resources across multiple\nedge devices to deploy and execute a large CNN by properly partitioning the CNN\nand running each CNN partition on a separate edge device. Although such\ndistribution, deployment, and execution of large CNNs on multiple edge devices\nis a desirable and beneficial approach, there currently does not exist a design\nand programming framework that takes a trained CNN model, together with a CNN\npartitioning specification, and fully automates the CNN model splitting and\ndeployment on multiple edge devices to facilitate distributed CNN inference at\nthe Edge. Therefore, in this paper, we propose a novel framework, called\nAutoDiCE, for automated splitting of a CNN model into a set of sub-models and\nautomated code generation for distributed and collaborative execution of these\nsub-models on multiple, possibly heterogeneous, edge devices, while supporting\nthe exploitation of parallelism among and within the edge devices. Our\nexperimental results show that AutoDiCE can deliver distributed CNN inference\nwith reduced energy consumption and memory usage per edge device, and improved\noverall system throughput at the same time.\nPublication Date: 2022-07-20\narXiv paper ID: 2207.12113v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n, Your response: ```FULL_TEXT\n2404.07217v2\n```\nStep #29, Phase: literature review, Feedback: Successfully added paper 2404.07217v2, Your response: ```SUMMARY\nedge device image classification early exit networks\n```\nStep #30, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification early exit networks, here was the response\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Resource-Constrained Edge AI with Early Exit Prediction\nSummary: By leveraging the data sample diversity, the early-exit network recently\nemerges as a prominent neural network architecture to accelerate the deep\nlearning inference process. However, intermediate classifiers of the early\nexits introduce additional computation overhead, which is unfavorable for\nresource-constrained edge artificial intelligence (AI). In this paper, we\npropose an early exit prediction mechanism to reduce the on-device computation\noverhead in a device-edge co-inference system supported by early-exit networks.\nSpecifically, we design a low-complexity module, namely the Exit Predictor, to\nguide some distinctly "hard" samples to bypass the computation of the early\nexits. Besides, considering the varying communication bandwidth, we extend the\nearly exit prediction mechanism for latency-aware edge inference, which adapts\nthe prediction thresholds of the Exit Predictor and the confidence thresholds\nof the early-exit network via a few simple regression models. Extensive\nexperiment results demonstrate the effectiveness of the Exit Predictor in\nachieving a better tradeoff between accuracy and on-device computation overhead\nfor early-exit networks. Besides, compared with the baseline methods, the\nproposed method for latency-aware edge inference attains higher inference\naccuracy under different bandwidth conditions.\nPublication Date: 2022-06-15\narXiv paper ID: 2206.07269v2\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n, Your response: ```FULL_TEXT\\n2311.18098v1\n```\nStep #31, Phase: literature review, Feedback: Error retrieving paper: Page request resulted in HTTP 400 (https://export.arxiv.org/api/query?search_query=&id_list=%5Cn2311.18098v1&sortBy=relevance&sortOrder=descending&start=0&max_results=100), Your response: ```SUMMARY\nedge image classification early exit dynamic inference\n```\nStep #32, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit dynamic inference, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\nStep #33, Phase: literature review, Feedback: 3\n--- Page 1 ---Single-Layer Vision Transformers for More Accurate Early Exits with Less OverheadArian Bakhtiarniaa,\u2217, Qi Zhanga, Alexandros Iosi\ufb01disaaDIGIT, Department of Electrical and Computer Engineering, Aarhus University, Finlandsgade 22, Aarhus, 8200, Midtjylland, DenmarkAbstractDeploying deep learning models in time-critical applications with limited computational resources, for instance in edgecomputing systems and IoT networks, is a challenging task that often relies on dynamic inference methods such as earlyexiting. In this paper, we introduce a novel architecture for early exiting based on the vision transformer architecture,as well as a \ufb01ne-tuning strategy that signi\ufb01cantly increase the accuracy of early exit branches compared to conventionalapproaches while introducing less overhead. Through extensive experiments on image and audio classi\ufb01cation as well asaudiovisual crowd counting, we show that our method works for both classi\ufb01cation and regression problems, and in bothsingle- and multi-modal settings. Additionally, we introduce a novel method for integrating audio and visual modalitieswithin early exits in audiovisual data analysis, that can lead to a more \ufb01ne-grained dynamic inference.Keywords: dynamic inference, early exiting, multi-exit architecture, vision transformer, multi-modal, deep learning1. IntroductionOver the past decade, deep learning has shown tremen-dous success across various \ufb01elds, such as computer vi-sion and natural language processing [1]. However, deeplearning models are by de\ufb01nition composed of many layersof interconnected neurons, even reaching billions of pa-rameters, which makes them computationally expensive.This has sparked a great deal of research in order to makedeep learning models more lightweight, for which manyapproaches have been proposed, for instance, model com-pression methods [2] such as quantization [3], pruning [4],low-rank approximation [5] and knowledge distillation [6].More and more emerging internet of things (IoT) appli-cations are integrating deep learning models, such as videosurveillance, voice assistants, augmented reality and coop-erative autonomous driving, which are often time-sensitiveand require inputs to be processed within speci\ufb01c deadlines[7, 8]. The heavy computational burden of deep learningbecomes problematic for these time-critical IoT applica-tions, due to resource-constrained IoT devices. Edge com-puting is a promising computing paradigm for addressingthis issue, in which the deep learning task is o\ufb04oaded toedge servers in the proximity of IoT devices.Since edge computing systems introduce computationo\ufb04oading over a communication network and involve mul-tiple nodes working collaboratively in order to completethe task in a timely manner, transmission time has tobe taken into account in addition to the deep learningcomputation time. However, transmission time may vary\u2217Corresponding authorEmail addresses: arianbakh@ece.au.dk (Arian Bakhtiarnia),qz@ece.au.dk (Qi Zhang), ai@ece.au.dk (Alexandros Iosi\ufb01dis)greatly over time and across di\ufb00erent channels. Conse-quently, deep learning models running on edge computingsystems and IoT networks should be capable of anytimeprediction, meaning they should be able to provide a validresponse even if they are interrupted before traversing theentire neural network, although the model is expected toprovide a better answer if it is allowed to run for longertime.Dynamic inference approaches [9] modify the computa-tion graph based on each input during the inference phasein order to \ufb01t the time constraints. A dynamic inferenceapproach that particularly suits anytime prediction isearlyexiting [10], also referred to as multi-exit architectures orauxiliary classi\ufb01ers in the literature. In multi-exit archi-tectures, one or more early exit branches are placed aftersome of the intermediate hidden layers of thebackbone net-work. The goal of each of these branches is to provide anearly result similar to the \ufb01nal result of the neural net-work using only the features extracted up to that particu-lar branch location. These early results are inevitably lessaccurate than the \ufb01nal result of the network. In order toachieve anytime prediction using early exiting, the latestearly result can be used whenever the execution is inter-rupted, for instance, whenever a hard deadline is reached.Computation time can be further decreased by applyingmodel compression techniques on the backbone of multi-exit architectures. Besides anytime prediction, early exit-ing can also be used in budgeted batch classi\ufb01cation wherea \ufb01xed amount of time is available in order to classify aset of input samples. In such a setting, the result of ear-lier branches can be used for \u201ceasier\u201d samples whereas theresult of later branches or the \ufb01nal result can be used for\u201charder\u201d ones. The di\ufb03culty of each sample can be de-termined based on the con\ufb01dence of the network about itsPreprint submitted to Neural Networks June 30, 2022arXiv:2105.09121v3  [cs.LG]  29 Jun 2022--- Page 2 ---output [11], although other approaches exist in the litera-ture [10].Early exit branches are expected to have a low over-head in terms of the extra computation they introduce,since a high overhead would defeat the purpose. There-fore, they often contain only a handful of layers. Ideally,we want the accuracy of the early results to be close to thatof the \ufb01nal result, since a higher accuracy for early exitbranches means that the overall reliability of the systemincreases. However, the low-overhead constraint makes itquite challenging to achieve a high accuracy since the earlyexit branches have signi\ufb01cantly less trainable parameterscompared to the rest of the network. Several approachesfor increasing the accuracy of early exits such as knowledgedistillation [12], curriculum learning [13] and architecturesdesigned speci\ufb01cally for early exit branches [14] have beensuggested. In this paper, we propose a novel architecturein order to obtain more accurate early exits for convolu-tional neural network (CNN) backbones.A neural architecture called vision transformer (ViT )[15] has been recently introduced for image classi\ufb01cationwhich is radically di\ufb00erent from convolutional neural net-works. The building blocks of Vision Transformer havebeen used for early exits placed on Vision Transformerbackbones [14], however, using Transformer-based earlyexit branches on CNN backbones is not intuitive and re-quires additional steps and architectural modi\ufb01cations. Weuse a modi\ufb01ed version of this architecture instead of theusual convolution and pooling layers in early exit branchesand show that our method can signi\ufb01cantly increase theaccuracy of early exits compared to conventional archi-tectures by fusing local and global receptive \ufb01elds 1. Thecontributions of this paper can be summarized as follows:\u2022 We propose a novel architecture for early exit branchesin multi-exit architectures based on vision transform-ers, called single-layer vision transformer (SL-ViT ).We compare our method with conventional CNN-based early exit architectures across 27 scenarios in-volving di\ufb00erent datasets, branch locations and back-bone networks and show that our method is signi\ufb01-cantly more accurate in 26 of these scenarios, whilehaving less overhead in terms of number of param-eters and \ufb02oating point operators (FLOPS). To thebest of our knowledge the fusion of global and localscope in early exits has never been used in multi-exitarchitectures before.\u2022 We show that our method is a general purpose ap-proach that works across di\ufb00erent modalities as wellas multi-modal settings by investigating image clas-si\ufb01cation, audio classi\ufb01cation and audiovisual crowdcounting scenarios. We also show that our methodworks for both classi\ufb01cation and regression prob-lems.1Our code will be available athttps://gitlab.au.dk/maleci/sl_vit.\u2022 We introduce a novel way of integrating audio andvisual features in early exits using vision transform-ers. To the best of our knowledge, this is the \ufb01rsttime early exits have been studied in multi-modalsettings.\u2022 We provide insight into why our method achievesbetter results compared to conventional CNN-basedarchitectures by investigating the role of attentionand receptive \ufb01eld.\u2022 We introduce a \ufb01ne-tuning strategy for SL-ViT calledcopycat single-layer vision transformer (CC-SL-ViT )which is based on the copycat strategy developed forCNNs [16] and show that this method can furtherincrease the accuracy of SL-ViT early exits. To thebest of our knowledge this is the \ufb01rst time the copy-cat strategy is used for vision transformers or earlyexits.The rest of this paper is organized as follows: Section2 provides an overview of the relevant literature; Section 3describes our proposed method in detail; Section 4 explainsthe details of our experiments; Section 5 showcases theexperiment results; and, \ufb01nally, Section 6 brie\ufb02y discussesthe results and concludes the paper.2. Related WorkThis section provides the necessary prerequisites forunderstanding our method and experiments. We startby describing the particulars of multi-exit architectures.Subsequently, we provide the details of the vision trans-former architecture, which is the foundation of the pro-posed method. Then, we brie\ufb02y touch on how audio clas-si\ufb01cation is normally carried out, which is included in sev-eral scenarios in our experiments. Finally, we explain an-other scenario investigated in our experiments, i.e. crowdcounting, and how it can be approached in a multi-modalmanner.2.1. Multi-Exit ArchitecturesIn order to describe multi-exit architectures, we use thesame notation as Scardapane et al. [10] where a neural net-work is formulated as a functionf(X) = fL(fL\u22121(...f1(X))).In this formulation Lsigni\ufb01es the total number of layers inthe network andfi is the operator corresponding to layeri,which can be a convolutional layer, a fully-connected layer,a normalization layer, or any other di\ufb00erentiable operator.hi = fi(hi\u22121) denotes the output of layeri, where h0 is theinput X. Finally, \u03b8i symbolizes the trainable parametersof layer i.Equation (1) formulates the training process for theneural network which is achieved by tuning its parametersusing an optimization algorithm on the landscape de\ufb01nedby a loss function. In this equation, the parameters of theneural network are denoted by \u03b8 = \u22c3Li=1 \u03b8i, the training2--- Page 3 ---samples are signi\ufb01ed by {(Xn,yn)}Nn=1, and l(\xb7,\xb7) is theloss function.f\u2217= arg min\u03b8N\u2211n=1l(yn,f(Xn)) (1)Extending this notation to multi-exit architectures,B \u2286{1,..,L }signi\ufb01es the set of selected branch locations afterwhich early exit branches will be placed. cb(hb) = yb is theclassi\ufb01er or regressor representing the early exit branch ateach branch location b, where yb denotes the early resultat that location. The schematic illustration of a multi-exitarchitecture is presented in Figure 1. However, since thereare multiple outputs, and thus multiple loss signals in amulti-exit architecture, its training is not as straightfor-ward.Figure 1: Schematic illustration of a multi-exit architecture with twoearly exits.Three di\ufb00erent approaches for training multi-exit ar-chitectures exist in the literature [10, 17, 13]. In the \ufb01rstapproach, called end-to-end training, the loss signals ofall exits are combined and backpropagated through thenetwork at the same time. With end-to-end training, thecontribution of each loss signal to the total loss is expressedwith weight values, which are therefore hyper-parametersof the model.The second approach, called layer-wise training, \ufb01rsttrains the network up to and including the \ufb01rst exit branch.Subsequently, the part of the network that has been trainedso far is frozen, meaning its parameters are not modi\ufb01edany further, and the remainder of the network up to andincluding the second exit branch is trained. This processcontinues until the entire network is trained. Note thatwith this approach, there is no guarantee that the accu-racy of the \ufb01nal exit remains unchanged.In the \ufb01nal approach, called classi\ufb01er-wise training,the backbone network is completely frozen and each branchis trained independent of the rest of the network and otherbranches, meaning the parameters \u03b8 are not modi\ufb01ed andonly the parameters of the classifers/regressors {cb},b \u2208Bare trained. With this approach, no new hyper-parametersare introduced and the backbone remains unchanged. How-ever, the early exit branches a\ufb00ect a lower number of train-able parameters compared to the other approaches.In this paper, we choose to follow the classi\ufb01er-wisetraining approach due to its practical importance. Thisis because with classi\ufb01er-wise training, early exit branchescan be easily added on top of existing backbone networkswithout the need for re-training and hyper-parameter op-timization, which can be computationally expensive andtime consuming. Furthermore, with end-to-end and layer-wise training strategies, the number of branches and theirplacement can lead to further trade-o\ufb00s and a\ufb00ect theoverall performance of the model. Since branches are inde-pendently trained in the classi\ufb01er-wise strategy, any num-ber of branches can exist and a branch can be placed atany location without a\ufb00ecting the performance of otherbranches or the backbone.It is important to mention that branches placed laterin the backbone network do not necessarily result in ahigher accuracy compared to branches placed earlier. Theusage of such branches would therefore not be sensiblesince earlier branches exist that require less computationand provide more accurate results. We hereby use the termimpractical to refer to such branches.As previously mentioned, there are several methodsthat try to improve the accuracy of early exits. The methodin [12] uses the combination of the distillation loss fromthe \ufb01nal exit and the loss signal from ground truth labelsto train more accurate early exits using in the end-to-endtraining setting. The method in [18] expands on this ideaby adding a third loss signal based on the di\ufb00erence be-tween features of the latest early exit with earlier exits.The method in [19] proposes a technique called gradientequilibrium to combat the problem of gradient imbalancethat surfaces when using the end-to-end strategy, whichis when the variance of the gradients becomes very largewhen loss signals from multiple exits are combined, lead-ing to unstable training. Moreover, this paper introducesforward and backward knowledge transfer that aims to en-courage collaboration among di\ufb00erent exits. The methodin [20] improves the accuracy of later exits by reusing pre-dictions from earlier exits. The method in [21] circumventsthe problem of impractical branches by adaptively select-ing the exit location based on time budget and the speci\ufb01cinput. The method in [22] simpli\ufb01es the design of multi-exit architectures by removing the hyper-parameters of theend-to-end training strategy that specify the contributionof each loss signal.Besides e\ufb03cient inference, early exits can prove use-ful in several other applications, for instance, the methodin [23] allows for parallel training of the segments of theDNN that exist between early exits, by training each seg-ment based on the loss signal of the next segment obtainedin the previous training stage. Moreover, early exits canbe added to the network during the training in order toincrease the accuracy of the backbone network and dis-carded after the training phase, for instance, the widelyused Inception model [24] was trained in this way.Besides early exiting, several other approaches existfor dynamic inference, for instance, layer skipping [25, 26,27, 28] where the execution of some of the layers of theDNN are skipped, and channel skipping [29] where lessimpactful channels of convolutional neural networks areignored and their computation is skipped during the infer-3--- Page 4 ---ence phase. However, unlike early exits, these approachescannot provide an output if the execution is interrupteddue to a strict deadline, as these methods need to performthe computations until the very last layer.2.2. Vision TransformerThe transformer architecture was \ufb01rst introduced byVaswani et al. [30] for natural language processing, andit has recently been adapted for solving computer visionproblems by Dosovitskiy et al. [15]. Vision transformerwas originally developed for the problem of image clas-si\ufb01cation, however, variations of vision transformer havesince been applied to many computer vision problems, suchas object detection, depth estimation, semantic segmenta-tion, image generation and action recognition, as well asmulti-modal data analysis tasks such as text-to-image syn-thesis and visual question answering [31, 32, 33].In order to describe the vision transformer architecture,we \ufb01rst explain the self-attention layer. The input of thislayer is in the form of a sequence X = (x1,...,x n) whereX \u2208Rn\xd7d and dis the embedding dimension to representeach entity. Its output is in the form of Z = (z1,...,z n)where Z \u2208Rn\xd7dv . The goal of self-attention is to capturethe interaction between the entities in the sequence. Forthis purpose, each vector xi in the sequence is transformedinto three separate vectors: the query vector qi \u2208Rdq ,the key vector ki \u2208Rdk and the value vector vi \u2208Rdv ,where dq = dk. To construct the output vector zi thatcorresponds to the input xi, for each vector xj in X (in-cluding xi itself), the scalar aij is calculated by the innerproduct of qi and kj. Output vector zi is then calculatedby summing the value vectors v1,...,v n weighted by theircorresponding scalars, that is, zi = \u2211nj=1 aijvj. The scalaraij basically speci\ufb01es how much attention the i-th entityshould pay to the j-th entity, since aij determines the con-tribution of vj to the combined output zi. In practice, thescalars are normalized by \u221adk and converted into proba-bilities using the softmax function.If the key, query and value vectors are packed into ma-trices Q= XWQ, K = XWK and V = XWV, where WQ,WK and WV are learnable weight matrices, the above op-eration can be rephrased as follows:Z = softmax(QKT\u221adk)V (2)In order to enable the model to capture more than onetype of relationship between the entities in the sequence,self-attention is extended to multi-head attention by con-catenating the output of h di\ufb00erent self-attention blocksZ1,...,Z h each with its own set of learnable weight ma-trices, into a single matrix Z\u2032 = [Z0,...,Z h] \u2208Rn\xd7h.dv ,which is then projected using a weight matrixW\u2032\u2208Rh.dv\xd7d.A transformer encoder is constructed by passing theinput sequence into a normalization layer, a multi-headattention layer, a second normalization layer and a multi-layer perceptron (MLP), respectively. Two residual con-nections are added, one by adding the input sequence tothe output of the multi-head attention, and the other byadding the output of the multi-head attention to the out-put of the MLP.Putting it all together, a vision transformer is createdby \ufb01rst splitting the input image into patches. Subse-quently, the sequence of patches is projected into a se-quence of vectors and a positional embedding is addedto the corresponding vector of each patch. An additionallearnable embedding called classi\ufb01cation token is added tothe beginning of the sequence. The sequence then passesthrough L transformer encoders. Finally, the \ufb01rst vectorin the output of the last transformer encoder, which cor-responds to the classi\ufb01cation token, is passed to a MLPwhich outputs the \ufb01nal classi\ufb01cation result. The architec-ture of vision transformer is depicted in Figure 2.Figure 2: The vision transformer (ViT) architecture for image clas-si\ufb01cation.ViT-EE is a method which uses transformer encodersfor early exits placed on ViT backbones [14]. ViT-EE usesthe exact same layer as the ViT backbone. Using the build-ing blocks of the backbone network for early exit branchesis simple and intuitive, and it is the reason why so far,mostly convolutional layers have been used for early exit-ing CNN backbones. However, as we show in this work,carefully designing the architecture of early exit branchescan lead to signi\ufb01cant improvements. Using Transformer-based early exit branches on CNN backbones is not intu-itive, and requires additional steps such as converting ten-sors to patches, dealing with the classi\ufb01cation token and\ufb01ne-tuning the architecture parameters including patchsize, attention heads, embedding representation, the sizeand number of layers for MLP, and dropout. Moreover,we show that removing the last residual connection in thetransformer encoder can improve the performance in somecases.Furthermore, ViT backbones have a global receptive\ufb01eld in every layer, this means that ViT-EE is not neces-sarily ideal for early exits at all layers, as it adds too muchoverhead without providing improvements in terms of re-4--- Page 5 ---ceptive \ufb01eld. On the other hand, CNN backbones have alimited receptive \ufb01eld particularly in earlier layers, there-fore fusing this receptive \ufb01eld with a global one leads toimprovements.2.3. Audio Classi\ufb01cationSimilar to image classi\ufb01cation, audio classi\ufb01cation isthe problem of categorizing a given audio waveform intoone of several predetermined classes. For instance, thegiven audio waveform could be a musical recording, andthe goal could be to specify which genre of music it belongsto. To represent the input features, spectrogramsobtainedby applying short-time Fourier transform (STFT) andMelspectrograms are commonly used [34], although raw audiowaveforms can been used as well [35]. Mel spectrogramsare spectrograms that are constructed using the Mel scalewhich is a nonlinear transformation of the frequency scaledesigned based on domain knowledge about the humanauditory system. Various deep learning models for au-dio classi\ufb01cation exist in the literature, including modelsthat are commonly used for image classi\ufb01cation, namelyResNet [36], DenseNet [37] and Inception [38], which havebeen shown to be quite e\ufb00ective for audio classi\ufb01cationas well [39]. Conveniently, the same three networks havepreviously been used as backbone networks when investi-gating early exiting for image classi\ufb01cation [13]. Thereforewe use these backbone networks for both image and audioclassi\ufb01cation in our experiments.2.4. Audiovisual Crowd CountingCrowd counting refers to the problem of identifying thetotal number of people present in a given image. Crowdcounting has many applications such as safety monitoring,disaster management, design of public spaces, intelligencegathering and analysis, creation of virtual environmentsand forensic search [40]. With many of these applications,it is vital for the model to perform in near real-time. How-ever, the input images in these scenarios often have highresolutions, such as HD or Full HD. Moreover, many ofthe available methods contain an immense number of pa-rameters [41]. This means that crowd counting modelsare often very computationally expensive, therefore, dy-namic inference methods such as early exiting and otherlightweight deep learning methods become essential in realworld applications.Although the main objective of this task is to obtain asingle count from an image, many methods treat this prob-lem as dense prediction where the output is a density mapdepicting the density of the crowd across the input image,and the total count is calculated by the sum of all valuesin the density map. Therefore, in most crowd countingdatasets, such as Shanghai Tech [42] and World Expo \u201910[43], the locations of the heads of individuals in the imageare annotated and provided as targets. A ground truthdensity map can then be obtained from these head anno-tations using Gaussian kernels or more complicated andFigure 3: An example image from the Shanghai Tech dataset and itscorresponding ground truth density map.specialized methods [41]. Figure 3 shows an image fromthe Shanghai Tech dataset and the ground truth densitymap that was generated from the provided head annota-tions using the method presented in Zhang et al [42]. Incrowd counting, Mean Absolute Error (MAE) is usuallyused as a measure of accuracy whereas Mean Squared Er-ror (MSE) is used as a measure of robustness [44].Many crowd counting methods exist in the literature[41], however, most of these methods are applied in asingle-modal fashion where the input is an image or a videoframe. In contrast, AudioCSRNet [45], a multi-modal ex-tension of the widely-used CSRNet model for crowd count-ing [46], takes as input the ambient audio of a scene inaddition to its image. The authors show that the ambi-ent audio improves the result in situations where the im-age quality is not ideal, for instance, low image resolution,presence of noise, occlusion and low illumination.In CSRNet, the features extracted from the input im-age by the \ufb01rst 10 layers of a VGG-16 [47] network pre-trained on the ImageNet dataset [48] are passed through6 dilated convolution layers and a 1 \xd71 convolution layerin order to obtain the density map. AudioCSRNet ex-tends this architecture by converting each of the dilatedconvolution layers into a fusion block. The architecture ofAudioCSRNet is depicted in Figure 4. First, a Mel spec-trogram is obtained from the raw audio waveform. Subse-quently, in each fusion block, the features extracted fromthe input Mel spectrogram by the \ufb01rst 6 layers of a VG-Gish [49] network pre-trained on the AudioSet dataset [49]are projected to two vectors called \u03b3 and \u03b2 which repre-sent the multiplicative and additive aspects of the audiofeatures. The \u03b3 and \u03b2 vectors are then tiled in order tomatch the size of the visual features. Finally, the outputof the dilated convolution is element-wise multiplied by \u03b3and added to \u03b2.The fusion operation can be summarized asvl+1 = Fl(\u03b3l \u2299Dl(vl) + \u03b2l), (3)where vl \u2208 RCl\xd7Wl\xd7Hl is the output of the l-th fusionblock, Fl denotes an activation function, \u03b3l and \u03b2l are thetiled vectors and Dl represents the l-th dilated convolu-tion.In practice, a batch normalization layer [50] is addedimmediately after each dilated convolution. Furthermore,the height and width of the intermediate features remain5--- Page 6 ---Figure 4: Architecture of AudioCSRNet.unchanged by using padding in the convolution operations,meaning Hl = Hl+1 and Wl = Wl+1. Additionally, sincethe \ufb01rst 10 layers of VGG-16 decrease both height andwidth by a factor of 8 via several pooling operations, the\ufb01nal result of the network needs to be upsampled by afactor of 8 in order to match the resolution of the inputimage. It is important to preserve the total sum of thedensity map during this upsampling operation, since itrepresents the total count.3. Single-Layer Vision Transformers for Early Ex-itsWe assume a pre-trained and high performing back-bone network is already available. Due to time constraintsarising from the particular application, it is desirable thatthe network provides a result within the speci\ufb01c deadlinerather than not providing a result at all, even though thisresult may be less accurate than it would be if time con-straints did not exist. Therefore, the backbone needs tobe augmented with early exit branches to allow for dy-namic inference and anytime prediction. As previouslymentioned, we use the classi\ufb01er-wise approach for trainingthe early exit branches since it results in \u201cplug-and-play\u201dbranches that can easily be added to the backbone networkwithout any re-training or hyper-parameter tuning.3.1. SL-ViTTypically, the architecture of early exit branches startswith one or more convolution layers, although some mayhave no convolutions at all. Afterwards, they may have apooling layer, which may be global pooling, and one MLP[51, 11]. Here, as a baseline, we choose to utilize the ar-chitecture depicted in Figure 5 with one 3 \xd73 convolution,followed by a 2 \xd72 max pooling layer and \ufb01nally a MLP.The size of the max pooling layer is increased to 4 \xd74 forcrowd counting since the input images have a very highresolution. Additionally, we use dropout [52] inside theMLP to avoid over\ufb01tting. We use a single convolutionsince early exits with two or more convolution layers havea high overhead and may even lead to lower accuracy [11].Early exits without convolutions are sometimes used verylate in the network, however, since they are straightfor-ward and leave no room for modi\ufb01cations, we do not applyour method for such cases. The resulting architecture is acommon setup within the literature, and is e\ufb00ectively thesame architecture used for earlier exits by Hu et al. [51].Figure 5: Architecture of CNN early exit branches. Size of the \ufb02at-tened feature vector depends on the dimensions of the features at thespeci\ufb01c branch location. For branches placed on the AudioCSRNetbackbone, max pooling size is increased to 4x4 since the input im-ages have a high resolution. Figure created using the NN-SVG tool[53].Our method called single-layer vision transformer orSL-ViT for short, is an alternative architecture for earlyexit branches that can achieve a higher accuracy comparedto the aforementioned baseline, while having less overheadin terms of the number of parameters as well as \ufb02oatingpoint operations per second (FLOPS). Our proposed ar-chitecture is based on the vision transformer architectureintroduced in section 2.2, where instead of the input im-age, we split the intermediate features at the branch loca-tion into patches (sub-tensors) and pass them to a visiontransformer.The choice of vision transformer architecture is pri-marily due to its global receptive \ufb01eld. Receptive \ufb01eld6--- Page 7 ---is crucial in many deep learning problems, including onesstudied in this work. The receptive \ufb01eld of state-of-the-art CNNs developed for image classi\ufb01cation has steadilyincreased over time and is correlated with increased classi-\ufb01cation accuracy [54]. Additionally, in audio classi\ufb01cationusing spectrograms, each location relates to a di\ufb00erent fre-quency band in a di\ufb00erent window of time. It is reasonableto assume that processing combinations of frequencies andwindows that are not necessarily adjacent could be of im-portance. Moreover, many crowd counting methods havemade use of global information through visual attentionmechanisms and dilated convolutions [41]. Since the re-ceptive \ufb01eld is particularly limited in early layers of CNNbackbones, choosing an architecture for early exit brancheswith a global receptive \ufb01eld could be bene\ufb01cial.Many other designs strive to increase the receptive \ufb01eldin their building blocks, for instance, the pyramid pool-ing module (PPM) in PSPNet [55] or atrous spatial pyra-mid pooling (ASPP) in DeepLab [56]. However, they allfall short in comparison with the global receptive \ufb01eld oftransformers. PPM increases the receptive \ufb01eld throughaggregating di\ufb00erent levels of pooling, which means far lo-cations have only access to coarse representations of eachother, and ASPP has holes in its receptive \ufb01eld.It is important to mention that the local receptive \ufb01eldof convolutional layers is not fundamentally bad. On thecontrary, it plays a key role in representation learning andextracting local information, especially in the early layersof the network where the receptive \ufb01eld of the convolu-tional \ufb01lters is small. Filters in successive convolutionallayers have increasingly larger receptive \ufb01elds, therefore,\ufb01nal layers in a CNN architecture have \ufb01lters of largeenough receptive \ufb01elds that can e\ufb00ectively aggregate in-formation from the entire input image to provide a properresponse. However, this process of cascading local recep-tive \ufb01elds of increasing size requires the number of layersin the CNN to be large, or at least all the layers in thenetwork to be traversed in order to provide the network\u2019sresponse. When an early exit is added at an early layer,this chain of increasingly larger receptive \ufb01elds is broken,and an early exit that has a local receptive \ufb01eld may notbe able to e\ufb00ectively aggregate all required informationin the image to provide a suitable response. This situa-tion is the motivation behind the proposed branch archi-tecture, which fuses the local receptive \ufb01eld of the layerin the network where the early exit branch is attached,with the global receptive \ufb01eld of the early exit, in orderto e\ufb00ectively aggregate information from the entire inputand provide a more accurate response. Indeed, the orig-inal Vision Transformer paper [15] attributes the successof their model to the combination of local and global re-ceptive \ufb01elds and shows that even in very early layers, thisability to integrate information globally is indeed used bythe model.There are some crucial di\ufb00erences between the origi-nal vision transformer and the architecture in our method.First, in order to introduce a low overhead for early exitbranches, we only use a single transformer encoder layerinstead of the original 12 to 36 layers, meaning that L= 1in our case. Secondly, we do not utilize a separate clas-si\ufb01cation token and instead pass the entire output of thetransformer encoder layer to the MLP head. This is pos-sible because the width and height of tensors are gener-ally reduced throughout CNN backbones by pooling oper-ations, and thus the number of patches in our architectureis lower than that of the original vision transformer. Inaddition to the number of patches, the size of the embed-ding dimension ( d) is also reduced in our proposed archi-tecture, introducing far less parameters when passing theentire output of the last transformer encoder layer to theMLP head, even with high-resolution inputs such as inour crowd counting experiments. Variations of our archi-tecture have 5 \xd75, 7 \xd77 or 16 \xd79 patches and embeddingdimensions of 32 or 36, whereas di\ufb00erent versions of theoriginal vision transformer have 14 \xd714 or 16 \xd716 patchesand embedding dimensions of 768, 1024 or 1280. We em-pirically found that using the entire transformer encoderoutput instead of just one classi\ufb01cation token can increasethe accuracy, perhaps because in a single-layer version,there are not enough layers for the classi\ufb01cation token tolearn to properly summarize other patches. Our proposedarchitecture is shown in Figure 6. It is also important tonote that the MLP head used in our architecture is exactlythe same as the MLP in the CNN early exit architecture.Figure 6: Architecture of SL-ViT early exit branches. Unlike typicalvision transformers, only a single transformer encoder layer is used,extra learnable classi\ufb01cation token is not added to the sequence andthe entire output of the transformer encoder is passed on to the MLPhead. The MLP head is the same as CNN early exit branches.Our model has several hyper-parameters, namely thesize of each patch, the embedding dimension d and thenumber of attention heads hin multi-head attention. Thepatch size creates a trade-o\ufb00 where smaller patches resultin a more \ufb01ne-grained attention mechanism while increas-ing the total number of parameters in a bi-quadratic fash-ion. Therefore, similar to the original vision transformer,we choose the size of the patch to be close to the square7--- Page 8 ---root of the height and width of the input features. We alsomake sure that the size of the patch can divide the size ofthe input features to avoid padding, for instance, a patchsize of 4 \xd74 for input features of size 28 \xd728. We performa grid search to \ufb01nd the values of d and h that result inthe highest accuracy, while keeping the total number ofparameters less than or equal to that of the CNN earlyexit counterpart.At a \ufb01rst glance, it might seem like the SL-ViT archi-tecture introduces more hyper-parameters than the con-ventional CNN architecture, however, the CNN architec-ture includes many design choices as well, such as the num-ber of \ufb01lters, \ufb01lter size, padding, dilation, stride, poolingtype and pooling size. The design choices for CNN archi-tectures might seem simpler since they have been studiedmore extensively compared to vision transformers whichwere introduced more recently.3.2. Audiovisual SL-ViTWith audiovisual backbones such as the AudioCSR-Net model for audiovisual crowd counting, described insection 2.4, it is desirable to have audiovisual early exitsthat use both visual and audio features in order to achievea higher accuracy. The simplest way to have such branchesis to add the branches after the blocks where the fusionof visual and audio features take place. However, withour proposed SL-ViT architecture, it is also possible toinclude audio features as one or more patches alongsideother patches, and directly fuse the features in the earlyexit. The advantage of this approach is that since in vi-sion transformers, any of the patches can pay attention toany other patch, the visual features can be fused with theaudio features without being directly impacted and mod-i\ufb01ed. In contrast, since convolutional \ufb01lters only take theimmediate vicinity into account, the audio features mustbe present in every location. One option is to concatenatethe visual features and the tiled audio features along thedepth. However, that would greatly increase the amount ofcomputation for each fusion operation, therefore intrusiveoperations such as element-wise multiplication and addi-tion are used instead.3.3. Copycat SL-ViTFinally, we introduce a \ufb01ne-tuning strategy for SL-ViTbranches that can further increase their accuracy. Correia-Silva et al. [16] developed a method called copycat CNNwhere they create a \u201cfake\u201d dataset by taking images fromanother domain, giving them as input to a network trainedon the target domain, and recording the output of thenetwork as labels for these images. For instance, imagesfrom the ImageNet dataset [48] can be given to a networktrained on the CIFAR-10 dataset [57], where the imageof a camel may be labelled as a \u201cdog\u201d since there are nolabels for \u201ccamel\u201d in CIFAR-10. This fake dataset is thencombined with a dataset for the target domain and usedto train a new network. We use this strategy to \ufb01ne-tunean already trained SL-ViT branch and obtain a copycatsingle-layer vision transformer (CC-SL-ViT ). Note thatthe ratio of the fake data mixed with the available datasetis a hyper-parameter of this \ufb01ne-tuning strategy.4. Experimental SetupIn this section, we provide the details of our experi-ments. We begin by giving a short summary of the datasetsas well as the training details for the backbone networks.We then lay out the details of the branch architectures,their training procedure and their placement on the back-bone networks, and \ufb01nally explain how the copycat strat-egy was used to \ufb01ne-tune the branches.A total of 27 di\ufb00erent scenarios were tested in our ex-periments. For both image and audio classi\ufb01cation, twodatasets, three backbone networks and two di\ufb00erent branchlocations on each backbone were tested. In addition, threedi\ufb00erent branch locations for the audiovisual crowd count-ing backbone network were covered. All experiments wererepeated 5 times and the average accuracy as well as thestandard deviation were recorded. 4 \xd7 Nvidia 2080TiGPUs were used for the training of our models.4.1. Datasets4.1.1. CIFAR-10 and CIFAR-100These are widely-used datasets for image classi\ufb01cation[57]. Both datasets consist of 60,000 color images of size32 \xd732 pixels and their corresponding class labels. Theimages in CIFAR-10 and CIFAR-100 are categorized into10 and 100 di\ufb00erent classes, respectively. We use 40,000examples for training, 10,000 for validation and another10,000 for testing. Since our backbone networks are pre-trained on ImageNet which consists of 224 \xd7224 pixelimages, we resize each image to these dimensions beforepassing them into the network.4.1.2. Speech Commands (SC)A well-known audio dataset of spoken words [58]. Itconsists of 100,503 1-second audio clips with a samplingrate of 16kHz, each labelled as one of 12 classes: 10 dif-ferent spoken words such as \u201cYes\u201d, \u201cNo\u201d, \u201cDown\u201d and\u201cStop\u201d as well as one class for background noise and an-other for unknown words. We use 85,511 examples fortraining, 10,102 for validation and 4,890 for testing. Weconvert the raw audio waveforms into spectrograms usingshort-time Fourier transform (STFT) with a window sizeof 255 samples and step size of 128 samples, and resize theresulting spectrograms to 224 \xd7224 before passing theminto the network.4.1.3. GTZANIt is the most widely-used dataset for music genre recog-nition [59]. The original dataset consists of 10 genres suchas \u201cPop\u201d and \u201cRock\u201d and 100 30-second audio clips pergenre with a sampling rate of 22,050Hz. We follow the8--- Page 9 ---common approach to split each audio clip into 10 separate3-second clips in order to increase the size of the datasetto 10,000. We use 8,000 examples for training, 1,000 forvalidation and another 1,000 for testing. Following the ap-proach of Palanisamy et al. [39] where di\ufb00erent spectro-grams with di\ufb00erent parameters are placed in each channelof the input image, we use one spectrogram obtained fromSTFT with window size of 512 samples and step size of 256samples as well as two Mel spectrograms with 128 bins andwindow sizes of 100 and 50 milliseconds, and step sizes of50 and 25 milliseconds, respectively.4.1.4. DISCOAn audiovisual dataset for crowd counting which con-tains 1,935 images of Full HD resolution (1920 \xd71080)[45]. For each image, a corresponding 1-second audio clipof ambient sounds with a sampling rate of 48kHz, starting0.5 seconds before the image was taken and ending 0.5 sec-onds afterwards, exists as well. The labels are provided inthe form of head annotations in the image. At the time ofthis writing, DISCO is the only publicly available datasetfor audiovisual crowd counting. We use 1435 examples fortraining, 200 for validation and 300 for testing. The inputimage is resized to 1024 \xd7576 pixels to reduce memoryand computation requirements. Similar to Hershey et al.[49], the input audio waveform is transformed into a Melspectrogram with 64 bins, window size of 25 millisecondsand step size of 10 milliseconds. Following Hu et al. [45]the ground truth density maps are obtained by convolv-ing the head annotations with a 15 \xd715 Gaussian kernelK\u223cN (0,4.0).4.2. Backbone networksTransfer learning is used to train the ResNet152,DenseNet201 and InceptionV3 backbone networks for bothimage and audio classi\ufb01cation. The backbone networks areall pre-trained on the ImageNet dataset and the top layeris replaced. We found that instead of adding just one denselayer at the top, as is common in transfer learning, usingtwo dense layers and a dropout layer in between leads to ahigher accuracy in our case. The resulting network is thentrained using the Adam optimizer [60] with a learning rateof 10\u22124 and categorical cross-entropy loss function. Thelearning rate is reduced by a factor of 0.6 on plateau witha tolerance of 2 epochs, and an early stopping mechanismwith a tolerance of 5 epochs is used.The audiovisual crowd counting backbone is trained intwo stages. We \ufb01rst train a network with the AudioCSR-Net architecture described in Section 2.4 for 100 epochs.L2 norm is used as loss function and AdamW [61] with alearning rate of 10 \u22125 and weight decay of 10 \u22124 is used asoptimizer, where the learning rate is multiplied by a factorof 0.99 each epoch. This is the same training procedureused in the original paper [45]. Subsequently, in order toconvert the problem from dense prediction to regression,a dense layer with an output size of one is added after thelast layer of the trained AudioCSRNet. This layer is ini-tialized as a sum, meaning the initial weights are all equalto one and no bias is used. Then the entire network isre-trained for another 100 epochs using MAE as loss func-tion instead of the previous L2 loss, a learning rate of 10\u22126and weight decay of 10 \u22125. The learning rate is similarlymultiplied by a factor of 0.99 every epoch. The resultingmodel achieves a MAE of 13.63 which is even lower thanthe MAE of 14.27 reported in the original paper. However,the output of the network is just a single number repre-senting the total count instead of a density map. The \ufb01nalaccuracy of all trained backbones can be seen in Table 1.When training the backbone networks, in order to \ufb01tthe limitations of our available computational resources,the batch sizes are adjusted and some layers of the back-bone networks are frozen. All backbone networks weretrained with a batch size of 32 except AudioCSRNet whichhas a batch size of 4 as well as InceptionV3 when trainedon CIFAR-10 and CIFAR-100 which has a batch size of 64.All layers of the backbone networks were trained, exceptin the case of ResNet152 and DenseNet201 when trainedon CIFAR-10 and CIFAR-100 where only the batch nor-malization layers were trained. We found that trainingonly the batch normalization layers is su\ufb03cient to achievea high-performing backbone network in these cases [62].Table 1: Performance of backbone networks on each datasetBackboneCIFAR-10 Acc. CIFAR-100 Acc. SC Acc. GTZAN Acc. DISCO MAEResNet152 95.36% 82.25% 95.85% 91.29% -DenseNet20196.48% 82.53% 96.36% 92.09% -InceptionV396.56% 83.80% 94.93% 87.79% -AudioCSRNet- - - - 13.634.3. BranchesAll branches were trained from scratch using the Heinitialization method [63] and the Adam optimizer with alearning rate of 10 \u22124 where the learning rate is reducedby a factor of 0.6 on plateau with a tolerance of 2 epochs,and an early stopping mechanism with a tolerance of 5epochs is utilized. The branches on classi\ufb01cation back-bones use a categorical cross-entropy loss function whereasthe branches on the audiovisual crowd counting backboneuse mean absolute error loss. The training batch size forbranches were 64 in scenarios involving CIFAR-10, CIFAR-100 and Speech Commands, 32 in scenarios involvingGTZAN and 4 in scenarios involving DISCO.Table 2 shows the location of the branches placed oneach backbone network. For the AudioCSRNet backbonenetwork, branch V1 uses only the output of the VGG-16layers, therefore, it only has access to the visual features.Branch AV1 uses the outputs of both VGG-16 and VG-Gish, therefore it has access to both audio and visual fea-tures. In this branch location, the fusion of audio andvisual features is performed as described in Section 3 forthe SL-ViT architecture, and similar to the fusion blocks9--- Page 10 ---in AudioCSRNet for the CNN architecture, however, with-out dilation. Finally, branch AV2 is placed after the \ufb01rstfusion block in AudioCSRNet, therefore audio and visualfeatures have already been fused and thus fusion operationis not required within the branches. Adding branches afterthe second fusion block or later would not be reasonablesince more than 85% of the computation of the backboneis carried out before that point, and thus the accelerationresulting from early exits would be negligible.Table 2: Placement of branches for each backbone betworkBackbone BN \u2217 Branch Placed AfterDenseNet201 1 Transition Layer 12 Transition Layer 2ResNet152 1 12th Convolution2 36th ConvolutionInceptionV3 1 First Filter Concat2 Second Filter ConcatAudioCSRNet V1 Last Layer of VGGAV1 Last Layers of VGG and VGGishAV2 First Fusion Block\u2217Branch Number4.4. SL-ViT and CC-SL-ViT ParametersTable 3 summarizes the hyper-parameters used for theSL-ViT branches in each scenario. \u201cPatch Size\u201d shows thewidth and height of each image patch, \u201cPatches\u201d denotesthe resulting number of patches across width and heightof the input image, d is the size of embedding dimensionand h is the number of heads in multi-head attention.For copycat SL-ViT, images from the Tiny ImageNetdataset, which are the images from ImageNet down-sampledto 32\xd732, were given to the InceptionV3 backbone trainedon CIFAR-10, and the outputs were used to create the fakedataset. Then the fake dataset was mixed with CIFAR-10with a 2-to-1 ratio and used for re-training.Table 3: Hyper-parameters of SL-ViT for di\ufb00erent backbone net-works and branchesBackbone Dataset BN \u2217 Patch Size Patchesd hDenseNet201 all all 4x4 7x7 32 12ResNet152 SC 2 4x4 7x7 32 24GTZAN 2 4x4 7x7 32 24Other 4x4 7x7 32 12InceptionV3 CIFAR-100 all 5x5 5x5 36 8Other 5x5 5x5 32 12AudioCSRNet DISCO all 8x8 16x9 32 12\u2217Branch Number5. ResultsThe results of our experiments are presented in Tables4 to 8. In these Tables, the \ufb01nal accuracy, the total FLOPSof the model up to and including the branch and the num-ber of parameters of just the early exit branch are com-pared between the CNN architecture and the SL-ViT ar-chitecture. Higher accuracies, lower errors, lower numberof parameters and lower total FLOPS are highlighted inthese tables. Furthermore, the acceleration caused by SL-ViT early exits, de\ufb01ned as the total FLOPS of the back-bone network divided by the total FLOPS of the modelup to and including the SL-ViT branch, is also provided.Several observations can be made about these results.First, in all scenarios except one, SL-ViT early exits achievea signi\ufb01cantly higher accuracy. Even in the one excep-tional scenario, namely branch 2 of ResNet152 in Table6, the accuracy of SL-ViT is very close to its CNN coun-terpart. Secondly, while in some cases SL-ViT brancheshave an equal number of parameters compared to CNNbranches, in all scenarios, the total FLOPS of SL-ViTbranches is, Your response: ```ADD_PAPER\n2105.09121v3\nThis paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with only one transformer encoder layer. Extensive experiments on image classification (CIFAR-10/100), audio classification (Speech Commands, GTZAN), and audiovisual crowd counting demonstrate SL-ViT's effectiveness across modalities. Key results show SL-ViT achieves higher accuracy in 26/27 tested scenarios while reducing parameters and FLOPS. The paper also introduces a copycat fine-tuning strategy (CC-SL-ViT) that further improves performance. This work enables more efficient dynamic inference for edge devices through improved early exit architectures.\n```\n~~~~~~~~~~\nCurrent Step #34, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2105.09121v3\nNotes: \nYour previous command was: ```ADD_PAPER\n2105.09121v3\nThis paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with only one transformer encoder layer. Extensive experiments on image classification (CIFAR-10/100), audio classification (Speech Commands, GTZAN), and audiovisual crowd counting demonstrate SL-ViT's effectiveness across modalities. Key results show SL-ViT achieves higher accuracy in 26/27 tested scenarios while reducing parameters and FLOPS. The paper also introduces a copycat fine-tuning strategy (CC-SL-ViT) that further improves performance. This work enables more efficient dynamic inference for edge devices through improved early exit architectures.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 36:
\n~~~~~~~~~~\nHistory: Step #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification accuracy-latency tradeoffs, here was the response\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Zero-shot Multi-level Feature Transmission Policy Powered by Semantic Knowledge Base\nSummary: Remote zero-shot object recognition, i.e., offloading zero-shot object\nrecognition task from one mobile device to remote mobile edge computing (MEC)\nserver or another mobile device, has become a common and important task to\nsolve for 6G. In order to tackle this problem, this paper first establishes a\nzero-shot multi-level feature extractor, which projects the image into visual,\nsemantic, as well as intermediate feature space in a lightweight way. Then,\nthis paper proposes a novel multi-level feature transmission framework powered\nby semantic knowledge base (SKB), and characterizes the semantic loss and\nrequired transmission latency at each level. Under this setup, this paper\nformulates the multi-level feature transmission optimization problem to\nminimize the semantic loss under the end-to-end latency constraint. The\noptimization problem, however, is a multi-choice knapsack problem, and thus\nvery difficult to be optimized. To resolve this issue, this paper proposes an\nefficient algorithm based on convex concave procedure to find a high-quality\nsolution. Numerical results show that the proposed design outperforms the\nbenchmarks, and illustrate the tradeoff between the transmission latency and\nzero-shot classification accuracy, as well as the effects of the SKBs at both\nthe transmitter and receiver on classification accuracy.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.12619v1\n\nTitle: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices\nSummary: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\nPublication Date: 2017-01-02\narXiv paper ID: 1701.00485v2\n\nTitle: SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms\nSummary: We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12896v1\n, Your response: ```SUMMARY\nedge image classification energy-accuracy optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\nStep #26, Phase: literature review, Feedback: Successfully added paper 1905.04159v1, Your response: ```SUMMARY\nedge device image classification dynamic inference\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification dynamic inference, here was the response\nTitle: AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and Dynamic Model Placements on Edge Devices\nSummary: Many real-world applications are widely adopting the edge computing paradigm\ndue to its low latency and better privacy protection. With notable success in\nAI and deep learning (DL), edge devices and AI accelerators play a crucial role\nin deploying DL inference services at the edge of the Internet. While prior\nworks quantified various edge devices' efficiency, most studies focused on the\nperformance of edge devices with single DL tasks. Therefore, there is an urgent\nneed to investigate AI multi-tenancy on edge devices, required by many advanced\nDL applications for edge computing.\n  This work investigates two techniques - concurrent model executions and\ndynamic model placements - for AI multi-tenancy on edge devices. With image\nclassification as an example scenario, we empirically evaluate AI multi-tenancy\non various edge devices, AI accelerators, and DL frameworks to identify its\nbenefits and limitations. Our results show that multi-tenancy significantly\nimproves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI\nmulti-tenancy techniques also open up new opportunities for flexible deployment\nof multiple DL services on edge devices and AI accelerators.\nPublication Date: 2021-07-26\narXiv paper ID: 2107.12486v1\n\nTitle: Edge-PRUNE: Flexible Distributed Deep Learning Inference\nSummary: Collaborative deep learning inference between low-resource endpoint devices\nand edge servers has received significant research interest in the last few\nyears. Such computation partitioning can help reducing endpoint device energy\nconsumption and improve latency, but equally importantly also contributes to\nprivacy-preserving of sensitive data. This paper describes Edge-PRUNE, a\nflexible but light-weight computation framework for distributing machine\nlearning inference between edge servers and one or more client devices.\nCompared to previous approaches, Edge-PRUNE is based on a formal dataflow\ncomputing model, and is agnostic towards machine learning training frameworks,\noffering at the same time wide support for leveraging deep learning\naccelerators such as embedded GPUs. The experimental section of the paper\ndemonstrates the use and performance of Edge-PRUNE by image classification and\nobject tracking applications on two heterogeneous endpoint devices and an edge\nserver, over wireless and physical connections. Endpoint device inference time\nfor SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by\ncollaborative inference.\nPublication Date: 2022-04-27\narXiv paper ID: 2204.12947v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: AutoDiCE: Fully Automated Distributed CNN Inference at the Edge\nSummary: Deep Learning approaches based on Convolutional Neural Networks (CNNs) are\nextensively utilized and very successful in a wide range of application areas,\nincluding image classification and speech recognition. For the execution of\ntrained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud\nto the Edge. Unfortunately, deploying and inferring large, compute and memory\nintensive CNNs on edge devices is challenging because these devices typically\nhave limited power budgets and compute/memory resources. One approach to\naddress this challenge is to leverage all available resources across multiple\nedge devices to deploy and execute a large CNN by properly partitioning the CNN\nand running each CNN partition on a separate edge device. Although such\ndistribution, deployment, and execution of large CNNs on multiple edge devices\nis a desirable and beneficial approach, there currently does not exist a design\nand programming framework that takes a trained CNN model, together with a CNN\npartitioning specification, and fully automates the CNN model splitting and\ndeployment on multiple edge devices to facilitate distributed CNN inference at\nthe Edge. Therefore, in this paper, we propose a novel framework, called\nAutoDiCE, for automated splitting of a CNN model into a set of sub-models and\nautomated code generation for distributed and collaborative execution of these\nsub-models on multiple, possibly heterogeneous, edge devices, while supporting\nthe exploitation of parallelism among and within the edge devices. Our\nexperimental results show that AutoDiCE can deliver distributed CNN inference\nwith reduced energy consumption and memory usage per edge device, and improved\noverall system throughput at the same time.\nPublication Date: 2022-07-20\narXiv paper ID: 2207.12113v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n, Your response: ```FULL_TEXT\n2404.07217v2\n```\nStep #29, Phase: literature review, Feedback: Successfully added paper 2404.07217v2, Your response: ```SUMMARY\nedge device image classification early exit networks\n```\nStep #30, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification early exit networks, here was the response\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Resource-Constrained Edge AI with Early Exit Prediction\nSummary: By leveraging the data sample diversity, the early-exit network recently\nemerges as a prominent neural network architecture to accelerate the deep\nlearning inference process. However, intermediate classifiers of the early\nexits introduce additional computation overhead, which is unfavorable for\nresource-constrained edge artificial intelligence (AI). In this paper, we\npropose an early exit prediction mechanism to reduce the on-device computation\noverhead in a device-edge co-inference system supported by early-exit networks.\nSpecifically, we design a low-complexity module, namely the Exit Predictor, to\nguide some distinctly "hard" samples to bypass the computation of the early\nexits. Besides, considering the varying communication bandwidth, we extend the\nearly exit prediction mechanism for latency-aware edge inference, which adapts\nthe prediction thresholds of the Exit Predictor and the confidence thresholds\nof the early-exit network via a few simple regression models. Extensive\nexperiment results demonstrate the effectiveness of the Exit Predictor in\nachieving a better tradeoff between accuracy and on-device computation overhead\nfor early-exit networks. Besides, compared with the baseline methods, the\nproposed method for latency-aware edge inference attains higher inference\naccuracy under different bandwidth conditions.\nPublication Date: 2022-06-15\narXiv paper ID: 2206.07269v2\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n, Your response: ```FULL_TEXT\\n2311.18098v1\n```\nStep #31, Phase: literature review, Feedback: Error retrieving paper: Page request resulted in HTTP 400 (https://export.arxiv.org/api/query?search_query=&id_list=%5Cn2311.18098v1&sortBy=relevance&sortOrder=descending&start=0&max_results=100), Your response: ```SUMMARY\nedge image classification early exit dynamic inference\n```\nStep #32, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit dynamic inference, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\nStep #33, Phase: literature review, Feedback: 3\n--- Page 1 ---Single-Layer Vision Transformers for More Accurate Early Exits with Less OverheadArian Bakhtiarniaa,\u2217, Qi Zhanga, Alexandros Iosi\ufb01disaaDIGIT, Department of Electrical and Computer Engineering, Aarhus University, Finlandsgade 22, Aarhus, 8200, Midtjylland, DenmarkAbstractDeploying deep learning models in time-critical applications with limited computational resources, for instance in edgecomputing systems and IoT networks, is a challenging task that often relies on dynamic inference methods such as earlyexiting. In this paper, we introduce a novel architecture for early exiting based on the vision transformer architecture,as well as a \ufb01ne-tuning strategy that signi\ufb01cantly increase the accuracy of early exit branches compared to conventionalapproaches while introducing less overhead. Through extensive experiments on image and audio classi\ufb01cation as well asaudiovisual crowd counting, we show that our method works for both classi\ufb01cation and regression problems, and in bothsingle- and multi-modal settings. Additionally, we introduce a novel method for integrating audio and visual modalitieswithin early exits in audiovisual data analysis, that can lead to a more \ufb01ne-grained dynamic inference.Keywords: dynamic inference, early exiting, multi-exit architecture, vision transformer, multi-modal, deep learning1. IntroductionOver the past decade, deep learning has shown tremen-dous success across various \ufb01elds, such as computer vi-sion and natural language processing [1]. However, deeplearning models are by de\ufb01nition composed of many layersof interconnected neurons, even reaching billions of pa-rameters, which makes them computationally expensive.This has sparked a great deal of research in order to makedeep learning models more lightweight, for which manyapproaches have been proposed, for instance, model com-pression methods [2] such as quantization [3], pruning [4],low-rank approximation [5] and knowledge distillation [6].More and more emerging internet of things (IoT) appli-cations are integrating deep learning models, such as videosurveillance, voice assistants, augmented reality and coop-erative autonomous driving, which are often time-sensitiveand require inputs to be processed within speci\ufb01c deadlines[7, 8]. The heavy computational burden of deep learningbecomes problematic for these time-critical IoT applica-tions, due to resource-constrained IoT devices. Edge com-puting is a promising computing paradigm for addressingthis issue, in which the deep learning task is o\ufb04oaded toedge servers in the proximity of IoT devices.Since edge computing systems introduce computationo\ufb04oading over a communication network and involve mul-tiple nodes working collaboratively in order to completethe task in a timely manner, transmission time has tobe taken into account in addition to the deep learningcomputation time. However, transmission time may vary\u2217Corresponding authorEmail addresses: arianbakh@ece.au.dk (Arian Bakhtiarnia),qz@ece.au.dk (Qi Zhang), ai@ece.au.dk (Alexandros Iosi\ufb01dis)greatly over time and across di\ufb00erent channels. Conse-quently, deep learning models running on edge computingsystems and IoT networks should be capable of anytimeprediction, meaning they should be able to provide a validresponse even if they are interrupted before traversing theentire neural network, although the model is expected toprovide a better answer if it is allowed to run for longertime.Dynamic inference approaches [9] modify the computa-tion graph based on each input during the inference phasein order to \ufb01t the time constraints. A dynamic inferenceapproach that particularly suits anytime prediction isearlyexiting [10], also referred to as multi-exit architectures orauxiliary classi\ufb01ers in the literature. In multi-exit archi-tectures, one or more early exit branches are placed aftersome of the intermediate hidden layers of thebackbone net-work. The goal of each of these branches is to provide anearly result similar to the \ufb01nal result of the neural net-work using only the features extracted up to that particu-lar branch location. These early results are inevitably lessaccurate than the \ufb01nal result of the network. In order toachieve anytime prediction using early exiting, the latestearly result can be used whenever the execution is inter-rupted, for instance, whenever a hard deadline is reached.Computation time can be further decreased by applyingmodel compression techniques on the backbone of multi-exit architectures. Besides anytime prediction, early exit-ing can also be used in budgeted batch classi\ufb01cation wherea \ufb01xed amount of time is available in order to classify aset of input samples. In such a setting, the result of ear-lier branches can be used for \u201ceasier\u201d samples whereas theresult of later branches or the \ufb01nal result can be used for\u201charder\u201d ones. The di\ufb03culty of each sample can be de-termined based on the con\ufb01dence of the network about itsPreprint submitted to Neural Networks June 30, 2022arXiv:2105.09121v3  [cs.LG]  29 Jun 2022--- Page 2 ---output [11], although other approaches exist in the litera-ture [10].Early exit branches are expected to have a low over-head in terms of the extra computation they introduce,since a high overhead would defeat the purpose. There-fore, they often contain only a handful of layers. Ideally,we want the accuracy of the early results to be close to thatof the \ufb01nal result, since a higher accuracy for early exitbranches means that the overall reliability of the systemincreases. However, the low-overhead constraint makes itquite challenging to achieve a high accuracy since the earlyexit branches have signi\ufb01cantly less trainable parameterscompared to the rest of the network. Several approachesfor increasing the accuracy of early exits such as knowledgedistillation [12], curriculum learning [13] and architecturesdesigned speci\ufb01cally for early exit branches [14] have beensuggested. In this paper, we propose a novel architecturein order to obtain more accurate early exits for convolu-tional neural network (CNN) backbones.A neural architecture called vision transformer (ViT )[15] has been recently introduced for image classi\ufb01cationwhich is radically di\ufb00erent from convolutional neural net-works. The building blocks of Vision Transformer havebeen used for early exits placed on Vision Transformerbackbones [14], however, using Transformer-based earlyexit branches on CNN backbones is not intuitive and re-quires additional steps and architectural modi\ufb01cations. Weuse a modi\ufb01ed version of this architecture instead of theusual convolution and pooling layers in early exit branchesand show that our method can signi\ufb01cantly increase theaccuracy of early exits compared to conventional archi-tectures by fusing local and global receptive \ufb01elds 1. Thecontributions of this paper can be summarized as follows:\u2022 We propose a novel architecture for early exit branchesin multi-exit architectures based on vision transform-ers, called single-layer vision transformer (SL-ViT ).We compare our method with conventional CNN-based early exit architectures across 27 scenarios in-volving di\ufb00erent datasets, branch locations and back-bone networks and show that our method is signi\ufb01-cantly more accurate in 26 of these scenarios, whilehaving less overhead in terms of number of param-eters and \ufb02oating point operators (FLOPS). To thebest of our knowledge the fusion of global and localscope in early exits has never been used in multi-exitarchitectures before.\u2022 We show that our method is a general purpose ap-proach that works across di\ufb00erent modalities as wellas multi-modal settings by investigating image clas-si\ufb01cation, audio classi\ufb01cation and audiovisual crowdcounting scenarios. We also show that our methodworks for both classi\ufb01cation and regression prob-lems.1Our code will be available athttps://gitlab.au.dk/maleci/sl_vit.\u2022 We introduce a novel way of integrating audio andvisual features in early exits using vision transform-ers. To the best of our knowledge, this is the \ufb01rsttime early exits have been studied in multi-modalsettings.\u2022 We provide insight into why our method achievesbetter results compared to conventional CNN-basedarchitectures by investigating the role of attentionand receptive \ufb01eld.\u2022 We introduce a \ufb01ne-tuning strategy for SL-ViT calledcopycat single-layer vision transformer (CC-SL-ViT )which is based on the copycat strategy developed forCNNs [16] and show that this method can furtherincrease the accuracy of SL-ViT early exits. To thebest of our knowledge this is the \ufb01rst time the copy-cat strategy is used for vision transformers or earlyexits.The rest of this paper is organized as follows: Section2 provides an overview of the relevant literature; Section 3describes our proposed method in detail; Section 4 explainsthe details of our experiments; Section 5 showcases theexperiment results; and, \ufb01nally, Section 6 brie\ufb02y discussesthe results and concludes the paper.2. Related WorkThis section provides the necessary prerequisites forunderstanding our method and experiments. We startby describing the particulars of multi-exit architectures.Subsequently, we provide the details of the vision trans-former architecture, which is the foundation of the pro-posed method. Then, we brie\ufb02y touch on how audio clas-si\ufb01cation is normally carried out, which is included in sev-eral scenarios in our experiments. Finally, we explain an-other scenario investigated in our experiments, i.e. crowdcounting, and how it can be approached in a multi-modalmanner.2.1. Multi-Exit ArchitecturesIn order to describe multi-exit architectures, we use thesame notation as Scardapane et al. [10] where a neural net-work is formulated as a functionf(X) = fL(fL\u22121(...f1(X))).In this formulation Lsigni\ufb01es the total number of layers inthe network andfi is the operator corresponding to layeri,which can be a convolutional layer, a fully-connected layer,a normalization layer, or any other di\ufb00erentiable operator.hi = fi(hi\u22121) denotes the output of layeri, where h0 is theinput X. Finally, \u03b8i symbolizes the trainable parametersof layer i.Equation (1) formulates the training process for theneural network which is achieved by tuning its parametersusing an optimization algorithm on the landscape de\ufb01nedby a loss function. In this equation, the parameters of theneural network are denoted by \u03b8 = \u22c3Li=1 \u03b8i, the training2--- Page 3 ---samples are signi\ufb01ed by {(Xn,yn)}Nn=1, and l(\xb7,\xb7) is theloss function.f\u2217= arg min\u03b8N\u2211n=1l(yn,f(Xn)) (1)Extending this notation to multi-exit architectures,B \u2286{1,..,L }signi\ufb01es the set of selected branch locations afterwhich early exit branches will be placed. cb(hb) = yb is theclassi\ufb01er or regressor representing the early exit branch ateach branch location b, where yb denotes the early resultat that location. The schematic illustration of a multi-exitarchitecture is presented in Figure 1. However, since thereare multiple outputs, and thus multiple loss signals in amulti-exit architecture, its training is not as straightfor-ward.Figure 1: Schematic illustration of a multi-exit architecture with twoearly exits.Three di\ufb00erent approaches for training multi-exit ar-chitectures exist in the literature [10, 17, 13]. In the \ufb01rstapproach, called end-to-end training, the loss signals ofall exits are combined and backpropagated through thenetwork at the same time. With end-to-end training, thecontribution of each loss signal to the total loss is expressedwith weight values, which are therefore hyper-parametersof the model.The second approach, called layer-wise training, \ufb01rsttrains the network up to and including the \ufb01rst exit branch.Subsequently, the part of the network that has been trainedso far is frozen, meaning its parameters are not modi\ufb01edany further, and the remainder of the network up to andincluding the second exit branch is trained. This processcontinues until the entire network is trained. Note thatwith this approach, there is no guarantee that the accu-racy of the \ufb01nal exit remains unchanged.In the \ufb01nal approach, called classi\ufb01er-wise training,the backbone network is completely frozen and each branchis trained independent of the rest of the network and otherbranches, meaning the parameters \u03b8 are not modi\ufb01ed andonly the parameters of the classifers/regressors {cb},b \u2208Bare trained. With this approach, no new hyper-parametersare introduced and the backbone remains unchanged. How-ever, the early exit branches a\ufb00ect a lower number of train-able parameters compared to the other approaches.In this paper, we choose to follow the classi\ufb01er-wisetraining approach due to its practical importance. Thisis because with classi\ufb01er-wise training, early exit branchescan be easily added on top of existing backbone networkswithout the need for re-training and hyper-parameter op-timization, which can be computationally expensive andtime consuming. Furthermore, with end-to-end and layer-wise training strategies, the number of branches and theirplacement can lead to further trade-o\ufb00s and a\ufb00ect theoverall performance of the model. Since branches are inde-pendently trained in the classi\ufb01er-wise strategy, any num-ber of branches can exist and a branch can be placed atany location without a\ufb00ecting the performance of otherbranches or the backbone.It is important to mention that branches placed laterin the backbone network do not necessarily result in ahigher accuracy compared to branches placed earlier. Theusage of such branches would therefore not be sensiblesince earlier branches exist that require less computationand provide more accurate results. We hereby use the termimpractical to refer to such branches.As previously mentioned, there are several methodsthat try to improve the accuracy of early exits. The methodin [12] uses the combination of the distillation loss fromthe \ufb01nal exit and the loss signal from ground truth labelsto train more accurate early exits using in the end-to-endtraining setting. The method in [18] expands on this ideaby adding a third loss signal based on the di\ufb00erence be-tween features of the latest early exit with earlier exits.The method in [19] proposes a technique called gradientequilibrium to combat the problem of gradient imbalancethat surfaces when using the end-to-end strategy, whichis when the variance of the gradients becomes very largewhen loss signals from multiple exits are combined, lead-ing to unstable training. Moreover, this paper introducesforward and backward knowledge transfer that aims to en-courage collaboration among di\ufb00erent exits. The methodin [20] improves the accuracy of later exits by reusing pre-dictions from earlier exits. The method in [21] circumventsthe problem of impractical branches by adaptively select-ing the exit location based on time budget and the speci\ufb01cinput. The method in [22] simpli\ufb01es the design of multi-exit architectures by removing the hyper-parameters of theend-to-end training strategy that specify the contributionof each loss signal.Besides e\ufb03cient inference, early exits can prove use-ful in several other applications, for instance, the methodin [23] allows for parallel training of the segments of theDNN that exist between early exits, by training each seg-ment based on the loss signal of the next segment obtainedin the previous training stage. Moreover, early exits canbe added to the network during the training in order toincrease the accuracy of the backbone network and dis-carded after the training phase, for instance, the widelyused Inception model [24] was trained in this way.Besides early exiting, several other approaches existfor dynamic inference, for instance, layer skipping [25, 26,27, 28] where the execution of some of the layers of theDNN are skipped, and channel skipping [29] where lessimpactful channels of convolutional neural networks areignored and their computation is skipped during the infer-3--- Page 4 ---ence phase. However, unlike early exits, these approachescannot provide an output if the execution is interrupteddue to a strict deadline, as these methods need to performthe computations until the very last layer.2.2. Vision TransformerThe transformer architecture was \ufb01rst introduced byVaswani et al. [30] for natural language processing, andit has recently been adapted for solving computer visionproblems by Dosovitskiy et al. [15]. Vision transformerwas originally developed for the problem of image clas-si\ufb01cation, however, variations of vision transformer havesince been applied to many computer vision problems, suchas object detection, depth estimation, semantic segmenta-tion, image generation and action recognition, as well asmulti-modal data analysis tasks such as text-to-image syn-thesis and visual question answering [31, 32, 33].In order to describe the vision transformer architecture,we \ufb01rst explain the self-attention layer. The input of thislayer is in the form of a sequence X = (x1,...,x n) whereX \u2208Rn\xd7d and dis the embedding dimension to representeach entity. Its output is in the form of Z = (z1,...,z n)where Z \u2208Rn\xd7dv . The goal of self-attention is to capturethe interaction between the entities in the sequence. Forthis purpose, each vector xi in the sequence is transformedinto three separate vectors: the query vector qi \u2208Rdq ,the key vector ki \u2208Rdk and the value vector vi \u2208Rdv ,where dq = dk. To construct the output vector zi thatcorresponds to the input xi, for each vector xj in X (in-cluding xi itself), the scalar aij is calculated by the innerproduct of qi and kj. Output vector zi is then calculatedby summing the value vectors v1,...,v n weighted by theircorresponding scalars, that is, zi = \u2211nj=1 aijvj. The scalaraij basically speci\ufb01es how much attention the i-th entityshould pay to the j-th entity, since aij determines the con-tribution of vj to the combined output zi. In practice, thescalars are normalized by \u221adk and converted into proba-bilities using the softmax function.If the key, query and value vectors are packed into ma-trices Q= XWQ, K = XWK and V = XWV, where WQ,WK and WV are learnable weight matrices, the above op-eration can be rephrased as follows:Z = softmax(QKT\u221adk)V (2)In order to enable the model to capture more than onetype of relationship between the entities in the sequence,self-attention is extended to multi-head attention by con-catenating the output of h di\ufb00erent self-attention blocksZ1,...,Z h each with its own set of learnable weight ma-trices, into a single matrix Z\u2032 = [Z0,...,Z h] \u2208Rn\xd7h.dv ,which is then projected using a weight matrixW\u2032\u2208Rh.dv\xd7d.A transformer encoder is constructed by passing theinput sequence into a normalization layer, a multi-headattention layer, a second normalization layer and a multi-layer perceptron (MLP), respectively. Two residual con-nections are added, one by adding the input sequence tothe output of the multi-head attention, and the other byadding the output of the multi-head attention to the out-put of the MLP.Putting it all together, a vision transformer is createdby \ufb01rst splitting the input image into patches. Subse-quently, the sequence of patches is projected into a se-quence of vectors and a positional embedding is addedto the corresponding vector of each patch. An additionallearnable embedding called classi\ufb01cation token is added tothe beginning of the sequence. The sequence then passesthrough L transformer encoders. Finally, the \ufb01rst vectorin the output of the last transformer encoder, which cor-responds to the classi\ufb01cation token, is passed to a MLPwhich outputs the \ufb01nal classi\ufb01cation result. The architec-ture of vision transformer is depicted in Figure 2.Figure 2: The vision transformer (ViT) architecture for image clas-si\ufb01cation.ViT-EE is a method which uses transformer encodersfor early exits placed on ViT backbones [14]. ViT-EE usesthe exact same layer as the ViT backbone. Using the build-ing blocks of the backbone network for early exit branchesis simple and intuitive, and it is the reason why so far,mostly convolutional layers have been used for early exit-ing CNN backbones. However, as we show in this work,carefully designing the architecture of early exit branchescan lead to signi\ufb01cant improvements. Using Transformer-based early exit branches on CNN backbones is not intu-itive, and requires additional steps such as converting ten-sors to patches, dealing with the classi\ufb01cation token and\ufb01ne-tuning the architecture parameters including patchsize, attention heads, embedding representation, the sizeand number of layers for MLP, and dropout. Moreover,we show that removing the last residual connection in thetransformer encoder can improve the performance in somecases.Furthermore, ViT backbones have a global receptive\ufb01eld in every layer, this means that ViT-EE is not neces-sarily ideal for early exits at all layers, as it adds too muchoverhead without providing improvements in terms of re-4--- Page 5 ---ceptive \ufb01eld. On the other hand, CNN backbones have alimited receptive \ufb01eld particularly in earlier layers, there-fore fusing this receptive \ufb01eld with a global one leads toimprovements.2.3. Audio Classi\ufb01cationSimilar to image classi\ufb01cation, audio classi\ufb01cation isthe problem of categorizing a given audio waveform intoone of several predetermined classes. For instance, thegiven audio waveform could be a musical recording, andthe goal could be to specify which genre of music it belongsto. To represent the input features, spectrogramsobtainedby applying short-time Fourier transform (STFT) andMelspectrograms are commonly used [34], although raw audiowaveforms can been used as well [35]. Mel spectrogramsare spectrograms that are constructed using the Mel scalewhich is a nonlinear transformation of the frequency scaledesigned based on domain knowledge about the humanauditory system. Various deep learning models for au-dio classi\ufb01cation exist in the literature, including modelsthat are commonly used for image classi\ufb01cation, namelyResNet [36], DenseNet [37] and Inception [38], which havebeen shown to be quite e\ufb00ective for audio classi\ufb01cationas well [39]. Conveniently, the same three networks havepreviously been used as backbone networks when investi-gating early exiting for image classi\ufb01cation [13]. Thereforewe use these backbone networks for both image and audioclassi\ufb01cation in our experiments.2.4. Audiovisual Crowd CountingCrowd counting refers to the problem of identifying thetotal number of people present in a given image. Crowdcounting has many applications such as safety monitoring,disaster management, design of public spaces, intelligencegathering and analysis, creation of virtual environmentsand forensic search [40]. With many of these applications,it is vital for the model to perform in near real-time. How-ever, the input images in these scenarios often have highresolutions, such as HD or Full HD. Moreover, many ofthe available methods contain an immense number of pa-rameters [41]. This means that crowd counting modelsare often very computationally expensive, therefore, dy-namic inference methods such as early exiting and otherlightweight deep learning methods become essential in realworld applications.Although the main objective of this task is to obtain asingle count from an image, many methods treat this prob-lem as dense prediction where the output is a density mapdepicting the density of the crowd across the input image,and the total count is calculated by the sum of all valuesin the density map. Therefore, in most crowd countingdatasets, such as Shanghai Tech [42] and World Expo \u201910[43], the locations of the heads of individuals in the imageare annotated and provided as targets. A ground truthdensity map can then be obtained from these head anno-tations using Gaussian kernels or more complicated andFigure 3: An example image from the Shanghai Tech dataset and itscorresponding ground truth density map.specialized methods [41]. Figure 3 shows an image fromthe Shanghai Tech dataset and the ground truth densitymap that was generated from the provided head annota-tions using the method presented in Zhang et al [42]. Incrowd counting, Mean Absolute Error (MAE) is usuallyused as a measure of accuracy whereas Mean Squared Er-ror (MSE) is used as a measure of robustness [44].Many crowd counting methods exist in the literature[41], however, most of these methods are applied in asingle-modal fashion where the input is an image or a videoframe. In contrast, AudioCSRNet [45], a multi-modal ex-tension of the widely-used CSRNet model for crowd count-ing [46], takes as input the ambient audio of a scene inaddition to its image. The authors show that the ambi-ent audio improves the result in situations where the im-age quality is not ideal, for instance, low image resolution,presence of noise, occlusion and low illumination.In CSRNet, the features extracted from the input im-age by the \ufb01rst 10 layers of a VGG-16 [47] network pre-trained on the ImageNet dataset [48] are passed through6 dilated convolution layers and a 1 \xd71 convolution layerin order to obtain the density map. AudioCSRNet ex-tends this architecture by converting each of the dilatedconvolution layers into a fusion block. The architecture ofAudioCSRNet is depicted in Figure 4. First, a Mel spec-trogram is obtained from the raw audio waveform. Subse-quently, in each fusion block, the features extracted fromthe input Mel spectrogram by the \ufb01rst 6 layers of a VG-Gish [49] network pre-trained on the AudioSet dataset [49]are projected to two vectors called \u03b3 and \u03b2 which repre-sent the multiplicative and additive aspects of the audiofeatures. The \u03b3 and \u03b2 vectors are then tiled in order tomatch the size of the visual features. Finally, the outputof the dilated convolution is element-wise multiplied by \u03b3and added to \u03b2.The fusion operation can be summarized asvl+1 = Fl(\u03b3l \u2299Dl(vl) + \u03b2l), (3)where vl \u2208 RCl\xd7Wl\xd7Hl is the output of the l-th fusionblock, Fl denotes an activation function, \u03b3l and \u03b2l are thetiled vectors and Dl represents the l-th dilated convolu-tion.In practice, a batch normalization layer [50] is addedimmediately after each dilated convolution. Furthermore,the height and width of the intermediate features remain5--- Page 6 ---Figure 4: Architecture of AudioCSRNet.unchanged by using padding in the convolution operations,meaning Hl = Hl+1 and Wl = Wl+1. Additionally, sincethe \ufb01rst 10 layers of VGG-16 decrease both height andwidth by a factor of 8 via several pooling operations, the\ufb01nal result of the network needs to be upsampled by afactor of 8 in order to match the resolution of the inputimage. It is important to preserve the total sum of thedensity map during this upsampling operation, since itrepresents the total count.3. Single-Layer Vision Transformers for Early Ex-itsWe assume a pre-trained and high performing back-bone network is already available. Due to time constraintsarising from the particular application, it is desirable thatthe network provides a result within the speci\ufb01c deadlinerather than not providing a result at all, even though thisresult may be less accurate than it would be if time con-straints did not exist. Therefore, the backbone needs tobe augmented with early exit branches to allow for dy-namic inference and anytime prediction. As previouslymentioned, we use the classi\ufb01er-wise approach for trainingthe early exit branches since it results in \u201cplug-and-play\u201dbranches that can easily be added to the backbone networkwithout any re-training or hyper-parameter tuning.3.1. SL-ViTTypically, the architecture of early exit branches startswith one or more convolution layers, although some mayhave no convolutions at all. Afterwards, they may have apooling layer, which may be global pooling, and one MLP[51, 11]. Here, as a baseline, we choose to utilize the ar-chitecture depicted in Figure 5 with one 3 \xd73 convolution,followed by a 2 \xd72 max pooling layer and \ufb01nally a MLP.The size of the max pooling layer is increased to 4 \xd74 forcrowd counting since the input images have a very highresolution. Additionally, we use dropout [52] inside theMLP to avoid over\ufb01tting. We use a single convolutionsince early exits with two or more convolution layers havea high overhead and may even lead to lower accuracy [11].Early exits without convolutions are sometimes used verylate in the network, however, since they are straightfor-ward and leave no room for modi\ufb01cations, we do not applyour method for such cases. The resulting architecture is acommon setup within the literature, and is e\ufb00ectively thesame architecture used for earlier exits by Hu et al. [51].Figure 5: Architecture of CNN early exit branches. Size of the \ufb02at-tened feature vector depends on the dimensions of the features at thespeci\ufb01c branch location. For branches placed on the AudioCSRNetbackbone, max pooling size is increased to 4x4 since the input im-ages have a high resolution. Figure created using the NN-SVG tool[53].Our method called single-layer vision transformer orSL-ViT for short, is an alternative architecture for earlyexit branches that can achieve a higher accuracy comparedto the aforementioned baseline, while having less overheadin terms of the number of parameters as well as \ufb02oatingpoint operations per second (FLOPS). Our proposed ar-chitecture is based on the vision transformer architectureintroduced in section 2.2, where instead of the input im-age, we split the intermediate features at the branch loca-tion into patches (sub-tensors) and pass them to a visiontransformer.The choice of vision transformer architecture is pri-marily due to its global receptive \ufb01eld. Receptive \ufb01eld6--- Page 7 ---is crucial in many deep learning problems, including onesstudied in this work. The receptive \ufb01eld of state-of-the-art CNNs developed for image classi\ufb01cation has steadilyincreased over time and is correlated with increased classi-\ufb01cation accuracy [54]. Additionally, in audio classi\ufb01cationusing spectrograms, each location relates to a di\ufb00erent fre-quency band in a di\ufb00erent window of time. It is reasonableto assume that processing combinations of frequencies andwindows that are not necessarily adjacent could be of im-portance. Moreover, many crowd counting methods havemade use of global information through visual attentionmechanisms and dilated convolutions [41]. Since the re-ceptive \ufb01eld is particularly limited in early layers of CNNbackbones, choosing an architecture for early exit brancheswith a global receptive \ufb01eld could be bene\ufb01cial.Many other designs strive to increase the receptive \ufb01eldin their building blocks, for instance, the pyramid pool-ing module (PPM) in PSPNet [55] or atrous spatial pyra-mid pooling (ASPP) in DeepLab [56]. However, they allfall short in comparison with the global receptive \ufb01eld oftransformers. PPM increases the receptive \ufb01eld throughaggregating di\ufb00erent levels of pooling, which means far lo-cations have only access to coarse representations of eachother, and ASPP has holes in its receptive \ufb01eld.It is important to mention that the local receptive \ufb01eldof convolutional layers is not fundamentally bad. On thecontrary, it plays a key role in representation learning andextracting local information, especially in the early layersof the network where the receptive \ufb01eld of the convolu-tional \ufb01lters is small. Filters in successive convolutionallayers have increasingly larger receptive \ufb01elds, therefore,\ufb01nal layers in a CNN architecture have \ufb01lters of largeenough receptive \ufb01elds that can e\ufb00ectively aggregate in-formation from the entire input image to provide a properresponse. However, this process of cascading local recep-tive \ufb01elds of increasing size requires the number of layersin the CNN to be large, or at least all the layers in thenetwork to be traversed in order to provide the network\u2019sresponse. When an early exit is added at an early layer,this chain of increasingly larger receptive \ufb01elds is broken,and an early exit that has a local receptive \ufb01eld may notbe able to e\ufb00ectively aggregate all required informationin the image to provide a suitable response. This situa-tion is the motivation behind the proposed branch archi-tecture, which fuses the local receptive \ufb01eld of the layerin the network where the early exit branch is attached,with the global receptive \ufb01eld of the early exit, in orderto e\ufb00ectively aggregate information from the entire inputand provide a more accurate response. Indeed, the orig-inal Vision Transformer paper [15] attributes the successof their model to the combination of local and global re-ceptive \ufb01elds and shows that even in very early layers, thisability to integrate information globally is indeed used bythe model.There are some crucial di\ufb00erences between the origi-nal vision transformer and the architecture in our method.First, in order to introduce a low overhead for early exitbranches, we only use a single transformer encoder layerinstead of the original 12 to 36 layers, meaning that L= 1in our case. Secondly, we do not utilize a separate clas-si\ufb01cation token and instead pass the entire output of thetransformer encoder layer to the MLP head. This is pos-sible because the width and height of tensors are gener-ally reduced throughout CNN backbones by pooling oper-ations, and thus the number of patches in our architectureis lower than that of the original vision transformer. Inaddition to the number of patches, the size of the embed-ding dimension ( d) is also reduced in our proposed archi-tecture, introducing far less parameters when passing theentire output of the last transformer encoder layer to theMLP head, even with high-resolution inputs such as inour crowd counting experiments. Variations of our archi-tecture have 5 \xd75, 7 \xd77 or 16 \xd79 patches and embeddingdimensions of 32 or 36, whereas di\ufb00erent versions of theoriginal vision transformer have 14 \xd714 or 16 \xd716 patchesand embedding dimensions of 768, 1024 or 1280. We em-pirically found that using the entire transformer encoderoutput instead of just one classi\ufb01cation token can increasethe accuracy, perhaps because in a single-layer version,there are not enough layers for the classi\ufb01cation token tolearn to properly summarize other patches. Our proposedarchitecture is shown in Figure 6. It is also important tonote that the MLP head used in our architecture is exactlythe same as the MLP in the CNN early exit architecture.Figure 6: Architecture of SL-ViT early exit branches. Unlike typicalvision transformers, only a single transformer encoder layer is used,extra learnable classi\ufb01cation token is not added to the sequence andthe entire output of the transformer encoder is passed on to the MLPhead. The MLP head is the same as CNN early exit branches.Our model has several hyper-parameters, namely thesize of each patch, the embedding dimension d and thenumber of attention heads hin multi-head attention. Thepatch size creates a trade-o\ufb00 where smaller patches resultin a more \ufb01ne-grained attention mechanism while increas-ing the total number of parameters in a bi-quadratic fash-ion. Therefore, similar to the original vision transformer,we choose the size of the patch to be close to the square7--- Page 8 ---root of the height and width of the input features. We alsomake sure that the size of the patch can divide the size ofthe input features to avoid padding, for instance, a patchsize of 4 \xd74 for input features of size 28 \xd728. We performa grid search to \ufb01nd the values of d and h that result inthe highest accuracy, while keeping the total number ofparameters less than or equal to that of the CNN earlyexit counterpart.At a \ufb01rst glance, it might seem like the SL-ViT archi-tecture introduces more hyper-parameters than the con-ventional CNN architecture, however, the CNN architec-ture includes many design choices as well, such as the num-ber of \ufb01lters, \ufb01lter size, padding, dilation, stride, poolingtype and pooling size. The design choices for CNN archi-tectures might seem simpler since they have been studiedmore extensively compared to vision transformers whichwere introduced more recently.3.2. Audiovisual SL-ViTWith audiovisual backbones such as the AudioCSR-Net model for audiovisual crowd counting, described insection 2.4, it is desirable to have audiovisual early exitsthat use both visual and audio features in order to achievea higher accuracy. The simplest way to have such branchesis to add the branches after the blocks where the fusionof visual and audio features take place. However, withour proposed SL-ViT architecture, it is also possible toinclude audio features as one or more patches alongsideother patches, and directly fuse the features in the earlyexit. The advantage of this approach is that since in vi-sion transformers, any of the patches can pay attention toany other patch, the visual features can be fused with theaudio features without being directly impacted and mod-i\ufb01ed. In contrast, since convolutional \ufb01lters only take theimmediate vicinity into account, the audio features mustbe present in every location. One option is to concatenatethe visual features and the tiled audio features along thedepth. However, that would greatly increase the amount ofcomputation for each fusion operation, therefore intrusiveoperations such as element-wise multiplication and addi-tion are used instead.3.3. Copycat SL-ViTFinally, we introduce a \ufb01ne-tuning strategy for SL-ViTbranches that can further increase their accuracy. Correia-Silva et al. [16] developed a method called copycat CNNwhere they create a \u201cfake\u201d dataset by taking images fromanother domain, giving them as input to a network trainedon the target domain, and recording the output of thenetwork as labels for these images. For instance, imagesfrom the ImageNet dataset [48] can be given to a networktrained on the CIFAR-10 dataset [57], where the imageof a camel may be labelled as a \u201cdog\u201d since there are nolabels for \u201ccamel\u201d in CIFAR-10. This fake dataset is thencombined with a dataset for the target domain and usedto train a new network. We use this strategy to \ufb01ne-tunean already trained SL-ViT branch and obtain a copycatsingle-layer vision transformer (CC-SL-ViT ). Note thatthe ratio of the fake data mixed with the available datasetis a hyper-parameter of this \ufb01ne-tuning strategy.4. Experimental SetupIn this section, we provide the details of our experi-ments. We begin by giving a short summary of the datasetsas well as the training details for the backbone networks.We then lay out the details of the branch architectures,their training procedure and their placement on the back-bone networks, and \ufb01nally explain how the copycat strat-egy was used to \ufb01ne-tune the branches.A total of 27 di\ufb00erent scenarios were tested in our ex-periments. For both image and audio classi\ufb01cation, twodatasets, three backbone networks and two di\ufb00erent branchlocations on each backbone were tested. In addition, threedi\ufb00erent branch locations for the audiovisual crowd count-ing backbone network were covered. All experiments wererepeated 5 times and the average accuracy as well as thestandard deviation were recorded. 4 \xd7 Nvidia 2080TiGPUs were used for the training of our models.4.1. Datasets4.1.1. CIFAR-10 and CIFAR-100These are widely-used datasets for image classi\ufb01cation[57]. Both datasets consist of 60,000 color images of size32 \xd732 pixels and their corresponding class labels. Theimages in CIFAR-10 and CIFAR-100 are categorized into10 and 100 di\ufb00erent classes, respectively. We use 40,000examples for training, 10,000 for validation and another10,000 for testing. Since our backbone networks are pre-trained on ImageNet which consists of 224 \xd7224 pixelimages, we resize each image to these dimensions beforepassing them into the network.4.1.2. Speech Commands (SC)A well-known audio dataset of spoken words [58]. Itconsists of 100,503 1-second audio clips with a samplingrate of 16kHz, each labelled as one of 12 classes: 10 dif-ferent spoken words such as \u201cYes\u201d, \u201cNo\u201d, \u201cDown\u201d and\u201cStop\u201d as well as one class for background noise and an-other for unknown words. We use 85,511 examples fortraining, 10,102 for validation and 4,890 for testing. Weconvert the raw audio waveforms into spectrograms usingshort-time Fourier transform (STFT) with a window sizeof 255 samples and step size of 128 samples, and resize theresulting spectrograms to 224 \xd7224 before passing theminto the network.4.1.3. GTZANIt is the most widely-used dataset for music genre recog-nition [59]. The original dataset consists of 10 genres suchas \u201cPop\u201d and \u201cRock\u201d and 100 30-second audio clips pergenre with a sampling rate of 22,050Hz. We follow the8--- Page 9 ---common approach to split each audio clip into 10 separate3-second clips in order to increase the size of the datasetto 10,000. We use 8,000 examples for training, 1,000 forvalidation and another 1,000 for testing. Following the ap-proach of Palanisamy et al. [39] where di\ufb00erent spectro-grams with di\ufb00erent parameters are placed in each channelof the input image, we use one spectrogram obtained fromSTFT with window size of 512 samples and step size of 256samples as well as two Mel spectrograms with 128 bins andwindow sizes of 100 and 50 milliseconds, and step sizes of50 and 25 milliseconds, respectively.4.1.4. DISCOAn audiovisual dataset for crowd counting which con-tains 1,935 images of Full HD resolution (1920 \xd71080)[45]. For each image, a corresponding 1-second audio clipof ambient sounds with a sampling rate of 48kHz, starting0.5 seconds before the image was taken and ending 0.5 sec-onds afterwards, exists as well. The labels are provided inthe form of head annotations in the image. At the time ofthis writing, DISCO is the only publicly available datasetfor audiovisual crowd counting. We use 1435 examples fortraining, 200 for validation and 300 for testing. The inputimage is resized to 1024 \xd7576 pixels to reduce memoryand computation requirements. Similar to Hershey et al.[49], the input audio waveform is transformed into a Melspectrogram with 64 bins, window size of 25 millisecondsand step size of 10 milliseconds. Following Hu et al. [45]the ground truth density maps are obtained by convolv-ing the head annotations with a 15 \xd715 Gaussian kernelK\u223cN (0,4.0).4.2. Backbone networksTransfer learning is used to train the ResNet152,DenseNet201 and InceptionV3 backbone networks for bothimage and audio classi\ufb01cation. The backbone networks areall pre-trained on the ImageNet dataset and the top layeris replaced. We found that instead of adding just one denselayer at the top, as is common in transfer learning, usingtwo dense layers and a dropout layer in between leads to ahigher accuracy in our case. The resulting network is thentrained using the Adam optimizer [60] with a learning rateof 10\u22124 and categorical cross-entropy loss function. Thelearning rate is reduced by a factor of 0.6 on plateau witha tolerance of 2 epochs, and an early stopping mechanismwith a tolerance of 5 epochs is used.The audiovisual crowd counting backbone is trained intwo stages. We \ufb01rst train a network with the AudioCSR-Net architecture described in Section 2.4 for 100 epochs.L2 norm is used as loss function and AdamW [61] with alearning rate of 10 \u22125 and weight decay of 10 \u22124 is used asoptimizer, where the learning rate is multiplied by a factorof 0.99 each epoch. This is the same training procedureused in the original paper [45]. Subsequently, in order toconvert the problem from dense prediction to regression,a dense layer with an output size of one is added after thelast layer of the trained AudioCSRNet. This layer is ini-tialized as a sum, meaning the initial weights are all equalto one and no bias is used. Then the entire network isre-trained for another 100 epochs using MAE as loss func-tion instead of the previous L2 loss, a learning rate of 10\u22126and weight decay of 10 \u22125. The learning rate is similarlymultiplied by a factor of 0.99 every epoch. The resultingmodel achieves a MAE of 13.63 which is even lower thanthe MAE of 14.27 reported in the original paper. However,the output of the network is just a single number repre-senting the total count instead of a density map. The \ufb01nalaccuracy of all trained backbones can be seen in Table 1.When training the backbone networks, in order to \ufb01tthe limitations of our available computational resources,the batch sizes are adjusted and some layers of the back-bone networks are frozen. All backbone networks weretrained with a batch size of 32 except AudioCSRNet whichhas a batch size of 4 as well as InceptionV3 when trainedon CIFAR-10 and CIFAR-100 which has a batch size of 64.All layers of the backbone networks were trained, exceptin the case of ResNet152 and DenseNet201 when trainedon CIFAR-10 and CIFAR-100 where only the batch nor-malization layers were trained. We found that trainingonly the batch normalization layers is su\ufb03cient to achievea high-performing backbone network in these cases [62].Table 1: Performance of backbone networks on each datasetBackboneCIFAR-10 Acc. CIFAR-100 Acc. SC Acc. GTZAN Acc. DISCO MAEResNet152 95.36% 82.25% 95.85% 91.29% -DenseNet20196.48% 82.53% 96.36% 92.09% -InceptionV396.56% 83.80% 94.93% 87.79% -AudioCSRNet- - - - 13.634.3. BranchesAll branches were trained from scratch using the Heinitialization method [63] and the Adam optimizer with alearning rate of 10 \u22124 where the learning rate is reducedby a factor of 0.6 on plateau with a tolerance of 2 epochs,and an early stopping mechanism with a tolerance of 5epochs is utilized. The branches on classi\ufb01cation back-bones use a categorical cross-entropy loss function whereasthe branches on the audiovisual crowd counting backboneuse mean absolute error loss. The training batch size forbranches were 64 in scenarios involving CIFAR-10, CIFAR-100 and Speech Commands, 32 in scenarios involvingGTZAN and 4 in scenarios involving DISCO.Table 2 shows the location of the branches placed oneach backbone network. For the AudioCSRNet backbonenetwork, branch V1 uses only the output of the VGG-16layers, therefore, it only has access to the visual features.Branch AV1 uses the outputs of both VGG-16 and VG-Gish, therefore it has access to both audio and visual fea-tures. In this branch location, the fusion of audio andvisual features is performed as described in Section 3 forthe SL-ViT architecture, and similar to the fusion blocks9--- Page 10 ---in AudioCSRNet for the CNN architecture, however, with-out dilation. Finally, branch AV2 is placed after the \ufb01rstfusion block in AudioCSRNet, therefore audio and visualfeatures have already been fused and thus fusion operationis not required within the branches. Adding branches afterthe second fusion block or later would not be reasonablesince more than 85% of the computation of the backboneis carried out before that point, and thus the accelerationresulting from early exits would be negligible.Table 2: Placement of branches for each backbone betworkBackbone BN \u2217 Branch Placed AfterDenseNet201 1 Transition Layer 12 Transition Layer 2ResNet152 1 12th Convolution2 36th ConvolutionInceptionV3 1 First Filter Concat2 Second Filter ConcatAudioCSRNet V1 Last Layer of VGGAV1 Last Layers of VGG and VGGishAV2 First Fusion Block\u2217Branch Number4.4. SL-ViT and CC-SL-ViT ParametersTable 3 summarizes the hyper-parameters used for theSL-ViT branches in each scenario. \u201cPatch Size\u201d shows thewidth and height of each image patch, \u201cPatches\u201d denotesthe resulting number of patches across width and heightof the input image, d is the size of embedding dimensionand h is the number of heads in multi-head attention.For copycat SL-ViT, images from the Tiny ImageNetdataset, which are the images from ImageNet down-sampledto 32\xd732, were given to the InceptionV3 backbone trainedon CIFAR-10, and the outputs were used to create the fakedataset. Then the fake dataset was mixed with CIFAR-10with a 2-to-1 ratio and used for re-training.Table 3: Hyper-parameters of SL-ViT for di\ufb00erent backbone net-works and branchesBackbone Dataset BN \u2217 Patch Size Patchesd hDenseNet201 all all 4x4 7x7 32 12ResNet152 SC 2 4x4 7x7 32 24GTZAN 2 4x4 7x7 32 24Other 4x4 7x7 32 12InceptionV3 CIFAR-100 all 5x5 5x5 36 8Other 5x5 5x5 32 12AudioCSRNet DISCO all 8x8 16x9 32 12\u2217Branch Number5. ResultsThe results of our experiments are presented in Tables4 to 8. In these Tables, the \ufb01nal accuracy, the total FLOPSof the model up to and including the branch and the num-ber of parameters of just the early exit branch are com-pared between the CNN architecture and the SL-ViT ar-chitecture. Higher accuracies, lower errors, lower numberof parameters and lower total FLOPS are highlighted inthese tables. Furthermore, the acceleration caused by SL-ViT early exits, de\ufb01ned as the total FLOPS of the back-bone network divided by the total FLOPS of the modelup to and including the SL-ViT branch, is also provided.Several observations can be made about these results.First, in all scenarios except one, SL-ViT early exits achievea signi\ufb01cantly higher accuracy. Even in the one excep-tional scenario, namely branch 2 of ResNet152 in Table6, the accuracy of SL-ViT is very close to its CNN coun-terpart. Secondly, while in some cases SL-ViT brancheshave an equal number of parameters compared to CNNbranches, in all scenarios, the total FLOPS of SL-ViTbranches is, Your response: ```ADD_PAPER\n2105.09121v3\nThis paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with only one transformer encoder layer. Extensive experiments on image classification (CIFAR-10/100), audio classification (Speech Commands, GTZAN), and audiovisual crowd counting demonstrate SL-ViT's effectiveness across modalities. Key results show SL-ViT achieves higher accuracy in 26/27 tested scenarios while reducing parameters and FLOPS. The paper also introduces a copycat fine-tuning strategy (CC-SL-ViT) that further improves performance. This work enables more efficient dynamic inference for edge devices through improved early exit architectures.\n```\nStep #34, Phase: literature review, Feedback: Successfully added paper 2105.09121v3, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\n~~~~~~~~~~\nCurrent Step #35, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification model compression techniques\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 37:
\n~~~~~~~~~~\nHistory: Step #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-accuracy optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #20, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge device image classification neural architecture search\n```\nStep #21, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification neural architecture search, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Enhanced Gradient for Differentiable Architecture Search\nSummary: In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\nPublication Date: 2021-03-23\narXiv paper ID: 2103.12529v1\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #23, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification hardware-aware NAS\n```\nStep #24, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification hardware-aware NAS, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Single-Path NAS: Device-Aware Efficient ConvNet Design\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\nPublication Date: 2019-05-10\narXiv paper ID: 1905.04159v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nTitle: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours\nSummary: Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.02877v1\n, Your response: ```FULL_TEXT\n1905.04159v1\n```\nStep #26, Phase: literature review, Feedback: Successfully added paper 1905.04159v1, Your response: ```SUMMARY\nedge device image classification dynamic inference\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification dynamic inference, here was the response\nTitle: AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and Dynamic Model Placements on Edge Devices\nSummary: Many real-world applications are widely adopting the edge computing paradigm\ndue to its low latency and better privacy protection. With notable success in\nAI and deep learning (DL), edge devices and AI accelerators play a crucial role\nin deploying DL inference services at the edge of the Internet. While prior\nworks quantified various edge devices' efficiency, most studies focused on the\nperformance of edge devices with single DL tasks. Therefore, there is an urgent\nneed to investigate AI multi-tenancy on edge devices, required by many advanced\nDL applications for edge computing.\n  This work investigates two techniques - concurrent model executions and\ndynamic model placements - for AI multi-tenancy on edge devices. With image\nclassification as an example scenario, we empirically evaluate AI multi-tenancy\non various edge devices, AI accelerators, and DL frameworks to identify its\nbenefits and limitations. Our results show that multi-tenancy significantly\nimproves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI\nmulti-tenancy techniques also open up new opportunities for flexible deployment\nof multiple DL services on edge devices and AI accelerators.\nPublication Date: 2021-07-26\narXiv paper ID: 2107.12486v1\n\nTitle: Edge-PRUNE: Flexible Distributed Deep Learning Inference\nSummary: Collaborative deep learning inference between low-resource endpoint devices\nand edge servers has received significant research interest in the last few\nyears. Such computation partitioning can help reducing endpoint device energy\nconsumption and improve latency, but equally importantly also contributes to\nprivacy-preserving of sensitive data. This paper describes Edge-PRUNE, a\nflexible but light-weight computation framework for distributing machine\nlearning inference between edge servers and one or more client devices.\nCompared to previous approaches, Edge-PRUNE is based on a formal dataflow\ncomputing model, and is agnostic towards machine learning training frameworks,\noffering at the same time wide support for leveraging deep learning\naccelerators such as embedded GPUs. The experimental section of the paper\ndemonstrates the use and performance of Edge-PRUNE by image classification and\nobject tracking applications on two heterogeneous endpoint devices and an edge\nserver, over wireless and physical connections. Endpoint device inference time\nfor SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by\ncollaborative inference.\nPublication Date: 2022-04-27\narXiv paper ID: 2204.12947v1\n\nTitle: Attention-aware Semantic Communications for Collaborative Inference\nSummary: We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.\nPublication Date: 2024-02-23\narXiv paper ID: 2404.07217v2\n\nTitle: AutoDiCE: Fully Automated Distributed CNN Inference at the Edge\nSummary: Deep Learning approaches based on Convolutional Neural Networks (CNNs) are\nextensively utilized and very successful in a wide range of application areas,\nincluding image classification and speech recognition. For the execution of\ntrained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud\nto the Edge. Unfortunately, deploying and inferring large, compute and memory\nintensive CNNs on edge devices is challenging because these devices typically\nhave limited power budgets and compute/memory resources. One approach to\naddress this challenge is to leverage all available resources across multiple\nedge devices to deploy and execute a large CNN by properly partitioning the CNN\nand running each CNN partition on a separate edge device. Although such\ndistribution, deployment, and execution of large CNNs on multiple edge devices\nis a desirable and beneficial approach, there currently does not exist a design\nand programming framework that takes a trained CNN model, together with a CNN\npartitioning specification, and fully automates the CNN model splitting and\ndeployment on multiple edge devices to facilitate distributed CNN inference at\nthe Edge. Therefore, in this paper, we propose a novel framework, called\nAutoDiCE, for automated splitting of a CNN model into a set of sub-models and\nautomated code generation for distributed and collaborative execution of these\nsub-models on multiple, possibly heterogeneous, edge devices, while supporting\nthe exploitation of parallelism among and within the edge devices. Our\nexperimental results show that AutoDiCE can deliver distributed CNN inference\nwith reduced energy consumption and memory usage per edge device, and improved\noverall system throughput at the same time.\nPublication Date: 2022-07-20\narXiv paper ID: 2207.12113v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n, Your response: ```FULL_TEXT\n2404.07217v2\n```\nStep #29, Phase: literature review, Feedback: Successfully added paper 2404.07217v2, Your response: ```SUMMARY\nedge device image classification early exit networks\n```\nStep #30, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification early exit networks, here was the response\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Resource-Constrained Edge AI with Early Exit Prediction\nSummary: By leveraging the data sample diversity, the early-exit network recently\nemerges as a prominent neural network architecture to accelerate the deep\nlearning inference process. However, intermediate classifiers of the early\nexits introduce additional computation overhead, which is unfavorable for\nresource-constrained edge artificial intelligence (AI). In this paper, we\npropose an early exit prediction mechanism to reduce the on-device computation\noverhead in a device-edge co-inference system supported by early-exit networks.\nSpecifically, we design a low-complexity module, namely the Exit Predictor, to\nguide some distinctly "hard" samples to bypass the computation of the early\nexits. Besides, considering the varying communication bandwidth, we extend the\nearly exit prediction mechanism for latency-aware edge inference, which adapts\nthe prediction thresholds of the Exit Predictor and the confidence thresholds\nof the early-exit network via a few simple regression models. Extensive\nexperiment results demonstrate the effectiveness of the Exit Predictor in\nachieving a better tradeoff between accuracy and on-device computation overhead\nfor early-exit networks. Besides, compared with the baseline methods, the\nproposed method for latency-aware edge inference attains higher inference\naccuracy under different bandwidth conditions.\nPublication Date: 2022-06-15\narXiv paper ID: 2206.07269v2\n\nTitle: Conditionally Deep Hybrid Neural Networks Across Edge and Cloud\nSummary: The pervasiveness of "Internet-of-Things" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\nPublication Date: 2020-05-21\narXiv paper ID: 2005.10851v1\n, Your response: ```FULL_TEXT\\n2311.18098v1\n```\nStep #31, Phase: literature review, Feedback: Error retrieving paper: Page request resulted in HTTP 400 (https://export.arxiv.org/api/query?search_query=&id_list=%5Cn2311.18098v1&sortBy=relevance&sortOrder=descending&start=0&max_results=100), Your response: ```SUMMARY\nedge image classification early exit dynamic inference\n```\nStep #32, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit dynamic inference, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n\nTitle: I-SplitEE: Image classification in Split Computing DNNs with Early Exits\nSummary: The recent advances in Deep Neural Networks (DNNs) stem from their\nexceptional performance across various domains. However, their inherent large\nsize hinders deploying these networks on resource-constrained devices like\nedge, mobile, and IoT platforms. Strategies have emerged, from partial cloud\ncomputation offloading (split computing) to integrating early exits within DNN\nlayers. Our work presents an innovative unified approach merging early exits\nand split computing. We determine the 'splitting layer', the optimal depth in\nthe DNN for edge device computations, and whether to infer on edge device or be\noffloaded to the cloud for inference considering accuracy, computational\nefficiency, and communication costs. Also, Image classification faces diverse\nenvironmental distortions, influenced by factors like time of day, lighting,\nand weather. To adapt to these distortions, we introduce I-SplitEE, an online\nunsupervised algorithm ideal for scenarios lacking ground truths and with\nsequential data. Experimental validation using Caltech-256 and Cifar-10\ndatasets subjected to varied distortions showcases I-SplitEE's ability to\nreduce costs by a minimum of 55% with marginal performance degradation of at\nmost 5%.\nPublication Date: 2024-01-19\narXiv paper ID: 2401.10541v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\nStep #33, Phase: literature review, Feedback: 3\n--- Page 1 ---Single-Layer Vision Transformers for More Accurate Early Exits with Less OverheadArian Bakhtiarniaa,\u2217, Qi Zhanga, Alexandros Iosi\ufb01disaaDIGIT, Department of Electrical and Computer Engineering, Aarhus University, Finlandsgade 22, Aarhus, 8200, Midtjylland, DenmarkAbstractDeploying deep learning models in time-critical applications with limited computational resources, for instance in edgecomputing systems and IoT networks, is a challenging task that often relies on dynamic inference methods such as earlyexiting. In this paper, we introduce a novel architecture for early exiting based on the vision transformer architecture,as well as a \ufb01ne-tuning strategy that signi\ufb01cantly increase the accuracy of early exit branches compared to conventionalapproaches while introducing less overhead. Through extensive experiments on image and audio classi\ufb01cation as well asaudiovisual crowd counting, we show that our method works for both classi\ufb01cation and regression problems, and in bothsingle- and multi-modal settings. Additionally, we introduce a novel method for integrating audio and visual modalitieswithin early exits in audiovisual data analysis, that can lead to a more \ufb01ne-grained dynamic inference.Keywords: dynamic inference, early exiting, multi-exit architecture, vision transformer, multi-modal, deep learning1. IntroductionOver the past decade, deep learning has shown tremen-dous success across various \ufb01elds, such as computer vi-sion and natural language processing [1]. However, deeplearning models are by de\ufb01nition composed of many layersof interconnected neurons, even reaching billions of pa-rameters, which makes them computationally expensive.This has sparked a great deal of research in order to makedeep learning models more lightweight, for which manyapproaches have been proposed, for instance, model com-pression methods [2] such as quantization [3], pruning [4],low-rank approximation [5] and knowledge distillation [6].More and more emerging internet of things (IoT) appli-cations are integrating deep learning models, such as videosurveillance, voice assistants, augmented reality and coop-erative autonomous driving, which are often time-sensitiveand require inputs to be processed within speci\ufb01c deadlines[7, 8]. The heavy computational burden of deep learningbecomes problematic for these time-critical IoT applica-tions, due to resource-constrained IoT devices. Edge com-puting is a promising computing paradigm for addressingthis issue, in which the deep learning task is o\ufb04oaded toedge servers in the proximity of IoT devices.Since edge computing systems introduce computationo\ufb04oading over a communication network and involve mul-tiple nodes working collaboratively in order to completethe task in a timely manner, transmission time has tobe taken into account in addition to the deep learningcomputation time. However, transmission time may vary\u2217Corresponding authorEmail addresses: arianbakh@ece.au.dk (Arian Bakhtiarnia),qz@ece.au.dk (Qi Zhang), ai@ece.au.dk (Alexandros Iosi\ufb01dis)greatly over time and across di\ufb00erent channels. Conse-quently, deep learning models running on edge computingsystems and IoT networks should be capable of anytimeprediction, meaning they should be able to provide a validresponse even if they are interrupted before traversing theentire neural network, although the model is expected toprovide a better answer if it is allowed to run for longertime.Dynamic inference approaches [9] modify the computa-tion graph based on each input during the inference phasein order to \ufb01t the time constraints. A dynamic inferenceapproach that particularly suits anytime prediction isearlyexiting [10], also referred to as multi-exit architectures orauxiliary classi\ufb01ers in the literature. In multi-exit archi-tectures, one or more early exit branches are placed aftersome of the intermediate hidden layers of thebackbone net-work. The goal of each of these branches is to provide anearly result similar to the \ufb01nal result of the neural net-work using only the features extracted up to that particu-lar branch location. These early results are inevitably lessaccurate than the \ufb01nal result of the network. In order toachieve anytime prediction using early exiting, the latestearly result can be used whenever the execution is inter-rupted, for instance, whenever a hard deadline is reached.Computation time can be further decreased by applyingmodel compression techniques on the backbone of multi-exit architectures. Besides anytime prediction, early exit-ing can also be used in budgeted batch classi\ufb01cation wherea \ufb01xed amount of time is available in order to classify aset of input samples. In such a setting, the result of ear-lier branches can be used for \u201ceasier\u201d samples whereas theresult of later branches or the \ufb01nal result can be used for\u201charder\u201d ones. The di\ufb03culty of each sample can be de-termined based on the con\ufb01dence of the network about itsPreprint submitted to Neural Networks June 30, 2022arXiv:2105.09121v3  [cs.LG]  29 Jun 2022--- Page 2 ---output [11], although other approaches exist in the litera-ture [10].Early exit branches are expected to have a low over-head in terms of the extra computation they introduce,since a high overhead would defeat the purpose. There-fore, they often contain only a handful of layers. Ideally,we want the accuracy of the early results to be close to thatof the \ufb01nal result, since a higher accuracy for early exitbranches means that the overall reliability of the systemincreases. However, the low-overhead constraint makes itquite challenging to achieve a high accuracy since the earlyexit branches have signi\ufb01cantly less trainable parameterscompared to the rest of the network. Several approachesfor increasing the accuracy of early exits such as knowledgedistillation [12], curriculum learning [13] and architecturesdesigned speci\ufb01cally for early exit branches [14] have beensuggested. In this paper, we propose a novel architecturein order to obtain more accurate early exits for convolu-tional neural network (CNN) backbones.A neural architecture called vision transformer (ViT )[15] has been recently introduced for image classi\ufb01cationwhich is radically di\ufb00erent from convolutional neural net-works. The building blocks of Vision Transformer havebeen used for early exits placed on Vision Transformerbackbones [14], however, using Transformer-based earlyexit branches on CNN backbones is not intuitive and re-quires additional steps and architectural modi\ufb01cations. Weuse a modi\ufb01ed version of this architecture instead of theusual convolution and pooling layers in early exit branchesand show that our method can signi\ufb01cantly increase theaccuracy of early exits compared to conventional archi-tectures by fusing local and global receptive \ufb01elds 1. Thecontributions of this paper can be summarized as follows:\u2022 We propose a novel architecture for early exit branchesin multi-exit architectures based on vision transform-ers, called single-layer vision transformer (SL-ViT ).We compare our method with conventional CNN-based early exit architectures across 27 scenarios in-volving di\ufb00erent datasets, branch locations and back-bone networks and show that our method is signi\ufb01-cantly more accurate in 26 of these scenarios, whilehaving less overhead in terms of number of param-eters and \ufb02oating point operators (FLOPS). To thebest of our knowledge the fusion of global and localscope in early exits has never been used in multi-exitarchitectures before.\u2022 We show that our method is a general purpose ap-proach that works across di\ufb00erent modalities as wellas multi-modal settings by investigating image clas-si\ufb01cation, audio classi\ufb01cation and audiovisual crowdcounting scenarios. We also show that our methodworks for both classi\ufb01cation and regression prob-lems.1Our code will be available athttps://gitlab.au.dk/maleci/sl_vit.\u2022 We introduce a novel way of integrating audio andvisual features in early exits using vision transform-ers. To the best of our knowledge, this is the \ufb01rsttime early exits have been studied in multi-modalsettings.\u2022 We provide insight into why our method achievesbetter results compared to conventional CNN-basedarchitectures by investigating the role of attentionand receptive \ufb01eld.\u2022 We introduce a \ufb01ne-tuning strategy for SL-ViT calledcopycat single-layer vision transformer (CC-SL-ViT )which is based on the copycat strategy developed forCNNs [16] and show that this method can furtherincrease the accuracy of SL-ViT early exits. To thebest of our knowledge this is the \ufb01rst time the copy-cat strategy is used for vision transformers or earlyexits.The rest of this paper is organized as follows: Section2 provides an overview of the relevant literature; Section 3describes our proposed method in detail; Section 4 explainsthe details of our experiments; Section 5 showcases theexperiment results; and, \ufb01nally, Section 6 brie\ufb02y discussesthe results and concludes the paper.2. Related WorkThis section provides the necessary prerequisites forunderstanding our method and experiments. We startby describing the particulars of multi-exit architectures.Subsequently, we provide the details of the vision trans-former architecture, which is the foundation of the pro-posed method. Then, we brie\ufb02y touch on how audio clas-si\ufb01cation is normally carried out, which is included in sev-eral scenarios in our experiments. Finally, we explain an-other scenario investigated in our experiments, i.e. crowdcounting, and how it can be approached in a multi-modalmanner.2.1. Multi-Exit ArchitecturesIn order to describe multi-exit architectures, we use thesame notation as Scardapane et al. [10] where a neural net-work is formulated as a functionf(X) = fL(fL\u22121(...f1(X))).In this formulation Lsigni\ufb01es the total number of layers inthe network andfi is the operator corresponding to layeri,which can be a convolutional layer, a fully-connected layer,a normalization layer, or any other di\ufb00erentiable operator.hi = fi(hi\u22121) denotes the output of layeri, where h0 is theinput X. Finally, \u03b8i symbolizes the trainable parametersof layer i.Equation (1) formulates the training process for theneural network which is achieved by tuning its parametersusing an optimization algorithm on the landscape de\ufb01nedby a loss function. In this equation, the parameters of theneural network are denoted by \u03b8 = \u22c3Li=1 \u03b8i, the training2--- Page 3 ---samples are signi\ufb01ed by {(Xn,yn)}Nn=1, and l(\xb7,\xb7) is theloss function.f\u2217= arg min\u03b8N\u2211n=1l(yn,f(Xn)) (1)Extending this notation to multi-exit architectures,B \u2286{1,..,L }signi\ufb01es the set of selected branch locations afterwhich early exit branches will be placed. cb(hb) = yb is theclassi\ufb01er or regressor representing the early exit branch ateach branch location b, where yb denotes the early resultat that location. The schematic illustration of a multi-exitarchitecture is presented in Figure 1. However, since thereare multiple outputs, and thus multiple loss signals in amulti-exit architecture, its training is not as straightfor-ward.Figure 1: Schematic illustration of a multi-exit architecture with twoearly exits.Three di\ufb00erent approaches for training multi-exit ar-chitectures exist in the literature [10, 17, 13]. In the \ufb01rstapproach, called end-to-end training, the loss signals ofall exits are combined and backpropagated through thenetwork at the same time. With end-to-end training, thecontribution of each loss signal to the total loss is expressedwith weight values, which are therefore hyper-parametersof the model.The second approach, called layer-wise training, \ufb01rsttrains the network up to and including the \ufb01rst exit branch.Subsequently, the part of the network that has been trainedso far is frozen, meaning its parameters are not modi\ufb01edany further, and the remainder of the network up to andincluding the second exit branch is trained. This processcontinues until the entire network is trained. Note thatwith this approach, there is no guarantee that the accu-racy of the \ufb01nal exit remains unchanged.In the \ufb01nal approach, called classi\ufb01er-wise training,the backbone network is completely frozen and each branchis trained independent of the rest of the network and otherbranches, meaning the parameters \u03b8 are not modi\ufb01ed andonly the parameters of the classifers/regressors {cb},b \u2208Bare trained. With this approach, no new hyper-parametersare introduced and the backbone remains unchanged. How-ever, the early exit branches a\ufb00ect a lower number of train-able parameters compared to the other approaches.In this paper, we choose to follow the classi\ufb01er-wisetraining approach due to its practical importance. Thisis because with classi\ufb01er-wise training, early exit branchescan be easily added on top of existing backbone networkswithout the need for re-training and hyper-parameter op-timization, which can be computationally expensive andtime consuming. Furthermore, with end-to-end and layer-wise training strategies, the number of branches and theirplacement can lead to further trade-o\ufb00s and a\ufb00ect theoverall performance of the model. Since branches are inde-pendently trained in the classi\ufb01er-wise strategy, any num-ber of branches can exist and a branch can be placed atany location without a\ufb00ecting the performance of otherbranches or the backbone.It is important to mention that branches placed laterin the backbone network do not necessarily result in ahigher accuracy compared to branches placed earlier. Theusage of such branches would therefore not be sensiblesince earlier branches exist that require less computationand provide more accurate results. We hereby use the termimpractical to refer to such branches.As previously mentioned, there are several methodsthat try to improve the accuracy of early exits. The methodin [12] uses the combination of the distillation loss fromthe \ufb01nal exit and the loss signal from ground truth labelsto train more accurate early exits using in the end-to-endtraining setting. The method in [18] expands on this ideaby adding a third loss signal based on the di\ufb00erence be-tween features of the latest early exit with earlier exits.The method in [19] proposes a technique called gradientequilibrium to combat the problem of gradient imbalancethat surfaces when using the end-to-end strategy, whichis when the variance of the gradients becomes very largewhen loss signals from multiple exits are combined, lead-ing to unstable training. Moreover, this paper introducesforward and backward knowledge transfer that aims to en-courage collaboration among di\ufb00erent exits. The methodin [20] improves the accuracy of later exits by reusing pre-dictions from earlier exits. The method in [21] circumventsthe problem of impractical branches by adaptively select-ing the exit location based on time budget and the speci\ufb01cinput. The method in [22] simpli\ufb01es the design of multi-exit architectures by removing the hyper-parameters of theend-to-end training strategy that specify the contributionof each loss signal.Besides e\ufb03cient inference, early exits can prove use-ful in several other applications, for instance, the methodin [23] allows for parallel training of the segments of theDNN that exist between early exits, by training each seg-ment based on the loss signal of the next segment obtainedin the previous training stage. Moreover, early exits canbe added to the network during the training in order toincrease the accuracy of the backbone network and dis-carded after the training phase, for instance, the widelyused Inception model [24] was trained in this way.Besides early exiting, several other approaches existfor dynamic inference, for instance, layer skipping [25, 26,27, 28] where the execution of some of the layers of theDNN are skipped, and channel skipping [29] where lessimpactful channels of convolutional neural networks areignored and their computation is skipped during the infer-3--- Page 4 ---ence phase. However, unlike early exits, these approachescannot provide an output if the execution is interrupteddue to a strict deadline, as these methods need to performthe computations until the very last layer.2.2. Vision TransformerThe transformer architecture was \ufb01rst introduced byVaswani et al. [30] for natural language processing, andit has recently been adapted for solving computer visionproblems by Dosovitskiy et al. [15]. Vision transformerwas originally developed for the problem of image clas-si\ufb01cation, however, variations of vision transformer havesince been applied to many computer vision problems, suchas object detection, depth estimation, semantic segmenta-tion, image generation and action recognition, as well asmulti-modal data analysis tasks such as text-to-image syn-thesis and visual question answering [31, 32, 33].In order to describe the vision transformer architecture,we \ufb01rst explain the self-attention layer. The input of thislayer is in the form of a sequence X = (x1,...,x n) whereX \u2208Rn\xd7d and dis the embedding dimension to representeach entity. Its output is in the form of Z = (z1,...,z n)where Z \u2208Rn\xd7dv . The goal of self-attention is to capturethe interaction between the entities in the sequence. Forthis purpose, each vector xi in the sequence is transformedinto three separate vectors: the query vector qi \u2208Rdq ,the key vector ki \u2208Rdk and the value vector vi \u2208Rdv ,where dq = dk. To construct the output vector zi thatcorresponds to the input xi, for each vector xj in X (in-cluding xi itself), the scalar aij is calculated by the innerproduct of qi and kj. Output vector zi is then calculatedby summing the value vectors v1,...,v n weighted by theircorresponding scalars, that is, zi = \u2211nj=1 aijvj. The scalaraij basically speci\ufb01es how much attention the i-th entityshould pay to the j-th entity, since aij determines the con-tribution of vj to the combined output zi. In practice, thescalars are normalized by \u221adk and converted into proba-bilities using the softmax function.If the key, query and value vectors are packed into ma-trices Q= XWQ, K = XWK and V = XWV, where WQ,WK and WV are learnable weight matrices, the above op-eration can be rephrased as follows:Z = softmax(QKT\u221adk)V (2)In order to enable the model to capture more than onetype of relationship between the entities in the sequence,self-attention is extended to multi-head attention by con-catenating the output of h di\ufb00erent self-attention blocksZ1,...,Z h each with its own set of learnable weight ma-trices, into a single matrix Z\u2032 = [Z0,...,Z h] \u2208Rn\xd7h.dv ,which is then projected using a weight matrixW\u2032\u2208Rh.dv\xd7d.A transformer encoder is constructed by passing theinput sequence into a normalization layer, a multi-headattention layer, a second normalization layer and a multi-layer perceptron (MLP), respectively. Two residual con-nections are added, one by adding the input sequence tothe output of the multi-head attention, and the other byadding the output of the multi-head attention to the out-put of the MLP.Putting it all together, a vision transformer is createdby \ufb01rst splitting the input image into patches. Subse-quently, the sequence of patches is projected into a se-quence of vectors and a positional embedding is addedto the corresponding vector of each patch. An additionallearnable embedding called classi\ufb01cation token is added tothe beginning of the sequence. The sequence then passesthrough L transformer encoders. Finally, the \ufb01rst vectorin the output of the last transformer encoder, which cor-responds to the classi\ufb01cation token, is passed to a MLPwhich outputs the \ufb01nal classi\ufb01cation result. The architec-ture of vision transformer is depicted in Figure 2.Figure 2: The vision transformer (ViT) architecture for image clas-si\ufb01cation.ViT-EE is a method which uses transformer encodersfor early exits placed on ViT backbones [14]. ViT-EE usesthe exact same layer as the ViT backbone. Using the build-ing blocks of the backbone network for early exit branchesis simple and intuitive, and it is the reason why so far,mostly convolutional layers have been used for early exit-ing CNN backbones. However, as we show in this work,carefully designing the architecture of early exit branchescan lead to signi\ufb01cant improvements. Using Transformer-based early exit branches on CNN backbones is not intu-itive, and requires additional steps such as converting ten-sors to patches, dealing with the classi\ufb01cation token and\ufb01ne-tuning the architecture parameters including patchsize, attention heads, embedding representation, the sizeand number of layers for MLP, and dropout. Moreover,we show that removing the last residual connection in thetransformer encoder can improve the performance in somecases.Furthermore, ViT backbones have a global receptive\ufb01eld in every layer, this means that ViT-EE is not neces-sarily ideal for early exits at all layers, as it adds too muchoverhead without providing improvements in terms of re-4--- Page 5 ---ceptive \ufb01eld. On the other hand, CNN backbones have alimited receptive \ufb01eld particularly in earlier layers, there-fore fusing this receptive \ufb01eld with a global one leads toimprovements.2.3. Audio Classi\ufb01cationSimilar to image classi\ufb01cation, audio classi\ufb01cation isthe problem of categorizing a given audio waveform intoone of several predetermined classes. For instance, thegiven audio waveform could be a musical recording, andthe goal could be to specify which genre of music it belongsto. To represent the input features, spectrogramsobtainedby applying short-time Fourier transform (STFT) andMelspectrograms are commonly used [34], although raw audiowaveforms can been used as well [35]. Mel spectrogramsare spectrograms that are constructed using the Mel scalewhich is a nonlinear transformation of the frequency scaledesigned based on domain knowledge about the humanauditory system. Various deep learning models for au-dio classi\ufb01cation exist in the literature, including modelsthat are commonly used for image classi\ufb01cation, namelyResNet [36], DenseNet [37] and Inception [38], which havebeen shown to be quite e\ufb00ective for audio classi\ufb01cationas well [39]. Conveniently, the same three networks havepreviously been used as backbone networks when investi-gating early exiting for image classi\ufb01cation [13]. Thereforewe use these backbone networks for both image and audioclassi\ufb01cation in our experiments.2.4. Audiovisual Crowd CountingCrowd counting refers to the problem of identifying thetotal number of people present in a given image. Crowdcounting has many applications such as safety monitoring,disaster management, design of public spaces, intelligencegathering and analysis, creation of virtual environmentsand forensic search [40]. With many of these applications,it is vital for the model to perform in near real-time. How-ever, the input images in these scenarios often have highresolutions, such as HD or Full HD. Moreover, many ofthe available methods contain an immense number of pa-rameters [41]. This means that crowd counting modelsare often very computationally expensive, therefore, dy-namic inference methods such as early exiting and otherlightweight deep learning methods become essential in realworld applications.Although the main objective of this task is to obtain asingle count from an image, many methods treat this prob-lem as dense prediction where the output is a density mapdepicting the density of the crowd across the input image,and the total count is calculated by the sum of all valuesin the density map. Therefore, in most crowd countingdatasets, such as Shanghai Tech [42] and World Expo \u201910[43], the locations of the heads of individuals in the imageare annotated and provided as targets. A ground truthdensity map can then be obtained from these head anno-tations using Gaussian kernels or more complicated andFigure 3: An example image from the Shanghai Tech dataset and itscorresponding ground truth density map.specialized methods [41]. Figure 3 shows an image fromthe Shanghai Tech dataset and the ground truth densitymap that was generated from the provided head annota-tions using the method presented in Zhang et al [42]. Incrowd counting, Mean Absolute Error (MAE) is usuallyused as a measure of accuracy whereas Mean Squared Er-ror (MSE) is used as a measure of robustness [44].Many crowd counting methods exist in the literature[41], however, most of these methods are applied in asingle-modal fashion where the input is an image or a videoframe. In contrast, AudioCSRNet [45], a multi-modal ex-tension of the widely-used CSRNet model for crowd count-ing [46], takes as input the ambient audio of a scene inaddition to its image. The authors show that the ambi-ent audio improves the result in situations where the im-age quality is not ideal, for instance, low image resolution,presence of noise, occlusion and low illumination.In CSRNet, the features extracted from the input im-age by the \ufb01rst 10 layers of a VGG-16 [47] network pre-trained on the ImageNet dataset [48] are passed through6 dilated convolution layers and a 1 \xd71 convolution layerin order to obtain the density map. AudioCSRNet ex-tends this architecture by converting each of the dilatedconvolution layers into a fusion block. The architecture ofAudioCSRNet is depicted in Figure 4. First, a Mel spec-trogram is obtained from the raw audio waveform. Subse-quently, in each fusion block, the features extracted fromthe input Mel spectrogram by the \ufb01rst 6 layers of a VG-Gish [49] network pre-trained on the AudioSet dataset [49]are projected to two vectors called \u03b3 and \u03b2 which repre-sent the multiplicative and additive aspects of the audiofeatures. The \u03b3 and \u03b2 vectors are then tiled in order tomatch the size of the visual features. Finally, the outputof the dilated convolution is element-wise multiplied by \u03b3and added to \u03b2.The fusion operation can be summarized asvl+1 = Fl(\u03b3l \u2299Dl(vl) + \u03b2l), (3)where vl \u2208 RCl\xd7Wl\xd7Hl is the output of the l-th fusionblock, Fl denotes an activation function, \u03b3l and \u03b2l are thetiled vectors and Dl represents the l-th dilated convolu-tion.In practice, a batch normalization layer [50] is addedimmediately after each dilated convolution. Furthermore,the height and width of the intermediate features remain5--- Page 6 ---Figure 4: Architecture of AudioCSRNet.unchanged by using padding in the convolution operations,meaning Hl = Hl+1 and Wl = Wl+1. Additionally, sincethe \ufb01rst 10 layers of VGG-16 decrease both height andwidth by a factor of 8 via several pooling operations, the\ufb01nal result of the network needs to be upsampled by afactor of 8 in order to match the resolution of the inputimage. It is important to preserve the total sum of thedensity map during this upsampling operation, since itrepresents the total count.3. Single-Layer Vision Transformers for Early Ex-itsWe assume a pre-trained and high performing back-bone network is already available. Due to time constraintsarising from the particular application, it is desirable thatthe network provides a result within the speci\ufb01c deadlinerather than not providing a result at all, even though thisresult may be less accurate than it would be if time con-straints did not exist. Therefore, the backbone needs tobe augmented with early exit branches to allow for dy-namic inference and anytime prediction. As previouslymentioned, we use the classi\ufb01er-wise approach for trainingthe early exit branches since it results in \u201cplug-and-play\u201dbranches that can easily be added to the backbone networkwithout any re-training or hyper-parameter tuning.3.1. SL-ViTTypically, the architecture of early exit branches startswith one or more convolution layers, although some mayhave no convolutions at all. Afterwards, they may have apooling layer, which may be global pooling, and one MLP[51, 11]. Here, as a baseline, we choose to utilize the ar-chitecture depicted in Figure 5 with one 3 \xd73 convolution,followed by a 2 \xd72 max pooling layer and \ufb01nally a MLP.The size of the max pooling layer is increased to 4 \xd74 forcrowd counting since the input images have a very highresolution. Additionally, we use dropout [52] inside theMLP to avoid over\ufb01tting. We use a single convolutionsince early exits with two or more convolution layers havea high overhead and may even lead to lower accuracy [11].Early exits without convolutions are sometimes used verylate in the network, however, since they are straightfor-ward and leave no room for modi\ufb01cations, we do not applyour method for such cases. The resulting architecture is acommon setup within the literature, and is e\ufb00ectively thesame architecture used for earlier exits by Hu et al. [51].Figure 5: Architecture of CNN early exit branches. Size of the \ufb02at-tened feature vector depends on the dimensions of the features at thespeci\ufb01c branch location. For branches placed on the AudioCSRNetbackbone, max pooling size is increased to 4x4 since the input im-ages have a high resolution. Figure created using the NN-SVG tool[53].Our method called single-layer vision transformer orSL-ViT for short, is an alternative architecture for earlyexit branches that can achieve a higher accuracy comparedto the aforementioned baseline, while having less overheadin terms of the number of parameters as well as \ufb02oatingpoint operations per second (FLOPS). Our proposed ar-chitecture is based on the vision transformer architectureintroduced in section 2.2, where instead of the input im-age, we split the intermediate features at the branch loca-tion into patches (sub-tensors) and pass them to a visiontransformer.The choice of vision transformer architecture is pri-marily due to its global receptive \ufb01eld. Receptive \ufb01eld6--- Page 7 ---is crucial in many deep learning problems, including onesstudied in this work. The receptive \ufb01eld of state-of-the-art CNNs developed for image classi\ufb01cation has steadilyincreased over time and is correlated with increased classi-\ufb01cation accuracy [54]. Additionally, in audio classi\ufb01cationusing spectrograms, each location relates to a di\ufb00erent fre-quency band in a di\ufb00erent window of time. It is reasonableto assume that processing combinations of frequencies andwindows that are not necessarily adjacent could be of im-portance. Moreover, many crowd counting methods havemade use of global information through visual attentionmechanisms and dilated convolutions [41]. Since the re-ceptive \ufb01eld is particularly limited in early layers of CNNbackbones, choosing an architecture for early exit brancheswith a global receptive \ufb01eld could be bene\ufb01cial.Many other designs strive to increase the receptive \ufb01eldin their building blocks, for instance, the pyramid pool-ing module (PPM) in PSPNet [55] or atrous spatial pyra-mid pooling (ASPP) in DeepLab [56]. However, they allfall short in comparison with the global receptive \ufb01eld oftransformers. PPM increases the receptive \ufb01eld throughaggregating di\ufb00erent levels of pooling, which means far lo-cations have only access to coarse representations of eachother, and ASPP has holes in its receptive \ufb01eld.It is important to mention that the local receptive \ufb01eldof convolutional layers is not fundamentally bad. On thecontrary, it plays a key role in representation learning andextracting local information, especially in the early layersof the network where the receptive \ufb01eld of the convolu-tional \ufb01lters is small. Filters in successive convolutionallayers have increasingly larger receptive \ufb01elds, therefore,\ufb01nal layers in a CNN architecture have \ufb01lters of largeenough receptive \ufb01elds that can e\ufb00ectively aggregate in-formation from the entire input image to provide a properresponse. However, this process of cascading local recep-tive \ufb01elds of increasing size requires the number of layersin the CNN to be large, or at least all the layers in thenetwork to be traversed in order to provide the network\u2019sresponse. When an early exit is added at an early layer,this chain of increasingly larger receptive \ufb01elds is broken,and an early exit that has a local receptive \ufb01eld may notbe able to e\ufb00ectively aggregate all required informationin the image to provide a suitable response. This situa-tion is the motivation behind the proposed branch archi-tecture, which fuses the local receptive \ufb01eld of the layerin the network where the early exit branch is attached,with the global receptive \ufb01eld of the early exit, in orderto e\ufb00ectively aggregate information from the entire inputand provide a more accurate response. Indeed, the orig-inal Vision Transformer paper [15] attributes the successof their model to the combination of local and global re-ceptive \ufb01elds and shows that even in very early layers, thisability to integrate information globally is indeed used bythe model.There are some crucial di\ufb00erences between the origi-nal vision transformer and the architecture in our method.First, in order to introduce a low overhead for early exitbranches, we only use a single transformer encoder layerinstead of the original 12 to 36 layers, meaning that L= 1in our case. Secondly, we do not utilize a separate clas-si\ufb01cation token and instead pass the entire output of thetransformer encoder layer to the MLP head. This is pos-sible because the width and height of tensors are gener-ally reduced throughout CNN backbones by pooling oper-ations, and thus the number of patches in our architectureis lower than that of the original vision transformer. Inaddition to the number of patches, the size of the embed-ding dimension ( d) is also reduced in our proposed archi-tecture, introducing far less parameters when passing theentire output of the last transformer encoder layer to theMLP head, even with high-resolution inputs such as inour crowd counting experiments. Variations of our archi-tecture have 5 \xd75, 7 \xd77 or 16 \xd79 patches and embeddingdimensions of 32 or 36, whereas di\ufb00erent versions of theoriginal vision transformer have 14 \xd714 or 16 \xd716 patchesand embedding dimensions of 768, 1024 or 1280. We em-pirically found that using the entire transformer encoderoutput instead of just one classi\ufb01cation token can increasethe accuracy, perhaps because in a single-layer version,there are not enough layers for the classi\ufb01cation token tolearn to properly summarize other patches. Our proposedarchitecture is shown in Figure 6. It is also important tonote that the MLP head used in our architecture is exactlythe same as the MLP in the CNN early exit architecture.Figure 6: Architecture of SL-ViT early exit branches. Unlike typicalvision transformers, only a single transformer encoder layer is used,extra learnable classi\ufb01cation token is not added to the sequence andthe entire output of the transformer encoder is passed on to the MLPhead. The MLP head is the same as CNN early exit branches.Our model has several hyper-parameters, namely thesize of each patch, the embedding dimension d and thenumber of attention heads hin multi-head attention. Thepatch size creates a trade-o\ufb00 where smaller patches resultin a more \ufb01ne-grained attention mechanism while increas-ing the total number of parameters in a bi-quadratic fash-ion. Therefore, similar to the original vision transformer,we choose the size of the patch to be close to the square7--- Page 8 ---root of the height and width of the input features. We alsomake sure that the size of the patch can divide the size ofthe input features to avoid padding, for instance, a patchsize of 4 \xd74 for input features of size 28 \xd728. We performa grid search to \ufb01nd the values of d and h that result inthe highest accuracy, while keeping the total number ofparameters less than or equal to that of the CNN earlyexit counterpart.At a \ufb01rst glance, it might seem like the SL-ViT archi-tecture introduces more hyper-parameters than the con-ventional CNN architecture, however, the CNN architec-ture includes many design choices as well, such as the num-ber of \ufb01lters, \ufb01lter size, padding, dilation, stride, poolingtype and pooling size. The design choices for CNN archi-tectures might seem simpler since they have been studiedmore extensively compared to vision transformers whichwere introduced more recently.3.2. Audiovisual SL-ViTWith audiovisual backbones such as the AudioCSR-Net model for audiovisual crowd counting, described insection 2.4, it is desirable to have audiovisual early exitsthat use both visual and audio features in order to achievea higher accuracy. The simplest way to have such branchesis to add the branches after the blocks where the fusionof visual and audio features take place. However, withour proposed SL-ViT architecture, it is also possible toinclude audio features as one or more patches alongsideother patches, and directly fuse the features in the earlyexit. The advantage of this approach is that since in vi-sion transformers, any of the patches can pay attention toany other patch, the visual features can be fused with theaudio features without being directly impacted and mod-i\ufb01ed. In contrast, since convolutional \ufb01lters only take theimmediate vicinity into account, the audio features mustbe present in every location. One option is to concatenatethe visual features and the tiled audio features along thedepth. However, that would greatly increase the amount ofcomputation for each fusion operation, therefore intrusiveoperations such as element-wise multiplication and addi-tion are used instead.3.3. Copycat SL-ViTFinally, we introduce a \ufb01ne-tuning strategy for SL-ViTbranches that can further increase their accuracy. Correia-Silva et al. [16] developed a method called copycat CNNwhere they create a \u201cfake\u201d dataset by taking images fromanother domain, giving them as input to a network trainedon the target domain, and recording the output of thenetwork as labels for these images. For instance, imagesfrom the ImageNet dataset [48] can be given to a networktrained on the CIFAR-10 dataset [57], where the imageof a camel may be labelled as a \u201cdog\u201d since there are nolabels for \u201ccamel\u201d in CIFAR-10. This fake dataset is thencombined with a dataset for the target domain and usedto train a new network. We use this strategy to \ufb01ne-tunean already trained SL-ViT branch and obtain a copycatsingle-layer vision transformer (CC-SL-ViT ). Note thatthe ratio of the fake data mixed with the available datasetis a hyper-parameter of this \ufb01ne-tuning strategy.4. Experimental SetupIn this section, we provide the details of our experi-ments. We begin by giving a short summary of the datasetsas well as the training details for the backbone networks.We then lay out the details of the branch architectures,their training procedure and their placement on the back-bone networks, and \ufb01nally explain how the copycat strat-egy was used to \ufb01ne-tune the branches.A total of 27 di\ufb00erent scenarios were tested in our ex-periments. For both image and audio classi\ufb01cation, twodatasets, three backbone networks and two di\ufb00erent branchlocations on each backbone were tested. In addition, threedi\ufb00erent branch locations for the audiovisual crowd count-ing backbone network were covered. All experiments wererepeated 5 times and the average accuracy as well as thestandard deviation were recorded. 4 \xd7 Nvidia 2080TiGPUs were used for the training of our models.4.1. Datasets4.1.1. CIFAR-10 and CIFAR-100These are widely-used datasets for image classi\ufb01cation[57]. Both datasets consist of 60,000 color images of size32 \xd732 pixels and their corresponding class labels. Theimages in CIFAR-10 and CIFAR-100 are categorized into10 and 100 di\ufb00erent classes, respectively. We use 40,000examples for training, 10,000 for validation and another10,000 for testing. Since our backbone networks are pre-trained on ImageNet which consists of 224 \xd7224 pixelimages, we resize each image to these dimensions beforepassing them into the network.4.1.2. Speech Commands (SC)A well-known audio dataset of spoken words [58]. Itconsists of 100,503 1-second audio clips with a samplingrate of 16kHz, each labelled as one of 12 classes: 10 dif-ferent spoken words such as \u201cYes\u201d, \u201cNo\u201d, \u201cDown\u201d and\u201cStop\u201d as well as one class for background noise and an-other for unknown words. We use 85,511 examples fortraining, 10,102 for validation and 4,890 for testing. Weconvert the raw audio waveforms into spectrograms usingshort-time Fourier transform (STFT) with a window sizeof 255 samples and step size of 128 samples, and resize theresulting spectrograms to 224 \xd7224 before passing theminto the network.4.1.3. GTZANIt is the most widely-used dataset for music genre recog-nition [59]. The original dataset consists of 10 genres suchas \u201cPop\u201d and \u201cRock\u201d and 100 30-second audio clips pergenre with a sampling rate of 22,050Hz. We follow the8--- Page 9 ---common approach to split each audio clip into 10 separate3-second clips in order to increase the size of the datasetto 10,000. We use 8,000 examples for training, 1,000 forvalidation and another 1,000 for testing. Following the ap-proach of Palanisamy et al. [39] where di\ufb00erent spectro-grams with di\ufb00erent parameters are placed in each channelof the input image, we use one spectrogram obtained fromSTFT with window size of 512 samples and step size of 256samples as well as two Mel spectrograms with 128 bins andwindow sizes of 100 and 50 milliseconds, and step sizes of50 and 25 milliseconds, respectively.4.1.4. DISCOAn audiovisual dataset for crowd counting which con-tains 1,935 images of Full HD resolution (1920 \xd71080)[45]. For each image, a corresponding 1-second audio clipof ambient sounds with a sampling rate of 48kHz, starting0.5 seconds before the image was taken and ending 0.5 sec-onds afterwards, exists as well. The labels are provided inthe form of head annotations in the image. At the time ofthis writing, DISCO is the only publicly available datasetfor audiovisual crowd counting. We use 1435 examples fortraining, 200 for validation and 300 for testing. The inputimage is resized to 1024 \xd7576 pixels to reduce memoryand computation requirements. Similar to Hershey et al.[49], the input audio waveform is transformed into a Melspectrogram with 64 bins, window size of 25 millisecondsand step size of 10 milliseconds. Following Hu et al. [45]the ground truth density maps are obtained by convolv-ing the head annotations with a 15 \xd715 Gaussian kernelK\u223cN (0,4.0).4.2. Backbone networksTransfer learning is used to train the ResNet152,DenseNet201 and InceptionV3 backbone networks for bothimage and audio classi\ufb01cation. The backbone networks areall pre-trained on the ImageNet dataset and the top layeris replaced. We found that instead of adding just one denselayer at the top, as is common in transfer learning, usingtwo dense layers and a dropout layer in between leads to ahigher accuracy in our case. The resulting network is thentrained using the Adam optimizer [60] with a learning rateof 10\u22124 and categorical cross-entropy loss function. Thelearning rate is reduced by a factor of 0.6 on plateau witha tolerance of 2 epochs, and an early stopping mechanismwith a tolerance of 5 epochs is used.The audiovisual crowd counting backbone is trained intwo stages. We \ufb01rst train a network with the AudioCSR-Net architecture described in Section 2.4 for 100 epochs.L2 norm is used as loss function and AdamW [61] with alearning rate of 10 \u22125 and weight decay of 10 \u22124 is used asoptimizer, where the learning rate is multiplied by a factorof 0.99 each epoch. This is the same training procedureused in the original paper [45]. Subsequently, in order toconvert the problem from dense prediction to regression,a dense layer with an output size of one is added after thelast layer of the trained AudioCSRNet. This layer is ini-tialized as a sum, meaning the initial weights are all equalto one and no bias is used. Then the entire network isre-trained for another 100 epochs using MAE as loss func-tion instead of the previous L2 loss, a learning rate of 10\u22126and weight decay of 10 \u22125. The learning rate is similarlymultiplied by a factor of 0.99 every epoch. The resultingmodel achieves a MAE of 13.63 which is even lower thanthe MAE of 14.27 reported in the original paper. However,the output of the network is just a single number repre-senting the total count instead of a density map. The \ufb01nalaccuracy of all trained backbones can be seen in Table 1.When training the backbone networks, in order to \ufb01tthe limitations of our available computational resources,the batch sizes are adjusted and some layers of the back-bone networks are frozen. All backbone networks weretrained with a batch size of 32 except AudioCSRNet whichhas a batch size of 4 as well as InceptionV3 when trainedon CIFAR-10 and CIFAR-100 which has a batch size of 64.All layers of the backbone networks were trained, exceptin the case of ResNet152 and DenseNet201 when trainedon CIFAR-10 and CIFAR-100 where only the batch nor-malization layers were trained. We found that trainingonly the batch normalization layers is su\ufb03cient to achievea high-performing backbone network in these cases [62].Table 1: Performance of backbone networks on each datasetBackboneCIFAR-10 Acc. CIFAR-100 Acc. SC Acc. GTZAN Acc. DISCO MAEResNet152 95.36% 82.25% 95.85% 91.29% -DenseNet20196.48% 82.53% 96.36% 92.09% -InceptionV396.56% 83.80% 94.93% 87.79% -AudioCSRNet- - - - 13.634.3. BranchesAll branches were trained from scratch using the Heinitialization method [63] and the Adam optimizer with alearning rate of 10 \u22124 where the learning rate is reducedby a factor of 0.6 on plateau with a tolerance of 2 epochs,and an early stopping mechanism with a tolerance of 5epochs is utilized. The branches on classi\ufb01cation back-bones use a categorical cross-entropy loss function whereasthe branches on the audiovisual crowd counting backboneuse mean absolute error loss. The training batch size forbranches were 64 in scenarios involving CIFAR-10, CIFAR-100 and Speech Commands, 32 in scenarios involvingGTZAN and 4 in scenarios involving DISCO.Table 2 shows the location of the branches placed oneach backbone network. For the AudioCSRNet backbonenetwork, branch V1 uses only the output of the VGG-16layers, therefore, it only has access to the visual features.Branch AV1 uses the outputs of both VGG-16 and VG-Gish, therefore it has access to both audio and visual fea-tures. In this branch location, the fusion of audio andvisual features is performed as described in Section 3 forthe SL-ViT architecture, and similar to the fusion blocks9--- Page 10 ---in AudioCSRNet for the CNN architecture, however, with-out dilation. Finally, branch AV2 is placed after the \ufb01rstfusion block in AudioCSRNet, therefore audio and visualfeatures have already been fused and thus fusion operationis not required within the branches. Adding branches afterthe second fusion block or later would not be reasonablesince more than 85% of the computation of the backboneis carried out before that point, and thus the accelerationresulting from early exits would be negligible.Table 2: Placement of branches for each backbone betworkBackbone BN \u2217 Branch Placed AfterDenseNet201 1 Transition Layer 12 Transition Layer 2ResNet152 1 12th Convolution2 36th ConvolutionInceptionV3 1 First Filter Concat2 Second Filter ConcatAudioCSRNet V1 Last Layer of VGGAV1 Last Layers of VGG and VGGishAV2 First Fusion Block\u2217Branch Number4.4. SL-ViT and CC-SL-ViT ParametersTable 3 summarizes the hyper-parameters used for theSL-ViT branches in each scenario. \u201cPatch Size\u201d shows thewidth and height of each image patch, \u201cPatches\u201d denotesthe resulting number of patches across width and heightof the input image, d is the size of embedding dimensionand h is the number of heads in multi-head attention.For copycat SL-ViT, images from the Tiny ImageNetdataset, which are the images from ImageNet down-sampledto 32\xd732, were given to the InceptionV3 backbone trainedon CIFAR-10, and the outputs were used to create the fakedataset. Then the fake dataset was mixed with CIFAR-10with a 2-to-1 ratio and used for re-training.Table 3: Hyper-parameters of SL-ViT for di\ufb00erent backbone net-works and branchesBackbone Dataset BN \u2217 Patch Size Patchesd hDenseNet201 all all 4x4 7x7 32 12ResNet152 SC 2 4x4 7x7 32 24GTZAN 2 4x4 7x7 32 24Other 4x4 7x7 32 12InceptionV3 CIFAR-100 all 5x5 5x5 36 8Other 5x5 5x5 32 12AudioCSRNet DISCO all 8x8 16x9 32 12\u2217Branch Number5. ResultsThe results of our experiments are presented in Tables4 to 8. In these Tables, the \ufb01nal accuracy, the total FLOPSof the model up to and including the branch and the num-ber of parameters of just the early exit branch are com-pared between the CNN architecture and the SL-ViT ar-chitecture. Higher accuracies, lower errors, lower numberof parameters and lower total FLOPS are highlighted inthese tables. Furthermore, the acceleration caused by SL-ViT early exits, de\ufb01ned as the total FLOPS of the back-bone network divided by the total FLOPS of the modelup to and including the SL-ViT branch, is also provided.Several observations can be made about these results.First, in all scenarios except one, SL-ViT early exits achievea signi\ufb01cantly higher accuracy. Even in the one excep-tional scenario, namely branch 2 of ResNet152 in Table6, the accuracy of SL-ViT is very close to its CNN coun-terpart. Secondly, while in some cases SL-ViT brancheshave an equal number of parameters compared to CNNbranches, in all scenarios, the total FLOPS of SL-ViTbranches is, Your response: ```ADD_PAPER\n2105.09121v3\nThis paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with only one transformer encoder layer. Extensive experiments on image classification (CIFAR-10/100), audio classification (Speech Commands, GTZAN), and audiovisual crowd counting demonstrate SL-ViT's effectiveness across modalities. Key results show SL-ViT achieves higher accuracy in 26/27 tested scenarios while reducing parameters and FLOPS. The paper also introduces a copycat fine-tuning strategy (CC-SL-ViT) that further improves performance. This work enables more efficient dynamic inference for edge devices through improved early exit architectures.\n```\nStep #34, Phase: literature review, Feedback: Successfully added paper 2105.09121v3, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #35, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\n~~~~~~~~~~\nCurrent Step #36, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---1Edge AI: Evaluation of Model CompressionTechniques for Convolutional Neural NetworksSamer Francy, Member, IEEE,Raghubir Singh, Member, IEEEAbstract\u2014This work evaluates the compression techniqueson ConvNeXt models in image classification tasks using theCIFAR-10 dataset. Structured pruning, unstructured pruning,and dynamic quantization methods are evaluated to reduce modelsize and computational complexity while maintaining accuracy.The experiments, conducted on cloud-based platforms and edgedevice, assess the performance of these techniques. Results showsignificant reductions in model size, with up to 75% reduc-tion achieved using structured pruning techniques. Additionally,dynamic quantization achieves a reduction of up to 95% inthe number of parameters. Fine-tuned models exhibit improvedcompression performance, indicating the benefits of pre-trainingin conjunction with compression techniques. Unstructured prun-ing methods reveal trends in accuracy and compression, withlimited reductions in computational complexity. The combinationof OTOV3 pruning and dynamic quantization further enhancescompression performance, resulting 89.7% reduction in size, 95%reduction with number of parameters and MACs, and 3.8%increase with accuracy. The deployment of the final compressedmodel on edge device demonstrates high accuracy 92.5% and lowinference time 20 ms, validating the effectiveness of compressiontechniques for real-world edge computing applications.Index Terms\u2014edge AI, ConvNeXt, CNN, pruning, quantiza-tion, compression, OTO.I. I NTRODUCTIONEDGE devices such as Internet of Things (IoT) are becom-ing increasingly important and widely used in our dailylives and industrial facilities. IoT is a network of things thatempowered by sensors, identifiers, software intelligence, andinternet connectivity, it can be considered as the intersectionof the internet, things/objects (anything/everything), and data[1]. The number of these devices is expected to increase evenmore [2]. These devices have the potential to perform complexArtificial Intelligence (AI) tasks locally, without relying heav-ily on cloud infrastructure [3]. The rapid advancement of AIhas led to the development of complex deep learning modelsthat show high performance in different domains. DeployingAI models on edge devices has many advantages such as lowlatency, privacy and data security, bandwidth optimization, andreduced network dependence. Low latency is achieved due toreal-time processing by instant data analysis on edge withoutwaiting for remote server processing, this data analysis on theedge reduces transmitting data to the cloud which enhancessecurity against breaches, reduces the bandwidth consumption,and reduces network dependence.Samer Francy is with the Department of Computer Science, University ofBath, Bath, England (e-mail: samer.francy@bath.edu).Raghubir Singh is with the Department of Computer Science, Universityof Bath, Bath, England (e-mail: rs3022@bath.ac.uk).A. Overview of Edge AIEdge AI represents a paradigm shift in the way AI isimplemented in the context of the IoT. It capitalizes on thecapabilities of IoT devices, enhancing real-time processing,analytics, and decision-making directly at the edge of thenetwork. The IoT architecture, which is the foundation forEdge AI, typically involves three core layers [1]. The layersare perceptual layer, where data is collected from varioussensors and devices, network layer, where data is transmittedand routed through this layer, which is responsible for commu-nication between devices and cloud services, and applicationlayer, which processes and utilizes the data, providing insightsand enabling actions.B. Convolutional Neural Networks (CNNs)CNN models are subsets of Deep Neural Networks (DNN)models. CNN models are effective for image and video-relatedtasks due to their ability to learn relevant features from thedata by recognizing patterns, shapes, and structures in images,which is challenging for traditional machine learning models,that\u2019s why they are used for computer vision tasks such asimage classification, object detection, and image segmentation[4].1) CNN Architecture: In general CNN models consist ofbelow parts:\u2022 Input Image : Pixels form the binary basis of computerimages, while the human visual system operates throughneurons with receptive fields. Similarly, CNNs functionwithin their receptive areas, starting with simple patternsand advancing to more complex ones, making CNNs apromising tool for computer vision [4].\u2022 Convolutional Layer : A convolutional layer in a CNNuses a small filter (e.g., 3x3 or 5x5) that slides over theinput image. At each position, it multiplies its valueswith the overlapping image pixels and sums the results toproduce an output. This sliding operation helps identifylocal features like edges and colors, building a hierar-chical representation. The depth of the filter matches theinput image\u2019s channels (e.g., 3 for RGB images). Stackingmultiple filters allows the network to learn features atdifferent abstraction levels [4].\u2022 Pooling Layer : Pooling reduces the spatial size of fea-ture maps. This not only lowers computational demandsbut also helps in extracting position and orientation-independent features essential for training. Two com-mon pooling methods are maximum pooling and averagepooling. In maximum pooling, a small kernel (e.g., 2x2)arXiv:2409.02134v1  [cs.LG]  2 Sep 2024--- Page 2 ---2Fig. 1. Unbalanced Demand For Computation (Left) and Memory (Right) in AlexNet [5].selects the maximum value within its range and places itin the output. In average pooling, a similar-sized kernelcomputes the average value within its area for eachchannel, maintaining the same depth. Pooling simplifiescomputation and weight requirements, with maximumpooling being the most commonly used method [4].\u2022 Activation Function : The activation function, applies amathematical operation to the filter\u2019s output to concludethe output of the network. The common choice is the Rec-tified Linear Unit (ReLU). They fall into two categories,linear and non-linear activation functions [4].\u2022 Fully Connected Layer : It functions as a feed-forwardneural network (NN) typically situated at the network\u2019slower layers. It receives input from the output of the lastpooling or convolutional layer, which is flattened into aone-dimensional vector, enabling it to learn complex datarelationships [4].2) Computation and Memory Demands: In CNNs, unbal-ance exists in resource demands between the layers. Convolu-tional layers primarily serve as feature extractors and heavilydominate the computational workload. In the case of AlexNet,for instance, the convolutional layers account for just 2 millionweights but demand a substantial 1.33 Giga Operations PerSecond (GOPS) of computation. In contrast, fully connectedlayers function as classifiers, accumulating information forhigh-level decisions, and bear the weight of the network witharound 59 million parameters, yet they contribute significantlyless to computation, requiring only 0.12 GOPS. This obviouscontrast in resource allocation (Figure 1) highlights the unbal-anced demands between these two layers in CNNs [5].3) Key CNN Architectures:In 1989, the use of a NN archi-tecture with convolutional layers for recognizing handwrittendigits in the context of ZIP code recognition was introduced[6], That architecture consisted of input layer, 3 hidden layers,and output layer. Since then, CNN models have developed(Figure 2) and became much deeper.4) CNN on Edge: Deploying CNN models on edge hasa wide range of practical and industrial applications acrossvarious sectors. Here are some specific examples:\u2022 Surveillance and Security : It can perform real-time ob-ject detection and facial recognition for security monitor-ing, identifying intruders, and managing access control.Face identification was deployed using VGGFace [7].Video analysis was deployed using YOLOX [8]. Infraredand Visible Image Fusion for security systems was de-ployed using DenseNet [9]. Human action recognitionapplications were deployed using ResNet [10].\u2022 Manufacturing and Quality Control : It can inspectproducts on assembly lines for defects, ensuring qualitycontrol and minimizing errors. Real-time detection ofsteel strip surface defects was deployed using Faster R-CNN model [11].\u2022 Agriculture: Drones can monitor crops, detect pests,diseases, and nutrient deficiencies, enabling precisionagriculture. Identifying rice leaf diseases in natural en-vironments was deployed using GoogLeNet [12]. Pepperleaf disease identification was deployed using GoogLeNet[13]. Detection for insect pests was deployed on YOLOX[14].\u2022 Healthcare and Wearables : Wearable devices can con-tinuously monitor vital signs, detect anomalies, and evendiagnose certain health conditions. Medical diagnosis(Covid and Lung Disease Detection) was deployed usingVGG, MobileNet, and AlexNet [15]. Automatically diag-nose pneumonia and COVID-19 from chest X-ray imageswas deployed on DenseNet [16]. Medical applications(e.g., COVID-19 detection, cardiomegaly diagnosis, braintumor classification) were deployed using ResNet [10].\u2022 Energy Management: It can monitor energy usage, opti-mize consumption patterns, and identify areas for energyefficiency improvements. Wind Turbine Maintenance andfault diagnosis was deployed using AlexNet [17].\u2022 Environmental Monitoring : It can monitor air quality,pollution levels, and weather conditions, providing valu-able insights for urban planning. A smartphone app toperform fine-grained classification of animals in the wildwas deployed using AlexNet, GoogLeNet, and ResNet[18]. Identification of mosquito species was deployedusing AlexNet, DenseNet, Inception, ResNet, and VGG[19].--- Page 3 ---3Fig. 2. Evolution of Key CNN Architectures Over Time.\u2022 Logistics and Inventory Management : It can automatepackage sorting, inventory tracking, and warehouse man-agement. Mobile robot to map its surroundings while de-tecting objects and people was deployed using AlexNet,GoogLeNet, and ResNet [18].\u2022 Autonomous Vehicles : Vehicles can process real-timedata from cameras and sensors using CNNs, aiding inautonomous navigation and collision avoidance. Instanceobjects detection system for intelligent service robots wasdeployed using Alexnet [20]. Advanced driving assistancesystems (ADASs) and automated vehicles (A Vs) weredeployed using Faster R-CNN [21].Deploying CNNs on edge devices presents significant chal-lenges mainly due to the limited computational resources,constrained memory, and power consumption constraints in-herent to these devices. CNN models, known for their depthand complexity, often demand substantial computational powerand memory, which may exceed the capabilities of edgehardware. Hence, compressing the model before deploymentbecomes imperative. Model compression techniques aim toreduce the size of the CNN model while preserving its perfor-mance, thereby enabling efficient utilization of computationalresources and memory on edge devices. By compressingthe model, we can mitigate the challenges associated withdeploying CNNs on edge devices, ensuring that they caneffectively perform tasks such as real-time image processing,object detection, and classification within resource-constrainedenvironments.With the enormous number of compression techniques pro-posed for CNNs, the rapid evolution of CNN architectures hascreated a gap in the field. This dynamic shift in architecturedesign requires an evaluation of existing compression methods,particularly in light of the demand to make these advancedCNN models suitable for deployment on edge devices. AsCNN designs continue to advance, the challenge lies inadapting compression techniques to smoothly integrate withthese modern architectures. This evaluation (either for eachindividual techniques or combined with each other) becomesimportant, as it not only ensures the continued relevance ofcompression techniques but also addresses the urgent need tomake resource-intensive CNN models accessible and deploy-able on edge devices.This work aims to evaluate CNN compression techniquesthat assure appropriate performance on edge devices. In thesubsequent sections, this work reveals in a structured mannerto evaluate the compression techniques for CNN models.section 2 provides a detailed review of related work, offeringinsights into prior research and establishing a foundationalunderstanding of the topic. section 3 explains the methodologyemployed in conducting the experiments, describing the designand execution of the study. Following this, section 4 presentsthe experimental results and analyzes the findings to recognizetrends and implications. Section 5 critically evaluates the re-sults. Section 6 draws conclusions regarding the effectivenessand significance of the compression techniques. This organizedframework aims to comprehensively explore and contribute tothe field of model compression for efficient deployment inresource-constrained environments.II. R ELATED WORKWithin the context of edge AI, it is important to address thecritical need for model compression. The resource constrainednature of these devices requires more efficient AI modelsby minimizing memory and computational demands, ensur-ing faster inference speeds, and enhancing energy efficiency.Below will explore various model compression techniques andtheir implications for edge AI applications.A. PruningPruning is a key technique in DNN, aimed at enhancingefficiency and model generalization. It involves the removalof redundant components, such as parameters, neurons, filters,or entire layers, leading to several advantages. By reducingunnecessary parameters, it cuts down on storage require-ments, and important for models deployed on devices withlimited memory. Furthermore, it streamlines computational--- Page 4 ---4Fig. 3. Weight Pruning (a) and Neuron Pruning (b). x: input, w: weight. [22].complexity during inference, resulting in faster predictions andlower power consumption. Pruning also mitigates overfittingby simplifying the network. Various pruning techniques, likeweight pruning, neuron pruning, filter pruning, and layerpruning, offer different levels of granularity in componentremoval. Whether applied during or after training, pruningenables the creation of more compact and efficient CNNmodels tailored to specific needs, effectively balancing modelsize, computational efficiency, and accuracy. Weight pruningsets weight connections in the network to zero if they fallbelow a predefined threshold or are considered redundant.Neuron pruning focuses on removing entire neurons if they arefound to be redundant. Layer pruning allows for the removalof entire layers that are considered less important [22].1) Pruning For Fully Connected Layer: Fully connectedlayers are dense that makes the layer with high memorydemand. Pruning them effectively reduces the memory burdenand reduce size of the model.It involves selectively removing weight connections andneurons to reduce the model\u2019s complexity while preservingperformance. In a typical feed-forward NN, inputs are multi-plied by corresponding weights, and a linear sum is computedat each neuron, which is then transformed by an activationfunction. As shown in Figure 3, a network with 3 inputneurons, 2 hidden neurons, and 1 output neuron may havemultiple weight connections. Pruning can be applied to elim-inate specific weight connections or entire neurons. By doingso, the total number of weight connections can be significantlyreduced, leading to a more compact network. The concept ofpruning was first introduced by [6], who proposed removingweights based on their saliency, with small-magnitude weightshaving less impact on training error. The process involvesiterative retraining to regain accuracy, and the technique isknown as \u2019Optimal Brain Damage (OBD)\u2019 where the secondderivative of the objective function with respect to parametersis used to calculate the small saliency, facilitating informedpruning decisions. Since then, other pruning approaches havebeen introduced for fully connected layers [22].2) Pruning For Convolutional Layer: Each convolutionallayer typically consists of numerous filters that makes the layerwith high computational demand. Pruning these less significantfilters directly from the convolutional layer effectively reducesthe computational burden and speeds up the model. Inspired byearly pruning methods, new approaches have been introducedto be used to prune convolutional layers [22].Bayesian was used to decide what to prune and the level ofpruning, in this context involves employing scale mixtures ofnormals as priors for parameter weights in LeNet and VGG[23]. Differential evolution based layer-wise weight methodalongside three other pruning techniques (Naive Cut, Itera-tive Pruning, and Multi-Objective NN Pruning) was used toprune LeNet, AlexNet, and VGG16 [24]. Two fully connectedlayers are removed from the AlexNet architecture, and BatchNormalization (BN) is introduced to mitigate overfitting [20].Filters Similarity in Consecutive Layers (FSCL) for CNNs wasused to reduce the number of filters while preserving impor-tant filters, ultimately improving model efficiency for VGG,GoogLeNet, and ResNet [25]. Structured pruning throughsparsity-induced pruning was used to enhance the real-timeimplementation of the DEtection TRansformer (DETR) [26].Structured pruning was used to compress YOLOX, this processincluded sparse training to prune unimportant channels, withfine-tuning to recover accuracy [14]. Evolutionary approachto filter pruning involved sequential application of multiplepruners in a specific order to sparsify LeNet and VGG-19while maintaining model accuracy [27]. Multilayer networkswere used to represent and compress ResNets, it involvedcreating class networks, calculating arc weights, and forminga multilayer network. The overall degree of nodes in themultilayer network is used to select a subset of nodes forcompression, and convolutional layers are pruned [10]. Tooptimize the Fused-DenseNet-Tiny model for efficient detec-tion of COVID-19 and pneumonia in chest X-ray images,three steps were implemented including removing insignif-icant weights, discarding pruning casings, and applying acompression algorithm [16]. Deep Scalable Zerotree-based(DeepSZ) framework was used to address resource constraints--- Page 5 ---5by achieving significant compression for LeNet, AlexNet,and VGG while maintaining acceptable inference accuracy[28]. Compressing without retraining that was used withResNet, AlexNet, VGGNet and SqueezeNet. It focused onconvolutional and fully connected layers, while maintainingor improving classification accuracy [29].B. QuantizationQuantization plays an important role in addressing theresource-intensive nature of CNNs. By reducing the bit pre-cision of model parameters, quantization not only conservesmemory and energy but also enhances inference speed, makingit an essential technique for deploying CNNs in resource-constrained environments such as edge devices. Weight clus-tering takes quantization to a more advanced level by orga-nizing weights into clusters, where each cluster shares thesame weight value. This approach minimizes the need forfine-tuning individual weights and can lead to substantialreductions in memory and computational overhead [22].Single Level Quantization (SLQ) and Multiple Level Quan-tization (MLQ) technique were used to quantize AlexNet,VGG, GoogleNet, and ResNet to the deployment of thesemodels on resource-constrained mobile devices like mobilephones and drones [30].C. Low-Rank Decomposition/FactorizationIt is a compression technique used with feed-forward NNsand CNNs, to reduce the size of weight matrices whilepreserving model performance. Singular Value Decomposition(SVD) is a popular factorization scheme that decomposes aweight matrix A into three smaller matrices: U, S, and V T . Urepresents the left singular vectors, S is a diagonal matrix ofsingular values, and V T is the transpose of the right singularvectors. This factorization offers several advantages, such asreduced storage requirements, which is crucial for memory-constrained environments, and accelerated inference, espe-cially in CNNs, as smaller matrices can be convolved faster.Low-rank factorization can be applied to fully connected andconvolutional layers, making models more storage-efficientand faster without sacrificing performance. Careful selectionof the rank is essential for achieving a balance between sizereduction and model accuracy. Later, more approaches havebeen introduced [22].Tucker decomposition for weight tensors was used tooptimizes weight tensor dimensions of LeNet and ResNetmodels [31]. Low-rank decomposition was used as an efficientmethod for compressing AlexNet, VGG, and ResNet withoutthe need for fine-tuning to significantly reduce model sizeand computational complexity to make them more suitablefor resource-constrained mobile and embedded devices [32].Hardware-Aware Automatic Low-Rank Compression frame-work HALOC was used to compress ResNet, VGG andMobileNet, with the goal of efficiently exploring the structure-level redundancy in NNs by integrating principles from neuralarchitecture search (NAS) [33]. Automatically DifferentiableTensor Network (ADTN) method was used to significantlyreduce the number of parameters of fully connected NN,LeNet, and VGG while maintaining or enhancing the per-formance [34]. Joint Matrix Decomposition, specifically JointSVD (JSVD) was used to address the challenge of deployingResNet with numerous parameters on resource-constrainedplatforms. It included Right JSVD, Left JSVD, and BinaryJSVD algorithms [35]. Tensor Ring Networks (TR-Nets) wasused as a method to effectively factorize LeNet and ResNet,thereby reducing computational and memory requirements[36]. Tucker decomposition with rank selection and fine tuningwas used as a one-shot whole network compression schemefor deploying AlexNet, VGG, and GoogLeNet on mobiledevices while maintaining reasonable accuracy [37]. TensorDynamic Low-Rank Training (TDLRT) was used to createa training algorithm with VGG and AlexNet that maintainshigh model performance while significantly reducing memoryrequirements for convolutional layers [38].D. Knowledge Distillation (KD)It is a technique used to transfer the knowledge learnedby a larger, more complex model (the teacher model) to asmaller and lighter model (the student model). The primarygoal of KD is to enable the student model to benefit from thegeneralization capabilities of the teacher model while beingmore lightweight in terms of parameters and computations.This technique helps to recover the accuracy drop occurs dueto implementing other compression techniques.Knowledge transfer and distillation, initially introduced by[39], aimed to compress large ensemble models into smaller,faster counterparts with minimal performance loss. [40] ex-tended this concept by empirically demonstrating that theintricate knowledge within larger DNNs could be effectivelytransferred to smaller, shallower models, yielding comparableaccuracy. This involved training a large DNN and transfer-ring its knowledge to a shallower network while minimizingthe squared difference between the logits produced by thetwo models. These foundational ideas produced knowledgedistillation, a widely used technique for training efficientmodels by transferring knowledge from larger ones. Later,more approaches have been introduced [22].KD was used to improve the compression of LeNet andResNet models when fresh training data is scarce, primarilythrough the use of synthetic data generated by GenerativeAdversarial Networks (GANs) [41]. To fuse information frominfrared and visible images while reducing DenseNet complex-ity and improving inference speed. Insights from pre-trainedteacher models are transferred to the smaller student model[9]. KD was used to develop a lightweight mosquito speciesidentification model (EfficientNet) that balances efficiency andaccuracy through the compression [42].E. Mixed TechniquesDifferent compression techniques are often combined andused together to achieve more effective and comprehensivemodel compression. Each compression technique targets spe-cific aspects of the model, such as reducing model size,computation complexity, or memory footprint.--- Page 6 ---6In-Parallel Pruning-Quantization CLIP-Q method combinesnetwork pruning and weight quantization was used to com-press AlexNet, GoogLeNet, and ResNet [18]. Pruning andquantization were used to optimize the compression ofAlexNet and reduce the number of parameters significantlywhile maintaining accuracy to be implemented on Field-Programmable Gate Array (FPGA) [5]. Pruning, quantization,and Huffman encoding combined with adversarial trainingwere used to enhance the robustness and compression ofAlexNet while also addressing the model vulnerability toadversarial attacks [43]. Pruning and quantization were usedto compress VGG and ResNet for remote sensing imageclassification, balancing computational complexity constraintswhile preserving model accuracy [44]. Low-rank decompo-sition and quantization were used to compress ResNet andMobileNet, and reduce the computational complexity whilepreserving high performance [45]. Pruning, quantization, andchanging the model architecture were used to design a com-pact SqueezeNet with competitive accuracy while significantlyreducing the number of parameters [46]. Quantization andpruning were used to develop an effective model compressionframework for ResNet and MobileNet. The objective was tooptimize the allocation of compression ratios to minimizeperformance degradation while reducing model size [47]. Jointquantization and pruning were used to develop a post-trainingmodel size compression method that efficiently combines lossyand lossless compression techniques to reduce the size ofResNet, MobileNet, RegNet, MNasNet, and YOLOv5 withoutsacrificing accuracy [48].F . Other TechniquesDepthwise separable convolutions was used to improvesteel strip defect detection by creating a real-time and ef-ficient model while maintaining high accuracy using FasterR-CNN [11]. Deferential Evolution was used to develop anefficient and optimized AlexNet, VGG, and MobileNet forCovid and liver disease detection [15]. Genetic Algorithmwas used to reduce the storage space and inference timeof VGG, ResNet, AlexNet, and SqueezeNet models [49].Factorization (changing kernel size) was used to improvethe accuracy and computing efficiency of pepper leaf diseasedetection using GoogLeNet, specifically for the agriculturalindustry [13]. Flexible and Separable Convolution (FSConv)was used to reduce computational costs without compromisingthe accuracy of VGG, ResNet, Faster R-CNN and RetinaNet[50]. Efficient Layer Compression (ELC) was used to enhancethe computational efficiency of VGG, ResNet, and ConvNeXtwhile preserving their representation capabilities [51].III. D ESIGN OF THE EXPERIMENTSThe experiments aimed to evaluate various compressiontechniques, namely pruning and quantization, on differenttypes of ConvNext [52] model. The experiments includedtraining, fine-tuning, and evaluating of models using CIFAR-10 dataset. The setup involved conducting experiments bothon cloud-based platforms and on edge devices to evaluate theperformance of the compressed models.Fig. 4. Block modifications and resulted specifications. (a) is a ResNeXtblock; in (b) we create an inverted bottleneck block and in (c) the positionof the spatial depthwise conv layer is moved up [52].A. ConvNeXt [52]Is a modern CNN family produced as a journey of graduallymodernize a standard ResNet toward the design of a visionTransformer. The journey starts from a ResNet-50 model, intoa CNN architecture that mirrors some aspects of Transformers,particularly Swin Transformers. The roadmap:1) Training Techniques: Vision Transformer training pro-cedures were used to train ResNet-50 model, this includedextending the training to 300 epochs (90 epochs originally),using AdamW optimizer, and data augmentation techniques(Mixup, Cutmix, RandAugment, Random Erasing, and regu-larization schemes including Stochastic Depth).2) Macro Design: Number of blocks in each stage wasadjusted from (3, 4, 6, 3) to (3, 3, 9, 3) and the stem wasreplaced with a patchify layer implemented using a 4x4, stride4 convolutional layer (non-overlapping convolution).3) ResNeXt-ify: ResNeXt approach was adopted which isutilize grouped convolutions, where convolutional filters aredivided into groups, each handling a subset of input channels,a variation of grouped convolution known as depthwise convo-lution was adopted, and the network\u2019s width was expanded byincreasing the number of channels in the convolutional layers.4) Inverted Bottleneck: The hidden dimension of the MLPblock was changed to be four times wider than the inputdimension as shown in Figure 4 (a and b)5) Large Kernel Sizes: The position of the convolutionallayer is moved up and the kernel size was changed from (3x3)to (7x7) as shown in Figure 4 (a and c).B. Micro DesignReplacing ReLU with Gaussian Error Linear Unit (GELU),fewer normalization layers, Substituting Batch Normalization(BN) with Layer Normalization (LN), and introducing separatedownsampling layers as shown in Figure 5.C. Compression Techniques1) Pruning: Different pruning techniques have been usedincluding structured and unstructured techniques.\u2022 Only Train Once (OTO) [53]: OTO version 3 (OTOV3)is automated framework for structured pruning which in-volves removing entire structures or groups of parametersfrom a DNN. OTOv3 begins by analyzing the depen-dencies between the vertices of the target DNN. Thisanalysis involves identifying accessory, Shape-Dependent--- Page 7 ---7Fig. 5. Block designs for a ResNet, a Swin Transformer, and a ConvNeXt.Swin Transformer\u2019s block is more sophisticated due to the presence of multiplespecialized modules and two residual connections [52].(SD) joint, and unknown vertices that are adjacent andestablishing their interdependencies. The goal is to formnode groups based on these dependencies, laying thefoundation for identifying interdependent vertices duringstructured pruning.Using the information gathered from the dependencyanalysis, OTOv3 constructs a pruning dependency graph.This graph represents the interdependencies between ver-tices, with vertices in the same node group indicatingtheir interdependency during structured pruning. Thepruning dependency graph ensures the validity of theproduced subnetwork by preserving essential connectionsbetween vertices.OTOv3 partitions the trainable variables of the DNNinto Pruning Zero-Invariant Groups (PZIGs) based on thepruning dependency graph. PZIGs consist of pairwisetrainable variables grouped together, with each grouprepresenting a potential pruning structure. Node groupsadjacent to the DNN output and containing unknownvertices are excluded from forming PZIGs to preserveoutput shapes and ensure model robustness as shown inFigure 6.To jointly search for redundant pruning structures andtrain the remaining groups for optimal performance,OTOv3 employs the Dual Half-Space Projected Gradient(DHSPG) algorithm. DHSPG minimizes the objectivefunction while introducing a sparsity constraint to identifyredundant groups for removal. It employs saliency-drivenredundant identification and a hybrid training paradigmto control sparsity and achieve better generalization per-formance as shown in Figure 7.\u2022 L1 Unstructured [54] L1 unstructured pruning is atechnique used in machine learning, to reduce the sizeof neural networks by eliminating less important con-nections. Each weight in the network is assigned a scorebased on its magnitude. This score reflects the importanceof the weight in the network\u2019s performance. In l1 pruning,this score is often the absolute value of the weight.A threshold is set, typically by selecting the top x% ofweights based on their magnitude scores. The thresholddetermines which weights will be pruned and which willbe retained.Weights that fall below the threshold are pruned, meaningthey are set to zero and effectively removed from thenetwork. This results in a sparser network architecturewith fewer connections.\u2022 Random Unstructured [55] Similar to l1 unstructuredpruning, random unstructured pruning is also a techniqueused in machine learning, to reduce the size of neuralnetworks by eliminating less important connections. Thedifference is the pruned weight are selected randomlyinstead of using l1 to decide the importance of theweights.2) Dynamic Quantization [30]:Dynamic quantization is anapproach aimed at optimizing the deployment of neural net-works by reducing the precision of the weights. Unlike tradi-tional quantization methods that apply a fixed quantization bit-width across all layers of the network, dynamic quantizationadapts the quantization bit-width for each layer individuallybased on its representation abilities and capacities. This isachieved through the use of a bit-width controller module,which employs a policy gradient-based training approach tolearn the optimal bit-width for each layer. By dynamicallyadjusting the quantization bit-width, dynamic quantization canstrike a balance between maintaining accuracy and reducingmemory size and computational costs.D. CIFAR-10 [56]CIFAR-10 is a dataset used for computer vision and ma-chine learning research, offering a rich resource for trainingand evaluating image classification algorithms. Comprising60,000 32x32 RGB color images across 10 distinct classes(Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse,Ship, and Truck), CIFAR-10 facilitates comprehensive explo-ration of diverse visual concepts. With each class containing6,000 images and a balanced distribution across the dataset,CIFAR-10 presents a well-structured foundation for model de-velopment. Its division into 50,000 training images and 10,000test images, further segmented into multiple batches, enablesstrict evaluation and benchmarking of algorithms. In terms ofcomputational requirements, CIFAR-10 generally requires lesscomputation compared to CIFAR-100 and ImageNet due to itssmaller image size and fewer classes which makes it suitablefor experiments with limited computational resources.E. Experiment SetupTwo types of experiments have been conducted, cloud-basedexperiments that focused on compressing the models and eval-uating the techniques and edge-based experiment experimentto evaluate the performance of one of the compressed models.--- Page 8 ---8Fig. 6. Automated PZIG Partition [53].Fig. 7. Search Direction in DHSPG [53].1) Cloud-based Experiment Setup: Google Colab Pro+was used to utilize GPU resources (NVIDIA A100 and V100Tensor Core GPUs), facilitating accelerated model trainingand evaluation and background execution. The integrationwith Google Drive reduced overheads associated with upload-ing and downloading model data to and from cloud. Theevaluation framework was designed to profile the originalmodel, compress it, profile the compressed model, and conductcomparison between the measurements before and after thecompression as shown in Figure 8.This profiling process involved measuring several key met-rics:\u2022 Accuracy: The classification accuracy achieved by themodel on the validation dataset.\u2022 Model Size: The size of the model in megabytes (MB).\u2022 Number of Parameters : The total count of trainableparameters in the model, measured in millions (M).\u2022 Number of MACs : The number of multiply-accumulateoperations performed during inference, measured in mil-lions (M).\u2022 Number of Non-Zero Parameters : The count of non-zero parameters in the model, essential for pruning-basedtechniques.2) Edge-based Experiment Setup: A compressed modelwas deployed on edge with CPU (11th Gen Intel(R) Core(TM)i7-1165G7 @ 2.80GHz 2.80 GHz), RAM (16GB), and laptopintegrated camera.2 samples from each of CIFAR-10 classes have been se-lected randomly from the internet, printed on A4 papers, andplaced in front of the camera to measure the accuracy and theinference time.--- Page 9 ---9Fig. 8. Compression Evaluation Framework.TABLE IOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT TINY, SMALL , BASE AND LARGE .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)Tiny Full 63.81 106.26 26.53 86.88 18.41Compressed 63.81 41.31 10.29 47.80 10.30Small Full 63.48 188.89 47.16 169.28 29.55Compressed 63.48 48.04 11.94 68.24 11.96Base Full 61.22 334.28 83.50 299.20 52.22Compressed 61.22 84.12 20.96 119.40 20.98Large Full 63.40 748.82 187.12 669.30 116.93Compressed 63.40 187.32 46.75 264.69 46.78IV. R UNNING THE EXPERIMENTS AND EXPERIMENTALRESULTSA. Cloud-Base Experimentsdifferent experiments have been conducted on cloud to eval-uate different compressing techniques and different versions ofConvNeXt model.1) Evaluate OTOV3 on Untrained Torch ConvNext Tiny,Small, Base, and Large: Untrained ConvNeXt tiny, small,base, and large have been imported from Torch and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and 200epochs were used for training and pruning. OTOV3 achievedhigh performance (Table I) with reducing the model size(61% for tiny and 75% for small, base, and large), numberof parameters (61% for tiny and 75% for small, base, andlarge), and MACs (45% for tiny and 60% for small, base,and large) as shown in Figure 9. Meanwhile OTOV3 wasable to increase both the full and compressed model accuracythrough the training and pruning without any accuracy dropafter pruning comparing to the full model.To investigate the effect of OTOV3 on the model archi-tecture, a comparison has been conducted between ConvNeXtsmall before and after compression. The Torch implementationof the model consist of many CNBlocks, each CNBlock con-sist of Conv2d, Permute, LayerNorm, Linear, GELU, Linear,and Permute layers. As shown in Figure 10, OTOV3 reducednumber of output features of the Linear layer (sequence 3)and the input features of the next Linear layer (sequence 5)and considering the big number of CNBlock in the modelarchitecture, the reduction in model size and number ofparameters after compression is justified as shown in TableII.2) Evaluate OTOV3 on Untrained ConvNext Small (Torchvs. TIMM): Two untrained ConvNeXt small have been im-ported, one from Torch and one from TIMM [57] and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and200 epochs were used for training and pruning. Although thecompression performance was same with size reduction (75%)but the accuracy after 200 epochs was less for Torch model(63%) comparing to TIMM model (73%) as shown in Figure11.To investigate the accuracy performance of OTOV3 withTorch and Timm ConvNeXt Small, a comparison has beenconducted between the two model architectures. The Torchmodel uses the CNBlock structure, which includes additionaloperations such as Permute and varying StochasticDepth prob-abilities. The TIMM model follows a simpler structure withConv2d and LayerNorm, lacking the additional complexitiesintroduced by CNBlock and associated operations in theTorch model as shown in Figure 12 which effects OTOV3performance regarding the accuracy.3) Evaluate OTOV3 on Fine-Tuned Torch ConvNext Small:A pre-trained ConvNeXt small have been imported from Torchand fine-tuned on CIFAR-10 with 100 epochs, the accuracyreached 89.5%. This fine-tuned ConvNeXt small will be usedfor the rest of cloud-base experiments. This model was usedto evaluate OTOV3, CIFAR-10 was used for training andevaluation, and 200 epochs were used for training and pruning.OTOV3 achieved high performance (Table III) 74% reductionin model size and number of parameters, 60% reduction inMACs, and 3.8% increase with accuracy as shown Figure13. The accuracy of the full model in (Table III) (92.86%)is different that the accuracy of the original model used inthe experiment (89.5%), that because OTOV3 trained the full--- Page 10 ---10Fig. 9. OTOV3 Compression Performance with ConvNeXt Tiny, Small, Base and Large.Fig. 10. Comparison Between ConvNeXt Small Full (Left) and Compressed (Right).Fig. 11. OTOV3 Comparison Between Torch and Timm ConvNeXt Small.model during the process which increased the model accuracy.4) Evaluate Unstructured Pruning: The Fine-tuned Con-vNext Small was used to evaluate Pytorch L1 UnstructuredPruning and Random Unstructured Pruning by using differentcombinations of weights pruning percentages for linear (.1 to.9) and convolutional (.1 to .9) layers. In both experiments,the accuracy and the number of non-zero parameters weredropping as the values of weights pruning percentages for bothlinear and convolutional amounts were increasing as shown inFigure 14 a and b. Although the accuracy dropped but themodel size, number of parameters, and MACs didn\u2019t changeas these techniques zero the weights instead of removing them.5) Evaluate Dynamic Quantization: The Fine-tuned Con-vNext Small was used to evaluate Pytorch dynamic quantiza-tion, 8-bit integer was used during the experiment and CIFAR-10 was used for evaluation. Dynamic quantization achievedhigh performance (Table IV) 71% reduction in model size,95% reduction with number of parameters and MACs, and0.1% drop with accuracy as shown in Figure 156) Evaluate Combination of OTOV3 and Dynamic Quan-tization: Two compression stages were evaluated together asone compression pipeline, OTOV3 and Dynamic Quantization.The compressed ConvNeXt small model that was produced inexperiment 4.1.3 (OTOV3) was furtherly compressed using--- Page 11 ---11Fig. 12. Comparison Between ConvNeXt Small TIMM (Left) and Torch (Right).Fig. 13. OTOV3 Compression Performance with ConvNeXt Small Tuned.Pytorch dynamic quantization using 8-bits integer (similar toexperiment 4.1.5). The pruned models using OTOV2 [58]had dependencies on Open Nural Network Exchange (ONNX)[59] which made it not applicable to be combined with othercompression technique like quantization. In OTOv3, there wasengineering changes produced the pruned model directly inPytorch format, which enhanced the flexibility to be combinedwith quantization as this experiment did [53].Pruning using OTOV3 and Quantization using Pytorchdynamic quantization achieved high performance (Table V)89.7% reduction in model size, 95% reduction with numberof parameters and MACs, and 3.8% increase with accuracy asshown in 16.B. Edge-Base ExperimentThe final compressed ConvNeXt Small model in experimentIV .A.6 (OTV3 and Dynamic Quantization) was deployed onedge and the printed samples used to measure the accuracyand inference time by placing them in front of the camera.The compressed model achieved 92.5% accuracy and 20msinference time. Figure 17 shows samples of the output.V. A NALYSIS OF EXPERIMENTAL RESULTSHere, the experimental results obtained from running aseries of experiments will be analyzed, the experiments aimedat evaluating various compression techniques applied to Con-vNeXt models. The experiments were designed to evaluate theperformance of techniques such as pruning and quantizationon different sizes of ConvNeXt models, with a focus on bothcloud-based compression experiments and edge-based deploy-ment experiment. The analysis will be conducted with respectto the work\u2019s aim of evaluating CNN compression techniquesthat assure appropriate performance (size and inference time)on edge devices and resource-constrained environments.A. OTOV3 Evaluation on Untrained ConvNeXt ModelsThe evaluation of OTOV3 on untrained ConvNeXt modelsdemonstrated its effectiveness in achieving substantial com-pression while increasing model accuracy for both full andcompressed models. Across varying sizes of ConvNeXt mod-els, OTOV3 consistently produced impressive reductions inmodel size, number of parameters, and MACs, highlighting itsability to prune redundant structures effectively. This suggests--- Page 12 ---12Fig. 14. Unstructured Pruning Performance with ConvMeXt Small.TABLE IICOMPARISON FOR NUMBER OF OUTPUT FEATURES AND INPUTFEATURES OF THE LINEAR LAYERS IN THE CNB LOCKS BEFORE ANDAFTER OTOV3 C OMPRESSION .Layer Index Input & Output FeaturesSequential CNBlock Full Model CompressedModel1 0 384 3841 1 384 3841 2 384 3843 0 768 7673 1 768 7043 2 768 7265 0 1536 2515 1 1536 3495 2 1536 2425 3 1536 3785 4 1536 2935 5 1536 3775 6 1536 3405 7 1536 4005 8 1536 3945 9 1536 4785 10 1536 4145 11 1536 4245 12 1536 4105 13 1536 3185 14 1536 4885 15 1536 4885 16 1536 4025 17 1536 2465 18 1536 4025 19 1536 4585 20 1536 3235 21 1536 4195 22 1536 4465 23 1536 4445 24 1536 4415 25 1536 4685 26 1536 10707 0 3072 2087 1 3072 2547 2 3072 262Fig. 15. Dynamic Quantization Compression Performance with ConvNeXtSmall Tuned.Fig. 16. OTOV3 and Dynamic Quantization Compression Performance withConvNeXt Small Tuned.--- Page 13 ---13TABLE IIIOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 92.86 188.89 47.16 169.28 29.80Compressed 92.86 50.03 12.44 67.41 12.46TABLE IVDYNAMIC QUANTIZATION COMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 89.53 188.89 47.16 169.28 47.17Compressed 89.40 54.21 2.15 7.27 2.17TABLE VOTOV3 AND DYNAMIC QUANTIZATION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy % Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number of NonZeroParameters (M)Full 89.53 188.89 47.16 169.28 47.17Phase 1 (OTOV3) 92.86 50.03 12.44 67.41 12.46Phase 2 (Dynamic Quantization) 92.93 19.39 2.15 7.27 2.17Fig. 17. Samples From the Compressed Model Output on Edge.that OTOV3 efficiently identifies and removes unnecessaryparameters from the models, leading to more streamlinedarchitectures without compromising predictive performance.However, an unexpected observation arose when comparingthe accuracy of the Torch implementation with that of theTIMM implementation of ConvNeXt Small. The Torch im-plementation exhibited lower accuracy compared to its TIMMcounterpart, indicating that OTOV3\u2019s performance regardingaccuracy may be influenced by the details of the modelarchitecture. This unexpected result suggests that differenttraining strategies or adjustments may be necessary to optimizeOTOV3\u2019s performance across various model implementations,emphasizing the importance of considering architectural dif-ferences when applying compression techniques like OTOV3.B. OTOV3 Evaluation on Fine-Tuned ConvNeXt ModelsWhen evaluating OTOV3\u2019s performance on a fine-tunedConvNeXt model, notable improvements in compression per-formance were observed, confirming its effectiveness in re-ducing model size, parameters, and MACs while marginallyenhancing accuracy. This outcome highlights the potential ofintegrating fine-tuning with structured pruning techniques toachieve even greater optimization of model performance. Thefine-tuned model displayed enhanced compression capabilitiescompared to untrained models, suggesting that pre-trainingcan significantly increase the effectiveness of compressiontechniques. This finding highlights the importance of lever-aging pre-existing knowledge within models to maximize thebenefits of compression, ultimately resulting in CNN modelswith higher performance.C. Unstructured Pruning TechniquesDuring the evaluation of l1 unstructured and random un-structured pruning techniques, expected trends were observedregarding accuracy and compression. As the pruning per-centages increased for both linear and convolutional layers,a corresponding decrease in accuracy was noted, while themodel size, parameters, and MACs remained unaltered. Thisoutcome aligns with the inherent nature of unstructured prun-ing, wherein weights are zeroed out but not entirely eliminated,resulting in sparse models without substantial reductions incomputational complexity. However, the lack of significantreductions in computational complexity may constrain their ef-fectiveness, particularly in resource-constrained environmentswhere efficient utilization of computational resources is essen-tial. This highlights the importance of considering the trade-offs between model compression and computational efficiencywhen selecting pruning techniques for deployment in real---- Page 14 ---14TABLE VIOTOV3 AND DYNAMIC QUANTIZATION REDUCTION NUMBERS WITH CONV NEXT SMALL TUNED .Technique AccuracyChange Size Reduction ParametersReductionMACsReductionOTOV3 3.75% 73.51% 73.62% 60.18%Dynamic Quantization -0.15% 71.30% 95.43% 95.71%OTOV3 and DynamicQuantization 3.80% 89.74% 95.43% 95.71%world applications, especially in edge computing scenarioswhere computational resources are limited.D. Dynamic QuantizationDynamic quantization emerged as a highly effective tech-nique for model compression, demonstrating remarkable re-ductions in model size, parameters, and \n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2409.02134v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================


=== PHD RESPONSE LOG ===

Interaction 1:
```SUMMARY
real-time edge image classification optimization
```
==================================================
Interaction 2:
```FULL_TEXT
2409.08943v1
```
==================================================
Interaction 3:
```SUMMARY
edge device image classification optimization
```
==================================================
Interaction 4:
```FULL_TEXT
2307.04443v1
```
==================================================
Interaction 5:
```ADD_PAPER
2409.08943v1
This paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, accuracy, and denoising performance. Key experimental results show their NAS-designed models outperform manually designed alternatives, achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency. The integrated approach demonstrates superior denoising quality compared to sequential models while maintaining competitive classification performance, making it particularly suitable for applications requiring human verification like surveillance and medical imaging.
```
==================================================
Interaction 6:
```ADD_PAPER
2307.04443v1
This paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.
```
==================================================
Interaction 7:
```SUMMARY
edge device image classification latency optimization
```
==================================================
Interaction 8:
```FULL_TEXT
2210.06223v1
```
==================================================
Interaction 9:
```ADD_PAPER
2210.06223v1
This paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach guided by a novel latency prediction model that accounts for algorithm design, scheduling strategies, and hardware properties. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method achieves these gains through optimized spatial granularity selection and operator fusion techniques, demonstrating significant improvements over pixel-level dynamic networks in practical deployment scenarios.
```
==================================================
Interaction 10:
```SUMMARY
edge device image classification energy efficiency
```
==================================================
Interaction 11:
```FULL_TEXT
2005.10851v1
```
==================================================
Interaction 12:
```ADD_PAPER
2005.10851v1
This paper presents a conditionally deep hybrid neural network for edge-cloud collaborative inference, combining quantized layers at the edge with full-precision layers in the cloud. The key innovation is an early-exit mechanism that allows easy samples to be classified at the edge, reducing both computation and communication costs. Experimental results show the approach achieves 5.5x energy reduction on CIFAR-10 (processing 65% of samples at edge) and 4.8x reduction on CIFAR-100 while maintaining accuracy. The hybrid quantization strategy and joint optimization of early exits demonstrate significant improvements over full-precision networks for distributed intelligence systems.
```
==================================================
Interaction 13:
```SUMMARY
edge device image classification model compression
```
==================================================
Interaction 14:
```FULL_TEXT
2409.02134v1
```
==================================================
Interaction 15:
```SUMMARY
edge device image classification model compression quantization
```
==================================================
Interaction 16:
```SUMMARY
edge image classification quantization efficiency tradeoffs
```
==================================================
Interaction 17:
```SUMMARY
edge device image classification accuracy-latency tradeoffs
```
==================================================
Interaction 18:
```SUMMARY
edge image classification energy-accuracy optimization
```
==================================================
Interaction 19:
```FULL_TEXT
2211.09897v1
```
==================================================
Interaction 20:
```ADD_PAPER
2211.09897v1
This paper presents an efficient feature compression method for edge-cloud image classification systems, focusing on optimizing the three-way trade-off between bit rate, classification accuracy, and encoding complexity (RAC trade-off). The authors propose a flexible encoder architecture with residual blocks that can be scaled based on edge device capabilities, along with a single-stage training strategy. Key results show their method achieves 51.4% lower encoding latency and 0.93% higher accuracy compared to prior work (Entropic Student), while supporting adjustable complexity levels. The system demonstrates superior RAC performance across different bit rates, particularly excelling at lower bit rates (0.1-0.8 bpp) where it maintains accuracy while reducing computational overhead.
```
==================================================
Interaction 21:
```SUMMARY
edge device image classification neural architecture search
```
==================================================
Interaction 22:
```FULL_TEXT
2409.08943v1
```
==================================================
Interaction 23:
```ADD_PAPER
2409.08943v1
This paper presents a novel approach for joint image denoising and classification optimized for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, classification accuracy, and denoising performance. Key results show their NAS-designed models outperform manually designed alternatives by significant margins - achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency, while reducing computational overhead. The integrated design proves particularly effective for human-in-the-loop applications like medical imaging and surveillance systems where both classification and visual clarity are crucial.
```
==================================================
Interaction 24:
```SUMMARY
edge device image classification hardware-aware NAS
```
==================================================
Interaction 25:
```FULL_TEXT
1905.04159v1
```
==================================================
Interaction 26:
```ADD_PAPER
1905.04159v1
This paper introduces Single-Path NAS, a novel differentiable neural architecture search method that optimizes convolutional networks for edge devices by viewing NAS as selecting subsets of weights from an over-parameterized "superkernel" rather than choosing between separate paths. Key innovations include: (1) A single-path formulation that reduces search cost to 8 epochs (30 TPU-hours), achieving up to 5,000x speedup over prior work; (2) Direct optimization for target device latency (79ms on Pixel 1) through a differentiable runtime model; (3) State-of-the-art 74.96% ImageNet accuracy in the <80ms latency range. The method demonstrates how hardware-aware NAS can efficiently discover Pareto-optimal architectures by jointly optimizing accuracy and on-device performance.
```
==================================================
Interaction 27:
```SUMMARY
edge device image classification dynamic inference
```
==================================================
Interaction 28:
```FULL_TEXT
2404.07217v2
```
==================================================
Interaction 29:
```ADD_PAPER
2404.07217v2
This paper presents an attention-aware semantic communication framework for collaborative inference between edge devices and cloud servers using Vision Transformers (ViTs). Key innovations include: (1) Attention-aware patch selection that leverages edge device attention scores to identify and transmit only critical image patches (reducing communication overhead by 68% on ImageNet); (2) Entropy-aware transmission that uses min-entropy to determine when to offload computation to the server; (3) A novel finding that lightweight edge models (DeiT-Tiny) can better identify semantically important patches than larger server models despite lower classification accuracy. The framework achieves server-level accuracy (80.84% vs DeiT-Base's 81.8%) while significantly reducing both communication costs and server computation through selective patch processing. Experimental results demonstrate superior performance compared to prior methods like MAE-based offloading, with particular advantages in low-bandwidth scenarios.
```
==================================================
Interaction 30:
```SUMMARY
edge device image classification early exit networks
```
==================================================
Interaction 31:
```FULL_TEXT\n2311.18098v1
```
==================================================
Interaction 32:
```SUMMARY
edge image classification early exit dynamic inference
```
==================================================
Interaction 33:
```FULL_TEXT
2105.09121v3
```
==================================================
Interaction 34:
```ADD_PAPER
2105.09121v3
This paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with only one transformer encoder layer. Extensive experiments on image classification (CIFAR-10/100), audio classification (Speech Commands, GTZAN), and audiovisual crowd counting demonstrate SL-ViT's effectiveness across modalities. Key results show SL-ViT achieves higher accuracy in 26/27 tested scenarios while reducing parameters and FLOPS. The paper also introduces a copycat fine-tuning strategy (CC-SL-ViT) that further improves performance. This work enables more efficient dynamic inference for edge devices through improved early exit architectures.
```
==================================================
Interaction 35:
```SUMMARY
edge image classification model compression techniques
```
==================================================
Interaction 36:
```FULL_TEXT
2409.02134v1
```
==================================================
Interaction 37:
```ADD_PAPER
2409.02134v1
This paper evaluates model compression techniques (structured/unstructured pruning and dynamic quantization) for ConvNeXt models on CIFAR-10 image classification. Key findings show structured pruning (OTOV3) achieves 75% model size reduction while maintaining accuracy, and dynamic quantization reduces parameters by 95%. The combination of OTOV3 pruning and quantization yields 89.7% size reduction, 95% parameter/MAC reduction, and 3.8% accuracy improvement. Edge deployment of the compressed model demonstrates 92.5% accuracy with 20ms inference time. The work provides empirical evidence that structured pruning combined with quantization is highly effective for optimizing CNNs for edge devices while preserving performance.
```
==================================================
