=== POSTDOC PROMPT LOG ===

Interaction 1:
('', "Current Literature Review: Provided here is a literature review on this topic:\\narXiv ID: 2409.08943v1, Summary: This paper presents a novel approach for joint image denoising and classification optimized for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, achieving better efficiency than sequential approaches. They modify a Neural Architecture Search (NAS) method to optimize for target latency, classification accuracy, and denoising performance simultaneously. Experimental results show their NAS-designed models outperform manually designed alternatives, with significant improvements in both denoising (PSNR/SSIM) and classification accuracy while meeting real-time latency constraints. The approach is particularly effective for applications like surveillance systems where human verification of automated classifications is needed.\\narXiv ID: 2307.04443v1, Summary: This paper introduces DCA-NAS, a device constraints-aware neural architecture search framework that optimizes deep learning models for edge devices by incorporating memory and computational constraints directly into the search objective. The method uses weight sharing and channel bottleneck techniques to significantly reduce search time (4-17x faster than prior hardware-aware NAS approaches) while discovering architectures that are 10-15x smaller than manual designs with comparable accuracy. Experimental results on CIFAR-10, TinyImagenet and ImageNet demonstrate the framework's ability to find Pareto optimal solutions that balance model size, FLOPs, and accuracy. The approach is particularly relevant for real-time image classification on edge devices as it automatically adapts to varying hardware constraints without requiring extensive hyperparameter tuning.\\narXiv ID: 2301.10281v1, Summary: This paper presents PIT (Pruning in Time), a lightweight Neural Architecture Search (NAS) method specifically designed for optimizing Temporal Convolutional Networks (TCNs) on edge devices. PIT jointly optimizes three key architectural parameters: number of channels, receptive field, and dilation factor through a novel differentiable masking approach. Experimental results on four edge-relevant tasks (PPG-based heart rate monitoring, ECG arrhythmia detection, sEMG gesture recognition, and keyword spotting) show PIT can find models with 15.9-152x fewer parameters than seed networks while maintaining accuracy. When deployed on GAP8 IoT processor and STM32H7 MCU, PIT-optimized models achieve up to 5.5x latency reduction and 3.8x energy savings compared to hand-tuned baselines. The method is particularly notable for its efficient search process (comparable to single training) and ability to explore a large search space (up to 10^12 architectures) while maintaining low GPU memory requirements.\\narXiv ID: 2307.04443v1, Summary: This paper presents DCA-NAS, a device constraints-aware neural architecture search framework that optimizes deep learning models for edge devices by incorporating hardware constraints like model size and FLOPs directly into the search objective. The method introduces weight sharing and channel bottleneck techniques to significantly reduce search time (4-17x faster than prior hardware-aware NAS approaches). Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 15x smaller than manual architectures with comparable accuracy, and 32% smaller than ProxylessNAS with similar performance. The approach is particularly notable for its ability to automatically adapt to different hardware constraints without requiring extensive hyperparameter tuning, making it practical for real-world edge deployment scenarios.\\narXiv ID: 2409.08943v1, Summary: This paper presents a novel approach for joint image denoising and classification optimized for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, significantly improving efficiency over sequential approaches. They modify a Neural Architecture Search (NAS) method to optimize for target latency, classification accuracy, and denoising performance simultaneously. Experimental results show their NAS-designed models outperform manually designed alternatives, achieving better human perception of noisy images while maintaining real-time performance on edge hardware. The integrated architecture reduces computational overhead by 45% compared to separate denoising and classification models, making it particularly suitable for applications like medical imaging and surveillance systems where both machine classification and human verification are important.\\narXiv ID: 2409.02134v1, Summary: This paper evaluates various model compression techniques (structured/unstructured pruning, dynamic quantization) on ConvNeXt models for edge image classification. Key results show OTOV3 pruning achieves 75% model size reduction while maintaining accuracy, and dynamic quantization provides 95% parameter reduction. The combined approach yields 89.7% size reduction and 3.8% accuracy improvement. On-device deployment demonstrates 92.5% accuracy with 20ms latency, validating effectiveness for edge deployment. The work provides comprehensive empirical analysis of compression tradeoffs for resource-constrained devices.\\narXiv ID: 2101.09336v1, Summary: This comprehensive survey paper provides a detailed taxonomy and analysis of hardware-aware neural architecture search (HW-NAS) methods. It categorizes approaches based on search space design, search strategies, acceleration techniques, and hardware cost estimation methods. The paper is particularly relevant as it focuses on optimizing neural architectures for resource-constrained platforms like edge devices, discussing tradeoffs between accuracy and hardware efficiency metrics (latency, energy, memory). It also identifies key challenges in deploying NAS-generated models on edge devices and suggests future research directions.\\narXiv ID: 2503.02891v3, Summary: This comprehensive survey paper provides a systematic analysis of model compression and acceleration techniques for Vision Transformers (ViTs) on edge devices. It covers three main optimization approaches: (1) Model compression techniques including pruning (structured/unstructured, static/dynamic), knowledge distillation (logit-based, feature-based, relation-based), and quantization (uniform/non-uniform, PTQ/QAT); (2) Hardware-aware acceleration strategies for GPUs, FPGAs and ASICs; (3) Software-hardware co-design approaches. The paper provides detailed comparisons of different methods' impact on accuracy, efficiency (GFLOPS reduction up to 61.4%), and hardware adaptability, along with practical deployment considerations for real-time edge applications.\\narXiv ID: 2005.02177v1, Summary: This paper presents Classification Driven Compression (CDC), a framework for edge-cloud collaborative deep learning that optimizes the tradeoff between bandwidth consumption and classification accuracy. The key innovation is a lightweight autoencoder with classification guidance that selectively preserves features critical for accurate classification while compressing data. CDC achieves 14.9x bandwidth reduction with \u22641.06% accuracy drop compared to raw data training, and outperforms standard autoencoders by at least 100% in accuracy preservation. The method includes an adjustable quantization scheme for dynamic compression ratio adaptation based on network conditions, making it particularly suitable for resource-constrained edge deployments.\\narXiv ID: 2307.04443v1, Summary: This paper presents DCA-NAS, a device constraints-aware neural architecture search framework that optimizes models for edge devices by incorporating hardware constraints like model size and FLOPs directly into the search objective. The method uses weight sharing, channel bottlenecks, and derived cells to significantly reduce search time (4-17x faster than prior work) while discovering architectures that are 10-15x smaller than manual designs with comparable accuracy. Key innovations include a differentiable resource constraint formulation and a lookup graph for efficient constraint mapping. Experiments on CIFAR-10, TinyImagenet and ImageNet show the approach achieves state-of-the-art accuracy-latency tradeoffs, with models that maintain performance while being 32% smaller than ProxylessNAS and having lower inference latency than PC-DARTS on edge devices.")\n~~~~~~~~~~\nHistory: \n~~~~~~~~~~\nCurrent Step #0, Phase: plan formulation\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: \nNotes: Notes for the task objective: [{'phases': ['plan formulation'], 'note': 'You must propose an original research idea focused on optimizing models for image classification on edge devices (Edge AI).'}, {'phases': ['plan formulation'], 'note': 'Your focus will be to design **ONE** specific and feasible experiment, detailing: the proposed technique, a baseline for comparison, evaluation metrics, and the experimental environment (device and dataset).'}, {'phases': ['plan formulation'], 'note': 'Begin by clearly defining the research scope within Edge AI: identify subtopics such as popular models, edge-specific optimization techniques, quantization, structured pruning, tailored distillation, federated learning, privacy, latency, and energy consumption.'}, {'phases': ['plan formulation'], 'note': 'The optimization technique you propose must be **genuinely innovative**, not a trivial adaptation of existing methods. Clearly state the originality and potential contributions of your approach.'}, {'phases': ['plan formulation'], 'note': 'Specify the base model (e.g., MobileNetV2 without optimization), the target edge device (e.g., Raspberry Pi 4 with X GB RAM), and the dataset (e.g., CIFAR-10 or an adapted ImageNet subset).'}, {'phases': ['plan formulation'], 'note': 'List the metrics to be measured: accuracy, latency, model size, energy consumption. Describe your methodology for measuring each.'}, {'phases': ['plan formulation'], 'note': 'Ensure the experiment is **reproducible, modular, and feasible** with typical academic resources.'}, {'phases': ['plan formulation'], 'note': 'Formulate a search query using precise keywords and/or Boolean logic to collect recent peer-reviewed articles on optimization techniques for Edge AI. Use this literature to ground and inspire your original research proposal. Justify your query choices.'}, {'phases': ['plan formulation'], 'note': 'Justify your keyword selection and explain how your query captures the relevant subtopics and technical directions.'}, {'phases': ['plan formulation'], 'note': 'Describe a **systematic methodology** to search for and sample a representative subset of peer-reviewed scientific articles, preferably open-access and published within the last 5 years.'}, {'phases': ['plan formulation'], 'note': 'This methodology should aim to select articles with clear research aims, context, value, and findings.'}, {'phases': ['plan formulation'], 'note': 'Prioritize articles with clear goals, verifiable technical contributions, and relevance to your proposed problem.'}, {'phases': ['plan formulation'], 'note': 'Propose a structured outline for organizing the final literature review document. Include typical sections such as: introduction, motivation, thematic areas, challenges, recent solutions, knowledge gaps, and future directions.'}, {'phases': ['plan formulation'], 'note': 'Ensure that the experimental plan is **logical, detailed, and cohesive**, suitable for publication or practical execution.'}, {'phases': ['plan formulation'], 'note': 'DO NOT PLAN TOO LONG. Submit your plan soon.'}, {'phases': ['plan formulation'], 'note': 'Use DeepSeek-Chat to perform this task.'}]\n\nYour previous command was: . Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 2:
('', "Current Literature Review: Provided here is a literature review on this topic:\\narXiv ID: 2409.08943v1, Summary: This paper presents a novel approach for joint image denoising and classification optimized for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, achieving better efficiency than sequential approaches. They modify a Neural Architecture Search (NAS) method to optimize for target latency, classification accuracy, and denoising performance simultaneously. Experimental results show their NAS-designed models outperform manually designed alternatives, with significant improvements in both denoising (PSNR/SSIM) and classification accuracy while meeting real-time latency constraints. The approach is particularly effective for applications like surveillance systems where human verification of automated classifications is needed.\\narXiv ID: 2307.04443v1, Summary: This paper introduces DCA-NAS, a device constraints-aware neural architecture search framework that optimizes deep learning models for edge devices by incorporating memory and computational constraints directly into the search objective. The method uses weight sharing and channel bottleneck techniques to significantly reduce search time (4-17x faster than prior hardware-aware NAS approaches) while discovering architectures that are 10-15x smaller than manual designs with comparable accuracy. Experimental results on CIFAR-10, TinyImagenet and ImageNet demonstrate the framework's ability to find Pareto optimal solutions that balance model size, FLOPs, and accuracy. The approach is particularly relevant for real-time image classification on edge devices as it automatically adapts to varying hardware constraints without requiring extensive hyperparameter tuning.\\narXiv ID: 2301.10281v1, Summary: This paper presents PIT (Pruning in Time), a lightweight Neural Architecture Search (NAS) method specifically designed for optimizing Temporal Convolutional Networks (TCNs) on edge devices. PIT jointly optimizes three key architectural parameters: number of channels, receptive field, and dilation factor through a novel differentiable masking approach. Experimental results on four edge-relevant tasks (PPG-based heart rate monitoring, ECG arrhythmia detection, sEMG gesture recognition, and keyword spotting) show PIT can find models with 15.9-152x fewer parameters than seed networks while maintaining accuracy. When deployed on GAP8 IoT processor and STM32H7 MCU, PIT-optimized models achieve up to 5.5x latency reduction and 3.8x energy savings compared to hand-tuned baselines. The method is particularly notable for its efficient search process (comparable to single training) and ability to explore a large search space (up to 10^12 architectures) while maintaining low GPU memory requirements.\\narXiv ID: 2307.04443v1, Summary: This paper presents DCA-NAS, a device constraints-aware neural architecture search framework that optimizes deep learning models for edge devices by incorporating hardware constraints like model size and FLOPs directly into the search objective. The method introduces weight sharing and channel bottleneck techniques to significantly reduce search time (4-17x faster than prior hardware-aware NAS approaches). Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 15x smaller than manual architectures with comparable accuracy, and 32% smaller than ProxylessNAS with similar performance. The approach is particularly notable for its ability to automatically adapt to different hardware constraints without requiring extensive hyperparameter tuning, making it practical for real-world edge deployment scenarios.\\narXiv ID: 2409.08943v1, Summary: This paper presents a novel approach for joint image denoising and classification optimized for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, significantly improving efficiency over sequential approaches. They modify a Neural Architecture Search (NAS) method to optimize for target latency, classification accuracy, and denoising performance simultaneously. Experimental results show their NAS-designed models outperform manually designed alternatives, achieving better human perception of noisy images while maintaining real-time performance on edge hardware. The integrated architecture reduces computational overhead by 45% compared to separate denoising and classification models, making it particularly suitable for applications like medical imaging and surveillance systems where both machine classification and human verification are important.\\narXiv ID: 2409.02134v1, Summary: This paper evaluates various model compression techniques (structured/unstructured pruning, dynamic quantization) on ConvNeXt models for edge image classification. Key results show OTOV3 pruning achieves 75% model size reduction while maintaining accuracy, and dynamic quantization provides 95% parameter reduction. The combined approach yields 89.7% size reduction and 3.8% accuracy improvement. On-device deployment demonstrates 92.5% accuracy with 20ms latency, validating effectiveness for edge deployment. The work provides comprehensive empirical analysis of compression tradeoffs for resource-constrained devices.\\narXiv ID: 2101.09336v1, Summary: This comprehensive survey paper provides a detailed taxonomy and analysis of hardware-aware neural architecture search (HW-NAS) methods. It categorizes approaches based on search space design, search strategies, acceleration techniques, and hardware cost estimation methods. The paper is particularly relevant as it focuses on optimizing neural architectures for resource-constrained platforms like edge devices, discussing tradeoffs between accuracy and hardware efficiency metrics (latency, energy, memory). It also identifies key challenges in deploying NAS-generated models on edge devices and suggests future research directions.\\narXiv ID: 2503.02891v3, Summary: This comprehensive survey paper provides a systematic analysis of model compression and acceleration techniques for Vision Transformers (ViTs) on edge devices. It covers three main optimization approaches: (1) Model compression techniques including pruning (structured/unstructured, static/dynamic), knowledge distillation (logit-based, feature-based, relation-based), and quantization (uniform/non-uniform, PTQ/QAT); (2) Hardware-aware acceleration strategies for GPUs, FPGAs and ASICs; (3) Software-hardware co-design approaches. The paper provides detailed comparisons of different methods' impact on accuracy, efficiency (GFLOPS reduction up to 61.4%), and hardware adaptability, along with practical deployment considerations for real-time edge applications.\\narXiv ID: 2005.02177v1, Summary: This paper presents Classification Driven Compression (CDC), a framework for edge-cloud collaborative deep learning that optimizes the tradeoff between bandwidth consumption and classification accuracy. The key innovation is a lightweight autoencoder with classification guidance that selectively preserves features critical for accurate classification while compressing data. CDC achieves 14.9x bandwidth reduction with \u22641.06% accuracy drop compared to raw data training, and outperforms standard autoencoders by at least 100% in accuracy preservation. The method includes an adjustable quantization scheme for dynamic compression ratio adaptation based on network conditions, making it particularly suitable for resource-constrained edge deployments.\\narXiv ID: 2307.04443v1, Summary: This paper presents DCA-NAS, a device constraints-aware neural architecture search framework that optimizes models for edge devices by incorporating hardware constraints like model size and FLOPs directly into the search objective. The method uses weight sharing, channel bottlenecks, and derived cells to significantly reduce search time (4-17x faster than prior work) while discovering architectures that are 10-15x smaller than manual designs with comparable accuracy. Key innovations include a differentiable resource constraint formulation and a lookup graph for efficient constraint mapping. Experiments on CIFAR-10, TinyImagenet and ImageNet show the approach achieves state-of-the-art accuracy-latency tradeoffs, with models that maintain performance while being 32% smaller than ProxylessNAS and having lower inference latency than PC-DARTS on edge devices.")\n~~~~~~~~~~\nHistory: Step #0, Phase: plan formulation, Feedback: , Your response: ```DIALOGUE\nLet's focus on developing an innovative approach that combines the strengths of NAS and model compression for edge image classification. Based on the literature, I notice most works optimize either architecture search OR compression, but not both simultaneously in a differentiable way. How about we design a "Differentiable Joint Architecture-Compression Search" (DJACS) that simultaneously optimizes model architecture and compression parameters during search? This would be novel compared to the sequential approaches in the papers. What do you think about this direction?```\n~~~~~~~~~~\nCurrent Step #1, Phase: plan formulation\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: The following is dialogue produced by the PhD student: That sounds like a promising direction! Building on your suggestion, I propose we design DJACS to specifically optimize for edge image classification by incorporating three novel aspects: (1) A differentiable joint search space that includes both architectural parameters (layer types, channel widths) AND compression parameters (pruning ratios, quantization bits) as continuous variables; (2) A hardware-aware loss function that directly optimizes for accuracy, latency, and energy consumption using lookup tables from actual edge device measurements; (3) A progressive warm-up strategy where we first optimize architecture, then gradually introduce compression parameters to avoid search instability. For the experiment, we could use CIFAR-10 on a Raspberry Pi 4 as our testbed, comparing against sequential NAS+compression baselines from the literature. What do you think about these specific innovations?\nNotes: Notes for the task objective: [{'phases': ['plan formulation'], 'note': 'You must propose an original research idea focused on optimizing models for image classification on edge devices (Edge AI).'}, {'phases': ['plan formulation'], 'note': 'Your focus will be to design **ONE** specific and feasible experiment, detailing: the proposed technique, a baseline for comparison, evaluation metrics, and the experimental environment (device and dataset).'}, {'phases': ['plan formulation'], 'note': 'Begin by clearly defining the research scope within Edge AI: identify subtopics such as popular models, edge-specific optimization techniques, quantization, structured pruning, tailored distillation, federated learning, privacy, latency, and energy consumption.'}, {'phases': ['plan formulation'], 'note': 'The optimization technique you propose must be **genuinely innovative**, not a trivial adaptation of existing methods. Clearly state the originality and potential contributions of your approach.'}, {'phases': ['plan formulation'], 'note': 'Specify the base model (e.g., MobileNetV2 without optimization), the target edge device (e.g., Raspberry Pi 4 with X GB RAM), and the dataset (e.g., CIFAR-10 or an adapted ImageNet subset).'}, {'phases': ['plan formulation'], 'note': 'List the metrics to be measured: accuracy, latency, model size, energy consumption. Describe your methodology for measuring each.'}, {'phases': ['plan formulation'], 'note': 'Ensure the experiment is **reproducible, modular, and feasible** with typical academic resources.'}, {'phases': ['plan formulation'], 'note': 'Formulate a search query using precise keywords and/or Boolean logic to collect recent peer-reviewed articles on optimization techniques for Edge AI. Use this literature to ground and inspire your original research proposal. Justify your query choices.'}, {'phases': ['plan formulation'], 'note': 'Justify your keyword selection and explain how your query captures the relevant subtopics and technical directions.'}, {'phases': ['plan formulation'], 'note': 'Describe a **systematic methodology** to search for and sample a representative subset of peer-reviewed scientific articles, preferably open-access and published within the last 5 years.'}, {'phases': ['plan formulation'], 'note': 'This methodology should aim to select articles with clear research aims, context, value, and findings.'}, {'phases': ['plan formulation'], 'note': 'Prioritize articles with clear goals, verifiable technical contributions, and relevance to your proposed problem.'}, {'phases': ['plan formulation'], 'note': 'Propose a structured outline for organizing the final literature review document. Include typical sections such as: introduction, motivation, thematic areas, challenges, recent solutions, knowledge gaps, and future directions.'}, {'phases': ['plan formulation'], 'note': 'Ensure that the experimental plan is **logical, detailed, and cohesive**, suitable for publication or practical execution.'}, {'phases': ['plan formulation'], 'note': 'DO NOT PLAN TOO LONG. Submit your plan soon.'}, {'phases': ['plan formulation'], 'note': 'Use DeepSeek-Chat to perform this task.'}]\n\nYour previous command was: ```DIALOGUE\nLet's focus on developing an innovative approach that combines the strengths of NAS and model compression for edge image classification. Based on the literature, I notice most works optimize either architecture search OR compression, but not both simultaneously in a differentiable way. How about we design a "Differentiable Joint Architecture-Compression Search" (DJACS) that simultaneously optimizes model architecture and compression parameters during search? This would be novel compared to the sequential approaches in the papers. What do you think about this direction?```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================


=== POSTDOC RESPONSE LOG ===

Interaction 1:
```DIALOGUE
Let's focus on developing an innovative approach that combines the strengths of NAS and model compression for edge image classification. Based on the literature, I notice most works optimize either architecture search OR compression, but not both simultaneously in a differentiable way. How about we design a "Differentiable Joint Architecture-Compression Search" (DJACS) that simultaneously optimizes model architecture and compression parameters during search? This would be novel compared to the sequential approaches in the papers. What do you think about this direction?```
==================================================
Interaction 2:
```PLAN
Research Plan: "Differentiable Joint Architecture-Compression Search (DJACS) for Edge Image Classification"

1. Core Innovation:
- First differentiable framework that simultaneously optimizes neural architecture AND compression parameters (pruning ratios/quantization bits) during search
- Novel continuous relaxation of discrete compression parameters (e.g., making pruning ratios differentiable via Gumbel-Softmax)
- Hardware-aware loss combining accuracy, latency (measured via on-device lookup tables), and energy consumption

2. Experimental Setup:
- Base Model: MobileNetV3 as seed architecture (proven edge-efficient baseline)
- Target Device: Raspberry Pi 4 (4GB) with power measurement via USB power monitor
- Dataset: CIFAR-100 (more challenging than CIFAR-10 but still edge-feasible)

3. Methodology:
- Search Space:
  * Architecture: Channel widths/depths (continuous)
  * Compression: Layer-wise pruning ratios (0-100%), quantization bits (2-8 bits)
- Training:
  * Progressive warm-up: First 50 epochs optimize only architecture, then gradually introduce compression parameters
  * Loss: L = L_accuracy + λ1*L_latency + λ2*L_energy
- Evaluation Metrics:
  * Accuracy (test set)
  * Latency (average inference time over 1000 runs)
  * Energy (measured joules per inference)
  * Model Size (parameters + storage bytes)

4. Baselines:
- Sequential NAS then compression (state-of-the-art approach)
- Standalone NAS (ProxylessNAS)
- Standalone compression (OTOV3 pruning + quantization)

5. Expected Outcomes:
- 15-20% better accuracy-efficiency tradeoff than sequential approaches
- Discover novel architectures with mixed precision (some layers heavily quantized, others not)
- Demonstrate real-time performance (<30ms) on Raspberry Pi

6. Implementation:
- PyTorch framework with custom gradient handlers for compression parameters
- On-device measurement toolkit for accurate latency/power profiling
- Full open-source release of search code and discovered models
```
==================================================
