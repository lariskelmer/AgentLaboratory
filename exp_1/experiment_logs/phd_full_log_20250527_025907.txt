=== PHD PROMPT LOG ===

Interaction 1:
\n~~~~~~~~~~\nHistory: \n~~~~~~~~~~\nCurrent Step #0, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: \nNotes: \nYour previous command was: . Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 2:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\n~~~~~~~~~~\nCurrent Step #1, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n\nNotes: \nYour previous command was: ```SUMMARY\nreal-time edge image classification optimization\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 3:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\n~~~~~~~~~~\nCurrent Step #2, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2409.08943v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 4:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #2, Phase: literature review, Feedback: 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co, Your response: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for latency, accuracy, and denoising performance. Experimental results show their NAS-designed models outperform manually designed alternatives, achieving significant improvements in both denoising (PSNR/SSIM) and classification accuracy while meeting real-time latency constraints. The method is particularly effective for applications like medical imaging and surveillance systems where human verification of automated decisions is required.\n```\n~~~~~~~~~~\nCurrent Step #3, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2409.08943v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for latency, accuracy, and denoising performance. Experimental results show their NAS-designed models outperform manually designed alternatives, achieving significant improvements in both denoising (PSNR/SSIM) and classification accuracy while meeting real-time latency constraints. The method is particularly effective for applications like medical imaging and surveillance systems where human verification of automated decisions is required.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 5:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #2, Phase: literature review, Feedback: 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co, Your response: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for latency, accuracy, and denoising performance. Experimental results show their NAS-designed models outperform manually designed alternatives, achieving significant improvements in both denoising (PSNR/SSIM) and classification accuracy while meeting real-time latency constraints. The method is particularly effective for applications like medical imaging and surveillance systems where human verification of automated decisions is required.\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\n~~~~~~~~~~\nCurrent Step #4, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification NAS optimization\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 6:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #2, Phase: literature review, Feedback: 3\n--- Page 1 ---Pushing Joint Image Denoising and Classificationto the EdgeThomas C. Markhorst* , Jan C. Van Gemert, and Osman S. KayhanCV Lab, Delft University of Technology{t.c.markhorst}@tudelft.nlAbstract. In this paper, we jointly combine image classification andimage denoising, aiming to enhance human perception of noisy imagescapturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,it is important to retain the ability of humans to verify the automaticclassification decision and thus jointly denoise the image to enhance hu-man perception. Since edge devices have little computational power, weexplicitly optimize for efficiency by proposing a novel architecture thatintegrates the two tasks. Additionally, we alter a Neural ArchitectureSearch (NAS) method, which searches for classifiers to search for theintegrated model while optimizing for a target latency, classification ac-curacy, and denoising performance. The NAS architectures outperformour manually designed alternatives in both denoising and classification,offering a significant improvement to human perception. Our approachempowers users to construct architectures tailored to domains like med-ical imaging, surveillance systems, and industrial inspections.Keywords: ImageDenoising \xb7 NeuralArchitectureSearch \xb7 ImageClassification1 IntroductionThe intersection of edge devices, such as security cameras, and deep learninghas sparked an interest in optimizing neural networks for inference time, furtherreferred to as latency. Common tasks to optimize for such efficiency are objectclassification and object detection, which unlock automatic recognition. How-ever, in noisy settings, the recognition accuracy might not be perfect and it isimportant to allow the ability to validate the automatic recognition by humaninspection. Thus, in addition to automatic recognition, the perceptual qualityof the processed image is equally significant. In particular, this is relevant forimages containing noise, which can arise from various sources such as low-lightconditions, sensor noise, or other recording conditions. We focus on using anefficient model that can be used on the edge with the aim of enhancing humanperception for validating the recognition output of noisy images.Domains relying on human image perception but challenged by noisy im-ages, like medical imaging [26], surveillance systems [35], and industrial in-arXiv:2409.08943v1  [cs.CV]  13 Sep 2024--- Page 2 ---2 T.C. Markhorst et al.(a) (b) (c) (d) Cls: HumanFig. 1:We take a noisy image (a), which can be interpreted as an animal (b) orhuman (c). We denoise and classify the image (a), aiming to improve human perceptionresulting in (d). Note, in a real application (b) and (c) would not be available, whichincreases the difficulty of interpreting the noisy image.Artist: DALL-E-2 [31]spections [11], can benefit from recently proposed denoising Convolutional Neu-ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-ods[8,14].FastCNN denoisers[13,49]arerequired toaccommodatethe real-timerequirement of the affected domains. However, denoisers are not able to removeall noise, which is not always enough for human image perception.We further improve human understanding of the image by combining de-noising with machine perception, like image classification. From the Human-Computer Cooperation strategies in [41], we use the Classifier as Aid to Human.Where the image classifier can be used as a direct advisor or an independentagent to the security guard, the latter being analogous to a second opinion inmedical diagnosis. In different use cases, fusing skills of humans and computershas been shown to improve performance beyond using only one type [2,24,41].Therefore, we investigate models that can leverage the benefits of both denoisingand classification to enhance human understanding in real time.A model combining both denoising and classification is studied in [21], fo-cusing on denoising performance. In addition, we optimize for efficiency, whichis required for edge devices, and classification. Our efficiency definition is basedon two elements: (i) latency reduction while (ii) retaining denoising performanceand classification accuracy. These elements could be optimized using indepen-dent classification and denoising models. However, we propose an architecturecombining the tasks more efficiently.First, we employ established model design approaches to enhance indepen-dent denoising and classification models, such as model scaling [25,37] and ef-ficient operators [33]. Although the models are optimized, they still operateseparately, resulting in unnecessary overhead. Hence we propose and comparetwo methods that join both tasks, yielding a novel and efficient architecture.Adjusting this architecture for each device and desired latency can be labori-ous and requires expert knowledge. These issues have recently garnered interest,leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,whichhave achieved competitive results in image classification [38,42]. Moreover, re-cent Neural Architecture Search (NAS) approaches incorporate latency in their--- Page 3 ---Pushing Joint Image Denoising and Classification to the Edge 3loss function, enabling the design of architectures tailored to specific latencyrequirements [16,19,42]. Combining NAS with the proposed architecture pro-vides a seamless and efficient approach to designing denoising and classificationmodels for diverse use cases.We find that our proposed efficiency-focused architecture consistently out-performs our more straightforward one. This is observed for both the manuallyand NAS designed models. In addition, our NAS models significantly outperformthe manually designed ones in denoising and classification performance.We have the following contributions. (i) We introduce a novel architecture toefficiently combine denoising and classification. The novelty lies in sharing an en-coder between the denoiser and the classifier. (ii) We propose modifications to anexisting NAS method for classification [19] to stabilize its search, improving theperformance of the found architectures. (iii) We extend an existing NAS methodto search for a model that combines denoising and classification, optimized fora target latency, classification accuracy, and denoising performance.Since no prior work proposes a joint efficient model for denoising and classi-fication, we study the tasks both separately and joint in Sec. 3. The findings areused as expert knowledge to construct the NAS method in Sec. 4.12 Related workDenoising. Image denoising aims to reconstruct a clean imagex from its ob-served noisy varianty. This relation can be formulated asy = x + n, where weassume n to be additive white Gaussian noise (AWGN). Neural network-baseddenoisers offer faster inference and good performance compared to traditionaldenoising methods like BM3D [8] and WNNM [14]. The interest in deep learningfor denoising started with DnCNN [48], a simple Convolutional Neural Network(CNN). Encoder-decoder architectures became popular due to their efficienthierarchical feature extraction. Specifically, UNet [32] whose skip-connectionsbetween the encoder and decoder enhance the denoising process as shown infollow-up methods [15,23,29]. The interest in the UNet structure continues withtransformer architectures [12,39]. In this paper, our denoisers are based on UNet,ensuring our findings can translate to most related work.Efficient classification. Optimization for efficiency is generally achievedby either compressing pre-trained networks [28] or designing small networks di-rectly [33,38]. We focus on efficient design, for which handcrafted models andneural architecture search (NAS) play essential roles. Studies proposing hand-crafted models often introduce efficient operators [17,33,50] or scaling meth-ods [37]. These efficient operators are used in NAS methods [38,42] aiming for theautomated design of efficient neural networks. Such an operator is the invertedresidual with a linear bottleneck (MBConv), as introduced in MobileNetV2 [33].In our models, we study scaling methods and MBConv\u2019s efficiency characteristic.Neural Architecture Search.The use of reinforcement learning (RL) forneural architecture search introduced efficient architectures with competitive1 Project site:https://thomas-markhorst.github.io--- Page 4 ---4 T.C. Markhorst et al.classification performance [16,30,36,38]. However, their discrete search space iscomputationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-nificantly reduce this cost by relaxing the search space to be continuous usinglearnable vectors\u03b1 for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS [22], whichsearches a cell structure. Due to the complex design and repetitiveness through-out the network of the cell structure, follow-up works [19,42] search operatorsfor every layer instead of constructing repeating cells.Pitfalls of DNAS are the collapse of search into some fixed operations and aperformance drop when converting from the continuous search network to thediscretized inference network [6,45,46]. TF-NAS [19] addresses these issues withan adaptation in the search algorithm, which lets the search model mimic thediscrete behavior of the inference model. In addition, TF-NAS searches an archi-tecture with a target latency by adding a latency loss to the search optimization.Because of these properties, we use TF-NAS as a baseline for our NAS study.Existing NAS methods for denoising are either not reproducible [27], have acell-based search space [47], or do not have an encoder-decoder [5] architecture.Instead, we use a layer-based search space and encoder-decoder structure.Jointclassificationanddenoising. In[43],thepositiveinfluenceofdenois-ing methods on classification performance is discussed. Moreover, [21] proposeda joint model where a VGG classifier [34] is attached to a denoiser similar toUNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimageswhen adding the classification loss to the denoiser\u2019s optimization, whereas [43]reports a quantitative improvement. Although these models denoise and classifywell, they are not optimized for efficiency. In this paper, we design a joint imagedenoising and classification method for edge devices.3 Exploiting Expert KnowledgeWe start in a controlled setting with separate baseline models for classificationand denoising. Additionally, methods to increase their respective efficiency arestudied, resulting in a reduced version of the baseline denoiser and classifier.Both the construction and efficiency improvement of the models are describedin Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)are used as baseline denoiser and classifier respectively. This section describeshow the different sizes of the classifiers and denoisers are used to study joiningmethods and their efficiency.Dataset & settings. For the experiments in this section, we generate acontrolled synthetic data set to study the behavior of the classifier and denoiserwhen applying model scaling, replacing the convolutional operations, and com-bining both models. The dataset consists of 30k images, each with a randomconstant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST [10] digits. We use two digits to increase the complexity ofthe denoising task. For experiments including classification, the two digits areextracted from the image using ground truth locations. These extracted digits--- Page 5 ---Pushing Joint Image Denoising and Classification to the Edge 5U-Net denoiser (i) Sequentiald=3\u22c5bm\u22c5b(ii) IntegratedEncoderDecoderClassi\ufb01erLinearConvMax poolSkipUpConvorc=2Fig. 2:A UNet, with hyperparameters base feature map width (b), depth (d), channelmultiplier (m) and convolutions per layer (c). For the joint model either attach theclassifier to form (i) the Sequential model or (ii) the Integrated model.are separately used as input for the classifier. In the experiments where noiseis required, for either denoising or noisy classification, synthetic Gaussian noiseis added. This noise is zero mean, and the intensity of the noise is controlledusing the standard deviation (\u03c3) of the distribution. Fig. 3a shows a sample,and Fig. 3b its noisy variant. To test the model behavior on an extensive noiserange, every model is trained and tested on eleven\u03c3 values evenly spaced on theinterval [0, 1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizerwith 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.Since the experiments with the controlled data set are not targeted at aspecific device, the metric defining efficiency should not depend on a device.Such a metric is computational power, most commonly defined as Floating PointOperations (FLOPs), which we use as the primary metric. Despite being devicedependent, we assess latency as a secondary metric. The latency is measuredwith a batch size of 32, 100 warm-up inference passes and averaged over 1000inference passes. Classification performance is quantified using accuracy, whilefordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarity Index (SSIM) metrics [40] are used. Higher is better for all our metrics.3.1 Joint model: DC-NetExperimentalsetup. Weconstructabaselineandreducedjointmodel,Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use thesame classifier (MB2.5-M Suppl. A). Whereas UNet-S and UNet are used for thereduced and baseline DC-Net respectively.Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequentialmodel where the classifier is attached after the denoiser (Fig. 2.i), and (ii) anIntegrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). Forthe Integrated model, classification and denoising branches share the encoder.The benefits of the Integrated model could come in threefold. First, using ashared encoder removes the need for a second large classifier, as in the Sequen-tial method. Second, the decoder and classifier branches could run in parallelcompared to running sequentially, which can result in lower latency. Thirdly, the--- Page 6 ---6 T.C. Markhorst et al.(a) GT (b) \u03c3=0.8 (c) Int. S (d) Seq. S (e) Int. L (f) Seq. LFig. 3:Ground-truth sample (a), which is the target for the denoiser when given noisyimage (b). S stands for the reduced model and L for the baseline. (c-f) are the croppeddenoised outputs for input (b) and the red squares indicate the zoomed-in regions. Forhigher noise levels, the denoising performance of the Sequential model is worse thanthe Integrated model.decoder is only optimized for denoising, since the optimization of the classifierdoes not influence it anymore. It can result in better image quality.The models are trained using a weighted combination of the Cross-Entropyand Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11noise levels,\u03c3 in [0, 1].L = 0.1 \xb7 LCE + 0.9 \xb7 LChar (1)Exp 1. Integrated vs. Sequential.Which joining method performs betterfor the baseline, and does the same conclusion hold when reducing its size?We compare theSequential and Integrated models. In Tab. 1, we see that forboth the baseline and reduced DC-Net models, the Integrated version performssignificantly better at denoising, while the Sequential version performs better atclassification. The difference in denoising performance is visualized in Fig. 3. Wesee that both the reduced (3c) and baseline (3e) Integrated models reconstructthe digit clearly. Whereas both sizes of the Sequential model (3d and f) fail toreconstruct the digit.Conclusion. The integrated model has a slightly lower classification accu-racy compared to the Sequential model, yet it has superior performance in termsof image quality. When aiming for improved human perception, it is still requiredfor the human to see the content of the image. Therefore, the Integrated modelis more suitable for joint denoising and classification and is called DC-Net.Table 1:Comparison of the reduced and baseline joint models. Both the Integratedand Sequential methods trained on the synthetic noise dataset. The integrated modelperforms significantly better in denoising and slightly worse in classification. The inte-grated model also scales down better.DC-Net Type FLOPs (M) \u2193 Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191 Acc. (%)\u2191BaselineIntegrated 1301.8 7.14 32.8 0.97 88.1Sequential 1302.1 7.55 27.1 0.95 89.6ReducedIntegrated 51.2 2.41 29.9 0.97 86.2Sequential 51.5 2.83 25.2 0.92 87.6--- Page 7 ---Pushing Joint Image Denoising and Classification to the Edge 70 25 50 75Epochs0.00.20.40.60.8 valueLearned MB-k3-e3MB-k3-e6MB-k5-e3MB-k5-e60 25 50 75EpochsRemoved Fig. 4: Stage-5:block-4\u2019s\u03b1 values for Removed andLearned \u03b2. Search is morestable when\u03b2 is removed.6.0 8.0 12.0T arget Latency (ms)85.085.586.086.587.087.588.088.5Accuracy (%)# Operations468Fig. 5: Acc for differentsearch spaces, showed fordifferent target latencies.Using fewer operations isthe most robust.7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6Latency (ms)84.084.585.085.586.086.587.087.588.0Accuracy (%)TF-NAS CResNet-18MobileNetV2C-NAS M (ours)Fig. 6: Comparing clas-sifiers with similar la-tency on Imagenet100.Our model outperformsother methods.4 Neural Architecture SearchWe follow similar experimentation strategies as in the previous section. TF-NAS [19] is used to construct a classifier, which we use as a basis for our denoiserand joint model. All our proposed models in this Section contain searchableblocks, the models and which parts are searchable are defined in Figure 7.Dataset & settings.The following experiments are conducted on Imagenet[9], randomly cropped to 224x224 pixels. To reduce search and training time, 100classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. Inthe experiments requiring noise, Gaussian noise is sampled uniformly with acontinuous range of\u03c3 in [0, 1] (Tabs. 2, 3, 4, 5 and 6).The models are searched using SGD with momentum, 2E-2 LR with 90epochs. Afterward, the found architecture is trained from scratch with 2E-1 LRfor 250 epochs. All other settings are similar to [19]. The loss function dependson the task of the experiment, Cross-Entropy with label smoothing for classifi-cation (LCE), combined Charbonnier and SSIM losses for denoising (LDen), anda weighted combination for the joint model (LBoth), see Eq. 2, 3.LDen = 0.8 \xb7 LChar + 0.2 \xb7 LSSIM (2)LBoth = 0.1 \xb7 LCE + 0.9 \xb7 LDen (3)Since our NAS method uses a latency look-up table constructed for our de-vice, these experiments target a specific device, GeForce RTX 3090 GPU. There-fore latency is suitable for defining efficiency in the NAS experiments.4.1 Classification: C-NASExperimental Setup.Since TF-NAS [19] learns\u03b2\u2019s to control the number ofconvolutional operators per stage,\u03b2\u2019s can reduce the model size. However, in--- Page 8 ---8 T.C. Markhorst et al.the models proposed by [19], only 2 out of 24 stages are reduced by\u03b2. So the\u03b2\u2019s have little effect on the found architectures, yet they make the search spacemore complex. Therefore, we propose a version of TF-NAS where the\u03b2\u2019s areremoved so that all convolutional blocks are used.The candidate operations in the search space of TF-NAS are MBConvs with8 different configurations, see Suppl. B. The configurations differ in kernel size,expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].The classification experiments are performed using data without noise, asthe aim is to examine the NAS method, which is designed for clean images.We investigate key components of TF-NAS and try to improve its stability andclassification performance.Exp 1. Learned vs. Removed\u03b2. We conduct an experiment to study theeffect of removing\u03b2 on the search quality. The SE-layer is excluded from thecandidate blocks, halving the search space to ensure the number of candidateoperations does not cause search instability. We set a low target latency of 6ms, as learning\u03b2 should have a positive effect on small networks. For both thelearned and removed settings, we run two searches, search 1 and 2.Fig. 4 shows that when\u03b2 is learned, the\u03b1\u2019s selecting a candidate operationoscillate and therefore do not decide on an architecture. Whereas with Removed\u03b2, the search is stable. This stability is reflected in the performance, as theaverage accuracy of the Removed\u03b2 models is 86.3%, compared to 84.2% forLearned \u03b2. The separate results for each model are shown in Suppl. C.Exp2.Numberofoperatorsinsearchspace. Doesreducingthenumberof operators during search positively influence the performance of the foundmodels? We test this by comparing the performance of architectures searchedwith three different search space sizes, {4, 6, or 8} operations, defined in Suppl.B. For each of these search spaces, three different latency targets are used: {6,8, and 12} ms.In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using feweroperations in the search space does not alter performance significantly. Whentargeting 12 ms latency, reducing the number of operations in the search spacedoes show a significant improvement. Additionally, we find that when using thelarger search spaces, the operators from the small search space are still preferredfor lower latencies.Exp 3. Compare with original TF-NAS.How do architectures foundusingourproposedchangestoTF-NASperformcomparedtomodelswithsimilarlatency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,and ResNet-18. MobileNetV2 our model have similar latency, architecture, andoperator types. ResNet only differs in that it uses the Conv operator. We includethese standard baseline architectures to indicate where C-NAS, see Fig. 7.i,stands on Imagenet100.Fig. 6 shows that the model found using our method has lower latency yethigher accuracy than TF-NAS C as proposed in [19]. The model is searchedwith target latency 8.0. We observe that our search method is able to find amodel that matches its target latency. Although ResNet-18 and MobileNetV2--- Page 9 ---Pushing Joint Image Denoising and Classification to the Edge 9run faster than our model, our classification accuracy is superior, especially whencompared to ResNet-18, which only uses Convs.Conclusion. By Removing \u03b2 and reducing the number of operators usedin the search, the search stability increases, and we find architectures that havebetter accuracy. An architecture found using our changes classifies better thana TF-NAS architecture with similar latency.The comparison between our model and ResNet-18 shows that our searchspace is able to compete with widely accepted Conv-based classifiers. Moreover,our model performs on par with MobileNetV2, a manually designed classifierusing MBConvs.4.2 Denoising: D-NASExperimental setup.To construct a denoiser, D-NAS (Fig. 7.ii), we use thefirst six stages of a found C-NAS classifier, which has four levels of resolution.Afterwards, we attach a UNet style decoder by using both a transposed convo-lution and two normal convolutions for each decoder level. Like UNet, we alsoadd skip connections between the encoder and decoder layers. The decoder isnot searched.Exp 1. D-NAS vs UNet denoiser.Does our denoiser D-NAS performsimilarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2){d = 4,b = 8,c = 2,m = 1.5}, with a latency of 9.2 ms and the larger UNet-M,{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare themwith our D-NAS M, with similar latency.Table 2:Comparison of D-NAS and UNet variants for denoising. D-NAS outperformsslightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.Model UNet params:Lat. (ms)\u2193 PSNR\u2191 SSIM\u2191d b mUNet-S 4 8 1.5 9.2 25.0 0.69UNet-M 4 16 2 16.9 25.9 0.72D-NAS M - - - 11.6 25.6 0.71Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.Conclusion. D-NAS performs similarly to our baseline UNets. ThereforeD-NAS is a suitable denoising architecture and it can form the backbone of ourIntegrated model.4.3 Joint Model: DC-NASExperimental setup. To construct the joint model, we use the Integratedsetup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We--- Page 10 ---10 T.C. Markhorst et al.Table 3:Comparison of DC-NAS models searched for three different latencies, withtheir corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-grated models perform similar or better than their corresponding baselines, with theadvantage of having a joint denoising and classification network. *See Suppl. A.2.Model Type Lat. (ms) \u2193 Classification DenoisingAcc. (%)\u2191 PSNR\u2191 SSIM\u2191MobileNetV3 [16] Classifier 4.9 70.4 - -C-NAS S (ours) Classifier 5.9 73.5 - -UNet-S* [32] Denoiser 9.2 - 25.0 0.69DC-Net S (ours) Integrated 10.0 61.9 24.5 0.68DC-NAS S (ours) Integrated 10.3 74.3 25.4 0.70EfficientNetV2-b0 [38] Classifier 9.0 75.4 - -C-NAS M (ours) Classifier 7.9 75.5 - -LPIENet 0.25x [7] Denoiser 12.7 - 24.1 0.65DC-NAS M (ours) Integrated 13.7 76.0 25.4 0.70EfficientNetV2-b1 [38] Classifier 11.8 76.7 - -C-NAS L (ours) Classifier 12.0 76.0 - -UNet-M* [32] Denoiser 16.9 - 25.9 0.72LPIENet 0.5x [7] Denoiser 19.8 - 24.7 0.68DC-NAS L (ours) Integrated 17.9 76.4 25.2 0.70connect the decoder after the first six stages of C-NAS (Fig. 7, but still use theremaining C-NAS stages(Fig. 7.iii)) as a classification branch. The design choicesfor DC-NAS are discussed in the ablations study (Sec. 4.4).Using our search method, we search for DC-NAS models of three differentsizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompareour searched models with separate state-of-the-art classifiers and denoisers, asthere are no existing models that jointly optimize denoising, classification, andefficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiserbaselines are chosen to have similar latency as the C-NAS or D-NAS model onwhich the corresponding DC-NAS is based.Results. We discuss the results in Tab. 3 in three separate sections for thedifferent target latencies. Our smallest Integrated model, DC-NAS S, outper-forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that thelatter shows that the classifier, C-NAS S, performs better when integrated intoDC-NAS S. Moreover, our Integrated model denoises better than its baselineUNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-signed DC-Net S (Reduced), which is the only Integrated baseline. We displaythe denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observebetter denoising on smooth areas, sharper edges, and more realistic color recon-struction for DC-NAS S.The results of DC-NAS M follow a similar pattern, where our DC-NAS out-performs its baseline denoiser and classifier, using C-NAS M in the Integratedmodel boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,--- Page 11 ---Pushing Joint Image Denoising and Classification to the Edge 11Decoder 4(iii) Integrated3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearDecoder 4Decoder 3Decoder 2Decoder 1(ii) D-NAS3x3 ConvMBConvStage 3Stage 4Stage 5Stage 6Stage 7Stage 81x1 ConvPool + LinearC-NAS(iv) SequentialDecoder 4Decoder 1-4:Up ConvConcatenate Conv x2(i) C-NASEncoder - SearchableEncoder - Manually pickedDecoder - Manually pickedFig. 7:C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block(iv) for DC-NASseq. During search models with various latencies can be obtained. Onlythe orange stages are searchable in the encoder and classifier.the classification performance improves by 1.7%, yet the denoising performanceplateaus. LPIENet denoises the worst. Comparing DC-NAS M\u2019s denoising per-formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-mance. However, our Integrated model denoises and classifies with lower latency.For DC-NAS L, we observe that both the classification and denoising baselineslightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higherclassification accuracy than DC-NAS L, and UNet-M improves with 0.7 dBPSNR and 2% SSIM. However, our Integrated model performs both denoisingand classification at a similar latency as UNet-M, which only denoises. Whencomparing DC-NAS L with DC-NAS M, we again note an improvement in classi-fication performance. However, the PSNR score drops by 0.2 dB while the moreimportant SSIM score remains at 0.70.Conclusion. Our results demonstrate that the proposed DC-NAS modelsperform similar or better than their denoising and classification baselines fortheir target latency. In addition, the searched model performs better than ourmanually designed joint denoiser and classifier.4.4 Ablation StudyExp 1. Encoder search.C-NAS forms the encoder of DC-NAS and containsthe searchable operations within DC-NAS. We test multiple search approaches:--- Page 12 ---12 T.C. Markhorst et al.(i) using clean images, and (ii) using noisy images. For both approaches, wesearch the encoder using only classification lossLCls. In addition, we also searchthe DC-NAS encoder on noisy images using the combined denoising and classi-fication lossLBoth. Therefore it searches for the optimal encoder for both taskswithin the Integrated model DC-NAS. Regardless of the search method, thefound models are trained using noisy images and the combined loss.Tab. 4, shows that when usingLCls with noisy images during search improvesclassification accuracy by 0.3%. Surprisingly, the denoising performance is thesame. Using both the denoising and classification objectives during the searchreduces the classification accuracy. Caused by the denoising loss complicatingthe search, without improving denoising performance. Therefore, we search ourDC-NAS models by only usingLCls loss.Exp 2. Compare Integrated vs. Sequential.We compare DC-NAS andDC-NASseq models with similar latency. Where DC-NASseq is our Sequentialmodel, which is constructed by attaching C-NAS to the output of D-NAS, seeFig. 7.iv. Since the searched classifier is used twice in DC-NASseq, the Sequen-tial model has a higher latency than the Integrated variant. To counter this, asmaller C-NAS model is used in both the encoder and classifier of DC-NASseq.The classifier, C-NAS, used to construct DC-NASseq L has a latency of 6.7 ms.Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that thesemodels were searched by using clean instead of noisy images, as this holds forboth models it is still a fair comparison.We see that both models have similar latency and the same classification ac-curacy, however, DC-NAS L improves denoising performance with 0.5 dB PSNRand 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L\u2019s Integrateddesign as this allows for a bigger encoder without increasing latency.Table 4:Different search strategies for DC-NAS, using (i) clean or noisy images and(ii) LCls or LCls + LDen. Searching onNoisy images with onlyLCls performs best.Search Lat. (ms) Acc. (%)\u2191 SSIM\u2191 PSNR\u2191Images LossClean LCls 13.9 75.7 25.4 0.70Noisy LCls 13.7 76.0 25.4 0.70Noisy LBoth 13.8 75.5 25.4 0.70Exp 3. Decoder tuning.The DC-NAS models found in Tab. 3 and 5, havesimilar denoising performance. These models differ only in the type of MBConvsthat are selected during search in the encoder. We test the hypothesis if thedenoising performance is influenced by adjusting the operators in the decoderwhile retaining the latency. DC-NAS M is used as a basis in this experiment. Weconstruct three alternatives. First, the convolutional operators in the decoderare replaced with MBConvs (MB-k3-e3) [33], which significantly increases thelatency of the model. To account for this, we scale down the decoder by (i)--- Page 13 ---Pushing Joint Image Denoising and Classification to the Edge 13Table 5:Comparing Sequential and Integrated DC-NAS, classification performance issimilar, yet the Integrated model is faster and denoises better.Model Lat. (ms) Acc. (%) \u2191 PSNR\u2191 SSIM\u2191DC-NASseq L 18.3 76.0 25.0 0.69DC-NAS L 17.9 76.0 25.5 0.70(a) \u03c3=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) Ground Truth(f) \u03c3=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) Ground TruthFig. 8:Denoising performance of DC-NAS S and its baselines. Left to right: noisyimage, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),we see better performance in smooth areas and more correct colors in (d). With (i)and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifactsthan (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3instead of 4 decoder layers.In Tab. 6, we see that using the MBConvs compared to Convs improves thedenoising performance. However, at the cost of a 14 ms latency increase, onlycaused by the MBConv decoder. When reducing the complexity of the MBConvdecoder with1 operatorand 3 layers, the denoising performance reduces to theoriginal level again, but the latency is still higher than for DC-NAS M whichhas only standard convolutional layers in the decoder block.Conclusion. We have seen that the Integrated combining method outper-forms its Sequential counterpart in denoising. To construct the integrated model(DC-NAS), we find that searching for a classifier on noisy data, without takingthedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.--- Page 14 ---14 T.C. Markhorst et al.Table 6: The influence of altering the Conv operators in the DC-NAS M decoderto MBConv and scaling down the MBConv alternative by reducing the number ofoperators or decoder layers. Using the standard convolutional layers is more efficient.Decoder Lat. (ms) Acc. (%)\u2191 PSNR\u2191 SSIM\u2191Operator ScalingConv - 13.7 76 25.4 0.70MBConv - 27.7 75.5 25.8 0.71MBConv 1 operator 16.4 75.4 25.3 0.70MBConv 3 layers 22.1 75.1 25.4 0.70Surprisingly, the search method does not influence the denoising performance.Furthermore, manually altering the decoder does not benefit denoising efficiencyeither. However, the NAS denoising experiments demonstrate that our denois-ing setup is competitive. Since tuning the decoder operators does not improveperformance, our method is focused on searching for only the encoder of theintegrated model. The models found by this approach, outperform our manuallydesigned models with similar latency.5 Limitations & ConclusionOne limitation of our NAS method is its inability to alter the decoder. It isdesigned this way as manually altering the decoder does not improve efficiency.However, when targeting a significantly different latency, a change in denoisingarchitecture could be required. Therefore, designing model scaling rules for thesearched models is of interest, similar to the EfficientNets [37,38].Another limitation is the fixation of\u03b2 in our NAS method. Although this im-provesthestabilityofsearchandnetworkperformance,learning \u03b2 whileretaininga stable search would be preferred. This would introduce more possibilities inthe search space for optimizing efficiency.In addition, the latency of Integrated models can be reduced further by run-ning the denoising and classification branches in parallel.To conclude, we show that using efficient operators and scaling methodsproposed in previous work [25,33,37] are relevant for denoising and noisy clas-sification. In addition, we present the Integrated model DC-Net to join the twotasks efficiently and show that the Integrated design is more suitable across var-ious latencies than the Sequential variant. To simplify the design process of thejoint model when targeting a latency, we present a NAS method. We alter anexisting NAS method to improve the stability and performance of the search.This method searches a classifier. Using the searched classifier as a basis, webuild the Integrated DC-NAS model. We demonstrate that the proposed modeloutperforms the manually constructed model. We believe that our study can bea precursor of efficient joint low-level and high-level computer vision tasks.--- Page 15 ---Pushing Joint Image Denoising and Classification to the Edge 15References1. Barron, J.T.: A more general robust loss function. CoRRabs/1701.03077 (2017),http://arxiv.org/abs/1701.030772. Bosch, N., D\u2019Mello, S.K.: Can computers outperform humans in detecting userzone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-teract. 29(2) (jan 2022).https://doi.org/10.1145/3481889, https://doi.org/10.1145/34818893. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-get task and hardware. CoRRabs/1812.00332 (2018), http://arxiv.org/abs/1812.003324. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministichalf-quadratic regularization algorithms for computed imaging. In: Proceedings of1st International Conference on Image Processing. vol. 2, pp. 168\u2013172 vol.2 (1994).https://doi.org/10.1109/ICIP.1994.4135535. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:neural architecture search for deep learning with differential privacy. CoRRabs/2110.08557 (2021), https://arxiv.org/abs/2110.085576. Chu, X., Zhou, T., Zhang, B., Li, J.: Fair DARTS: eliminating unfair advantages indifferentiable architecture search. CoRRabs/1911.12126 (2019), http://arxiv.org/abs/1911.121267. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-age enhancement for smartphone real-time applications. In: Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp.1848\u20131858 (January 2023)8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on Image Processing16(8), 2080\u20132095 (2007).https://doi.org/10.1109/TIP.2007.9012389. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer visionand pattern recognition. pp. 248\u2013255. Ieee (2009)10. Deng, L.: The mnist database of handwritten digit images for machine learningresearch. IEEE Signal Processing Magazine29(6), 141\u2013142 (2012)11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,M.A.: SEM image denoising with unsupervised machine learning for better de-fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.258480312. Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: Swin transformer unet for image denoising.In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE(May 2022).https://doi.org/10.1109/iscas48785.2022.9937486 , http://dx.doi.org/10.1109/ISCAS48785.2022.993748613. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.In: Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV) (October 2019)14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization withapplication to image denoising. In: 2014 IEEE Conference on Computer Visionand Pattern Recognition. pp. 2862\u20132869 (2014).https://doi.org/10.1109/CVPR.2014.366--- Page 16 ---16 T.C. Markhorst et al.15. Gurrola-Ramos, J., Dalmau, O., Alarc\xf3n, T.E.: A residual dense u-net neu-ral network for image denoising. IEEE Access 9, 31742\u201331754 (2021). https://doi.org/10.1109/ACCESS.2021.306106216. Howard, A., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y.,Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRRabs/1905.02244 (2019), http://arxiv.org/abs/1905.0224417. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications. CoRRabs/1704.04861 (2017), http://arxiv.org/abs/1704.0486118. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRRabs/1709.01507 (2017), http://arxiv.org/abs/1709.0150719. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-constrained differentiable neural architecture search. CoRR abs/2008.05314(2020), https://arxiv.org/abs/2008.0531420. Ignatov, A., Timofte, R., Liu, S., Feng, C., Bai, F., Wang, X., Lei, L., Yi, Z., Xiang,Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,Wang, Z., Conde, M.V., Choi, U.J., Perevozchikov, G., Ershov, E., Hui, Z., Dong,M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp onmobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),https://arxiv.org/abs/2211.0388521. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting imagedenoising and high-level vision tasks via deep learning. CoRRabs/1809.01826(2018), http://arxiv.org/abs/1809.0182622. Liu, H., Simonyan, K., Yang, Y.: DARTS: differentiable architecture search. CoRRabs/1806.09055 (2018), http://arxiv.org/abs/1806.0905523. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for imagerestoration. CoRRabs/1805.07071 (2018), http://arxiv.org/abs/1805.0707124. Lopera Tellez, O.: Underwater threat recognition: Are automatic target classifica-tion algorithms going to replace expert human operators in the near future? In:OCEANS 2019 - Marseille. pp. 1\u20134 (2019).https://doi.org/10.1109/OCEANSE.2019.886716825. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:Virus recognition in electron microscopy images. Computer Methods and Programsin Biomedicine178, 31\u201339 (2019).https://doi.org/https://doi.org/10.1016/j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/S016926071930085926. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https://doi.org/https://doi.org/10.1016/j.bspc.2020.102036 , https://www.sciencedirect.com/science/article/pii/S174680942030192027. Mo\u017cejko, M., Latkowski, T., \u0141ukasz Treszczotko, Szafraniuk, M., Trojanowski, K.:Superkernel neural architecture search for image denoising (2020)28. O\u2019Neill, J.: An overview of neural network compression. CoRRabs/2006.03669(2020), https://arxiv.org/abs/2006.0366929. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-noising. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops (CVPRW). pp. 2104\u20132113 (2019).https://doi.org/10.1109/CVPRW.2019.00263--- Page 17 ---Pushing Joint Image Denoising and Classification to the Edge 1730. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearchviaparametersharing.CoRR abs/1802.03268 (2018),http://arxiv.org/abs/1802.0326831. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents (2022)32. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-cal image segmentation. CoRRabs/1505.04597 (2015), http://arxiv.org/abs/1505.0459733. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residualsand linear bottlenecks: Mobile networks for classification, detection and segmenta-tion. CoRRabs/1801.04381 (2018), http://arxiv.org/abs/1801.0438134. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition (2015)35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-tional neural network and anisotropic diffusion for real-time surveillance applica-tions. Journal of Real-Time Image Processing18(5), 1711\u20131728 (Oct 2021).https://doi.org/10.1007/s11554-020-01060-0 , https://doi.org/10.1007/s11554-020-01060-036. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-awareneural architecture search for mobile. CoRRabs/1807.11626 (2018), http://arxiv.org/abs/1807.1162637. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. CoRRabs/1905.11946 (2019), http://arxiv.org/abs/1905.1194638. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRRabs/2104.00298 (2021), https://arxiv.org/abs/2104.0029839. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformerfor image restoration. CoRRabs/2106.03106 (2021), https://arxiv.org/abs/2106.0310640. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: fromerror visibility to structural similarity. IEEE Transactions on Image Processing13(4), 600\u2013612 (2004).https://doi.org/10.1109/TIP.2003.81986141. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-matic target recognition: Strategies for human-computer cooperation. In: 201422nd International Conference on Pattern Recognition. pp. 4690\u20134695 (2014).https://doi.org/10.1109/ICPR.2014.80242. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiableneural architecture search. CoRRabs/1812.03443 (2018), http://arxiv.org/abs/1812.0344343. Wu, J., Timofte, R., Huang, Z., Van Gool, L.: On the relation between color imagedenoising and classification. arXiv preprint arXiv:1704.01372 (2017)44. Wu, Y., He, K.: Group normalization. CoRRabs/1803.08494 (2018), http://arxiv.org/abs/1803.0849445. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.0992646. Ye, P., Li, B., Li, Y., Chen, T., Fan, J., Ouyang, W.:\u03b2-darts: Beta-decay regular-ization for differentiable architecture search (2022)47. Zhang, H., Li, Y., Chen, H., Shen, C.: IR-NAS: neural architecture search for imagerestoration. CoRRabs/1909.08228 (2019), http://arxiv.org/abs/1909.08228--- Page 18 ---18 T.C. Markhorst et al.48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:Residual learning of deep CNN for image denoising. CoRR abs/1608.03981(2016), http://arxiv.org/abs/1608.0398149. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: Toward a fast and flexible solution for CNNbased image denoising. CoRRabs/1710.04026 (2017), http://arxiv.org/abs/1710.0402650. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),http://arxiv.org/abs/1707.01083--- Page 19 ---Pushing Joint Image Denoising and Classification to the Edge 1A Efficient Classification & Denoising: Additional resultsThe joint models in Section 3 are constructed using a separate denoiser andclassifier. We describe the baseline models and several methods to construct thereduced versions.Overview of the models used in the main paper.UNet-S: {d = 4, b= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is alsocalled Baseline UNet. MB2.5-M: the classifier described in Section A.1 with anMBConv (expansion rate = 2.5) as second convolutional layer.A.1 Efficient ClassificationExperimental setup.Our baseline classifier (Conv-L) consists of two convolu-tional, one global max pooling, and a linear layer. Each convolutional layer alsohas a group normalization [44], max pooling, and ReLU activation function.To construct the reduced version, we use two methods similar to previousworks [33,37]. In the first method, we replace the second convolutional layer withan MBConv layer. Three expansion rates are used{1, 2.5, 4}: (i) rate 1 is thelowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,and (iii) rate 2.5 is in the middle of those two. The second reduction methodis to lower the number of filters in the baseline, also called the model width.Using these techniques, models with three different FLOP sizes are constructed,{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where xrepresents the FLOP size ande is the expansion rate of the MBConv.The models are trained using Cross Entropy loss. We report the accuracyaveraged over all 11 noise levels.Exp. 1: Conv vs MBConv comparison.According to [33], the MBConvlayer should be more efficient than a normal convolutional layer. Therefore, whencomparing the two operators in our network, we expect the version with anMBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MBmodels with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better thanthe Conv-L model with fewer FLOPs. However, with an expansion rate of 1(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]\u2019s theoryalso holds for the noisy classifier, but only for the higher expansion rates.Exp. 2: MBConv width & expansion rate scaling. Since MBConvlayers can be used to improve efficiency, we question how to further reduce theMB model\u2019s FLOP size. We co, Your response: ```ADD_PAPER\n2409.08943v1\nThis paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for latency, accuracy, and denoising performance. Experimental results show their NAS-designed models outperform manually designed alternatives, achieving significant improvements in both denoising (PSNR/SSIM) and classification accuracy while meeting real-time latency constraints. The method is particularly effective for applications like medical imaging and surveillance systems where human verification of automated decisions is required.\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\n~~~~~~~~~~\nCurrent Step #5, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance.\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2307.04443v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 7:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance., Your response: ```ADD_PAPER\n2307.04443v1\nThis paper presents DCA-NAS, a device constraints-aware neural architecture search framework that optimizes models for edge devices with strict memory and computational constraints. The method incorporates weight sharing and channel bottleneck techniques to significantly reduce search time while maintaining performance. Key innovations include a differentiable hardware-aware objective function that directly optimizes for device constraints like FLOPs and memory usage without requiring tunable scaling factors. Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 10-15x smaller than manual architectures with comparable accuracy, while reducing search time by 4-17x compared to prior hardware-aware NAS methods. The approach demonstrates strong transferability across datasets and maintains stable performance across different resource constraints.\n```\n~~~~~~~~~~\nCurrent Step #6, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2307.04443v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2307.04443v1\nThis paper presents DCA-NAS, a device constraints-aware neural architecture search framework that optimizes models for edge devices with strict memory and computational constraints. The method incorporates weight sharing and channel bottleneck techniques to significantly reduce search time while maintaining performance. Key innovations include a differentiable hardware-aware objective function that directly optimizes for device constraints like FLOPs and memory usage without requiring tunable scaling factors. Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 10-15x smaller than manual architectures with comparable accuracy, while reducing search time by 4-17x compared to prior hardware-aware NAS methods. The approach demonstrates strong transferability across datasets and maintains stable performance across different resource constraints.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 8:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance., Your response: ```ADD_PAPER\n2307.04443v1\nThis paper presents DCA-NAS, a device constraints-aware neural architecture search framework that optimizes models for edge devices with strict memory and computational constraints. The method incorporates weight sharing and channel bottleneck techniques to significantly reduce search time while maintaining performance. Key innovations include a differentiable hardware-aware objective function that directly optimizes for device constraints like FLOPs and memory usage without requiring tunable scaling factors. Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 10-15x smaller than manual architectures with comparable accuracy, while reducing search time by 4-17x compared to prior hardware-aware NAS methods. The approach demonstrates strong transferability across datasets and maintains stable performance across different resource constraints.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\n~~~~~~~~~~\nCurrent Step #7, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge device image classification latency optimization\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 9:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #5, Phase: literature review, Feedback: 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance., Your response: ```ADD_PAPER\n2307.04443v1\nThis paper presents DCA-NAS, a device constraints-aware neural architecture search framework that optimizes models for edge devices with strict memory and computational constraints. The method incorporates weight sharing and channel bottleneck techniques to significantly reduce search time while maintaining performance. Key innovations include a differentiable hardware-aware objective function that directly optimizes for device constraints like FLOPs and memory usage without requiring tunable scaling factors. Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 10-15x smaller than manual architectures with comparable accuracy, while reducing search time by 4-17x compared to prior hardware-aware NAS methods. The approach demonstrates strong transferability across datasets and maintains stable performance across different resource constraints.\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\n~~~~~~~~~~\nCurrent Step #8, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Latency-aware Spatial-wise Dynamic NetworksYizeng Han1\u2217Zhihang Yuan2\u2217 Yifan Pu1\u2217 Chenhao Xue2Shiji Song1 Guangyu Sun2 Gao Huang1\u20201 Department of Automation, BNRist, Tsinghua University, Beijing, China2 School of Electronics Engineering and Computer Science, Peking University, Beijing, China{hanyz18, pyf20}@mails.tsinghua.edu.cn, {shijis, gaohuang}@tsinghua.edu.cn{yuanzhihang, xch927027, gsun}@pku.edu.cnAbstractSpatial-wise dynamic convolution has become a promising approach to improvingthe inference ef\ufb01ciency of deep networks. By allocating more computation to themost informative pixels, such an adaptive inference paradigm reduces the spatialredundancy in image features and saves a considerable amount of unnecessarycomputation. However, the theoretical ef\ufb01ciency achieved by previous methodscan hardly translate into a realistic speedup, especially on the multi-core processors(e.g. GPUs). The key challenge is that the existing literature has only focusedon designing algorithms with minimal computation, ignoring the fact that thepractical latency can also be in\ufb02uenced by scheduling strategies and hardwareproperties. To bridge the gap between theoretical computation and practical ef-\ufb01ciency, we propose a latency-aware spatial-wise dynamic network (LASNet),which performs coarse-grained spatially adaptive inference under the guidanceof a novel latency prediction model. The latency prediction model can ef\ufb01cientlyestimate the inference latency of dynamic networks by simultaneously consideringalgorithms, scheduling strategies, and hardware properties. We use the latencypredictor to guide both the algorithm design and the scheduling optimization onvarious hardware platforms. Experiments on image classi\ufb01cation, object detectionand instance segmentation demonstrate that the proposed framework signi\ufb01cantlyimproves the practical inference ef\ufb01ciency of deep networks. For example, theaverage latency of a ResNet-101 on the ImageNet validation set could be reducedby 36% and 46% on a server GPU (Nvidia Tesla-V100) and an edge device (NvidiaJetson TX2 GPU) respectively without sacri\ufb01cing the accuracy. Code is availableat https://github.com/LeapLabTHU/LASNet.1 IntroductionDynamic neural networks [7] have attracted great research interests in recent years. Compared tostatic models [11, 17, 13, 23] which treat different inputs equally during inference, dynamic networkscan allocate the computation in a data-dependent manner. For example, they can conditionally skipthe computation of network layers [ 15, 9, 32, 30] or convolutional channels [ 19, 1], or performspatially adaptive inference on the most informative image regions ( e.g. the foreground areas)[6, 5, 31, 35, 33, 8]. Spatial-wise dynamic networks, which typically decide whether to computeeach feature pixel with masker modules [5, 31, 35, 8] (Figure 1 (a)), have shown promising results inimproving the inference ef\ufb01ciency of convolution neural networks (CNNs).Despite the remarkable theoretical ef\ufb01ciency achieved by spatial-wise dynamic networks [5, 31, 35],researchers have found it challenging to translate the theoretical results into realistic speedup,\u2217Equal contribution.\u2020Corresponding author.36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.06223v1  [cs.CV]  12 Oct 2022--- Page 2 ---Maskergather ...convolution(s) scatter...Dynamic Convolution(s) AlgorithmSchedulingHardwareLatency(a) Algorithm: Spatially adaptive inference with a masker.(b) Scheduling: the practical inference pipeline of dynamic convolutions.(c) The general idea of our work.in\ufb02uence guidanceselected pixel locationsFigure 1: An overview of our method. (a) illustrates the spatially adaptive inference algorithm; (b) isthe scheduling strategy; and (c) presents the three key factors to the practical latency. For a givenhardware, the latency is used to guide our algorithm design and scheduling optimization.especially on some multi-core processors, e.g., GPUs [35, 3, 8]. The challenges are two-fold: 1) mostprevious approaches [5, 31, 35] perform spatially adaptive inference at the \ufb01nest granularity: everypixel is \ufb02exibly decided whether to be computed or not. Such \ufb02exibility induces non-contiguousmemory access [35] and requires specialized scheduling strategies (Figure 1 (b)); 2) the existingliterature has only adopted the hardware-agnostic FLOPs (\ufb02oating-point operations) as an inaccurateproxy for the ef\ufb01ciency, lacking latency-aware guidance on the algorithm design. For dynamicnetworks, the adaptive computation with sub-optimal scheduling strategies further enlarges thediscrepancy between the theoretical FLOPs and the practical latency. Note that it has been validatedby previous works that the latency on CPUs has a strong correlation with FLOPs [8, 35]. Therefore,we mainly focus on the GPU platform in this paper, which is more challenging and less explored.We address the above challenges by proposing a latency-aware spatial-wise dynamic network(LASNet). Three key factors to the inference latency are considered: the algorithm, the schedulingstrategy, and the hardware properties. Given a target hardware device, we directly use the latency,rather than the FLOPs, to guide our algorithm design and scheduling optimization (Figure 1 (c)).Because the memory access pattern and the scheduling strategies in our dynamic operators differfrom those in static networks, the libraries developed for static models (e.g. cuDNN) are sub-optimalfor the acceleration of dynamic models. Without the support of libraries, each dynamic operatorrequires scheduling optimization, code optimization, compiling, and deployment for each device.Therefore, it is laborious to evaluate the network latency on different hardware platforms. To this end,we propose a novel latency prediction model to ef\ufb01ciently estimate the realistic latency of a networkby simultaneously considering the aforementioned three factors. Compared to the hardware-agnosticFLOPs, our predicted latency can better re\ufb02ect the practical ef\ufb01ciency of dynamic models.Guided by this latency prediction model, we establish our latency-aware spatial-wise dynamic network(LASNet), which adaptively decides whether to allocate computation on feature patches insteadof pixels [5, 31, 35] (Figure 2 top). We name this paradigm as spatially adaptive inference at acoarse granularity. While less \ufb02exible than the pixel-level adaptive computation in previous works[5, 31, 35], it facilitates more contiguous memory access, bene\ufb01ting the realistic speedup on hardware.The scheduling strategy and the implementation are further ameliorated for faster inference.It is worth noting that LASNet is designed as a general framework in two aspects: 1) the coarse-grained spatially adaptive inference paradigm can be conveniently implemented in various CNNbackbones, e.g., ResNets [11], DenseNets [17] and RegNets [25]; and 2) the latency predictor is anoff-the-shell tool which can be directly used for various computing platforms (e.g. server GPUs andedge devices).We evaluate the performance of LASNet on multiple CNN architectures on image classi\ufb01cation,object detection, and instance segmentation tasks. Experiment results show that our LASNet improvesthe ef\ufb01ciency of deep CNNs both theoretically and practically. For example, the inference latencyof ResNet-101 on ImageNet [ 4] is reduced by 36% and 46% on an Nvidia Tesla V100 GPU andan Nvidia Jetson TX2 GPU, respectively, without sacri\ufb01cing the accuracy. Moreover, the proposedmethod outperforms various lightweight networks in a low-FLOPs regime.Our main contributions are summarized as follows:2--- Page 3 ---1. We propose LASNet, which performs coarse-grained spatially adaptive inference guidedby the practical latency instead of the theoretical FLOPs. To the best of our knowledge,LASNet is the \ufb01rst framework that directly considers the real latency in the design phase ofdynamic neural networks;2. We propose a latency prediction model, which can ef\ufb01ciently and accurately estimate thelatency of dynamic operators by simultaneously considering the algorithm, the schedulingstrategy, and the hardware properties;3. Experiments on image classi\ufb01cation and downstream tasks verify that our proposed LAS-Net can effectively improve the practical ef\ufb01ciency of different CNN architectures.2 Related worksSpatial-wise dynamic network is a common type of dynamic neural networks [ 7]. Compared tostatic models which treat different feature locations evenly during inference, these networks performspatially adaptive inference on the most informative regions ( e.g., foregrounds), and reduce theunnecessary computation on less important areas (e.g., backgrounds). Existing works mainly includethree levels of dynamic computation: resolution level [ 36, 37], region level [ 33] and pixel level[5, 31, 35]. The former two generally manipulate the network inputs [ 33, 37] or require specialarchitecture design [36]. In contrast, pixel-level dynamic networks can \ufb02exibly skip the convolutionson certain feature pixels in arbitrary CNN backbones [5, 31, 35]. Despite its remarkable theoreticalef\ufb01ciency, pixel-wise dynamic computation brings considerable dif\ufb01culty to achieving realisticspeedup on multi-core processors, e.g., GPUs. Compared to the previous approaches [ 5, 31, 35]which only focus on reducing the theoretical computation, we propose to directly use the latency toguide our algorithm design and scheduling optimization.Hardware-aware network design. To bridge the gap between theoretical and practical ef\ufb01ciencyof deep models, researchers have started to consider the real latency in the network design phase.There are two lines of works in this direction. One directly performs speed tests on targeteddevices, and summarizes some guidelines to facilitate hand-designing lightweight models [23]. Theother line of work searches for fast models using the neural architecture search (NAS) technique[29, 34]. However, all existing works try to build static models, which have intrinsic computationalredundancy by treating different inputs in the same way. However, speed tests for dynamic operatorson different hardware devices can be very laborious and impractical. In contrast, our proposed latencyprediction model can ef\ufb01ciently estimate the inference latency on any given computing platforms bysimultaneously considering algorithm design, scheduling strategies and hardware properties.3 MethodologyIn this section, we \ufb01rst introduce the preliminaries of spatially adaptive inference, and then demon-strate the architecture design of our LASNet. The latency prediction model is then explained, whichguides the granularity settings and the scheduling optimization for LASNet. We further present theimplementation improvements for faster inference, followed by the training strategies.3.1 PreliminariesSpatially adaptive inference. The existing spatial-wise dynamic networks are usually establishedby attaching a masker Min each convolutional block of a CNN backbone (Figure 1 (a)). Speci\ufb01cally,let x\u2208RH\xd7W\xd7C denote the input of a block, where H and W are the feature height and width, andC is the channel number. The masker Mtakes x as input, and generates a binary-valued spatialmask M = M(x) \u2208{0,1}H\xd7W . Each element of M determines whether to perform convolutionoperations on the corresponding location of the output feature. The unselected regions will be \ufb01lledwith the values from the input [5, 31] or obtained via interpolation [35]. We de\ufb01ne the activation rateof a block as r=\u2211i,j Mi,jH\xd7W , representing the ratio of the calculated pixels.Scheduling strategy. During inference, the current scheduling strategy for spatial-wise dynamicconvolutions generally involve three steps [26] (Figure 1 (b)): 1) gathering, which re-organizes theselected pixels (if the convolution kernel size is greater than 1 \xd71, the neighbors are also required)along the batch dimension; 2) computation, which performs convolution on the gathered input; and3) scattering, which \ufb01lls the computed pixels on their corresponding locations of the output feature.3--- Page 4 ---MaskerMasker-Conv1x1 Conv1x1 ...Scatter-Add Conv1x1 Conv3x3 Gumbel NoiseT raining Inference Input OutputUpsample Element-wise  multiplication Selected pixelGather-Conv3x3 Conv1x1 Regular convolution Operator fusionInput OutputFigure 2: Our proposed LASNet block. Top: we \ufb01rst generate a low-resolution spatial mask Mcoarse,which is then upsampled to obtain the mask M with the same size as the output feature. GumbelSoftmax [18, 24] is used for end-to-end training (Sec. 3.5). Bottom: the scheduling optimization isperformed to decrease the memory access for faster inference (Sec. 3.4).Limitations. Compared to performing convolutions on the entire feature map, the aforementionedscheduling strategy reduces the computation while bringing considerable overhead to memory accessdue to the mask generation and the non-contiguous memory access. Such overhead would increasethe overall latency, especially when the granularity of dynamic convolution is at the \ufb01nest pixel level.3.2 Architecture designSpatial granularity. As mentioned above, pixel-level dynamic convolutions [5, 31, 35] raise sub-stantial challenges to achieving realistic speedup on multi-core processors due to the non-contiguousmemory access. To this end, we propose to optimize the granularity of spatially adaptive inference.Speci\ufb01cally, take the commonly used bottleneck structure in [11] as an example, our coarse-grainedspatial-wise dynamic convolutional block is illustrated in Figure 2. Instead of directly producinga mask with the shape of H\xd7W, we \ufb01rst generate a low-resolution mask Mcoarse \u2208{0,1}HS \xd7WS ,where Sis named as the spatial granularity. Each element in Mcoarse determines the computationof an S\xd7S-sized feature patch. For instance, the feature size in the \ufb01rst ResNet stage3 is 56 \xd756.Then the possible choices for Sare {1,2,4,7,8,14,28,56}.The mask Mcoarse is then upsampledto the size of H\xd7W. Notably, S = 1means that the granularity is still at the pixel level as previousmethods [5, 31, 35]. Note that the other extreme situation (S = 56) is not considered in this paper,when the masker directly determines whether to skip the whole block (i.e. layer skipping [30, 32]).Such an overly aggressive approach will lead to considerable drop of accuracy, as we presented inAppendix C.2. The masker is composed of a pooling layer followed by a 1 \xd71 convolution.Differences to existing works. Without using the interpolation operation [ 35] or the carefullydesigned two-branch structure [8], the proposed block architecture is simple and suf\ufb01ciently generalto be plugged into most backbones with minimal modi\ufb01cation. Our formulation is mostly similar tothat in [31], which could be viewed as a variant of our method with the spatial granularity S=1 for allblocks. Instead of performing spatially adaptive inference at the \ufb01nest pixel level, our granularity Sisoptimized under the guidance of our latency prediction model (details are presented in the followingSec. 4.2) to achieve realistic speedup on target computing platforms.3.3 Latency prediction modelAs stated before, it is laborious to evaluate the latency of dynamic operators on different hardwareplatforms. To ef\ufb01ciently seek preferable granularity settings on arbitrary hardware devices, wepropose a latency prediction model G, which can directly predict the delay of executing dynamicoperators on any target devices. For a spatial-wise dynamic convolutional block, the latency predictorGtakes the hardware propertiesH, the layer parametersP, the spatial granularityS, and the activationrate ras input and predicts the latency \u2113of a dynamic convolutional block: \u2113= G(H,P,S,r ).Hardware modeling. We model a hardware device as multiple processing engines (PEs), and parallelcomputation can be executed on these PEs. As shown in Figure 3, we model the memory system as a3Here we refer to a stage as the cascading of multiple blocks which process features with the same resolution.4--- Page 5 ---Off-chip Memory(GDDR/DDR/HBM)On-chip Memory(LLC/Global Buffer)PE1PEMemoryComputation UnitsPE2PEMemoryComputation UnitsPEnPEMemoryComputation Units\u2026Figure 3: Our hardware model.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)101520253035Latency ( s)PredictedReal Figure 4: Latency prediction results.three-level structure [12]: 1) off-chip memory, 2) on-chip global memory, and 3) memory in PE. Sucha hardware model enables us to accurately predict the cost on both data movement and computation.Latency prediction. When simulating the data movement procedure, the ef\ufb01ciency of non-contiguous memory accesses under different granularity Ssettings is considered. As for the com-putation latency, it is important to adopt a proper scheduling strategy to increase the parallelism ofcomputation. Therefore, we search for the optimal scheduling (the con\ufb01guration of tiling and in-PEparallel computing) of dynamic operations to maximize the utilization of hardware resources. A moredetailed description of our latency prediction model is presented in Appendix A.Empirical validation. We take the \ufb01rst block in ResNet-101 as an example and vary the activationrate rto evaluate the performance of our prediction model. The comparison between our predictionsand the real testing latency on the Nvidia V100 GPU is illustrated in Figure 4, from which we canobserve that our predictor can accurately estimate the real latency in a wide range of activation rates.3.4 Implementation detailsWe use general optimization methods like fusing activation functions and batch normalizationlayers into convolution layers. We also optimize the speci\ufb01c operators in our spatial-wise dynamicconvolutional blocks as follows (see also Figure 2 for an overview).Fusing the masker and the \ufb01rst convolution. As mentioned in Sec. 3.1, the masker in each blockconsumes very little computation, but it takes the whole feature map as input. Therefore, it is amemory-bounded operation (the inference time is mainly spent on memory access). Since the maskerand the \ufb01rst convolution in the block share the same input, there is an opportunity to fuse these twooperations to avoid the repeated access of the input data. Note that a spatial-wise dynamic convolutionrequires the output of the masker. If we fuse the two layers, the \ufb01rst convolution will be changed to astatic operation, which may increase the inference latency. There exists a threshold of activation raterth, when r>r th, the overall latency can be reduced. We decide whether to fuse them according tothe average activation rate. See more details in Appendix B.1.Fusing the gather operation and the dynamic convolution. Traditional approaches \ufb01rst gather theinput pixels of the \ufb01rst dynamic convolution in a block (Figure 1 (b)). The gather operation is alsoa memory-bounded operation. Furthermore, when the size of the convolution kernel exceeds 1\xd71,the area of input patches may overlap, resulting in repeated memory load/store. We fuse the gatheroperation into the dynamic convolution to reduce the memory access.Fusing the scatter operation and the add operation. Traditional approaches scatter the outputpixels of the last dynamic convolution, and then execute the element-wise addition (Figure 1 (b)). Wefuse these two operators to reduce the memory access. The ablation study in Sec. 4.4 validates theeffectiveness of the proposed fusing methods.3.5 TrainingOptimization of non-differentiable maskers. The masker modules are required to produce binary-valued spatial masks for making discrete decisions, and cannot be directly optimized with backpropagation. Following [35, 31, 8], we adopt straight-through Gumbel Softmax [18, 24] to train thenetwork in an end-to-end fashion. Speci\ufb01cally, let \u02dcM \u2208RH\xd7W\xd72 denote the output of the maskgenerator. The decisions are obtained with the argmax function during inference. In the trainingphase, a differentiable approximation is de\ufb01ned by replacing the argmax operation with a Softmax:\u02c6M =exp{(log(\u02dcM:,:,0)+ G:,:,0)/\u03c4}\u22111k=0 exp{(log(\u02dcM:,:,k)+ G:,:,k)/\u03c4} \u2208 [0,1]H\xd7W , (1)5--- Page 6 ---0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1(a) Relationship between r\u2113 and rfor LAS-ResNet blocks on the Nvidia Tesla V100 GPU.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.75Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.752.00Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.4Latency ratio (r )S = 1(b) Relationship between r\u2113 and rfor LAS-RegNetY-800MF blocks on the Nvidia Jetson TX2 GPU.Figure 5: Latency prediction results of LAS-ResNet blocks on V100 (a) and LAS-RegNet blocks onTX2 (b). For both networks, we plot the relationship between the latency ratio r\u2113 and the activationrate r for the blocks in 4 stages with the convolutional stride 1. The practical ef\ufb01ciency is onlyimproved when r\u2113 <1. Note that S = 1can harm the practical latency even for a small r(reducedcomputation), while a larger Swill alleviate this problem. See detailed analysis in Sec. 4.2.where \u03c4 is the Softmax temperature. Following the common practice [ 31, 8], we let \u03c4 decreaseexponentially from 5.0 to 0.1 in training to facilitate the optimization of maskers.Training objective. The FLOPs of each spatial-wise dynamic convolutional block can be calculatedbased on our de\ufb01ned activation rate r[31]. Then we can obtain the FLOPs of the overall dynamicnetwork Fdyn. Let Fstat denotes the FLOPs of its static counterpart. We optimize their ratio toapproximate a target 0 <t< 1:LFLOPs = (FdynFstat\u2212t)2.In addition, we de\ufb01ne loss item Lbounds asin [31] to constrain the upper bound and the lower bound of activation rates in early training epochs.We further propose to leverage the static counterparts of our dynamic networks as \u201cteachers\u201d to guidethe optimization procedure. Let y and y\u2032denote the output logits of a dynamic model (\u201cstudent\u201d)and its \u201cteacher\u201d, respectively. Our \ufb01nal loss can be written asL= Ltask + \u03b1(LFLOPs + Lbounds) +\u03b2T2 \xb7KL(\u03c3(y/T)||\u03c3(y\u2032/T)), (2)where Ltask represents the task-related loss, e.g., cross-entropy loss in image classi\ufb01cation. KL(\xb7||\xb7)denotes the Kullback\u2013Leibler divergence, and \u03b1,\u03b2 are the coef\ufb01cients balancing these items. We use\u03c3to denote the log-Softmax function, and T is the temperature for computing KL-divergence.4 ExperimentsIn this section, we \ufb01rst introduce the experiment settings in Sec. 4.1. Then the latency of differentgranularity settings are analyzed in Sec. 4.2. The performance of our LASNet on ImageNet is furtherevaluated in Sec. 4.3, followed by the ablation studies in Sec. 4.4. Visualization results are illustratedin Sec. 4.5, and we \ufb01nally validate our method on the object detection task (Sec. 4.6). The results onthe instance segmentation task are presented in . For simplicity, we add \u201cLAS-\u201d as a pre\ufb01x beforemodel names to denote our LASNet, e.g., LAS-ResNet-50.4.1 Experiment settingsLatency prediction. Various types of hardware platforms are tested, including a server GPU (TeslaV100), a desktop GPU (GTX1080) and edge devices ( e.g., Nvidia Nano and Jetson TX2). Themajor properties considered by our latency prediction model include the number of processingengines (#PE), the \ufb02oating-point computation in a processing engine (#FP32), the frequency and thebandwidth. It can be observed that the server GPUs generally have a larger #PE than the IoT devices.The batch size is set as 1 for all dynamic models and computing platforms.Image classi\ufb01cation. The image classi\ufb01cation experiments are conducted on the ImageNet [ 4]dataset. Following [31], we initialize the backbone parameter from a pre-trained checkpoint4, and4We use the torchvision pre-trained models at https://pytorch.org/vision/stable/models.html.6--- Page 7 ---1 2 4 7 8 14 28Spatial granularity (S)12345Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.51.01.52.02.53.03.54.0Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.51.01.52.02.53.0Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(a) Relationship between r\u2113 and Sfor LAS-ResNet blocks on V100.1 2 4 7 8 14 28Spatial granularity (S)0.250.500.751.001.251.501.75Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.250.500.751.001.251.501.752.00Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.20.40.60.81.01.21.41.6Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(b) Relationship between r\u2113 and Sfor LAS-RegNetY-800MF blocks on Nvidia Jetson TX2 GPU.Figure 6: The relationship between the latency ratio r\u2113 and the spatial granularity S.\ufb01netune the whole network for 100 epochs with the loss function in Eq. (2). We \ufb01x \u03b1= 10,\u03b2 = 0.5and T = 4.0 for all dynamic models. More details are provided in Appendix B.4.2 Latency prediction resultsIn this subsection, we present the latency prediction results of the spatial-wise dynamic convolutionalblocks in two different models: LAS-ResNet-101 [11] (on V100) and LAS-RegNetY-800MF [25] (onTX2). All the blocks have the bottleneck structure with different channel numbers and convolutiongroups, and the RegNetY is equipped with Squeeze-and-Excitation (SE) [14] modules.We \ufb01rst de\ufb01ne \u2113dyn as the latency of a spatial-wise dynamic convolutional block, and \u2113stat as that ofa static block without a masker. Their ratio is denoted as r\u2113 = \u2113dyn\u2113stat. We investigate the relationshipbetween r\u2113 and the activation rate r(cf. Sec. 3.5) for different granularity settings. The results inFigure 5 demonstrate that: 1) even equipped with our special optimization on the scheduling strategies,pixel-level spatially adaptive inference (S=1) cannot always improve the practical ef\ufb01ciency. Such\ufb01ne-grained adaptive inference is adopted by most previous works [31, 35], and our result can explainthe reason why they can only achieve realistic speedup on less powerful CPUs [35] or specializeddevices [3]; 2) a proper granularity S >1 effectively alleviates this problem on both hardwaredevices. By setting S >1, realistic speedup could be achieved with larger activation rates.The latency prediction results are further used to seek for a preferable granularity setting for the\ufb01rst 3 stages (we \ufb01x S = 1 for the last stage, where the feature resolution is 7 \xd77). Therefore,we plot the relationship between r\u2113 and S in Figure 10. It can be observed that: 1) r\u2113 generallydecreases with Sincreasing for a given r; 2) an overly largeS(less \ufb02exible adaptive inference) bringsinsigni\ufb01cant improvement on both devices. Especially, enlarging S from 8 to 28 in the \ufb01rst stageof a LAS-ResNet brings very little improvement on V100. Based on the results in Figure 10, wecan trade off between \ufb02exibility and ef\ufb01ciency by selecting appropriate Sfor different models andhardware devices. For example, we can simply set Snet=8-4-7-15 in a LAS-ResNet-101 to achiverealistic speedup. The accuracy-latency plots in Figure 7 also validate this observation. More resultsof our latency prediction model on the desktop-level GPU, GTX1080, are presented in Appendix C.1.4.3 ImageNet classi\ufb01cation resultsWe now empirically evaluate our proposed LASNet on the ImageNet dataset. The network perfor-mance is measured in terms of the trade-off between classi\ufb01cation accuracy and inference ef\ufb01ciency.Both theoretical (i.e. FLOPs) and practical ef\ufb01ciency (i.e. latency) are tested in our experiments.5We use this form to represnet the Ssettings for the 4 stages of a network.7--- Page 8 ---2 3 4 5 6 7 8FLOPs (x1e9)73747576777879Accuracy (%)static ResNetsConvAIG-ResNet-101SkipNet-ResNet-101BAS-ResNet-50DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)15 20 25 30 35 40Latency on Nvidia Jetson TX2 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)1.5 2.0 2.5 3.0 3.5Latency on Nvidia Tesla V100 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)(a) LAS-ResNet results.0.2 0.3 0.4 0.5 0.6 0.7 0.8FLOPs (x1e9)717273747576Accuracy (%)RegNets-XRegNets-YMobileNets-v2ShuffleNets-v2CondenseNetsLAS-RegNetY-400MF (Snet=1-1-1-1)LAS-RegNetY-400MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)3 4 5 6 7 8Latency on Nvidia Jetson TX2 GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-XRegNets-YMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)8 10 12 14 16 18 20Latency on Nvidia nano GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-YRegNets-XMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)(b) LAS-RegNetY results.Figure 7: Experimental results on ImageNet. The proposed coarse-grained spatially adaptive inferenceis tested on standard ResNets (a) and lightweight RegNets (b).4.3.1 Standard baseline comparison: ResNetsWe \ufb01rst establish our LASNet based on the standard ResNets [ 11]. Speci\ufb01cally, we build LAS-ResNet-50 and LAS-ResNet-101 by plugging our maskers in the two common ResNet structures.The baselines include various types of dynamic inference approaches: 1) layer skipping (SkipNet[32] and Conv-AIG [30]); 2) channel skipping (BAS [1]); and 3) pixel-level spatial-wise dynamicnetwork (DynConv [31]). For our LASNet, we compare various settings of the spatial granularitySnet. We set training targets (cf. Sec. 3.5) t\u2208{0,4,0.5,0.6,0.7}for our dynamic models to evaluatetheir performance in different sparsity regimes. We apply the same operator fusion (Sec. 3.4) for bothour models and the compared baselines [30, 31] for fair comparison.Results are presented in Figure 7 (a). On the left we plot the relationship of accuracy v.s. FLOPs. Itcan be observed that our LAS-ResNets with different granularity settings signi\ufb01cantly outperformthe competing dynamic neural networks. Surprisingly, coarse-grained spatially adaptive inference(Snet=4-4-2-1 and Snet=8-4-7-1 for the 4 stages) can achieve even higher accuracy when consumingsimilar FLOPs on ResNets, despite the sacri\ufb01ced \ufb02exibility compared toSnet=1-1-1-1. We conjecturethat a larger Sis also bene\ufb01cial to the optimization of maskers.We compare the practical latency of three granularity settings in Figure 7 (a) predicted by our latencyprediction model (middle on TX2 and right on V100). We can witness that although they achievecomparable theoretical ef\ufb01ciency (Figure 7 (a) left), larger Sis more hardware-friendly comparedto the \ufb01nest granularity. For example, the inference latency of LAS-ResNet-101 ( Snet=1-1-1-1)is signi\ufb01cantly higher than the ResNet-101 baseline on V100 (Figure 7 (a) right), even though itstheoretical computation is much smaller than that of the static model. However, larger granularities(Snet=4-4-2-1 and Snet=8-4-7-1) can effectively improve the inference latency due to its lower burdenon the memory access. Remarkably, the latency of ResNet-101 could be reduced by 36% and 46% onV100 and TX2 respectively without sacri\ufb01cing the accuracy when t=0.4. The classi\ufb01cation accuracyis increased by 1.9% with similar inference ef\ufb01ciency. It can be observed that the realistic speedupratio r\u2113 is more close to the theoretical FLOPs ratio target ton the less powerful TX2, because thelatency is computation-bounded (i.e. the latency is mainly spent on computation) on such IoT devices.In contrast, there is a larger gap between practical and theoretical ef\ufb01ciency on the more powerfulV100, as the latency is bounded by the memory access cost.4.3.2 Lightweight baseline comparison: RegNetsWe further evaluate our LASNet in lightweight CNN architectures,i.e. RegNets-Y [25]. Two differentsized models are tested: RegNetY-400MF and RegNetY-800MF. Compared baselines include othertypes of ef\ufb01cient models, e.g., MobileNets-v2 [28], Shuf\ufb02etNets-v2 [23] and CondenseNets [16].The results are presented in Figure 7 (b). The x-axis for the three sub-\ufb01gures are the FLOPs, thelatency on TX2, and the latency on the Nvidia nano GPU, respectively. We can observe that our8--- Page 9 ---1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2Latency on Nvidia Tesla V100 GPU (ms)76.577.077.578.078.579.0Accuracy (%) static ResNetsLAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=4-4-7-1)LAS-ResNet-101 (Snet=8-4-7-1)Figure 8: Ablation studies on S. Figure 9: Visualization results.method outperforms various types of static models in terms of the trade-off between accuracy andef\ufb01ciency. More results on image classi\ufb01cation are provided in Appendix C.2.4.4 Ablation studiesWe conduct ablation studies to validate the effectiveness of our coarse-grained spatially adaptiveinference (Sec. 3.2) and operator fusion operations (Sec. 3.4).More granularities settings. We test various granularity settings on LAS-ResNet-101 to examinethe effects of Sin different stages. The results on the Tesla-V100 GPU are presented in Figure 8. Itcan be found that the \ufb01nest granularity (Snet=1-1-1-1) leads to substantial inef\ufb01ciency despite thereduced FLOPs (cf. Figure 7 (a) left). Coarse-grained spatially adaptive inference in the \ufb01rst twostages (Snet=4-4-1-1) effectively reduces the inference latency. We further increase Sin the thirdstage to 2 and 7, and this procedure consistently improves the realistic ef\ufb01ciency on the V100 GPU.This trend also holds on the GTX 1080 GPU (see the results in Appendix C.2.Table 1: Ablation studies on operator fusion.Masker- Gather- Scatter- LatencyConv1x1 Conv3x3 Add (\xb5s)   163.2   90.1   86.7   71.4Operator fusion. We investigate the effect of ouroperator fusion introduced in Sec. 3.4. One convolu-tional block in the \ufb01rst stage of a LAS-ResNet-101(S=4, r=0.6) is tested. The results in Table 1 vali-date that every step of operator fusion bene\ufb01ts thepractical latency of a block, as the overhead onmemory access is effectively reduced. Especially,the fusion of the masker operation and the \ufb01rst con-volution is crucial to reducing the latency.4.5 VisualizationWe visualize the masks generated by our masker in the third block of a LAS-ResNet-101 (Snet=4-4-2-1) in Figure 12. The brilliant areas correspond to the locations of 1 elements in a mask, andthe computation on the dimmed regions is skipped by our dynamic model. We can witness that themasker accurately locate the most task-related regions (even the tiny aircraft at the corner), whichhelps reduce the unnecessary computation on background areas. These resultsalso suggest that forthe \ufb01rst stage, the granularity S=4 is suf\ufb01ciently \ufb02exible to recognize the important regions, anda win-win can be achieved between accuracy and ef\ufb01ciency. Interestingly, the masker could selectsome objects that are not labeled for the sample, e.g., the \ufb02ower beside the hummingbird and thehuman holding the camera. This indicates that our spatial-wise dynamic networks can automaticallyrecognize the regions with semantics, and their capability is not limited by the classi\ufb01cation labels.This property is helpful in some downstream tasks, such as object detection (Sec. 4.6) and instancesegmentation (Sec. 4.7), which require detecting multiple classes and objects in an image. Morevisualization results could be found in Appendix C.3.4.6 COCO Object detectionWe further evaluate our LASNet on the COCO [22] object detection task. The mean average precision(mAP), the average backbone FLOPs, and the average backbone latency on the validation set areused to measure the network performance. We test two commonly used detection frameworks: FasterR-CNN [27] with Feature Pyramid Network [20] and RetinaNet [21]. Thanks to the generality ofour method, we can conveniently replace the backbones with ours pre-trained on ImageNet, andthe whole models are \ufb01netuned on COCO with the standard setting for 12 epochs (see detailed9--- Page 10 ---Table 2: Object detection results on the COCO dataset.Detection Backbone Backbone Backbone Latency (ms)mAP (%)Framework FLOPs (G) V100 GTX1080 TX2Faster R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 39.4LAS-ResNet-101 (Snet=4-4-2-1,t=0.6) 90.7 33.8 90.6 524.9 40.3LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 79.3 30.7 82.5 477.1 39.8LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.5 29.0 79.9 464.8 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 67.9 25.3 69.1 401.3 39.5RetinaNetResNet-101 (Baseline) 141.2 39.5 119.3 729.4 38.5LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 77.8 30.4 81.8 472.6 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.4 28.9 79.9 464.8 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 66.4 25.3 69.1 401.3 38.9Table 3: Instance Segmentation results on the COCO dataset.Segmentation Backbone Backbone Backbone Latency (ms)APmask(%) APbox(%)Framework FLOPs (G) V100 GTX1080 TX2Mask R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 36.1 40.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 80.5 31.1 83.3 481.9 37.0 41.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.4) 69.2 27.9 74.8 431.6 36.1 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 68.8 25.8 70.9 411.8 36.2 40.0setup in Appendix B.3). The input images are resized to a short side of 800 and a long side notexceeding 1333. The results of our LAS-ResNet-101 with different Snet settings are presented inTable 2. We can observe from the results that when setting the training target as 0.4, the latency ofour LAS-ResNet-101 with Snet=4-4-7-1 is signi\ufb01cantly lower than the static baseline on all deviceswithout sacri\ufb01cing mAP in both detection frameworks. With a larger training targets, our LASNetcan increase the mAP by 0.9% and 0.8% in Faster R-CNN and RetinaNet respectively, while stillbeing faster than the baseline method.4.7 COCO instance segmentationWe also present the results of instance segmentation on COCO, which demonstrate the effectivenessof our LASNet on the dense prediction task. From the results in Table 3, we can observe that whensetting the training target as 0.4, the Mask R-CNN [ 10] models (Snet=4-4-2-1 and Snet=4-4-7-1)runs faster on all tested hardware devices without sacri\ufb01cing the performance. With a training targetof 0.5, the APmask and APbox of the Mask R-CNN model could be increased by 0.9% and 1.0%respectively while still running faster than the baseline method.5 ConclusionIn this paper, we propose to build latency-aware spatial-wise dynamic networks (LASNet) under theguidance of a latency prediction model. By simultaneously considering the algorithm, the schedulingstrategy and the hardware properties, we can ef\ufb01ciently estimate the practical latency of spatial-wise dynamic operators on arbitrary computing platforms. Based on the empirical analysis on therelationship between the latency and the granularity of spatially adaptive inference, we optimizeboth the algorithm and the scheduling strategies to achieve realistic speedup on many multi-coreprocessors, e.g., the Tesla V100 GPU and the Jetson TX2 GPU. Experiments on image classi\ufb01cation,object detection and instance segmentation tasks validate that the proposed method signi\ufb01cantlyimproves the practical ef\ufb01ciency of deep CNNs, and outperforms various competing approaches.AcknowledgementThis work is supported in part by the National Key R&D Program of China under Grant2020AAA0105200, the National Natural Science Foundation of China under Grants 62022048and the Tsinghua University-China Mobile Communications Group Co.,Ltd. Joint Institute..10--- Page 11 ---References[1] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping for learningconditional channel gated networks. In ICLR, 2020.[2] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, MeghanCowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. Tvm: An automated end-to-end optimizingcompiler for deep learning. In OSDI, 2018.[3] Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian Verhelst. Proces-sor architecture optimization for spatially dynamic neural networks. In VLSI-SoC, 2021.[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In CVPR, 2009.[5] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicatednetwork with less inference complexity. In CVPR, 2017.[6] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov,and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In CVPR,2017.[7] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamicneural networks: A survey. TPAMI, 2021.[8] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, and Haojun Jiang. Spatiallyadaptive feature re\ufb01nement for ef\ufb01cient inference. TIP, 2021.[9] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfen Cao, Wenhui Huang,Chao Deng, and Gao Huang. Learning to weight samples for dynamic early-exiting networks.In ECCV, 2022.[10] Kaiming He, Georgia Gkioxari, Piotr Doll\xe1r, and Ross Girshick. Mask r-cnn. In Proceedings ofthe IEEE international conference on computer vision, pages 2961\u20132969, 2017.[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In CVPR, 2016.[12] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach .Elsevier, 2011.[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, TobiasWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ef\ufb01cient convolutional neuralnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.[15] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Wein-berger. Multi-scale dense networks for resource ef\ufb01cient image classi\ufb01cation. In ICLR, 2018.[16] Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: Anef\ufb01cient densenet using learned group convolutions. In CVPR, 2018.[17] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Weinberger.Densely connected convolutional networks. TPAMI, 2019.[18] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.In ICLR, 2017.[19] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In NeurIPS, 2017.[20] Tsung-Yi Lin, Piotr Doll\xe1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.Feature pyramid networks for object detection. In CVPR, 2017.[21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Focal loss for denseobject detection. In ICCV, 2017.11--- Page 12 ---[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,Piotr Doll\xe1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,2014.[23] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shuf\ufb02enet v2: Practical guidelinesfor ef\ufb01cient cnn architecture design. In ECCV, 2018.[24] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuousrelaxation of discrete random variables. In ICLR, 2017.[25] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Design-ing network design spaces. In CVPR, 2020.[26] Mengye Ren, Andrei Pokrovsky, Bin Yang, and Raquel Urtasun. SBNet: Sparse Blocks Networkfor Fast Inference. CVPR, 2018.[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-timeobject detection with region proposal networks. In NeurIPS, 2015.[28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.[29] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, andQuoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820\u20132828,2019.[30] Andreas Veit and Serge Belongie. Convolutional networks with adaptive inference graphs. InECCV, 2018.[31] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: Exploiting Spatial Sparsity forFaster Inference. In CVPR, 2020.[32] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learningdynamic routing in convolutional networks. In ECCV, 2018.[33] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus:a dynamic approach to reducing spatial redundancy in image classi\ufb01cation. In NeurIPS, 2020.[34] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, YuandongTian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware ef\ufb01cient convnetdesign via differentiable neural architecture search. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 10734\u201310742, 2019.[35] Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptiveinference with stochastic feature sampling and interpolation. In ECCV, 2020.[36] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptivenetworks for ef\ufb01cient inference. In CVPR, 2020.[37] Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan, and Yunhe Wang.Dynamic resolution network. NeurIPS, 2021.12--- Page 13 ---AppendixA Latency prediction model.As the dynamic operators in our method have not been supported by current deep learning libraries,we propose a latency prediction model to ef\ufb01ciently estimate the real latency of these operators onhardware device. The inputs of the latency prediction model include: 1) the structural con\ufb01gurationof a convolutional block, 2) its activation rate rwhich decides the computation amount, 3) the spatialgranularity S, and 4) the hardware properties mentioned in Table 4. The latency of a dynamic blockis predicted as follows.Input/output shape de\ufb01nition. The \ufb01rst step of predicting the latency of an operation is to calculatethe shape of input and output. Taking the gather-conv2 operation as an example, the input of thisoperation is the activation with the shape of Cin \xd7H \xd7W, where Cin is the number of inputchannels, and H and W are the resolution of the feature map. The shape of the output tensor isP \xd7Cout \xd7S\xd7S, where P is the number of output patches, Cout is the number of output channelsand Sis the spatial granularity. Note that P is obtained based on the output of our maskers.Operation-to-hardware mapping. Next, we map the operations to hardware. As is mentioned in thepaper, we model a hardware device as multiple processing engines (PEs). We assign the computationof each element in the output feature map to a PE. Speci\ufb01cally, we consecutively split the outputfeature map into multiple tiles. The shape of each tile is TP \xd7TC \xd7TS1 \xd7TS2. These split tilesare assigned to multiple PEs. The computation of the elements in each tile is executed in a PE. Wecan con\ufb01gure different shapes of tiles. In order to determine the optimal shape of the tile, we makea search space of different tile shapes. The tile shape has 4 dimensions. The candidates of eachdimension are power-of-2 and do not exceed the corresponding dimension of the feature map.Latency estimation. Then, we evaluate the latency of each tile shape in the search space and selectthe optimal tile shape with the lowest latency. The latency includes the data movement latency andthe computation latency: \u2113= \u2113data + \u2113computation.1) Data movement latency \u2113data. The estimation of the latency for data movement requires us tomodel the memory system of a hardware device. We model the memory system of hardware as athree-level architecture [12]: off-chip memory, on-chip global memory, and local memory in PE.The input data and weight data are \ufb01rst transferred from the off-chip memory to the on-chip globalmemory. We assume the hardware can make full use of the off-chip memory bandwidth to simplifythe latency prediction model.After that, the data used to compute the output tiles is transferred from on-chip global memory tothe local m\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2210.06223v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 10:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #8, Phase: literature review, Feedback: 3\n--- Page 1 ---Latency-aware Spatial-wise Dynamic NetworksYizeng Han1\u2217Zhihang Yuan2\u2217 Yifan Pu1\u2217 Chenhao Xue2Shiji Song1 Guangyu Sun2 Gao Huang1\u20201 Department of Automation, BNRist, Tsinghua University, Beijing, China2 School of Electronics Engineering and Computer Science, Peking University, Beijing, China{hanyz18, pyf20}@mails.tsinghua.edu.cn, {shijis, gaohuang}@tsinghua.edu.cn{yuanzhihang, xch927027, gsun}@pku.edu.cnAbstractSpatial-wise dynamic convolution has become a promising approach to improvingthe inference ef\ufb01ciency of deep networks. By allocating more computation to themost informative pixels, such an adaptive inference paradigm reduces the spatialredundancy in image features and saves a considerable amount of unnecessarycomputation. However, the theoretical ef\ufb01ciency achieved by previous methodscan hardly translate into a realistic speedup, especially on the multi-core processors(e.g. GPUs). The key challenge is that the existing literature has only focusedon designing algorithms with minimal computation, ignoring the fact that thepractical latency can also be in\ufb02uenced by scheduling strategies and hardwareproperties. To bridge the gap between theoretical computation and practical ef-\ufb01ciency, we propose a latency-aware spatial-wise dynamic network (LASNet),which performs coarse-grained spatially adaptive inference under the guidanceof a novel latency prediction model. The latency prediction model can ef\ufb01cientlyestimate the inference latency of dynamic networks by simultaneously consideringalgorithms, scheduling strategies, and hardware properties. We use the latencypredictor to guide both the algorithm design and the scheduling optimization onvarious hardware platforms. Experiments on image classi\ufb01cation, object detectionand instance segmentation demonstrate that the proposed framework signi\ufb01cantlyimproves the practical inference ef\ufb01ciency of deep networks. For example, theaverage latency of a ResNet-101 on the ImageNet validation set could be reducedby 36% and 46% on a server GPU (Nvidia Tesla-V100) and an edge device (NvidiaJetson TX2 GPU) respectively without sacri\ufb01cing the accuracy. Code is availableat https://github.com/LeapLabTHU/LASNet.1 IntroductionDynamic neural networks [7] have attracted great research interests in recent years. Compared tostatic models [11, 17, 13, 23] which treat different inputs equally during inference, dynamic networkscan allocate the computation in a data-dependent manner. For example, they can conditionally skipthe computation of network layers [ 15, 9, 32, 30] or convolutional channels [ 19, 1], or performspatially adaptive inference on the most informative image regions ( e.g. the foreground areas)[6, 5, 31, 35, 33, 8]. Spatial-wise dynamic networks, which typically decide whether to computeeach feature pixel with masker modules [5, 31, 35, 8] (Figure 1 (a)), have shown promising results inimproving the inference ef\ufb01ciency of convolution neural networks (CNNs).Despite the remarkable theoretical ef\ufb01ciency achieved by spatial-wise dynamic networks [5, 31, 35],researchers have found it challenging to translate the theoretical results into realistic speedup,\u2217Equal contribution.\u2020Corresponding author.36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.06223v1  [cs.CV]  12 Oct 2022--- Page 2 ---Maskergather ...convolution(s) scatter...Dynamic Convolution(s) AlgorithmSchedulingHardwareLatency(a) Algorithm: Spatially adaptive inference with a masker.(b) Scheduling: the practical inference pipeline of dynamic convolutions.(c) The general idea of our work.in\ufb02uence guidanceselected pixel locationsFigure 1: An overview of our method. (a) illustrates the spatially adaptive inference algorithm; (b) isthe scheduling strategy; and (c) presents the three key factors to the practical latency. For a givenhardware, the latency is used to guide our algorithm design and scheduling optimization.especially on some multi-core processors, e.g., GPUs [35, 3, 8]. The challenges are two-fold: 1) mostprevious approaches [5, 31, 35] perform spatially adaptive inference at the \ufb01nest granularity: everypixel is \ufb02exibly decided whether to be computed or not. Such \ufb02exibility induces non-contiguousmemory access [35] and requires specialized scheduling strategies (Figure 1 (b)); 2) the existingliterature has only adopted the hardware-agnostic FLOPs (\ufb02oating-point operations) as an inaccurateproxy for the ef\ufb01ciency, lacking latency-aware guidance on the algorithm design. For dynamicnetworks, the adaptive computation with sub-optimal scheduling strategies further enlarges thediscrepancy between the theoretical FLOPs and the practical latency. Note that it has been validatedby previous works that the latency on CPUs has a strong correlation with FLOPs [8, 35]. Therefore,we mainly focus on the GPU platform in this paper, which is more challenging and less explored.We address the above challenges by proposing a latency-aware spatial-wise dynamic network(LASNet). Three key factors to the inference latency are considered: the algorithm, the schedulingstrategy, and the hardware properties. Given a target hardware device, we directly use the latency,rather than the FLOPs, to guide our algorithm design and scheduling optimization (Figure 1 (c)).Because the memory access pattern and the scheduling strategies in our dynamic operators differfrom those in static networks, the libraries developed for static models (e.g. cuDNN) are sub-optimalfor the acceleration of dynamic models. Without the support of libraries, each dynamic operatorrequires scheduling optimization, code optimization, compiling, and deployment for each device.Therefore, it is laborious to evaluate the network latency on different hardware platforms. To this end,we propose a novel latency prediction model to ef\ufb01ciently estimate the realistic latency of a networkby simultaneously considering the aforementioned three factors. Compared to the hardware-agnosticFLOPs, our predicted latency can better re\ufb02ect the practical ef\ufb01ciency of dynamic models.Guided by this latency prediction model, we establish our latency-aware spatial-wise dynamic network(LASNet), which adaptively decides whether to allocate computation on feature patches insteadof pixels [5, 31, 35] (Figure 2 top). We name this paradigm as spatially adaptive inference at acoarse granularity. While less \ufb02exible than the pixel-level adaptive computation in previous works[5, 31, 35], it facilitates more contiguous memory access, bene\ufb01ting the realistic speedup on hardware.The scheduling strategy and the implementation are further ameliorated for faster inference.It is worth noting that LASNet is designed as a general framework in two aspects: 1) the coarse-grained spatially adaptive inference paradigm can be conveniently implemented in various CNNbackbones, e.g., ResNets [11], DenseNets [17] and RegNets [25]; and 2) the latency predictor is anoff-the-shell tool which can be directly used for various computing platforms (e.g. server GPUs andedge devices).We evaluate the performance of LASNet on multiple CNN architectures on image classi\ufb01cation,object detection, and instance segmentation tasks. Experiment results show that our LASNet improvesthe ef\ufb01ciency of deep CNNs both theoretically and practically. For example, the inference latencyof ResNet-101 on ImageNet [ 4] is reduced by 36% and 46% on an Nvidia Tesla V100 GPU andan Nvidia Jetson TX2 GPU, respectively, without sacri\ufb01cing the accuracy. Moreover, the proposedmethod outperforms various lightweight networks in a low-FLOPs regime.Our main contributions are summarized as follows:2--- Page 3 ---1. We propose LASNet, which performs coarse-grained spatially adaptive inference guidedby the practical latency instead of the theoretical FLOPs. To the best of our knowledge,LASNet is the \ufb01rst framework that directly considers the real latency in the design phase ofdynamic neural networks;2. We propose a latency prediction model, which can ef\ufb01ciently and accurately estimate thelatency of dynamic operators by simultaneously considering the algorithm, the schedulingstrategy, and the hardware properties;3. Experiments on image classi\ufb01cation and downstream tasks verify that our proposed LAS-Net can effectively improve the practical ef\ufb01ciency of different CNN architectures.2 Related worksSpatial-wise dynamic network is a common type of dynamic neural networks [ 7]. Compared tostatic models which treat different feature locations evenly during inference, these networks performspatially adaptive inference on the most informative regions ( e.g., foregrounds), and reduce theunnecessary computation on less important areas (e.g., backgrounds). Existing works mainly includethree levels of dynamic computation: resolution level [ 36, 37], region level [ 33] and pixel level[5, 31, 35]. The former two generally manipulate the network inputs [ 33, 37] or require specialarchitecture design [36]. In contrast, pixel-level dynamic networks can \ufb02exibly skip the convolutionson certain feature pixels in arbitrary CNN backbones [5, 31, 35]. Despite its remarkable theoreticalef\ufb01ciency, pixel-wise dynamic computation brings considerable dif\ufb01culty to achieving realisticspeedup on multi-core processors, e.g., GPUs. Compared to the previous approaches [ 5, 31, 35]which only focus on reducing the theoretical computation, we propose to directly use the latency toguide our algorithm design and scheduling optimization.Hardware-aware network design. To bridge the gap between theoretical and practical ef\ufb01ciencyof deep models, researchers have started to consider the real latency in the network design phase.There are two lines of works in this direction. One directly performs speed tests on targeteddevices, and summarizes some guidelines to facilitate hand-designing lightweight models [23]. Theother line of work searches for fast models using the neural architecture search (NAS) technique[29, 34]. However, all existing works try to build static models, which have intrinsic computationalredundancy by treating different inputs in the same way. However, speed tests for dynamic operatorson different hardware devices can be very laborious and impractical. In contrast, our proposed latencyprediction model can ef\ufb01ciently estimate the inference latency on any given computing platforms bysimultaneously considering algorithm design, scheduling strategies and hardware properties.3 MethodologyIn this section, we \ufb01rst introduce the preliminaries of spatially adaptive inference, and then demon-strate the architecture design of our LASNet. The latency prediction model is then explained, whichguides the granularity settings and the scheduling optimization for LASNet. We further present theimplementation improvements for faster inference, followed by the training strategies.3.1 PreliminariesSpatially adaptive inference. The existing spatial-wise dynamic networks are usually establishedby attaching a masker Min each convolutional block of a CNN backbone (Figure 1 (a)). Speci\ufb01cally,let x\u2208RH\xd7W\xd7C denote the input of a block, where H and W are the feature height and width, andC is the channel number. The masker Mtakes x as input, and generates a binary-valued spatialmask M = M(x) \u2208{0,1}H\xd7W . Each element of M determines whether to perform convolutionoperations on the corresponding location of the output feature. The unselected regions will be \ufb01lledwith the values from the input [5, 31] or obtained via interpolation [35]. We de\ufb01ne the activation rateof a block as r=\u2211i,j Mi,jH\xd7W , representing the ratio of the calculated pixels.Scheduling strategy. During inference, the current scheduling strategy for spatial-wise dynamicconvolutions generally involve three steps [26] (Figure 1 (b)): 1) gathering, which re-organizes theselected pixels (if the convolution kernel size is greater than 1 \xd71, the neighbors are also required)along the batch dimension; 2) computation, which performs convolution on the gathered input; and3) scattering, which \ufb01lls the computed pixels on their corresponding locations of the output feature.3--- Page 4 ---MaskerMasker-Conv1x1 Conv1x1 ...Scatter-Add Conv1x1 Conv3x3 Gumbel NoiseT raining Inference Input OutputUpsample Element-wise  multiplication Selected pixelGather-Conv3x3 Conv1x1 Regular convolution Operator fusionInput OutputFigure 2: Our proposed LASNet block. Top: we \ufb01rst generate a low-resolution spatial mask Mcoarse,which is then upsampled to obtain the mask M with the same size as the output feature. GumbelSoftmax [18, 24] is used for end-to-end training (Sec. 3.5). Bottom: the scheduling optimization isperformed to decrease the memory access for faster inference (Sec. 3.4).Limitations. Compared to performing convolutions on the entire feature map, the aforementionedscheduling strategy reduces the computation while bringing considerable overhead to memory accessdue to the mask generation and the non-contiguous memory access. Such overhead would increasethe overall latency, especially when the granularity of dynamic convolution is at the \ufb01nest pixel level.3.2 Architecture designSpatial granularity. As mentioned above, pixel-level dynamic convolutions [5, 31, 35] raise sub-stantial challenges to achieving realistic speedup on multi-core processors due to the non-contiguousmemory access. To this end, we propose to optimize the granularity of spatially adaptive inference.Speci\ufb01cally, take the commonly used bottleneck structure in [11] as an example, our coarse-grainedspatial-wise dynamic convolutional block is illustrated in Figure 2. Instead of directly producinga mask with the shape of H\xd7W, we \ufb01rst generate a low-resolution mask Mcoarse \u2208{0,1}HS \xd7WS ,where Sis named as the spatial granularity. Each element in Mcoarse determines the computationof an S\xd7S-sized feature patch. For instance, the feature size in the \ufb01rst ResNet stage3 is 56 \xd756.Then the possible choices for Sare {1,2,4,7,8,14,28,56}.The mask Mcoarse is then upsampledto the size of H\xd7W. Notably, S = 1means that the granularity is still at the pixel level as previousmethods [5, 31, 35]. Note that the other extreme situation (S = 56) is not considered in this paper,when the masker directly determines whether to skip the whole block (i.e. layer skipping [30, 32]).Such an overly aggressive approach will lead to considerable drop of accuracy, as we presented inAppendix C.2. The masker is composed of a pooling layer followed by a 1 \xd71 convolution.Differences to existing works. Without using the interpolation operation [ 35] or the carefullydesigned two-branch structure [8], the proposed block architecture is simple and suf\ufb01ciently generalto be plugged into most backbones with minimal modi\ufb01cation. Our formulation is mostly similar tothat in [31], which could be viewed as a variant of our method with the spatial granularity S=1 for allblocks. Instead of performing spatially adaptive inference at the \ufb01nest pixel level, our granularity Sisoptimized under the guidance of our latency prediction model (details are presented in the followingSec. 4.2) to achieve realistic speedup on target computing platforms.3.3 Latency prediction modelAs stated before, it is laborious to evaluate the latency of dynamic operators on different hardwareplatforms. To ef\ufb01ciently seek preferable granularity settings on arbitrary hardware devices, wepropose a latency prediction model G, which can directly predict the delay of executing dynamicoperators on any target devices. For a spatial-wise dynamic convolutional block, the latency predictorGtakes the hardware propertiesH, the layer parametersP, the spatial granularityS, and the activationrate ras input and predicts the latency \u2113of a dynamic convolutional block: \u2113= G(H,P,S,r ).Hardware modeling. We model a hardware device as multiple processing engines (PEs), and parallelcomputation can be executed on these PEs. As shown in Figure 3, we model the memory system as a3Here we refer to a stage as the cascading of multiple blocks which process features with the same resolution.4--- Page 5 ---Off-chip Memory(GDDR/DDR/HBM)On-chip Memory(LLC/Global Buffer)PE1PEMemoryComputation UnitsPE2PEMemoryComputation UnitsPEnPEMemoryComputation Units\u2026Figure 3: Our hardware model.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)101520253035Latency ( s)PredictedReal Figure 4: Latency prediction results.three-level structure [12]: 1) off-chip memory, 2) on-chip global memory, and 3) memory in PE. Sucha hardware model enables us to accurately predict the cost on both data movement and computation.Latency prediction. When simulating the data movement procedure, the ef\ufb01ciency of non-contiguous memory accesses under different granularity Ssettings is considered. As for the com-putation latency, it is important to adopt a proper scheduling strategy to increase the parallelism ofcomputation. Therefore, we search for the optimal scheduling (the con\ufb01guration of tiling and in-PEparallel computing) of dynamic operations to maximize the utilization of hardware resources. A moredetailed description of our latency prediction model is presented in Appendix A.Empirical validation. We take the \ufb01rst block in ResNet-101 as an example and vary the activationrate rto evaluate the performance of our prediction model. The comparison between our predictionsand the real testing latency on the Nvidia V100 GPU is illustrated in Figure 4, from which we canobserve that our predictor can accurately estimate the real latency in a wide range of activation rates.3.4 Implementation detailsWe use general optimization methods like fusing activation functions and batch normalizationlayers into convolution layers. We also optimize the speci\ufb01c operators in our spatial-wise dynamicconvolutional blocks as follows (see also Figure 2 for an overview).Fusing the masker and the \ufb01rst convolution. As mentioned in Sec. 3.1, the masker in each blockconsumes very little computation, but it takes the whole feature map as input. Therefore, it is amemory-bounded operation (the inference time is mainly spent on memory access). Since the maskerand the \ufb01rst convolution in the block share the same input, there is an opportunity to fuse these twooperations to avoid the repeated access of the input data. Note that a spatial-wise dynamic convolutionrequires the output of the masker. If we fuse the two layers, the \ufb01rst convolution will be changed to astatic operation, which may increase the inference latency. There exists a threshold of activation raterth, when r>r th, the overall latency can be reduced. We decide whether to fuse them according tothe average activation rate. See more details in Appendix B.1.Fusing the gather operation and the dynamic convolution. Traditional approaches \ufb01rst gather theinput pixels of the \ufb01rst dynamic convolution in a block (Figure 1 (b)). The gather operation is alsoa memory-bounded operation. Furthermore, when the size of the convolution kernel exceeds 1\xd71,the area of input patches may overlap, resulting in repeated memory load/store. We fuse the gatheroperation into the dynamic convolution to reduce the memory access.Fusing the scatter operation and the add operation. Traditional approaches scatter the outputpixels of the last dynamic convolution, and then execute the element-wise addition (Figure 1 (b)). Wefuse these two operators to reduce the memory access. The ablation study in Sec. 4.4 validates theeffectiveness of the proposed fusing methods.3.5 TrainingOptimization of non-differentiable maskers. The masker modules are required to produce binary-valued spatial masks for making discrete decisions, and cannot be directly optimized with backpropagation. Following [35, 31, 8], we adopt straight-through Gumbel Softmax [18, 24] to train thenetwork in an end-to-end fashion. Speci\ufb01cally, let \u02dcM \u2208RH\xd7W\xd72 denote the output of the maskgenerator. The decisions are obtained with the argmax function during inference. In the trainingphase, a differentiable approximation is de\ufb01ned by replacing the argmax operation with a Softmax:\u02c6M =exp{(log(\u02dcM:,:,0)+ G:,:,0)/\u03c4}\u22111k=0 exp{(log(\u02dcM:,:,k)+ G:,:,k)/\u03c4} \u2208 [0,1]H\xd7W , (1)5--- Page 6 ---0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1(a) Relationship between r\u2113 and rfor LAS-ResNet blocks on the Nvidia Tesla V100 GPU.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.75Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.752.00Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.4Latency ratio (r )S = 1(b) Relationship between r\u2113 and rfor LAS-RegNetY-800MF blocks on the Nvidia Jetson TX2 GPU.Figure 5: Latency prediction results of LAS-ResNet blocks on V100 (a) and LAS-RegNet blocks onTX2 (b). For both networks, we plot the relationship between the latency ratio r\u2113 and the activationrate r for the blocks in 4 stages with the convolutional stride 1. The practical ef\ufb01ciency is onlyimproved when r\u2113 <1. Note that S = 1can harm the practical latency even for a small r(reducedcomputation), while a larger Swill alleviate this problem. See detailed analysis in Sec. 4.2.where \u03c4 is the Softmax temperature. Following the common practice [ 31, 8], we let \u03c4 decreaseexponentially from 5.0 to 0.1 in training to facilitate the optimization of maskers.Training objective. The FLOPs of each spatial-wise dynamic convolutional block can be calculatedbased on our de\ufb01ned activation rate r[31]. Then we can obtain the FLOPs of the overall dynamicnetwork Fdyn. Let Fstat denotes the FLOPs of its static counterpart. We optimize their ratio toapproximate a target 0 <t< 1:LFLOPs = (FdynFstat\u2212t)2.In addition, we de\ufb01ne loss item Lbounds asin [31] to constrain the upper bound and the lower bound of activation rates in early training epochs.We further propose to leverage the static counterparts of our dynamic networks as \u201cteachers\u201d to guidethe optimization procedure. Let y and y\u2032denote the output logits of a dynamic model (\u201cstudent\u201d)and its \u201cteacher\u201d, respectively. Our \ufb01nal loss can be written asL= Ltask + \u03b1(LFLOPs + Lbounds) +\u03b2T2 \xb7KL(\u03c3(y/T)||\u03c3(y\u2032/T)), (2)where Ltask represents the task-related loss, e.g., cross-entropy loss in image classi\ufb01cation. KL(\xb7||\xb7)denotes the Kullback\u2013Leibler divergence, and \u03b1,\u03b2 are the coef\ufb01cients balancing these items. We use\u03c3to denote the log-Softmax function, and T is the temperature for computing KL-divergence.4 ExperimentsIn this section, we \ufb01rst introduce the experiment settings in Sec. 4.1. Then the latency of differentgranularity settings are analyzed in Sec. 4.2. The performance of our LASNet on ImageNet is furtherevaluated in Sec. 4.3, followed by the ablation studies in Sec. 4.4. Visualization results are illustratedin Sec. 4.5, and we \ufb01nally validate our method on the object detection task (Sec. 4.6). The results onthe instance segmentation task are presented in . For simplicity, we add \u201cLAS-\u201d as a pre\ufb01x beforemodel names to denote our LASNet, e.g., LAS-ResNet-50.4.1 Experiment settingsLatency prediction. Various types of hardware platforms are tested, including a server GPU (TeslaV100), a desktop GPU (GTX1080) and edge devices ( e.g., Nvidia Nano and Jetson TX2). Themajor properties considered by our latency prediction model include the number of processingengines (#PE), the \ufb02oating-point computation in a processing engine (#FP32), the frequency and thebandwidth. It can be observed that the server GPUs generally have a larger #PE than the IoT devices.The batch size is set as 1 for all dynamic models and computing platforms.Image classi\ufb01cation. The image classi\ufb01cation experiments are conducted on the ImageNet [ 4]dataset. Following [31], we initialize the backbone parameter from a pre-trained checkpoint4, and4We use the torchvision pre-trained models at https://pytorch.org/vision/stable/models.html.6--- Page 7 ---1 2 4 7 8 14 28Spatial granularity (S)12345Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.51.01.52.02.53.03.54.0Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.51.01.52.02.53.0Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(a) Relationship between r\u2113 and Sfor LAS-ResNet blocks on V100.1 2 4 7 8 14 28Spatial granularity (S)0.250.500.751.001.251.501.75Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.250.500.751.001.251.501.752.00Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.20.40.60.81.01.21.41.6Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(b) Relationship between r\u2113 and Sfor LAS-RegNetY-800MF blocks on Nvidia Jetson TX2 GPU.Figure 6: The relationship between the latency ratio r\u2113 and the spatial granularity S.\ufb01netune the whole network for 100 epochs with the loss function in Eq. (2). We \ufb01x \u03b1= 10,\u03b2 = 0.5and T = 4.0 for all dynamic models. More details are provided in Appendix B.4.2 Latency prediction resultsIn this subsection, we present the latency prediction results of the spatial-wise dynamic convolutionalblocks in two different models: LAS-ResNet-101 [11] (on V100) and LAS-RegNetY-800MF [25] (onTX2). All the blocks have the bottleneck structure with different channel numbers and convolutiongroups, and the RegNetY is equipped with Squeeze-and-Excitation (SE) [14] modules.We \ufb01rst de\ufb01ne \u2113dyn as the latency of a spatial-wise dynamic convolutional block, and \u2113stat as that ofa static block without a masker. Their ratio is denoted as r\u2113 = \u2113dyn\u2113stat. We investigate the relationshipbetween r\u2113 and the activation rate r(cf. Sec. 3.5) for different granularity settings. The results inFigure 5 demonstrate that: 1) even equipped with our special optimization on the scheduling strategies,pixel-level spatially adaptive inference (S=1) cannot always improve the practical ef\ufb01ciency. Such\ufb01ne-grained adaptive inference is adopted by most previous works [31, 35], and our result can explainthe reason why they can only achieve realistic speedup on less powerful CPUs [35] or specializeddevices [3]; 2) a proper granularity S >1 effectively alleviates this problem on both hardwaredevices. By setting S >1, realistic speedup could be achieved with larger activation rates.The latency prediction results are further used to seek for a preferable granularity setting for the\ufb01rst 3 stages (we \ufb01x S = 1 for the last stage, where the feature resolution is 7 \xd77). Therefore,we plot the relationship between r\u2113 and S in Figure 10. It can be observed that: 1) r\u2113 generallydecreases with Sincreasing for a given r; 2) an overly largeS(less \ufb02exible adaptive inference) bringsinsigni\ufb01cant improvement on both devices. Especially, enlarging S from 8 to 28 in the \ufb01rst stageof a LAS-ResNet brings very little improvement on V100. Based on the results in Figure 10, wecan trade off between \ufb02exibility and ef\ufb01ciency by selecting appropriate Sfor different models andhardware devices. For example, we can simply set Snet=8-4-7-15 in a LAS-ResNet-101 to achiverealistic speedup. The accuracy-latency plots in Figure 7 also validate this observation. More resultsof our latency prediction model on the desktop-level GPU, GTX1080, are presented in Appendix C.1.4.3 ImageNet classi\ufb01cation resultsWe now empirically evaluate our proposed LASNet on the ImageNet dataset. The network perfor-mance is measured in terms of the trade-off between classi\ufb01cation accuracy and inference ef\ufb01ciency.Both theoretical (i.e. FLOPs) and practical ef\ufb01ciency (i.e. latency) are tested in our experiments.5We use this form to represnet the Ssettings for the 4 stages of a network.7--- Page 8 ---2 3 4 5 6 7 8FLOPs (x1e9)73747576777879Accuracy (%)static ResNetsConvAIG-ResNet-101SkipNet-ResNet-101BAS-ResNet-50DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)15 20 25 30 35 40Latency on Nvidia Jetson TX2 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)1.5 2.0 2.5 3.0 3.5Latency on Nvidia Tesla V100 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)(a) LAS-ResNet results.0.2 0.3 0.4 0.5 0.6 0.7 0.8FLOPs (x1e9)717273747576Accuracy (%)RegNets-XRegNets-YMobileNets-v2ShuffleNets-v2CondenseNetsLAS-RegNetY-400MF (Snet=1-1-1-1)LAS-RegNetY-400MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)3 4 5 6 7 8Latency on Nvidia Jetson TX2 GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-XRegNets-YMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)8 10 12 14 16 18 20Latency on Nvidia nano GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-YRegNets-XMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)(b) LAS-RegNetY results.Figure 7: Experimental results on ImageNet. The proposed coarse-grained spatially adaptive inferenceis tested on standard ResNets (a) and lightweight RegNets (b).4.3.1 Standard baseline comparison: ResNetsWe \ufb01rst establish our LASNet based on the standard ResNets [ 11]. Speci\ufb01cally, we build LAS-ResNet-50 and LAS-ResNet-101 by plugging our maskers in the two common ResNet structures.The baselines include various types of dynamic inference approaches: 1) layer skipping (SkipNet[32] and Conv-AIG [30]); 2) channel skipping (BAS [1]); and 3) pixel-level spatial-wise dynamicnetwork (DynConv [31]). For our LASNet, we compare various settings of the spatial granularitySnet. We set training targets (cf. Sec. 3.5) t\u2208{0,4,0.5,0.6,0.7}for our dynamic models to evaluatetheir performance in different sparsity regimes. We apply the same operator fusion (Sec. 3.4) for bothour models and the compared baselines [30, 31] for fair comparison.Results are presented in Figure 7 (a). On the left we plot the relationship of accuracy v.s. FLOPs. Itcan be observed that our LAS-ResNets with different granularity settings signi\ufb01cantly outperformthe competing dynamic neural networks. Surprisingly, coarse-grained spatially adaptive inference(Snet=4-4-2-1 and Snet=8-4-7-1 for the 4 stages) can achieve even higher accuracy when consumingsimilar FLOPs on ResNets, despite the sacri\ufb01ced \ufb02exibility compared toSnet=1-1-1-1. We conjecturethat a larger Sis also bene\ufb01cial to the optimization of maskers.We compare the practical latency of three granularity settings in Figure 7 (a) predicted by our latencyprediction model (middle on TX2 and right on V100). We can witness that although they achievecomparable theoretical ef\ufb01ciency (Figure 7 (a) left), larger Sis more hardware-friendly comparedto the \ufb01nest granularity. For example, the inference latency of LAS-ResNet-101 ( Snet=1-1-1-1)is signi\ufb01cantly higher than the ResNet-101 baseline on V100 (Figure 7 (a) right), even though itstheoretical computation is much smaller than that of the static model. However, larger granularities(Snet=4-4-2-1 and Snet=8-4-7-1) can effectively improve the inference latency due to its lower burdenon the memory access. Remarkably, the latency of ResNet-101 could be reduced by 36% and 46% onV100 and TX2 respectively without sacri\ufb01cing the accuracy when t=0.4. The classi\ufb01cation accuracyis increased by 1.9% with similar inference ef\ufb01ciency. It can be observed that the realistic speedupratio r\u2113 is more close to the theoretical FLOPs ratio target ton the less powerful TX2, because thelatency is computation-bounded (i.e. the latency is mainly spent on computation) on such IoT devices.In contrast, there is a larger gap between practical and theoretical ef\ufb01ciency on the more powerfulV100, as the latency is bounded by the memory access cost.4.3.2 Lightweight baseline comparison: RegNetsWe further evaluate our LASNet in lightweight CNN architectures,i.e. RegNets-Y [25]. Two differentsized models are tested: RegNetY-400MF and RegNetY-800MF. Compared baselines include othertypes of ef\ufb01cient models, e.g., MobileNets-v2 [28], Shuf\ufb02etNets-v2 [23] and CondenseNets [16].The results are presented in Figure 7 (b). The x-axis for the three sub-\ufb01gures are the FLOPs, thelatency on TX2, and the latency on the Nvidia nano GPU, respectively. We can observe that our8--- Page 9 ---1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2Latency on Nvidia Tesla V100 GPU (ms)76.577.077.578.078.579.0Accuracy (%) static ResNetsLAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=4-4-7-1)LAS-ResNet-101 (Snet=8-4-7-1)Figure 8: Ablation studies on S. Figure 9: Visualization results.method outperforms various types of static models in terms of the trade-off between accuracy andef\ufb01ciency. More results on image classi\ufb01cation are provided in Appendix C.2.4.4 Ablation studiesWe conduct ablation studies to validate the effectiveness of our coarse-grained spatially adaptiveinference (Sec. 3.2) and operator fusion operations (Sec. 3.4).More granularities settings. We test various granularity settings on LAS-ResNet-101 to examinethe effects of Sin different stages. The results on the Tesla-V100 GPU are presented in Figure 8. Itcan be found that the \ufb01nest granularity (Snet=1-1-1-1) leads to substantial inef\ufb01ciency despite thereduced FLOPs (cf. Figure 7 (a) left). Coarse-grained spatially adaptive inference in the \ufb01rst twostages (Snet=4-4-1-1) effectively reduces the inference latency. We further increase Sin the thirdstage to 2 and 7, and this procedure consistently improves the realistic ef\ufb01ciency on the V100 GPU.This trend also holds on the GTX 1080 GPU (see the results in Appendix C.2.Table 1: Ablation studies on operator fusion.Masker- Gather- Scatter- LatencyConv1x1 Conv3x3 Add (\xb5s)   163.2   90.1   86.7   71.4Operator fusion. We investigate the effect of ouroperator fusion introduced in Sec. 3.4. One convolu-tional block in the \ufb01rst stage of a LAS-ResNet-101(S=4, r=0.6) is tested. The results in Table 1 vali-date that every step of operator fusion bene\ufb01ts thepractical latency of a block, as the overhead onmemory access is effectively reduced. Especially,the fusion of the masker operation and the \ufb01rst con-volution is crucial to reducing the latency.4.5 VisualizationWe visualize the masks generated by our masker in the third block of a LAS-ResNet-101 (Snet=4-4-2-1) in Figure 12. The brilliant areas correspond to the locations of 1 elements in a mask, andthe computation on the dimmed regions is skipped by our dynamic model. We can witness that themasker accurately locate the most task-related regions (even the tiny aircraft at the corner), whichhelps reduce the unnecessary computation on background areas. These resultsalso suggest that forthe \ufb01rst stage, the granularity S=4 is suf\ufb01ciently \ufb02exible to recognize the important regions, anda win-win can be achieved between accuracy and ef\ufb01ciency. Interestingly, the masker could selectsome objects that are not labeled for the sample, e.g., the \ufb02ower beside the hummingbird and thehuman holding the camera. This indicates that our spatial-wise dynamic networks can automaticallyrecognize the regions with semantics, and their capability is not limited by the classi\ufb01cation labels.This property is helpful in some downstream tasks, such as object detection (Sec. 4.6) and instancesegmentation (Sec. 4.7), which require detecting multiple classes and objects in an image. Morevisualization results could be found in Appendix C.3.4.6 COCO Object detectionWe further evaluate our LASNet on the COCO [22] object detection task. The mean average precision(mAP), the average backbone FLOPs, and the average backbone latency on the validation set areused to measure the network performance. We test two commonly used detection frameworks: FasterR-CNN [27] with Feature Pyramid Network [20] and RetinaNet [21]. Thanks to the generality ofour method, we can conveniently replace the backbones with ours pre-trained on ImageNet, andthe whole models are \ufb01netuned on COCO with the standard setting for 12 epochs (see detailed9--- Page 10 ---Table 2: Object detection results on the COCO dataset.Detection Backbone Backbone Backbone Latency (ms)mAP (%)Framework FLOPs (G) V100 GTX1080 TX2Faster R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 39.4LAS-ResNet-101 (Snet=4-4-2-1,t=0.6) 90.7 33.8 90.6 524.9 40.3LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 79.3 30.7 82.5 477.1 39.8LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.5 29.0 79.9 464.8 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 67.9 25.3 69.1 401.3 39.5RetinaNetResNet-101 (Baseline) 141.2 39.5 119.3 729.4 38.5LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 77.8 30.4 81.8 472.6 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.4 28.9 79.9 464.8 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 66.4 25.3 69.1 401.3 38.9Table 3: Instance Segmentation results on the COCO dataset.Segmentation Backbone Backbone Backbone Latency (ms)APmask(%) APbox(%)Framework FLOPs (G) V100 GTX1080 TX2Mask R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 36.1 40.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 80.5 31.1 83.3 481.9 37.0 41.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.4) 69.2 27.9 74.8 431.6 36.1 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 68.8 25.8 70.9 411.8 36.2 40.0setup in Appendix B.3). The input images are resized to a short side of 800 and a long side notexceeding 1333. The results of our LAS-ResNet-101 with different Snet settings are presented inTable 2. We can observe from the results that when setting the training target as 0.4, the latency ofour LAS-ResNet-101 with Snet=4-4-7-1 is signi\ufb01cantly lower than the static baseline on all deviceswithout sacri\ufb01cing mAP in both detection frameworks. With a larger training targets, our LASNetcan increase the mAP by 0.9% and 0.8% in Faster R-CNN and RetinaNet respectively, while stillbeing faster than the baseline method.4.7 COCO instance segmentationWe also present the results of instance segmentation on COCO, which demonstrate the effectivenessof our LASNet on the dense prediction task. From the results in Table 3, we can observe that whensetting the training target as 0.4, the Mask R-CNN [ 10] models (Snet=4-4-2-1 and Snet=4-4-7-1)runs faster on all tested hardware devices without sacri\ufb01cing the performance. With a training targetof 0.5, the APmask and APbox of the Mask R-CNN model could be increased by 0.9% and 1.0%respectively while still running faster than the baseline method.5 ConclusionIn this paper, we propose to build latency-aware spatial-wise dynamic networks (LASNet) under theguidance of a latency prediction model. By simultaneously considering the algorithm, the schedulingstrategy and the hardware properties, we can ef\ufb01ciently estimate the practical latency of spatial-wise dynamic operators on arbitrary computing platforms. Based on the empirical analysis on therelationship between the latency and the granularity of spatially adaptive inference, we optimizeboth the algorithm and the scheduling strategies to achieve realistic speedup on many multi-coreprocessors, e.g., the Tesla V100 GPU and the Jetson TX2 GPU. Experiments on image classi\ufb01cation,object detection and instance segmentation tasks validate that the proposed method signi\ufb01cantlyimproves the practical ef\ufb01ciency of deep CNNs, and outperforms various competing approaches.AcknowledgementThis work is supported in part by the National Key R&D Program of China under Grant2020AAA0105200, the National Natural Science Foundation of China under Grants 62022048and the Tsinghua University-China Mobile Communications Group Co.,Ltd. Joint Institute..10--- Page 11 ---References[1] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping for learningconditional channel gated networks. In ICLR, 2020.[2] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, MeghanCowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. Tvm: An automated end-to-end optimizingcompiler for deep learning. In OSDI, 2018.[3] Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian Verhelst. Proces-sor architecture optimization for spatially dynamic neural networks. In VLSI-SoC, 2021.[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In CVPR, 2009.[5] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicatednetwork with less inference complexity. In CVPR, 2017.[6] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov,and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In CVPR,2017.[7] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamicneural networks: A survey. TPAMI, 2021.[8] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, and Haojun Jiang. Spatiallyadaptive feature re\ufb01nement for ef\ufb01cient inference. TIP, 2021.[9] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfen Cao, Wenhui Huang,Chao Deng, and Gao Huang. Learning to weight samples for dynamic early-exiting networks.In ECCV, 2022.[10] Kaiming He, Georgia Gkioxari, Piotr Doll\xe1r, and Ross Girshick. Mask r-cnn. In Proceedings ofthe IEEE international conference on computer vision, pages 2961\u20132969, 2017.[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In CVPR, 2016.[12] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach .Elsevier, 2011.[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, TobiasWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ef\ufb01cient convolutional neuralnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.[15] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Wein-berger. Multi-scale dense networks for resource ef\ufb01cient image classi\ufb01cation. In ICLR, 2018.[16] Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: Anef\ufb01cient densenet using learned group convolutions. In CVPR, 2018.[17] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Weinberger.Densely connected convolutional networks. TPAMI, 2019.[18] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.In ICLR, 2017.[19] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In NeurIPS, 2017.[20] Tsung-Yi Lin, Piotr Doll\xe1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.Feature pyramid networks for object detection. In CVPR, 2017.[21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Focal loss for denseobject detection. In ICCV, 2017.11--- Page 12 ---[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,Piotr Doll\xe1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,2014.[23] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shuf\ufb02enet v2: Practical guidelinesfor ef\ufb01cient cnn architecture design. In ECCV, 2018.[24] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuousrelaxation of discrete random variables. In ICLR, 2017.[25] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Design-ing network design spaces. In CVPR, 2020.[26] Mengye Ren, Andrei Pokrovsky, Bin Yang, and Raquel Urtasun. SBNet: Sparse Blocks Networkfor Fast Inference. CVPR, 2018.[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-timeobject detection with region proposal networks. In NeurIPS, 2015.[28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.[29] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, andQuoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820\u20132828,2019.[30] Andreas Veit and Serge Belongie. Convolutional networks with adaptive inference graphs. InECCV, 2018.[31] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: Exploiting Spatial Sparsity forFaster Inference. In CVPR, 2020.[32] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learningdynamic routing in convolutional networks. In ECCV, 2018.[33] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus:a dynamic approach to reducing spatial redundancy in image classi\ufb01cation. In NeurIPS, 2020.[34] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, YuandongTian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware ef\ufb01cient convnetdesign via differentiable neural architecture search. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 10734\u201310742, 2019.[35] Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptiveinference with stochastic feature sampling and interpolation. In ECCV, 2020.[36] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptivenetworks for ef\ufb01cient inference. In CVPR, 2020.[37] Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan, and Yunhe Wang.Dynamic resolution network. NeurIPS, 2021.12--- Page 13 ---AppendixA Latency prediction model.As the dynamic operators in our method have not been supported by current deep learning libraries,we propose a latency prediction model to ef\ufb01ciently estimate the real latency of these operators onhardware device. The inputs of the latency prediction model include: 1) the structural con\ufb01gurationof a convolutional block, 2) its activation rate rwhich decides the computation amount, 3) the spatialgranularity S, and 4) the hardware properties mentioned in Table 4. The latency of a dynamic blockis predicted as follows.Input/output shape de\ufb01nition. The \ufb01rst step of predicting the latency of an operation is to calculatethe shape of input and output. Taking the gather-conv2 operation as an example, the input of thisoperation is the activation with the shape of Cin \xd7H \xd7W, where Cin is the number of inputchannels, and H and W are the resolution of the feature map. The shape of the output tensor isP \xd7Cout \xd7S\xd7S, where P is the number of output patches, Cout is the number of output channelsand Sis the spatial granularity. Note that P is obtained based on the output of our maskers.Operation-to-hardware mapping. Next, we map the operations to hardware. As is mentioned in thepaper, we model a hardware device as multiple processing engines (PEs). We assign the computationof each element in the output feature map to a PE. Speci\ufb01cally, we consecutively split the outputfeature map into multiple tiles. The shape of each tile is TP \xd7TC \xd7TS1 \xd7TS2. These split tilesare assigned to multiple PEs. The computation of the elements in each tile is executed in a PE. Wecan con\ufb01gure different shapes of tiles. In order to determine the optimal shape of the tile, we makea search space of different tile shapes. The tile shape has 4 dimensions. The candidates of eachdimension are power-of-2 and do not exceed the corresponding dimension of the feature map.Latency estimation. Then, we evaluate the latency of each tile shape in the search space and selectthe optimal tile shape with the lowest latency. The latency includes the data movement latency andthe computation latency: \u2113= \u2113data + \u2113computation.1) Data movement latency \u2113data. The estimation of the latency for data movement requires us tomodel the memory system of a hardware device. We model the memory system of hardware as athree-level architecture [12]: off-chip memory, on-chip global memory, and local memory in PE.The input data and weight data are \ufb01rst transferred from the off-chip memory to the on-chip globalmemory. We assume the hardware can make full use of the off-chip memory bandwidth to simplifythe latency prediction model.After that, the data used to compute the output tiles is transferred from on-chip global memory tothe local m, Your response: ```ADD_PAPER\n2210.06223v1\nThis paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach that balances computation allocation with practical latency considerations through a novel latency prediction model. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method outperforms pixel-level dynamic networks by optimizing spatial granularity and scheduling strategies, demonstrating significant improvements in real-world deployment scenarios. The latency predictor considers algorithm design, scheduling, and hardware properties to guide network optimization, making it particularly effective for resource-constrained edge devices.\n```\n~~~~~~~~~~\nCurrent Step #9, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2210.06223v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2210.06223v1\nThis paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach that balances computation allocation with practical latency considerations through a novel latency prediction model. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method outperforms pixel-level dynamic networks by optimizing spatial granularity and scheduling strategies, demonstrating significant improvements in real-world deployment scenarios. The latency predictor considers algorithm design, scheduling, and hardware properties to guide network optimization, making it particularly effective for resource-constrained edge devices.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 11:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #8, Phase: literature review, Feedback: 3\n--- Page 1 ---Latency-aware Spatial-wise Dynamic NetworksYizeng Han1\u2217Zhihang Yuan2\u2217 Yifan Pu1\u2217 Chenhao Xue2Shiji Song1 Guangyu Sun2 Gao Huang1\u20201 Department of Automation, BNRist, Tsinghua University, Beijing, China2 School of Electronics Engineering and Computer Science, Peking University, Beijing, China{hanyz18, pyf20}@mails.tsinghua.edu.cn, {shijis, gaohuang}@tsinghua.edu.cn{yuanzhihang, xch927027, gsun}@pku.edu.cnAbstractSpatial-wise dynamic convolution has become a promising approach to improvingthe inference ef\ufb01ciency of deep networks. By allocating more computation to themost informative pixels, such an adaptive inference paradigm reduces the spatialredundancy in image features and saves a considerable amount of unnecessarycomputation. However, the theoretical ef\ufb01ciency achieved by previous methodscan hardly translate into a realistic speedup, especially on the multi-core processors(e.g. GPUs). The key challenge is that the existing literature has only focusedon designing algorithms with minimal computation, ignoring the fact that thepractical latency can also be in\ufb02uenced by scheduling strategies and hardwareproperties. To bridge the gap between theoretical computation and practical ef-\ufb01ciency, we propose a latency-aware spatial-wise dynamic network (LASNet),which performs coarse-grained spatially adaptive inference under the guidanceof a novel latency prediction model. The latency prediction model can ef\ufb01cientlyestimate the inference latency of dynamic networks by simultaneously consideringalgorithms, scheduling strategies, and hardware properties. We use the latencypredictor to guide both the algorithm design and the scheduling optimization onvarious hardware platforms. Experiments on image classi\ufb01cation, object detectionand instance segmentation demonstrate that the proposed framework signi\ufb01cantlyimproves the practical inference ef\ufb01ciency of deep networks. For example, theaverage latency of a ResNet-101 on the ImageNet validation set could be reducedby 36% and 46% on a server GPU (Nvidia Tesla-V100) and an edge device (NvidiaJetson TX2 GPU) respectively without sacri\ufb01cing the accuracy. Code is availableat https://github.com/LeapLabTHU/LASNet.1 IntroductionDynamic neural networks [7] have attracted great research interests in recent years. Compared tostatic models [11, 17, 13, 23] which treat different inputs equally during inference, dynamic networkscan allocate the computation in a data-dependent manner. For example, they can conditionally skipthe computation of network layers [ 15, 9, 32, 30] or convolutional channels [ 19, 1], or performspatially adaptive inference on the most informative image regions ( e.g. the foreground areas)[6, 5, 31, 35, 33, 8]. Spatial-wise dynamic networks, which typically decide whether to computeeach feature pixel with masker modules [5, 31, 35, 8] (Figure 1 (a)), have shown promising results inimproving the inference ef\ufb01ciency of convolution neural networks (CNNs).Despite the remarkable theoretical ef\ufb01ciency achieved by spatial-wise dynamic networks [5, 31, 35],researchers have found it challenging to translate the theoretical results into realistic speedup,\u2217Equal contribution.\u2020Corresponding author.36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.06223v1  [cs.CV]  12 Oct 2022--- Page 2 ---Maskergather ...convolution(s) scatter...Dynamic Convolution(s) AlgorithmSchedulingHardwareLatency(a) Algorithm: Spatially adaptive inference with a masker.(b) Scheduling: the practical inference pipeline of dynamic convolutions.(c) The general idea of our work.in\ufb02uence guidanceselected pixel locationsFigure 1: An overview of our method. (a) illustrates the spatially adaptive inference algorithm; (b) isthe scheduling strategy; and (c) presents the three key factors to the practical latency. For a givenhardware, the latency is used to guide our algorithm design and scheduling optimization.especially on some multi-core processors, e.g., GPUs [35, 3, 8]. The challenges are two-fold: 1) mostprevious approaches [5, 31, 35] perform spatially adaptive inference at the \ufb01nest granularity: everypixel is \ufb02exibly decided whether to be computed or not. Such \ufb02exibility induces non-contiguousmemory access [35] and requires specialized scheduling strategies (Figure 1 (b)); 2) the existingliterature has only adopted the hardware-agnostic FLOPs (\ufb02oating-point operations) as an inaccurateproxy for the ef\ufb01ciency, lacking latency-aware guidance on the algorithm design. For dynamicnetworks, the adaptive computation with sub-optimal scheduling strategies further enlarges thediscrepancy between the theoretical FLOPs and the practical latency. Note that it has been validatedby previous works that the latency on CPUs has a strong correlation with FLOPs [8, 35]. Therefore,we mainly focus on the GPU platform in this paper, which is more challenging and less explored.We address the above challenges by proposing a latency-aware spatial-wise dynamic network(LASNet). Three key factors to the inference latency are considered: the algorithm, the schedulingstrategy, and the hardware properties. Given a target hardware device, we directly use the latency,rather than the FLOPs, to guide our algorithm design and scheduling optimization (Figure 1 (c)).Because the memory access pattern and the scheduling strategies in our dynamic operators differfrom those in static networks, the libraries developed for static models (e.g. cuDNN) are sub-optimalfor the acceleration of dynamic models. Without the support of libraries, each dynamic operatorrequires scheduling optimization, code optimization, compiling, and deployment for each device.Therefore, it is laborious to evaluate the network latency on different hardware platforms. To this end,we propose a novel latency prediction model to ef\ufb01ciently estimate the realistic latency of a networkby simultaneously considering the aforementioned three factors. Compared to the hardware-agnosticFLOPs, our predicted latency can better re\ufb02ect the practical ef\ufb01ciency of dynamic models.Guided by this latency prediction model, we establish our latency-aware spatial-wise dynamic network(LASNet), which adaptively decides whether to allocate computation on feature patches insteadof pixels [5, 31, 35] (Figure 2 top). We name this paradigm as spatially adaptive inference at acoarse granularity. While less \ufb02exible than the pixel-level adaptive computation in previous works[5, 31, 35], it facilitates more contiguous memory access, bene\ufb01ting the realistic speedup on hardware.The scheduling strategy and the implementation are further ameliorated for faster inference.It is worth noting that LASNet is designed as a general framework in two aspects: 1) the coarse-grained spatially adaptive inference paradigm can be conveniently implemented in various CNNbackbones, e.g., ResNets [11], DenseNets [17] and RegNets [25]; and 2) the latency predictor is anoff-the-shell tool which can be directly used for various computing platforms (e.g. server GPUs andedge devices).We evaluate the performance of LASNet on multiple CNN architectures on image classi\ufb01cation,object detection, and instance segmentation tasks. Experiment results show that our LASNet improvesthe ef\ufb01ciency of deep CNNs both theoretically and practically. For example, the inference latencyof ResNet-101 on ImageNet [ 4] is reduced by 36% and 46% on an Nvidia Tesla V100 GPU andan Nvidia Jetson TX2 GPU, respectively, without sacri\ufb01cing the accuracy. Moreover, the proposedmethod outperforms various lightweight networks in a low-FLOPs regime.Our main contributions are summarized as follows:2--- Page 3 ---1. We propose LASNet, which performs coarse-grained spatially adaptive inference guidedby the practical latency instead of the theoretical FLOPs. To the best of our knowledge,LASNet is the \ufb01rst framework that directly considers the real latency in the design phase ofdynamic neural networks;2. We propose a latency prediction model, which can ef\ufb01ciently and accurately estimate thelatency of dynamic operators by simultaneously considering the algorithm, the schedulingstrategy, and the hardware properties;3. Experiments on image classi\ufb01cation and downstream tasks verify that our proposed LAS-Net can effectively improve the practical ef\ufb01ciency of different CNN architectures.2 Related worksSpatial-wise dynamic network is a common type of dynamic neural networks [ 7]. Compared tostatic models which treat different feature locations evenly during inference, these networks performspatially adaptive inference on the most informative regions ( e.g., foregrounds), and reduce theunnecessary computation on less important areas (e.g., backgrounds). Existing works mainly includethree levels of dynamic computation: resolution level [ 36, 37], region level [ 33] and pixel level[5, 31, 35]. The former two generally manipulate the network inputs [ 33, 37] or require specialarchitecture design [36]. In contrast, pixel-level dynamic networks can \ufb02exibly skip the convolutionson certain feature pixels in arbitrary CNN backbones [5, 31, 35]. Despite its remarkable theoreticalef\ufb01ciency, pixel-wise dynamic computation brings considerable dif\ufb01culty to achieving realisticspeedup on multi-core processors, e.g., GPUs. Compared to the previous approaches [ 5, 31, 35]which only focus on reducing the theoretical computation, we propose to directly use the latency toguide our algorithm design and scheduling optimization.Hardware-aware network design. To bridge the gap between theoretical and practical ef\ufb01ciencyof deep models, researchers have started to consider the real latency in the network design phase.There are two lines of works in this direction. One directly performs speed tests on targeteddevices, and summarizes some guidelines to facilitate hand-designing lightweight models [23]. Theother line of work searches for fast models using the neural architecture search (NAS) technique[29, 34]. However, all existing works try to build static models, which have intrinsic computationalredundancy by treating different inputs in the same way. However, speed tests for dynamic operatorson different hardware devices can be very laborious and impractical. In contrast, our proposed latencyprediction model can ef\ufb01ciently estimate the inference latency on any given computing platforms bysimultaneously considering algorithm design, scheduling strategies and hardware properties.3 MethodologyIn this section, we \ufb01rst introduce the preliminaries of spatially adaptive inference, and then demon-strate the architecture design of our LASNet. The latency prediction model is then explained, whichguides the granularity settings and the scheduling optimization for LASNet. We further present theimplementation improvements for faster inference, followed by the training strategies.3.1 PreliminariesSpatially adaptive inference. The existing spatial-wise dynamic networks are usually establishedby attaching a masker Min each convolutional block of a CNN backbone (Figure 1 (a)). Speci\ufb01cally,let x\u2208RH\xd7W\xd7C denote the input of a block, where H and W are the feature height and width, andC is the channel number. The masker Mtakes x as input, and generates a binary-valued spatialmask M = M(x) \u2208{0,1}H\xd7W . Each element of M determines whether to perform convolutionoperations on the corresponding location of the output feature. The unselected regions will be \ufb01lledwith the values from the input [5, 31] or obtained via interpolation [35]. We de\ufb01ne the activation rateof a block as r=\u2211i,j Mi,jH\xd7W , representing the ratio of the calculated pixels.Scheduling strategy. During inference, the current scheduling strategy for spatial-wise dynamicconvolutions generally involve three steps [26] (Figure 1 (b)): 1) gathering, which re-organizes theselected pixels (if the convolution kernel size is greater than 1 \xd71, the neighbors are also required)along the batch dimension; 2) computation, which performs convolution on the gathered input; and3) scattering, which \ufb01lls the computed pixels on their corresponding locations of the output feature.3--- Page 4 ---MaskerMasker-Conv1x1 Conv1x1 ...Scatter-Add Conv1x1 Conv3x3 Gumbel NoiseT raining Inference Input OutputUpsample Element-wise  multiplication Selected pixelGather-Conv3x3 Conv1x1 Regular convolution Operator fusionInput OutputFigure 2: Our proposed LASNet block. Top: we \ufb01rst generate a low-resolution spatial mask Mcoarse,which is then upsampled to obtain the mask M with the same size as the output feature. GumbelSoftmax [18, 24] is used for end-to-end training (Sec. 3.5). Bottom: the scheduling optimization isperformed to decrease the memory access for faster inference (Sec. 3.4).Limitations. Compared to performing convolutions on the entire feature map, the aforementionedscheduling strategy reduces the computation while bringing considerable overhead to memory accessdue to the mask generation and the non-contiguous memory access. Such overhead would increasethe overall latency, especially when the granularity of dynamic convolution is at the \ufb01nest pixel level.3.2 Architecture designSpatial granularity. As mentioned above, pixel-level dynamic convolutions [5, 31, 35] raise sub-stantial challenges to achieving realistic speedup on multi-core processors due to the non-contiguousmemory access. To this end, we propose to optimize the granularity of spatially adaptive inference.Speci\ufb01cally, take the commonly used bottleneck structure in [11] as an example, our coarse-grainedspatial-wise dynamic convolutional block is illustrated in Figure 2. Instead of directly producinga mask with the shape of H\xd7W, we \ufb01rst generate a low-resolution mask Mcoarse \u2208{0,1}HS \xd7WS ,where Sis named as the spatial granularity. Each element in Mcoarse determines the computationof an S\xd7S-sized feature patch. For instance, the feature size in the \ufb01rst ResNet stage3 is 56 \xd756.Then the possible choices for Sare {1,2,4,7,8,14,28,56}.The mask Mcoarse is then upsampledto the size of H\xd7W. Notably, S = 1means that the granularity is still at the pixel level as previousmethods [5, 31, 35]. Note that the other extreme situation (S = 56) is not considered in this paper,when the masker directly determines whether to skip the whole block (i.e. layer skipping [30, 32]).Such an overly aggressive approach will lead to considerable drop of accuracy, as we presented inAppendix C.2. The masker is composed of a pooling layer followed by a 1 \xd71 convolution.Differences to existing works. Without using the interpolation operation [ 35] or the carefullydesigned two-branch structure [8], the proposed block architecture is simple and suf\ufb01ciently generalto be plugged into most backbones with minimal modi\ufb01cation. Our formulation is mostly similar tothat in [31], which could be viewed as a variant of our method with the spatial granularity S=1 for allblocks. Instead of performing spatially adaptive inference at the \ufb01nest pixel level, our granularity Sisoptimized under the guidance of our latency prediction model (details are presented in the followingSec. 4.2) to achieve realistic speedup on target computing platforms.3.3 Latency prediction modelAs stated before, it is laborious to evaluate the latency of dynamic operators on different hardwareplatforms. To ef\ufb01ciently seek preferable granularity settings on arbitrary hardware devices, wepropose a latency prediction model G, which can directly predict the delay of executing dynamicoperators on any target devices. For a spatial-wise dynamic convolutional block, the latency predictorGtakes the hardware propertiesH, the layer parametersP, the spatial granularityS, and the activationrate ras input and predicts the latency \u2113of a dynamic convolutional block: \u2113= G(H,P,S,r ).Hardware modeling. We model a hardware device as multiple processing engines (PEs), and parallelcomputation can be executed on these PEs. As shown in Figure 3, we model the memory system as a3Here we refer to a stage as the cascading of multiple blocks which process features with the same resolution.4--- Page 5 ---Off-chip Memory(GDDR/DDR/HBM)On-chip Memory(LLC/Global Buffer)PE1PEMemoryComputation UnitsPE2PEMemoryComputation UnitsPEnPEMemoryComputation Units\u2026Figure 3: Our hardware model.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)101520253035Latency ( s)PredictedReal Figure 4: Latency prediction results.three-level structure [12]: 1) off-chip memory, 2) on-chip global memory, and 3) memory in PE. Sucha hardware model enables us to accurately predict the cost on both data movement and computation.Latency prediction. When simulating the data movement procedure, the ef\ufb01ciency of non-contiguous memory accesses under different granularity Ssettings is considered. As for the com-putation latency, it is important to adopt a proper scheduling strategy to increase the parallelism ofcomputation. Therefore, we search for the optimal scheduling (the con\ufb01guration of tiling and in-PEparallel computing) of dynamic operations to maximize the utilization of hardware resources. A moredetailed description of our latency prediction model is presented in Appendix A.Empirical validation. We take the \ufb01rst block in ResNet-101 as an example and vary the activationrate rto evaluate the performance of our prediction model. The comparison between our predictionsand the real testing latency on the Nvidia V100 GPU is illustrated in Figure 4, from which we canobserve that our predictor can accurately estimate the real latency in a wide range of activation rates.3.4 Implementation detailsWe use general optimization methods like fusing activation functions and batch normalizationlayers into convolution layers. We also optimize the speci\ufb01c operators in our spatial-wise dynamicconvolutional blocks as follows (see also Figure 2 for an overview).Fusing the masker and the \ufb01rst convolution. As mentioned in Sec. 3.1, the masker in each blockconsumes very little computation, but it takes the whole feature map as input. Therefore, it is amemory-bounded operation (the inference time is mainly spent on memory access). Since the maskerand the \ufb01rst convolution in the block share the same input, there is an opportunity to fuse these twooperations to avoid the repeated access of the input data. Note that a spatial-wise dynamic convolutionrequires the output of the masker. If we fuse the two layers, the \ufb01rst convolution will be changed to astatic operation, which may increase the inference latency. There exists a threshold of activation raterth, when r>r th, the overall latency can be reduced. We decide whether to fuse them according tothe average activation rate. See more details in Appendix B.1.Fusing the gather operation and the dynamic convolution. Traditional approaches \ufb01rst gather theinput pixels of the \ufb01rst dynamic convolution in a block (Figure 1 (b)). The gather operation is alsoa memory-bounded operation. Furthermore, when the size of the convolution kernel exceeds 1\xd71,the area of input patches may overlap, resulting in repeated memory load/store. We fuse the gatheroperation into the dynamic convolution to reduce the memory access.Fusing the scatter operation and the add operation. Traditional approaches scatter the outputpixels of the last dynamic convolution, and then execute the element-wise addition (Figure 1 (b)). Wefuse these two operators to reduce the memory access. The ablation study in Sec. 4.4 validates theeffectiveness of the proposed fusing methods.3.5 TrainingOptimization of non-differentiable maskers. The masker modules are required to produce binary-valued spatial masks for making discrete decisions, and cannot be directly optimized with backpropagation. Following [35, 31, 8], we adopt straight-through Gumbel Softmax [18, 24] to train thenetwork in an end-to-end fashion. Speci\ufb01cally, let \u02dcM \u2208RH\xd7W\xd72 denote the output of the maskgenerator. The decisions are obtained with the argmax function during inference. In the trainingphase, a differentiable approximation is de\ufb01ned by replacing the argmax operation with a Softmax:\u02c6M =exp{(log(\u02dcM:,:,0)+ G:,:,0)/\u03c4}\u22111k=0 exp{(log(\u02dcM:,:,k)+ G:,:,k)/\u03c4} \u2208 [0,1]H\xd7W , (1)5--- Page 6 ---0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1(a) Relationship between r\u2113 and rfor LAS-ResNet blocks on the Nvidia Tesla V100 GPU.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.75Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.752.00Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.4Latency ratio (r )S = 1(b) Relationship between r\u2113 and rfor LAS-RegNetY-800MF blocks on the Nvidia Jetson TX2 GPU.Figure 5: Latency prediction results of LAS-ResNet blocks on V100 (a) and LAS-RegNet blocks onTX2 (b). For both networks, we plot the relationship between the latency ratio r\u2113 and the activationrate r for the blocks in 4 stages with the convolutional stride 1. The practical ef\ufb01ciency is onlyimproved when r\u2113 <1. Note that S = 1can harm the practical latency even for a small r(reducedcomputation), while a larger Swill alleviate this problem. See detailed analysis in Sec. 4.2.where \u03c4 is the Softmax temperature. Following the common practice [ 31, 8], we let \u03c4 decreaseexponentially from 5.0 to 0.1 in training to facilitate the optimization of maskers.Training objective. The FLOPs of each spatial-wise dynamic convolutional block can be calculatedbased on our de\ufb01ned activation rate r[31]. Then we can obtain the FLOPs of the overall dynamicnetwork Fdyn. Let Fstat denotes the FLOPs of its static counterpart. We optimize their ratio toapproximate a target 0 <t< 1:LFLOPs = (FdynFstat\u2212t)2.In addition, we de\ufb01ne loss item Lbounds asin [31] to constrain the upper bound and the lower bound of activation rates in early training epochs.We further propose to leverage the static counterparts of our dynamic networks as \u201cteachers\u201d to guidethe optimization procedure. Let y and y\u2032denote the output logits of a dynamic model (\u201cstudent\u201d)and its \u201cteacher\u201d, respectively. Our \ufb01nal loss can be written asL= Ltask + \u03b1(LFLOPs + Lbounds) +\u03b2T2 \xb7KL(\u03c3(y/T)||\u03c3(y\u2032/T)), (2)where Ltask represents the task-related loss, e.g., cross-entropy loss in image classi\ufb01cation. KL(\xb7||\xb7)denotes the Kullback\u2013Leibler divergence, and \u03b1,\u03b2 are the coef\ufb01cients balancing these items. We use\u03c3to denote the log-Softmax function, and T is the temperature for computing KL-divergence.4 ExperimentsIn this section, we \ufb01rst introduce the experiment settings in Sec. 4.1. Then the latency of differentgranularity settings are analyzed in Sec. 4.2. The performance of our LASNet on ImageNet is furtherevaluated in Sec. 4.3, followed by the ablation studies in Sec. 4.4. Visualization results are illustratedin Sec. 4.5, and we \ufb01nally validate our method on the object detection task (Sec. 4.6). The results onthe instance segmentation task are presented in . For simplicity, we add \u201cLAS-\u201d as a pre\ufb01x beforemodel names to denote our LASNet, e.g., LAS-ResNet-50.4.1 Experiment settingsLatency prediction. Various types of hardware platforms are tested, including a server GPU (TeslaV100), a desktop GPU (GTX1080) and edge devices ( e.g., Nvidia Nano and Jetson TX2). Themajor properties considered by our latency prediction model include the number of processingengines (#PE), the \ufb02oating-point computation in a processing engine (#FP32), the frequency and thebandwidth. It can be observed that the server GPUs generally have a larger #PE than the IoT devices.The batch size is set as 1 for all dynamic models and computing platforms.Image classi\ufb01cation. The image classi\ufb01cation experiments are conducted on the ImageNet [ 4]dataset. Following [31], we initialize the backbone parameter from a pre-trained checkpoint4, and4We use the torchvision pre-trained models at https://pytorch.org/vision/stable/models.html.6--- Page 7 ---1 2 4 7 8 14 28Spatial granularity (S)12345Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.51.01.52.02.53.03.54.0Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.51.01.52.02.53.0Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(a) Relationship between r\u2113 and Sfor LAS-ResNet blocks on V100.1 2 4 7 8 14 28Spatial granularity (S)0.250.500.751.001.251.501.75Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.250.500.751.001.251.501.752.00Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.20.40.60.81.01.21.41.6Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(b) Relationship between r\u2113 and Sfor LAS-RegNetY-800MF blocks on Nvidia Jetson TX2 GPU.Figure 6: The relationship between the latency ratio r\u2113 and the spatial granularity S.\ufb01netune the whole network for 100 epochs with the loss function in Eq. (2). We \ufb01x \u03b1= 10,\u03b2 = 0.5and T = 4.0 for all dynamic models. More details are provided in Appendix B.4.2 Latency prediction resultsIn this subsection, we present the latency prediction results of the spatial-wise dynamic convolutionalblocks in two different models: LAS-ResNet-101 [11] (on V100) and LAS-RegNetY-800MF [25] (onTX2). All the blocks have the bottleneck structure with different channel numbers and convolutiongroups, and the RegNetY is equipped with Squeeze-and-Excitation (SE) [14] modules.We \ufb01rst de\ufb01ne \u2113dyn as the latency of a spatial-wise dynamic convolutional block, and \u2113stat as that ofa static block without a masker. Their ratio is denoted as r\u2113 = \u2113dyn\u2113stat. We investigate the relationshipbetween r\u2113 and the activation rate r(cf. Sec. 3.5) for different granularity settings. The results inFigure 5 demonstrate that: 1) even equipped with our special optimization on the scheduling strategies,pixel-level spatially adaptive inference (S=1) cannot always improve the practical ef\ufb01ciency. Such\ufb01ne-grained adaptive inference is adopted by most previous works [31, 35], and our result can explainthe reason why they can only achieve realistic speedup on less powerful CPUs [35] or specializeddevices [3]; 2) a proper granularity S >1 effectively alleviates this problem on both hardwaredevices. By setting S >1, realistic speedup could be achieved with larger activation rates.The latency prediction results are further used to seek for a preferable granularity setting for the\ufb01rst 3 stages (we \ufb01x S = 1 for the last stage, where the feature resolution is 7 \xd77). Therefore,we plot the relationship between r\u2113 and S in Figure 10. It can be observed that: 1) r\u2113 generallydecreases with Sincreasing for a given r; 2) an overly largeS(less \ufb02exible adaptive inference) bringsinsigni\ufb01cant improvement on both devices. Especially, enlarging S from 8 to 28 in the \ufb01rst stageof a LAS-ResNet brings very little improvement on V100. Based on the results in Figure 10, wecan trade off between \ufb02exibility and ef\ufb01ciency by selecting appropriate Sfor different models andhardware devices. For example, we can simply set Snet=8-4-7-15 in a LAS-ResNet-101 to achiverealistic speedup. The accuracy-latency plots in Figure 7 also validate this observation. More resultsof our latency prediction model on the desktop-level GPU, GTX1080, are presented in Appendix C.1.4.3 ImageNet classi\ufb01cation resultsWe now empirically evaluate our proposed LASNet on the ImageNet dataset. The network perfor-mance is measured in terms of the trade-off between classi\ufb01cation accuracy and inference ef\ufb01ciency.Both theoretical (i.e. FLOPs) and practical ef\ufb01ciency (i.e. latency) are tested in our experiments.5We use this form to represnet the Ssettings for the 4 stages of a network.7--- Page 8 ---2 3 4 5 6 7 8FLOPs (x1e9)73747576777879Accuracy (%)static ResNetsConvAIG-ResNet-101SkipNet-ResNet-101BAS-ResNet-50DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)15 20 25 30 35 40Latency on Nvidia Jetson TX2 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)1.5 2.0 2.5 3.0 3.5Latency on Nvidia Tesla V100 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)(a) LAS-ResNet results.0.2 0.3 0.4 0.5 0.6 0.7 0.8FLOPs (x1e9)717273747576Accuracy (%)RegNets-XRegNets-YMobileNets-v2ShuffleNets-v2CondenseNetsLAS-RegNetY-400MF (Snet=1-1-1-1)LAS-RegNetY-400MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)3 4 5 6 7 8Latency on Nvidia Jetson TX2 GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-XRegNets-YMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)8 10 12 14 16 18 20Latency on Nvidia nano GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-YRegNets-XMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)(b) LAS-RegNetY results.Figure 7: Experimental results on ImageNet. The proposed coarse-grained spatially adaptive inferenceis tested on standard ResNets (a) and lightweight RegNets (b).4.3.1 Standard baseline comparison: ResNetsWe \ufb01rst establish our LASNet based on the standard ResNets [ 11]. Speci\ufb01cally, we build LAS-ResNet-50 and LAS-ResNet-101 by plugging our maskers in the two common ResNet structures.The baselines include various types of dynamic inference approaches: 1) layer skipping (SkipNet[32] and Conv-AIG [30]); 2) channel skipping (BAS [1]); and 3) pixel-level spatial-wise dynamicnetwork (DynConv [31]). For our LASNet, we compare various settings of the spatial granularitySnet. We set training targets (cf. Sec. 3.5) t\u2208{0,4,0.5,0.6,0.7}for our dynamic models to evaluatetheir performance in different sparsity regimes. We apply the same operator fusion (Sec. 3.4) for bothour models and the compared baselines [30, 31] for fair comparison.Results are presented in Figure 7 (a). On the left we plot the relationship of accuracy v.s. FLOPs. Itcan be observed that our LAS-ResNets with different granularity settings signi\ufb01cantly outperformthe competing dynamic neural networks. Surprisingly, coarse-grained spatially adaptive inference(Snet=4-4-2-1 and Snet=8-4-7-1 for the 4 stages) can achieve even higher accuracy when consumingsimilar FLOPs on ResNets, despite the sacri\ufb01ced \ufb02exibility compared toSnet=1-1-1-1. We conjecturethat a larger Sis also bene\ufb01cial to the optimization of maskers.We compare the practical latency of three granularity settings in Figure 7 (a) predicted by our latencyprediction model (middle on TX2 and right on V100). We can witness that although they achievecomparable theoretical ef\ufb01ciency (Figure 7 (a) left), larger Sis more hardware-friendly comparedto the \ufb01nest granularity. For example, the inference latency of LAS-ResNet-101 ( Snet=1-1-1-1)is signi\ufb01cantly higher than the ResNet-101 baseline on V100 (Figure 7 (a) right), even though itstheoretical computation is much smaller than that of the static model. However, larger granularities(Snet=4-4-2-1 and Snet=8-4-7-1) can effectively improve the inference latency due to its lower burdenon the memory access. Remarkably, the latency of ResNet-101 could be reduced by 36% and 46% onV100 and TX2 respectively without sacri\ufb01cing the accuracy when t=0.4. The classi\ufb01cation accuracyis increased by 1.9% with similar inference ef\ufb01ciency. It can be observed that the realistic speedupratio r\u2113 is more close to the theoretical FLOPs ratio target ton the less powerful TX2, because thelatency is computation-bounded (i.e. the latency is mainly spent on computation) on such IoT devices.In contrast, there is a larger gap between practical and theoretical ef\ufb01ciency on the more powerfulV100, as the latency is bounded by the memory access cost.4.3.2 Lightweight baseline comparison: RegNetsWe further evaluate our LASNet in lightweight CNN architectures,i.e. RegNets-Y [25]. Two differentsized models are tested: RegNetY-400MF and RegNetY-800MF. Compared baselines include othertypes of ef\ufb01cient models, e.g., MobileNets-v2 [28], Shuf\ufb02etNets-v2 [23] and CondenseNets [16].The results are presented in Figure 7 (b). The x-axis for the three sub-\ufb01gures are the FLOPs, thelatency on TX2, and the latency on the Nvidia nano GPU, respectively. We can observe that our8--- Page 9 ---1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2Latency on Nvidia Tesla V100 GPU (ms)76.577.077.578.078.579.0Accuracy (%) static ResNetsLAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=4-4-7-1)LAS-ResNet-101 (Snet=8-4-7-1)Figure 8: Ablation studies on S. Figure 9: Visualization results.method outperforms various types of static models in terms of the trade-off between accuracy andef\ufb01ciency. More results on image classi\ufb01cation are provided in Appendix C.2.4.4 Ablation studiesWe conduct ablation studies to validate the effectiveness of our coarse-grained spatially adaptiveinference (Sec. 3.2) and operator fusion operations (Sec. 3.4).More granularities settings. We test various granularity settings on LAS-ResNet-101 to examinethe effects of Sin different stages. The results on the Tesla-V100 GPU are presented in Figure 8. Itcan be found that the \ufb01nest granularity (Snet=1-1-1-1) leads to substantial inef\ufb01ciency despite thereduced FLOPs (cf. Figure 7 (a) left). Coarse-grained spatially adaptive inference in the \ufb01rst twostages (Snet=4-4-1-1) effectively reduces the inference latency. We further increase Sin the thirdstage to 2 and 7, and this procedure consistently improves the realistic ef\ufb01ciency on the V100 GPU.This trend also holds on the GTX 1080 GPU (see the results in Appendix C.2.Table 1: Ablation studies on operator fusion.Masker- Gather- Scatter- LatencyConv1x1 Conv3x3 Add (\xb5s)   163.2   90.1   86.7   71.4Operator fusion. We investigate the effect of ouroperator fusion introduced in Sec. 3.4. One convolu-tional block in the \ufb01rst stage of a LAS-ResNet-101(S=4, r=0.6) is tested. The results in Table 1 vali-date that every step of operator fusion bene\ufb01ts thepractical latency of a block, as the overhead onmemory access is effectively reduced. Especially,the fusion of the masker operation and the \ufb01rst con-volution is crucial to reducing the latency.4.5 VisualizationWe visualize the masks generated by our masker in the third block of a LAS-ResNet-101 (Snet=4-4-2-1) in Figure 12. The brilliant areas correspond to the locations of 1 elements in a mask, andthe computation on the dimmed regions is skipped by our dynamic model. We can witness that themasker accurately locate the most task-related regions (even the tiny aircraft at the corner), whichhelps reduce the unnecessary computation on background areas. These resultsalso suggest that forthe \ufb01rst stage, the granularity S=4 is suf\ufb01ciently \ufb02exible to recognize the important regions, anda win-win can be achieved between accuracy and ef\ufb01ciency. Interestingly, the masker could selectsome objects that are not labeled for the sample, e.g., the \ufb02ower beside the hummingbird and thehuman holding the camera. This indicates that our spatial-wise dynamic networks can automaticallyrecognize the regions with semantics, and their capability is not limited by the classi\ufb01cation labels.This property is helpful in some downstream tasks, such as object detection (Sec. 4.6) and instancesegmentation (Sec. 4.7), which require detecting multiple classes and objects in an image. Morevisualization results could be found in Appendix C.3.4.6 COCO Object detectionWe further evaluate our LASNet on the COCO [22] object detection task. The mean average precision(mAP), the average backbone FLOPs, and the average backbone latency on the validation set areused to measure the network performance. We test two commonly used detection frameworks: FasterR-CNN [27] with Feature Pyramid Network [20] and RetinaNet [21]. Thanks to the generality ofour method, we can conveniently replace the backbones with ours pre-trained on ImageNet, andthe whole models are \ufb01netuned on COCO with the standard setting for 12 epochs (see detailed9--- Page 10 ---Table 2: Object detection results on the COCO dataset.Detection Backbone Backbone Backbone Latency (ms)mAP (%)Framework FLOPs (G) V100 GTX1080 TX2Faster R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 39.4LAS-ResNet-101 (Snet=4-4-2-1,t=0.6) 90.7 33.8 90.6 524.9 40.3LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 79.3 30.7 82.5 477.1 39.8LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.5 29.0 79.9 464.8 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 67.9 25.3 69.1 401.3 39.5RetinaNetResNet-101 (Baseline) 141.2 39.5 119.3 729.4 38.5LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 77.8 30.4 81.8 472.6 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.4 28.9 79.9 464.8 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 66.4 25.3 69.1 401.3 38.9Table 3: Instance Segmentation results on the COCO dataset.Segmentation Backbone Backbone Backbone Latency (ms)APmask(%) APbox(%)Framework FLOPs (G) V100 GTX1080 TX2Mask R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 36.1 40.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 80.5 31.1 83.3 481.9 37.0 41.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.4) 69.2 27.9 74.8 431.6 36.1 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 68.8 25.8 70.9 411.8 36.2 40.0setup in Appendix B.3). The input images are resized to a short side of 800 and a long side notexceeding 1333. The results of our LAS-ResNet-101 with different Snet settings are presented inTable 2. We can observe from the results that when setting the training target as 0.4, the latency ofour LAS-ResNet-101 with Snet=4-4-7-1 is signi\ufb01cantly lower than the static baseline on all deviceswithout sacri\ufb01cing mAP in both detection frameworks. With a larger training targets, our LASNetcan increase the mAP by 0.9% and 0.8% in Faster R-CNN and RetinaNet respectively, while stillbeing faster than the baseline method.4.7 COCO instance segmentationWe also present the results of instance segmentation on COCO, which demonstrate the effectivenessof our LASNet on the dense prediction task. From the results in Table 3, we can observe that whensetting the training target as 0.4, the Mask R-CNN [ 10] models (Snet=4-4-2-1 and Snet=4-4-7-1)runs faster on all tested hardware devices without sacri\ufb01cing the performance. With a training targetof 0.5, the APmask and APbox of the Mask R-CNN model could be increased by 0.9% and 1.0%respectively while still running faster than the baseline method.5 ConclusionIn this paper, we propose to build latency-aware spatial-wise dynamic networks (LASNet) under theguidance of a latency prediction model. By simultaneously considering the algorithm, the schedulingstrategy and the hardware properties, we can ef\ufb01ciently estimate the practical latency of spatial-wise dynamic operators on arbitrary computing platforms. Based on the empirical analysis on therelationship between the latency and the granularity of spatially adaptive inference, we optimizeboth the algorithm and the scheduling strategies to achieve realistic speedup on many multi-coreprocessors, e.g., the Tesla V100 GPU and the Jetson TX2 GPU. Experiments on image classi\ufb01cation,object detection and instance segmentation tasks validate that the proposed method signi\ufb01cantlyimproves the practical ef\ufb01ciency of deep CNNs, and outperforms various competing approaches.AcknowledgementThis work is supported in part by the National Key R&D Program of China under Grant2020AAA0105200, the National Natural Science Foundation of China under Grants 62022048and the Tsinghua University-China Mobile Communications Group Co.,Ltd. Joint Institute..10--- Page 11 ---References[1] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping for learningconditional channel gated networks. In ICLR, 2020.[2] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, MeghanCowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. Tvm: An automated end-to-end optimizingcompiler for deep learning. In OSDI, 2018.[3] Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian Verhelst. Proces-sor architecture optimization for spatially dynamic neural networks. In VLSI-SoC, 2021.[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In CVPR, 2009.[5] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicatednetwork with less inference complexity. In CVPR, 2017.[6] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov,and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In CVPR,2017.[7] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamicneural networks: A survey. TPAMI, 2021.[8] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, and Haojun Jiang. Spatiallyadaptive feature re\ufb01nement for ef\ufb01cient inference. TIP, 2021.[9] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfen Cao, Wenhui Huang,Chao Deng, and Gao Huang. Learning to weight samples for dynamic early-exiting networks.In ECCV, 2022.[10] Kaiming He, Georgia Gkioxari, Piotr Doll\xe1r, and Ross Girshick. Mask r-cnn. In Proceedings ofthe IEEE international conference on computer vision, pages 2961\u20132969, 2017.[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In CVPR, 2016.[12] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach .Elsevier, 2011.[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, TobiasWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ef\ufb01cient convolutional neuralnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.[15] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Wein-berger. Multi-scale dense networks for resource ef\ufb01cient image classi\ufb01cation. In ICLR, 2018.[16] Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: Anef\ufb01cient densenet using learned group convolutions. In CVPR, 2018.[17] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Weinberger.Densely connected convolutional networks. TPAMI, 2019.[18] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.In ICLR, 2017.[19] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In NeurIPS, 2017.[20] Tsung-Yi Lin, Piotr Doll\xe1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.Feature pyramid networks for object detection. In CVPR, 2017.[21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Focal loss for denseobject detection. In ICCV, 2017.11--- Page 12 ---[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,Piotr Doll\xe1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,2014.[23] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shuf\ufb02enet v2: Practical guidelinesfor ef\ufb01cient cnn architecture design. In ECCV, 2018.[24] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuousrelaxation of discrete random variables. In ICLR, 2017.[25] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Design-ing network design spaces. In CVPR, 2020.[26] Mengye Ren, Andrei Pokrovsky, Bin Yang, and Raquel Urtasun. SBNet: Sparse Blocks Networkfor Fast Inference. CVPR, 2018.[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-timeobject detection with region proposal networks. In NeurIPS, 2015.[28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.[29] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, andQuoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820\u20132828,2019.[30] Andreas Veit and Serge Belongie. Convolutional networks with adaptive inference graphs. InECCV, 2018.[31] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: Exploiting Spatial Sparsity forFaster Inference. In CVPR, 2020.[32] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learningdynamic routing in convolutional networks. In ECCV, 2018.[33] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus:a dynamic approach to reducing spatial redundancy in image classi\ufb01cation. In NeurIPS, 2020.[34] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, YuandongTian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware ef\ufb01cient convnetdesign via differentiable neural architecture search. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 10734\u201310742, 2019.[35] Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptiveinference with stochastic feature sampling and interpolation. In ECCV, 2020.[36] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptivenetworks for ef\ufb01cient inference. In CVPR, 2020.[37] Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan, and Yunhe Wang.Dynamic resolution network. NeurIPS, 2021.12--- Page 13 ---AppendixA Latency prediction model.As the dynamic operators in our method have not been supported by current deep learning libraries,we propose a latency prediction model to ef\ufb01ciently estimate the real latency of these operators onhardware device. The inputs of the latency prediction model include: 1) the structural con\ufb01gurationof a convolutional block, 2) its activation rate rwhich decides the computation amount, 3) the spatialgranularity S, and 4) the hardware properties mentioned in Table 4. The latency of a dynamic blockis predicted as follows.Input/output shape de\ufb01nition. The \ufb01rst step of predicting the latency of an operation is to calculatethe shape of input and output. Taking the gather-conv2 operation as an example, the input of thisoperation is the activation with the shape of Cin \xd7H \xd7W, where Cin is the number of inputchannels, and H and W are the resolution of the feature map. The shape of the output tensor isP \xd7Cout \xd7S\xd7S, where P is the number of output patches, Cout is the number of output channelsand Sis the spatial granularity. Note that P is obtained based on the output of our maskers.Operation-to-hardware mapping. Next, we map the operations to hardware. As is mentioned in thepaper, we model a hardware device as multiple processing engines (PEs). We assign the computationof each element in the output feature map to a PE. Speci\ufb01cally, we consecutively split the outputfeature map into multiple tiles. The shape of each tile is TP \xd7TC \xd7TS1 \xd7TS2. These split tilesare assigned to multiple PEs. The computation of the elements in each tile is executed in a PE. Wecan con\ufb01gure different shapes of tiles. In order to determine the optimal shape of the tile, we makea search space of different tile shapes. The tile shape has 4 dimensions. The candidates of eachdimension are power-of-2 and do not exceed the corresponding dimension of the feature map.Latency estimation. Then, we evaluate the latency of each tile shape in the search space and selectthe optimal tile shape with the lowest latency. The latency includes the data movement latency andthe computation latency: \u2113= \u2113data + \u2113computation.1) Data movement latency \u2113data. The estimation of the latency for data movement requires us tomodel the memory system of a hardware device. We model the memory system of hardware as athree-level architecture [12]: off-chip memory, on-chip global memory, and local memory in PE.The input data and weight data are \ufb01rst transferred from the off-chip memory to the on-chip globalmemory. We assume the hardware can make full use of the off-chip memory bandwidth to simplifythe latency prediction model.After that, the data used to compute the output tiles is transferred from on-chip global memory tothe local m, Your response: ```ADD_PAPER\n2210.06223v1\nThis paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach that balances computation allocation with practical latency considerations through a novel latency prediction model. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method outperforms pixel-level dynamic networks by optimizing spatial granularity and scheduling strategies, demonstrating significant improvements in real-world deployment scenarios. The latency predictor considers algorithm design, scheduling, and hardware properties to guide network optimization, making it particularly effective for resource-constrained edge devices.\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\n~~~~~~~~~~\nCurrent Step #10, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification energy efficiency optimization\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 12:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #8, Phase: literature review, Feedback: 3\n--- Page 1 ---Latency-aware Spatial-wise Dynamic NetworksYizeng Han1\u2217Zhihang Yuan2\u2217 Yifan Pu1\u2217 Chenhao Xue2Shiji Song1 Guangyu Sun2 Gao Huang1\u20201 Department of Automation, BNRist, Tsinghua University, Beijing, China2 School of Electronics Engineering and Computer Science, Peking University, Beijing, China{hanyz18, pyf20}@mails.tsinghua.edu.cn, {shijis, gaohuang}@tsinghua.edu.cn{yuanzhihang, xch927027, gsun}@pku.edu.cnAbstractSpatial-wise dynamic convolution has become a promising approach to improvingthe inference ef\ufb01ciency of deep networks. By allocating more computation to themost informative pixels, such an adaptive inference paradigm reduces the spatialredundancy in image features and saves a considerable amount of unnecessarycomputation. However, the theoretical ef\ufb01ciency achieved by previous methodscan hardly translate into a realistic speedup, especially on the multi-core processors(e.g. GPUs). The key challenge is that the existing literature has only focusedon designing algorithms with minimal computation, ignoring the fact that thepractical latency can also be in\ufb02uenced by scheduling strategies and hardwareproperties. To bridge the gap between theoretical computation and practical ef-\ufb01ciency, we propose a latency-aware spatial-wise dynamic network (LASNet),which performs coarse-grained spatially adaptive inference under the guidanceof a novel latency prediction model. The latency prediction model can ef\ufb01cientlyestimate the inference latency of dynamic networks by simultaneously consideringalgorithms, scheduling strategies, and hardware properties. We use the latencypredictor to guide both the algorithm design and the scheduling optimization onvarious hardware platforms. Experiments on image classi\ufb01cation, object detectionand instance segmentation demonstrate that the proposed framework signi\ufb01cantlyimproves the practical inference ef\ufb01ciency of deep networks. For example, theaverage latency of a ResNet-101 on the ImageNet validation set could be reducedby 36% and 46% on a server GPU (Nvidia Tesla-V100) and an edge device (NvidiaJetson TX2 GPU) respectively without sacri\ufb01cing the accuracy. Code is availableat https://github.com/LeapLabTHU/LASNet.1 IntroductionDynamic neural networks [7] have attracted great research interests in recent years. Compared tostatic models [11, 17, 13, 23] which treat different inputs equally during inference, dynamic networkscan allocate the computation in a data-dependent manner. For example, they can conditionally skipthe computation of network layers [ 15, 9, 32, 30] or convolutional channels [ 19, 1], or performspatially adaptive inference on the most informative image regions ( e.g. the foreground areas)[6, 5, 31, 35, 33, 8]. Spatial-wise dynamic networks, which typically decide whether to computeeach feature pixel with masker modules [5, 31, 35, 8] (Figure 1 (a)), have shown promising results inimproving the inference ef\ufb01ciency of convolution neural networks (CNNs).Despite the remarkable theoretical ef\ufb01ciency achieved by spatial-wise dynamic networks [5, 31, 35],researchers have found it challenging to translate the theoretical results into realistic speedup,\u2217Equal contribution.\u2020Corresponding author.36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.06223v1  [cs.CV]  12 Oct 2022--- Page 2 ---Maskergather ...convolution(s) scatter...Dynamic Convolution(s) AlgorithmSchedulingHardwareLatency(a) Algorithm: Spatially adaptive inference with a masker.(b) Scheduling: the practical inference pipeline of dynamic convolutions.(c) The general idea of our work.in\ufb02uence guidanceselected pixel locationsFigure 1: An overview of our method. (a) illustrates the spatially adaptive inference algorithm; (b) isthe scheduling strategy; and (c) presents the three key factors to the practical latency. For a givenhardware, the latency is used to guide our algorithm design and scheduling optimization.especially on some multi-core processors, e.g., GPUs [35, 3, 8]. The challenges are two-fold: 1) mostprevious approaches [5, 31, 35] perform spatially adaptive inference at the \ufb01nest granularity: everypixel is \ufb02exibly decided whether to be computed or not. Such \ufb02exibility induces non-contiguousmemory access [35] and requires specialized scheduling strategies (Figure 1 (b)); 2) the existingliterature has only adopted the hardware-agnostic FLOPs (\ufb02oating-point operations) as an inaccurateproxy for the ef\ufb01ciency, lacking latency-aware guidance on the algorithm design. For dynamicnetworks, the adaptive computation with sub-optimal scheduling strategies further enlarges thediscrepancy between the theoretical FLOPs and the practical latency. Note that it has been validatedby previous works that the latency on CPUs has a strong correlation with FLOPs [8, 35]. Therefore,we mainly focus on the GPU platform in this paper, which is more challenging and less explored.We address the above challenges by proposing a latency-aware spatial-wise dynamic network(LASNet). Three key factors to the inference latency are considered: the algorithm, the schedulingstrategy, and the hardware properties. Given a target hardware device, we directly use the latency,rather than the FLOPs, to guide our algorithm design and scheduling optimization (Figure 1 (c)).Because the memory access pattern and the scheduling strategies in our dynamic operators differfrom those in static networks, the libraries developed for static models (e.g. cuDNN) are sub-optimalfor the acceleration of dynamic models. Without the support of libraries, each dynamic operatorrequires scheduling optimization, code optimization, compiling, and deployment for each device.Therefore, it is laborious to evaluate the network latency on different hardware platforms. To this end,we propose a novel latency prediction model to ef\ufb01ciently estimate the realistic latency of a networkby simultaneously considering the aforementioned three factors. Compared to the hardware-agnosticFLOPs, our predicted latency can better re\ufb02ect the practical ef\ufb01ciency of dynamic models.Guided by this latency prediction model, we establish our latency-aware spatial-wise dynamic network(LASNet), which adaptively decides whether to allocate computation on feature patches insteadof pixels [5, 31, 35] (Figure 2 top). We name this paradigm as spatially adaptive inference at acoarse granularity. While less \ufb02exible than the pixel-level adaptive computation in previous works[5, 31, 35], it facilitates more contiguous memory access, bene\ufb01ting the realistic speedup on hardware.The scheduling strategy and the implementation are further ameliorated for faster inference.It is worth noting that LASNet is designed as a general framework in two aspects: 1) the coarse-grained spatially adaptive inference paradigm can be conveniently implemented in various CNNbackbones, e.g., ResNets [11], DenseNets [17] and RegNets [25]; and 2) the latency predictor is anoff-the-shell tool which can be directly used for various computing platforms (e.g. server GPUs andedge devices).We evaluate the performance of LASNet on multiple CNN architectures on image classi\ufb01cation,object detection, and instance segmentation tasks. Experiment results show that our LASNet improvesthe ef\ufb01ciency of deep CNNs both theoretically and practically. For example, the inference latencyof ResNet-101 on ImageNet [ 4] is reduced by 36% and 46% on an Nvidia Tesla V100 GPU andan Nvidia Jetson TX2 GPU, respectively, without sacri\ufb01cing the accuracy. Moreover, the proposedmethod outperforms various lightweight networks in a low-FLOPs regime.Our main contributions are summarized as follows:2--- Page 3 ---1. We propose LASNet, which performs coarse-grained spatially adaptive inference guidedby the practical latency instead of the theoretical FLOPs. To the best of our knowledge,LASNet is the \ufb01rst framework that directly considers the real latency in the design phase ofdynamic neural networks;2. We propose a latency prediction model, which can ef\ufb01ciently and accurately estimate thelatency of dynamic operators by simultaneously considering the algorithm, the schedulingstrategy, and the hardware properties;3. Experiments on image classi\ufb01cation and downstream tasks verify that our proposed LAS-Net can effectively improve the practical ef\ufb01ciency of different CNN architectures.2 Related worksSpatial-wise dynamic network is a common type of dynamic neural networks [ 7]. Compared tostatic models which treat different feature locations evenly during inference, these networks performspatially adaptive inference on the most informative regions ( e.g., foregrounds), and reduce theunnecessary computation on less important areas (e.g., backgrounds). Existing works mainly includethree levels of dynamic computation: resolution level [ 36, 37], region level [ 33] and pixel level[5, 31, 35]. The former two generally manipulate the network inputs [ 33, 37] or require specialarchitecture design [36]. In contrast, pixel-level dynamic networks can \ufb02exibly skip the convolutionson certain feature pixels in arbitrary CNN backbones [5, 31, 35]. Despite its remarkable theoreticalef\ufb01ciency, pixel-wise dynamic computation brings considerable dif\ufb01culty to achieving realisticspeedup on multi-core processors, e.g., GPUs. Compared to the previous approaches [ 5, 31, 35]which only focus on reducing the theoretical computation, we propose to directly use the latency toguide our algorithm design and scheduling optimization.Hardware-aware network design. To bridge the gap between theoretical and practical ef\ufb01ciencyof deep models, researchers have started to consider the real latency in the network design phase.There are two lines of works in this direction. One directly performs speed tests on targeteddevices, and summarizes some guidelines to facilitate hand-designing lightweight models [23]. Theother line of work searches for fast models using the neural architecture search (NAS) technique[29, 34]. However, all existing works try to build static models, which have intrinsic computationalredundancy by treating different inputs in the same way. However, speed tests for dynamic operatorson different hardware devices can be very laborious and impractical. In contrast, our proposed latencyprediction model can ef\ufb01ciently estimate the inference latency on any given computing platforms bysimultaneously considering algorithm design, scheduling strategies and hardware properties.3 MethodologyIn this section, we \ufb01rst introduce the preliminaries of spatially adaptive inference, and then demon-strate the architecture design of our LASNet. The latency prediction model is then explained, whichguides the granularity settings and the scheduling optimization for LASNet. We further present theimplementation improvements for faster inference, followed by the training strategies.3.1 PreliminariesSpatially adaptive inference. The existing spatial-wise dynamic networks are usually establishedby attaching a masker Min each convolutional block of a CNN backbone (Figure 1 (a)). Speci\ufb01cally,let x\u2208RH\xd7W\xd7C denote the input of a block, where H and W are the feature height and width, andC is the channel number. The masker Mtakes x as input, and generates a binary-valued spatialmask M = M(x) \u2208{0,1}H\xd7W . Each element of M determines whether to perform convolutionoperations on the corresponding location of the output feature. The unselected regions will be \ufb01lledwith the values from the input [5, 31] or obtained via interpolation [35]. We de\ufb01ne the activation rateof a block as r=\u2211i,j Mi,jH\xd7W , representing the ratio of the calculated pixels.Scheduling strategy. During inference, the current scheduling strategy for spatial-wise dynamicconvolutions generally involve three steps [26] (Figure 1 (b)): 1) gathering, which re-organizes theselected pixels (if the convolution kernel size is greater than 1 \xd71, the neighbors are also required)along the batch dimension; 2) computation, which performs convolution on the gathered input; and3) scattering, which \ufb01lls the computed pixels on their corresponding locations of the output feature.3--- Page 4 ---MaskerMasker-Conv1x1 Conv1x1 ...Scatter-Add Conv1x1 Conv3x3 Gumbel NoiseT raining Inference Input OutputUpsample Element-wise  multiplication Selected pixelGather-Conv3x3 Conv1x1 Regular convolution Operator fusionInput OutputFigure 2: Our proposed LASNet block. Top: we \ufb01rst generate a low-resolution spatial mask Mcoarse,which is then upsampled to obtain the mask M with the same size as the output feature. GumbelSoftmax [18, 24] is used for end-to-end training (Sec. 3.5). Bottom: the scheduling optimization isperformed to decrease the memory access for faster inference (Sec. 3.4).Limitations. Compared to performing convolutions on the entire feature map, the aforementionedscheduling strategy reduces the computation while bringing considerable overhead to memory accessdue to the mask generation and the non-contiguous memory access. Such overhead would increasethe overall latency, especially when the granularity of dynamic convolution is at the \ufb01nest pixel level.3.2 Architecture designSpatial granularity. As mentioned above, pixel-level dynamic convolutions [5, 31, 35] raise sub-stantial challenges to achieving realistic speedup on multi-core processors due to the non-contiguousmemory access. To this end, we propose to optimize the granularity of spatially adaptive inference.Speci\ufb01cally, take the commonly used bottleneck structure in [11] as an example, our coarse-grainedspatial-wise dynamic convolutional block is illustrated in Figure 2. Instead of directly producinga mask with the shape of H\xd7W, we \ufb01rst generate a low-resolution mask Mcoarse \u2208{0,1}HS \xd7WS ,where Sis named as the spatial granularity. Each element in Mcoarse determines the computationof an S\xd7S-sized feature patch. For instance, the feature size in the \ufb01rst ResNet stage3 is 56 \xd756.Then the possible choices for Sare {1,2,4,7,8,14,28,56}.The mask Mcoarse is then upsampledto the size of H\xd7W. Notably, S = 1means that the granularity is still at the pixel level as previousmethods [5, 31, 35]. Note that the other extreme situation (S = 56) is not considered in this paper,when the masker directly determines whether to skip the whole block (i.e. layer skipping [30, 32]).Such an overly aggressive approach will lead to considerable drop of accuracy, as we presented inAppendix C.2. The masker is composed of a pooling layer followed by a 1 \xd71 convolution.Differences to existing works. Without using the interpolation operation [ 35] or the carefullydesigned two-branch structure [8], the proposed block architecture is simple and suf\ufb01ciently generalto be plugged into most backbones with minimal modi\ufb01cation. Our formulation is mostly similar tothat in [31], which could be viewed as a variant of our method with the spatial granularity S=1 for allblocks. Instead of performing spatially adaptive inference at the \ufb01nest pixel level, our granularity Sisoptimized under the guidance of our latency prediction model (details are presented in the followingSec. 4.2) to achieve realistic speedup on target computing platforms.3.3 Latency prediction modelAs stated before, it is laborious to evaluate the latency of dynamic operators on different hardwareplatforms. To ef\ufb01ciently seek preferable granularity settings on arbitrary hardware devices, wepropose a latency prediction model G, which can directly predict the delay of executing dynamicoperators on any target devices. For a spatial-wise dynamic convolutional block, the latency predictorGtakes the hardware propertiesH, the layer parametersP, the spatial granularityS, and the activationrate ras input and predicts the latency \u2113of a dynamic convolutional block: \u2113= G(H,P,S,r ).Hardware modeling. We model a hardware device as multiple processing engines (PEs), and parallelcomputation can be executed on these PEs. As shown in Figure 3, we model the memory system as a3Here we refer to a stage as the cascading of multiple blocks which process features with the same resolution.4--- Page 5 ---Off-chip Memory(GDDR/DDR/HBM)On-chip Memory(LLC/Global Buffer)PE1PEMemoryComputation UnitsPE2PEMemoryComputation UnitsPEnPEMemoryComputation Units\u2026Figure 3: Our hardware model.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)101520253035Latency ( s)PredictedReal Figure 4: Latency prediction results.three-level structure [12]: 1) off-chip memory, 2) on-chip global memory, and 3) memory in PE. Sucha hardware model enables us to accurately predict the cost on both data movement and computation.Latency prediction. When simulating the data movement procedure, the ef\ufb01ciency of non-contiguous memory accesses under different granularity Ssettings is considered. As for the com-putation latency, it is important to adopt a proper scheduling strategy to increase the parallelism ofcomputation. Therefore, we search for the optimal scheduling (the con\ufb01guration of tiling and in-PEparallel computing) of dynamic operations to maximize the utilization of hardware resources. A moredetailed description of our latency prediction model is presented in Appendix A.Empirical validation. We take the \ufb01rst block in ResNet-101 as an example and vary the activationrate rto evaluate the performance of our prediction model. The comparison between our predictionsand the real testing latency on the Nvidia V100 GPU is illustrated in Figure 4, from which we canobserve that our predictor can accurately estimate the real latency in a wide range of activation rates.3.4 Implementation detailsWe use general optimization methods like fusing activation functions and batch normalizationlayers into convolution layers. We also optimize the speci\ufb01c operators in our spatial-wise dynamicconvolutional blocks as follows (see also Figure 2 for an overview).Fusing the masker and the \ufb01rst convolution. As mentioned in Sec. 3.1, the masker in each blockconsumes very little computation, but it takes the whole feature map as input. Therefore, it is amemory-bounded operation (the inference time is mainly spent on memory access). Since the maskerand the \ufb01rst convolution in the block share the same input, there is an opportunity to fuse these twooperations to avoid the repeated access of the input data. Note that a spatial-wise dynamic convolutionrequires the output of the masker. If we fuse the two layers, the \ufb01rst convolution will be changed to astatic operation, which may increase the inference latency. There exists a threshold of activation raterth, when r>r th, the overall latency can be reduced. We decide whether to fuse them according tothe average activation rate. See more details in Appendix B.1.Fusing the gather operation and the dynamic convolution. Traditional approaches \ufb01rst gather theinput pixels of the \ufb01rst dynamic convolution in a block (Figure 1 (b)). The gather operation is alsoa memory-bounded operation. Furthermore, when the size of the convolution kernel exceeds 1\xd71,the area of input patches may overlap, resulting in repeated memory load/store. We fuse the gatheroperation into the dynamic convolution to reduce the memory access.Fusing the scatter operation and the add operation. Traditional approaches scatter the outputpixels of the last dynamic convolution, and then execute the element-wise addition (Figure 1 (b)). Wefuse these two operators to reduce the memory access. The ablation study in Sec. 4.4 validates theeffectiveness of the proposed fusing methods.3.5 TrainingOptimization of non-differentiable maskers. The masker modules are required to produce binary-valued spatial masks for making discrete decisions, and cannot be directly optimized with backpropagation. Following [35, 31, 8], we adopt straight-through Gumbel Softmax [18, 24] to train thenetwork in an end-to-end fashion. Speci\ufb01cally, let \u02dcM \u2208RH\xd7W\xd72 denote the output of the maskgenerator. The decisions are obtained with the argmax function during inference. In the trainingphase, a differentiable approximation is de\ufb01ned by replacing the argmax operation with a Softmax:\u02c6M =exp{(log(\u02dcM:,:,0)+ G:,:,0)/\u03c4}\u22111k=0 exp{(log(\u02dcM:,:,k)+ G:,:,k)/\u03c4} \u2208 [0,1]H\xd7W , (1)5--- Page 6 ---0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.00.20.40.60.81.01.21.41.6Latency ratio (r )S = 1(a) Relationship between r\u2113 and rfor LAS-ResNet blocks on the Nvidia Tesla V100 GPU.0.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.75Latency ratio (r )S = 1S = 2S = 4S = 7S = 8S = 14S = 280.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.250.500.751.001.251.501.752.00Latency ratio (r )S = 1S = 2S = 4S = 7S = 140.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.41.6Latency ratio (r )S = 1S = 2S = 70.0 0.2 0.4 0.6 0.8 1.0Activation rate (r)0.20.40.60.81.01.21.4Latency ratio (r )S = 1(b) Relationship between r\u2113 and rfor LAS-RegNetY-800MF blocks on the Nvidia Jetson TX2 GPU.Figure 5: Latency prediction results of LAS-ResNet blocks on V100 (a) and LAS-RegNet blocks onTX2 (b). For both networks, we plot the relationship between the latency ratio r\u2113 and the activationrate r for the blocks in 4 stages with the convolutional stride 1. The practical ef\ufb01ciency is onlyimproved when r\u2113 <1. Note that S = 1can harm the practical latency even for a small r(reducedcomputation), while a larger Swill alleviate this problem. See detailed analysis in Sec. 4.2.where \u03c4 is the Softmax temperature. Following the common practice [ 31, 8], we let \u03c4 decreaseexponentially from 5.0 to 0.1 in training to facilitate the optimization of maskers.Training objective. The FLOPs of each spatial-wise dynamic convolutional block can be calculatedbased on our de\ufb01ned activation rate r[31]. Then we can obtain the FLOPs of the overall dynamicnetwork Fdyn. Let Fstat denotes the FLOPs of its static counterpart. We optimize their ratio toapproximate a target 0 <t< 1:LFLOPs = (FdynFstat\u2212t)2.In addition, we de\ufb01ne loss item Lbounds asin [31] to constrain the upper bound and the lower bound of activation rates in early training epochs.We further propose to leverage the static counterparts of our dynamic networks as \u201cteachers\u201d to guidethe optimization procedure. Let y and y\u2032denote the output logits of a dynamic model (\u201cstudent\u201d)and its \u201cteacher\u201d, respectively. Our \ufb01nal loss can be written asL= Ltask + \u03b1(LFLOPs + Lbounds) +\u03b2T2 \xb7KL(\u03c3(y/T)||\u03c3(y\u2032/T)), (2)where Ltask represents the task-related loss, e.g., cross-entropy loss in image classi\ufb01cation. KL(\xb7||\xb7)denotes the Kullback\u2013Leibler divergence, and \u03b1,\u03b2 are the coef\ufb01cients balancing these items. We use\u03c3to denote the log-Softmax function, and T is the temperature for computing KL-divergence.4 ExperimentsIn this section, we \ufb01rst introduce the experiment settings in Sec. 4.1. Then the latency of differentgranularity settings are analyzed in Sec. 4.2. The performance of our LASNet on ImageNet is furtherevaluated in Sec. 4.3, followed by the ablation studies in Sec. 4.4. Visualization results are illustratedin Sec. 4.5, and we \ufb01nally validate our method on the object detection task (Sec. 4.6). The results onthe instance segmentation task are presented in . For simplicity, we add \u201cLAS-\u201d as a pre\ufb01x beforemodel names to denote our LASNet, e.g., LAS-ResNet-50.4.1 Experiment settingsLatency prediction. Various types of hardware platforms are tested, including a server GPU (TeslaV100), a desktop GPU (GTX1080) and edge devices ( e.g., Nvidia Nano and Jetson TX2). Themajor properties considered by our latency prediction model include the number of processingengines (#PE), the \ufb02oating-point computation in a processing engine (#FP32), the frequency and thebandwidth. It can be observed that the server GPUs generally have a larger #PE than the IoT devices.The batch size is set as 1 for all dynamic models and computing platforms.Image classi\ufb01cation. The image classi\ufb01cation experiments are conducted on the ImageNet [ 4]dataset. Following [31], we initialize the backbone parameter from a pre-trained checkpoint4, and4We use the torchvision pre-trained models at https://pytorch.org/vision/stable/models.html.6--- Page 7 ---1 2 4 7 8 14 28Spatial granularity (S)12345Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.51.01.52.02.53.03.54.0Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.51.01.52.02.53.0Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(a) Relationship between r\u2113 and Sfor LAS-ResNet blocks on V100.1 2 4 7 8 14 28Spatial granularity (S)0.250.500.751.001.251.501.75Latency ratio (r )stage 1r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 4 7 14Spatial granularity (S)0.250.500.751.001.251.501.752.00Latency ratio (r )stage 2r = 0.00r = 0.25r = 0.50r = 0.75r = 1.001 2 7Spatial granularity (S)0.20.40.60.81.01.21.41.6Latency ratio (r )stage 3r = 0.00r = 0.25r = 0.50r = 0.75r = 1.00(b) Relationship between r\u2113 and Sfor LAS-RegNetY-800MF blocks on Nvidia Jetson TX2 GPU.Figure 6: The relationship between the latency ratio r\u2113 and the spatial granularity S.\ufb01netune the whole network for 100 epochs with the loss function in Eq. (2). We \ufb01x \u03b1= 10,\u03b2 = 0.5and T = 4.0 for all dynamic models. More details are provided in Appendix B.4.2 Latency prediction resultsIn this subsection, we present the latency prediction results of the spatial-wise dynamic convolutionalblocks in two different models: LAS-ResNet-101 [11] (on V100) and LAS-RegNetY-800MF [25] (onTX2). All the blocks have the bottleneck structure with different channel numbers and convolutiongroups, and the RegNetY is equipped with Squeeze-and-Excitation (SE) [14] modules.We \ufb01rst de\ufb01ne \u2113dyn as the latency of a spatial-wise dynamic convolutional block, and \u2113stat as that ofa static block without a masker. Their ratio is denoted as r\u2113 = \u2113dyn\u2113stat. We investigate the relationshipbetween r\u2113 and the activation rate r(cf. Sec. 3.5) for different granularity settings. The results inFigure 5 demonstrate that: 1) even equipped with our special optimization on the scheduling strategies,pixel-level spatially adaptive inference (S=1) cannot always improve the practical ef\ufb01ciency. Such\ufb01ne-grained adaptive inference is adopted by most previous works [31, 35], and our result can explainthe reason why they can only achieve realistic speedup on less powerful CPUs [35] or specializeddevices [3]; 2) a proper granularity S >1 effectively alleviates this problem on both hardwaredevices. By setting S >1, realistic speedup could be achieved with larger activation rates.The latency prediction results are further used to seek for a preferable granularity setting for the\ufb01rst 3 stages (we \ufb01x S = 1 for the last stage, where the feature resolution is 7 \xd77). Therefore,we plot the relationship between r\u2113 and S in Figure 10. It can be observed that: 1) r\u2113 generallydecreases with Sincreasing for a given r; 2) an overly largeS(less \ufb02exible adaptive inference) bringsinsigni\ufb01cant improvement on both devices. Especially, enlarging S from 8 to 28 in the \ufb01rst stageof a LAS-ResNet brings very little improvement on V100. Based on the results in Figure 10, wecan trade off between \ufb02exibility and ef\ufb01ciency by selecting appropriate Sfor different models andhardware devices. For example, we can simply set Snet=8-4-7-15 in a LAS-ResNet-101 to achiverealistic speedup. The accuracy-latency plots in Figure 7 also validate this observation. More resultsof our latency prediction model on the desktop-level GPU, GTX1080, are presented in Appendix C.1.4.3 ImageNet classi\ufb01cation resultsWe now empirically evaluate our proposed LASNet on the ImageNet dataset. The network perfor-mance is measured in terms of the trade-off between classi\ufb01cation accuracy and inference ef\ufb01ciency.Both theoretical (i.e. FLOPs) and practical ef\ufb01ciency (i.e. latency) are tested in our experiments.5We use this form to represnet the Ssettings for the 4 stages of a network.7--- Page 8 ---2 3 4 5 6 7 8FLOPs (x1e9)73747576777879Accuracy (%)static ResNetsConvAIG-ResNet-101SkipNet-ResNet-101BAS-ResNet-50DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)15 20 25 30 35 40Latency on Nvidia Jetson TX2 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)1.5 2.0 2.5 3.0 3.5Latency on Nvidia Tesla V100 GPU (ms)76.076.577.077.578.078.579.0Accuracy (%)static ResNetsConvAIG-ResNet-101DynConv-ResNet-101LAS-ResNet-50 (Snet=1-1-1-1)LAS-ResNet-50 (Snet=4-4-2-1)LAS-ResNet-50 (Snet=8-4-7-1)LAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=8-4-7-1)(a) LAS-ResNet results.0.2 0.3 0.4 0.5 0.6 0.7 0.8FLOPs (x1e9)717273747576Accuracy (%)RegNets-XRegNets-YMobileNets-v2ShuffleNets-v2CondenseNetsLAS-RegNetY-400MF (Snet=1-1-1-1)LAS-RegNetY-400MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)3 4 5 6 7 8Latency on Nvidia Jetson TX2 GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-XRegNets-YMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)8 10 12 14 16 18 20Latency on Nvidia nano GPU (ms)72.573.073.574.074.575.075.576.076.5Accuracy (%)RegNets-YRegNets-XMobileNets-v2LAS-RegNetY-800MF (Snet=1-1-1-1)LAS-RegNetY-800MF (Snet=4-4-2-1)LAS-RegNetY-800MF (Snet=4-4-7-1)(b) LAS-RegNetY results.Figure 7: Experimental results on ImageNet. The proposed coarse-grained spatially adaptive inferenceis tested on standard ResNets (a) and lightweight RegNets (b).4.3.1 Standard baseline comparison: ResNetsWe \ufb01rst establish our LASNet based on the standard ResNets [ 11]. Speci\ufb01cally, we build LAS-ResNet-50 and LAS-ResNet-101 by plugging our maskers in the two common ResNet structures.The baselines include various types of dynamic inference approaches: 1) layer skipping (SkipNet[32] and Conv-AIG [30]); 2) channel skipping (BAS [1]); and 3) pixel-level spatial-wise dynamicnetwork (DynConv [31]). For our LASNet, we compare various settings of the spatial granularitySnet. We set training targets (cf. Sec. 3.5) t\u2208{0,4,0.5,0.6,0.7}for our dynamic models to evaluatetheir performance in different sparsity regimes. We apply the same operator fusion (Sec. 3.4) for bothour models and the compared baselines [30, 31] for fair comparison.Results are presented in Figure 7 (a). On the left we plot the relationship of accuracy v.s. FLOPs. Itcan be observed that our LAS-ResNets with different granularity settings signi\ufb01cantly outperformthe competing dynamic neural networks. Surprisingly, coarse-grained spatially adaptive inference(Snet=4-4-2-1 and Snet=8-4-7-1 for the 4 stages) can achieve even higher accuracy when consumingsimilar FLOPs on ResNets, despite the sacri\ufb01ced \ufb02exibility compared toSnet=1-1-1-1. We conjecturethat a larger Sis also bene\ufb01cial to the optimization of maskers.We compare the practical latency of three granularity settings in Figure 7 (a) predicted by our latencyprediction model (middle on TX2 and right on V100). We can witness that although they achievecomparable theoretical ef\ufb01ciency (Figure 7 (a) left), larger Sis more hardware-friendly comparedto the \ufb01nest granularity. For example, the inference latency of LAS-ResNet-101 ( Snet=1-1-1-1)is signi\ufb01cantly higher than the ResNet-101 baseline on V100 (Figure 7 (a) right), even though itstheoretical computation is much smaller than that of the static model. However, larger granularities(Snet=4-4-2-1 and Snet=8-4-7-1) can effectively improve the inference latency due to its lower burdenon the memory access. Remarkably, the latency of ResNet-101 could be reduced by 36% and 46% onV100 and TX2 respectively without sacri\ufb01cing the accuracy when t=0.4. The classi\ufb01cation accuracyis increased by 1.9% with similar inference ef\ufb01ciency. It can be observed that the realistic speedupratio r\u2113 is more close to the theoretical FLOPs ratio target ton the less powerful TX2, because thelatency is computation-bounded (i.e. the latency is mainly spent on computation) on such IoT devices.In contrast, there is a larger gap between practical and theoretical ef\ufb01ciency on the more powerfulV100, as the latency is bounded by the memory access cost.4.3.2 Lightweight baseline comparison: RegNetsWe further evaluate our LASNet in lightweight CNN architectures,i.e. RegNets-Y [25]. Two differentsized models are tested: RegNetY-400MF and RegNetY-800MF. Compared baselines include othertypes of ef\ufb01cient models, e.g., MobileNets-v2 [28], Shuf\ufb02etNets-v2 [23] and CondenseNets [16].The results are presented in Figure 7 (b). The x-axis for the three sub-\ufb01gures are the FLOPs, thelatency on TX2, and the latency on the Nvidia nano GPU, respectively. We can observe that our8--- Page 9 ---1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2Latency on Nvidia Tesla V100 GPU (ms)76.577.077.578.078.579.0Accuracy (%) static ResNetsLAS-ResNet-101 (Snet=1-1-1-1)LAS-ResNet-101 (Snet=4-4-1-1)LAS-ResNet-101 (Snet=4-4-2-1)LAS-ResNet-101 (Snet=4-4-7-1)LAS-ResNet-101 (Snet=8-4-7-1)Figure 8: Ablation studies on S. Figure 9: Visualization results.method outperforms various types of static models in terms of the trade-off between accuracy andef\ufb01ciency. More results on image classi\ufb01cation are provided in Appendix C.2.4.4 Ablation studiesWe conduct ablation studies to validate the effectiveness of our coarse-grained spatially adaptiveinference (Sec. 3.2) and operator fusion operations (Sec. 3.4).More granularities settings. We test various granularity settings on LAS-ResNet-101 to examinethe effects of Sin different stages. The results on the Tesla-V100 GPU are presented in Figure 8. Itcan be found that the \ufb01nest granularity (Snet=1-1-1-1) leads to substantial inef\ufb01ciency despite thereduced FLOPs (cf. Figure 7 (a) left). Coarse-grained spatially adaptive inference in the \ufb01rst twostages (Snet=4-4-1-1) effectively reduces the inference latency. We further increase Sin the thirdstage to 2 and 7, and this procedure consistently improves the realistic ef\ufb01ciency on the V100 GPU.This trend also holds on the GTX 1080 GPU (see the results in Appendix C.2.Table 1: Ablation studies on operator fusion.Masker- Gather- Scatter- LatencyConv1x1 Conv3x3 Add (\xb5s)   163.2   90.1   86.7   71.4Operator fusion. We investigate the effect of ouroperator fusion introduced in Sec. 3.4. One convolu-tional block in the \ufb01rst stage of a LAS-ResNet-101(S=4, r=0.6) is tested. The results in Table 1 vali-date that every step of operator fusion bene\ufb01ts thepractical latency of a block, as the overhead onmemory access is effectively reduced. Especially,the fusion of the masker operation and the \ufb01rst con-volution is crucial to reducing the latency.4.5 VisualizationWe visualize the masks generated by our masker in the third block of a LAS-ResNet-101 (Snet=4-4-2-1) in Figure 12. The brilliant areas correspond to the locations of 1 elements in a mask, andthe computation on the dimmed regions is skipped by our dynamic model. We can witness that themasker accurately locate the most task-related regions (even the tiny aircraft at the corner), whichhelps reduce the unnecessary computation on background areas. These resultsalso suggest that forthe \ufb01rst stage, the granularity S=4 is suf\ufb01ciently \ufb02exible to recognize the important regions, anda win-win can be achieved between accuracy and ef\ufb01ciency. Interestingly, the masker could selectsome objects that are not labeled for the sample, e.g., the \ufb02ower beside the hummingbird and thehuman holding the camera. This indicates that our spatial-wise dynamic networks can automaticallyrecognize the regions with semantics, and their capability is not limited by the classi\ufb01cation labels.This property is helpful in some downstream tasks, such as object detection (Sec. 4.6) and instancesegmentation (Sec. 4.7), which require detecting multiple classes and objects in an image. Morevisualization results could be found in Appendix C.3.4.6 COCO Object detectionWe further evaluate our LASNet on the COCO [22] object detection task. The mean average precision(mAP), the average backbone FLOPs, and the average backbone latency on the validation set areused to measure the network performance. We test two commonly used detection frameworks: FasterR-CNN [27] with Feature Pyramid Network [20] and RetinaNet [21]. Thanks to the generality ofour method, we can conveniently replace the backbones with ours pre-trained on ImageNet, andthe whole models are \ufb01netuned on COCO with the standard setting for 12 epochs (see detailed9--- Page 10 ---Table 2: Object detection results on the COCO dataset.Detection Backbone Backbone Backbone Latency (ms)mAP (%)Framework FLOPs (G) V100 GTX1080 TX2Faster R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 39.4LAS-ResNet-101 (Snet=4-4-2-1,t=0.6) 90.7 33.8 90.6 524.9 40.3LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 79.3 30.7 82.5 477.1 39.8LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.5 29.0 79.9 464.8 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 67.9 25.3 69.1 401.3 39.5RetinaNetResNet-101 (Baseline) 141.2 39.5 119.3 729.4 38.5LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 77.8 30.4 81.8 472.6 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.5) 79.4 28.9 79.9 464.8 39.3LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 66.4 25.3 69.1 401.3 38.9Table 3: Instance Segmentation results on the COCO dataset.Segmentation Backbone Backbone Backbone Latency (ms)APmask(%) APbox(%)Framework FLOPs (G) V100 GTX1080 TX2Mask R-CNNResNet-101 (Baseline) 141.2 39.5 119.3 729.4 36.1 40.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.5) 80.5 31.1 83.3 481.9 37.0 41.0LAS-ResNet-101 (Snet=4-4-2-1,t=0.4) 69.2 27.9 74.8 431.6 36.1 40.0LAS-ResNet-101 (Snet=4-4-7-1,t=0.4) 68.8 25.8 70.9 411.8 36.2 40.0setup in Appendix B.3). The input images are resized to a short side of 800 and a long side notexceeding 1333. The results of our LAS-ResNet-101 with different Snet settings are presented inTable 2. We can observe from the results that when setting the training target as 0.4, the latency ofour LAS-ResNet-101 with Snet=4-4-7-1 is signi\ufb01cantly lower than the static baseline on all deviceswithout sacri\ufb01cing mAP in both detection frameworks. With a larger training targets, our LASNetcan increase the mAP by 0.9% and 0.8% in Faster R-CNN and RetinaNet respectively, while stillbeing faster than the baseline method.4.7 COCO instance segmentationWe also present the results of instance segmentation on COCO, which demonstrate the effectivenessof our LASNet on the dense prediction task. From the results in Table 3, we can observe that whensetting the training target as 0.4, the Mask R-CNN [ 10] models (Snet=4-4-2-1 and Snet=4-4-7-1)runs faster on all tested hardware devices without sacri\ufb01cing the performance. With a training targetof 0.5, the APmask and APbox of the Mask R-CNN model could be increased by 0.9% and 1.0%respectively while still running faster than the baseline method.5 ConclusionIn this paper, we propose to build latency-aware spatial-wise dynamic networks (LASNet) under theguidance of a latency prediction model. By simultaneously considering the algorithm, the schedulingstrategy and the hardware properties, we can ef\ufb01ciently estimate the practical latency of spatial-wise dynamic operators on arbitrary computing platforms. Based on the empirical analysis on therelationship between the latency and the granularity of spatially adaptive inference, we optimizeboth the algorithm and the scheduling strategies to achieve realistic speedup on many multi-coreprocessors, e.g., the Tesla V100 GPU and the Jetson TX2 GPU. Experiments on image classi\ufb01cation,object detection and instance segmentation tasks validate that the proposed method signi\ufb01cantlyimproves the practical ef\ufb01ciency of deep CNNs, and outperforms various competing approaches.AcknowledgementThis work is supported in part by the National Key R&D Program of China under Grant2020AAA0105200, the National Natural Science Foundation of China under Grants 62022048and the Tsinghua University-China Mobile Communications Group Co.,Ltd. Joint Institute..10--- Page 11 ---References[1] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping for learningconditional channel gated networks. In ICLR, 2020.[2] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, MeghanCowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. Tvm: An automated end-to-end optimizingcompiler for deep learning. In OSDI, 2018.[3] Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian Verhelst. Proces-sor architecture optimization for spatially dynamic neural networks. In VLSI-SoC, 2021.[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In CVPR, 2009.[5] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicatednetwork with less inference complexity. In CVPR, 2017.[6] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov,and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In CVPR,2017.[7] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamicneural networks: A survey. TPAMI, 2021.[8] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, and Haojun Jiang. Spatiallyadaptive feature re\ufb01nement for ef\ufb01cient inference. TIP, 2021.[9] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfen Cao, Wenhui Huang,Chao Deng, and Gao Huang. Learning to weight samples for dynamic early-exiting networks.In ECCV, 2022.[10] Kaiming He, Georgia Gkioxari, Piotr Doll\xe1r, and Ross Girshick. Mask r-cnn. In Proceedings ofthe IEEE international conference on computer vision, pages 2961\u20132969, 2017.[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In CVPR, 2016.[12] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach .Elsevier, 2011.[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, TobiasWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ef\ufb01cient convolutional neuralnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.[15] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Wein-berger. Multi-scale dense networks for resource ef\ufb01cient image classi\ufb01cation. In ICLR, 2018.[16] Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: Anef\ufb01cient densenet using learned group convolutions. In CVPR, 2018.[17] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Weinberger.Densely connected convolutional networks. TPAMI, 2019.[18] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.In ICLR, 2017.[19] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In NeurIPS, 2017.[20] Tsung-Yi Lin, Piotr Doll\xe1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.Feature pyramid networks for object detection. In CVPR, 2017.[21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Focal loss for denseobject detection. In ICCV, 2017.11--- Page 12 ---[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,Piotr Doll\xe1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,2014.[23] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shuf\ufb02enet v2: Practical guidelinesfor ef\ufb01cient cnn architecture design. In ECCV, 2018.[24] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuousrelaxation of discrete random variables. In ICLR, 2017.[25] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\xe1r. Design-ing network design spaces. In CVPR, 2020.[26] Mengye Ren, Andrei Pokrovsky, Bin Yang, and Raquel Urtasun. SBNet: Sparse Blocks Networkfor Fast Inference. CVPR, 2018.[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-timeobject detection with region proposal networks. In NeurIPS, 2015.[28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.[29] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, andQuoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820\u20132828,2019.[30] Andreas Veit and Serge Belongie. Convolutional networks with adaptive inference graphs. InECCV, 2018.[31] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: Exploiting Spatial Sparsity forFaster Inference. In CVPR, 2020.[32] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learningdynamic routing in convolutional networks. In ECCV, 2018.[33] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus:a dynamic approach to reducing spatial redundancy in image classi\ufb01cation. In NeurIPS, 2020.[34] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, YuandongTian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware ef\ufb01cient convnetdesign via differentiable neural architecture search. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 10734\u201310742, 2019.[35] Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptiveinference with stochastic feature sampling and interpolation. In ECCV, 2020.[36] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptivenetworks for ef\ufb01cient inference. In CVPR, 2020.[37] Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan, and Yunhe Wang.Dynamic resolution network. NeurIPS, 2021.12--- Page 13 ---AppendixA Latency prediction model.As the dynamic operators in our method have not been supported by current deep learning libraries,we propose a latency prediction model to ef\ufb01ciently estimate the real latency of these operators onhardware device. The inputs of the latency prediction model include: 1) the structural con\ufb01gurationof a convolutional block, 2) its activation rate rwhich decides the computation amount, 3) the spatialgranularity S, and 4) the hardware properties mentioned in Table 4. The latency of a dynamic blockis predicted as follows.Input/output shape de\ufb01nition. The \ufb01rst step of predicting the latency of an operation is to calculatethe shape of input and output. Taking the gather-conv2 operation as an example, the input of thisoperation is the activation with the shape of Cin \xd7H \xd7W, where Cin is the number of inputchannels, and H and W are the resolution of the feature map. The shape of the output tensor isP \xd7Cout \xd7S\xd7S, where P is the number of output patches, Cout is the number of output channelsand Sis the spatial granularity. Note that P is obtained based on the output of our maskers.Operation-to-hardware mapping. Next, we map the operations to hardware. As is mentioned in thepaper, we model a hardware device as multiple processing engines (PEs). We assign the computationof each element in the output feature map to a PE. Speci\ufb01cally, we consecutively split the outputfeature map into multiple tiles. The shape of each tile is TP \xd7TC \xd7TS1 \xd7TS2. These split tilesare assigned to multiple PEs. The computation of the elements in each tile is executed in a PE. Wecan con\ufb01gure different shapes of tiles. In order to determine the optimal shape of the tile, we makea search space of different tile shapes. The tile shape has 4 dimensions. The candidates of eachdimension are power-of-2 and do not exceed the corresponding dimension of the feature map.Latency estimation. Then, we evaluate the latency of each tile shape in the search space and selectthe optimal tile shape with the lowest latency. The latency includes the data movement latency andthe computation latency: \u2113= \u2113data + \u2113computation.1) Data movement latency \u2113data. The estimation of the latency for data movement requires us tomodel the memory system of a hardware device. We model the memory system of hardware as athree-level architecture [12]: off-chip memory, on-chip global memory, and local memory in PE.The input data and weight data are \ufb01rst transferred from the off-chip memory to the on-chip globalmemory. We assume the hardware can make full use of the off-chip memory bandwidth to simplifythe latency prediction model.After that, the data used to compute the output tiles is transferred from on-chip global memory tothe local m, Your response: ```ADD_PAPER\n2210.06223v1\nThis paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach that balances computation allocation with practical latency considerations through a novel latency prediction model. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method outperforms pixel-level dynamic networks by optimizing spatial granularity and scheduling strategies, demonstrating significant improvements in real-world deployment scenarios. The latency predictor considers algorithm design, scheduling, and hardware properties to guide network optimization, making it particularly effective for resource-constrained edge devices.\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\n~~~~~~~~~~\nCurrent Step #11, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Ef\ufb01cient Feature Compression forEdge-Cloud SystemsZhihao DuanElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.duan90@purdue.eduFengqing ZhuElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.zhu0@purdue.eduAbstract\u2014Optimizing computation in an edge-cloud system isan important yet challenging problem. In this paper, we considera three way trade-off between bit rate, classi\ufb01cation accuracy,and encoding complexity in an edge-cloud image classi\ufb01cationsystem. Our method includes a new training strategy andan ef\ufb01cient encoder architecture to improve the rate-accuracyperformance. Our design can also be easily scaled accordingto different computation resources on the edge device, taking astep towards achieving rate-accuracy-complexity (RAC) trade-off.Under various settings, our feature coding system consistentlyoutperforms previous methods in terms of the RAC performance.Code is made publicly available 1.Index Terms\u2014Feature compression, Coding for machine vision,Image classi\ufb01cation, Edge-cloud systemI. I NTRODUCTIONMobile devices and wearable sensors are essential gatewaysto many autonomous systems. These edge devices are oftendeployed with vision tasks in applications involving robotics,drones, surveillance cameras, and smart home devices. How-ever, due to constraints on the device size and power supply,edge devices cannot support the real-time execution of neuralnetworks-based computer vision methods, which are typicallycomputationally expensive. A common solution is to takeadvantage of the wireless connection to a powerful cloudcomputing resource, forming an edge-cloud system [1], [2]as illustrated in Fig. 1. In such a system, we obtain data (inour context, images) from edge device sensors (cameras) andsend it to the cloud for task execution.Designing practical methods for edge-cloud systems ischallenging from several aspects. First, in most cases wherethe edge-cloud connection is band-limited, we have constraintson the transmission bit rate, requiring the captured images tobe compressed by an encoder on the edge as ef\ufb01ciently as pos-sible. Second, we also have constraints on the computationalcapacity of the edge device, and thus the encoder cannot betoo complex. Finally, for common vision tasks such as theimage classi\ufb01cation, we also desire high accuracy. Taking allthese into consideration, we face a three-way trade-off betweenencoder complexity, bit rate, and vision task accuracy, whichWe term as the rate-accuracy-complexity (RAC) trade-off.1github.com/duanzhiihao/edge-cloud-racEdge device(sender)Cloud computing(receiver)Bits \u201ccat\u201dRateEncoder complexityAccuracyFig. 1: Illustration of an edge-cloud machine vision system,where three objectives are of interest: encoder computationalcomplexity on the edge device, bit rate in transmission, andtask accuracy in the cloud. In this paper, we present a featurecompression method for the edge-cloud image classi\ufb01cation.A straightforward baseline for RAC trade-off is to com-press and transmit images using different codecs. However,image codecs are typically developed for image reconstruction,making them suboptimal for vision task execution. Recently,research focused on feature compression has gained attention,which can be optimized for different vision tasks [1], [3].Another advantage to transmit features rather than images isprivacy protection. By transmitting features, one could skippixel reconstruction on the cloud and thus reduce potentiallyexposing image contents to the attackers [4].Entropic Student [2] is a recently proposed feature com-pression framework that provides promising rate-accuracyperformance. However, it does not take into account theencoder complexity as part of the performance trade-off. Toextend the scope to the three-way RAC trade-off, we proposea \ufb02exible encoder whose complexity can be easily adjustedfor different device capacities. Together with a new trainingstrategy, we achieve better RAC performance compared toprevious methods, including image coding baselines and thestate-of-the-art Entropic Student model.Our contributions can be summarized as follows:\u2022 We expand the scope of previous feature compressionsystems to include the RAC trade-off.\u2022 We design a \ufb02exible neural network architecture forfeature coding, which is both ef\ufb01cient and achieves goodclassi\ufb01cation performance.\u2022 We conduct extensive experiments to verify the effec-tiveness of our approach, showing its advantages overprevious methods.arXiv:2211.09897v1  [eess.IV]  17 Nov 2022--- Page 2 ---II. R ELATED WORKA. Image CompressionTraditional image coders . Traditional image codecs typi-cally follow a linear transform coding paradigm, such as block-based discrete cosine transform (DCT) and discrete wavelettransform (DWT). Representative codecs include JPEG, WebP,and BPG (i.e., HEVC intra [5]). Their encoding complexityvaries depending on different factors such as the choice of theentropy coding algorithms, number of prediction modes, andhow much optimization is done during training. In general,more complex computation devoted at the encoder leads tobetter compression performance.Learned image coders . With the rise of deep learning,learned image coders (LICs) extend the traditional lineartransform coding paradigm to a non-linear fashion, which isachieved by transforming images using neural networks [6].Despite improved compression performance compared to tra-ditional codecs, LICs typically require more computationalresources due to highly parameterized neural network layersand autoregressive entropy models [7].The paradigm of image coding share a common disadvan-tage: they are designed for image reconstruction and may besuboptimal for vision tasks in most edge-cloud settings.B. Feature compressionVarious methods have been proposed to compress andtransmit features rather than images for machine vision.Hand-crafted feature coding. Earlier works have exploredcompressing deep features using hand-crafted algorithms. Choiand Baji \xb4c use a block-based intra and inter coding algorithmfor video feature compression in [8] and a HEVC-based fea-ture compression method for object detection in [9]. Followingtheir works, methods have been proposed for feature compres-sion using pruning [10], quantization [11], and bidirectionalprediction [10].Learned feature compression . Instead of manually de-signed methods, recent work has shifted attention to end-to-end learned feature compression and showed promisingresults. Singh et al. use a fully factorized entropy model toconstrain the entropy of features, which results in improvedrate-accuracy trade-off over previous methods [3]. Dubois etal. show that, given enough computational capacity, featurescan be extremely (around 1,000\xd7) compressed without affect-ing the vision task accuracy [12]. More recently, Matsubaraet al. train vision models with feature compressor jointly andshow state-of-the-art rate-accuracy performance [2].While existing methods mostly focus on improving therate-accuracy performance, their approach lacks scalabilityfor different encoder complexity. In this work, we extendthe scope to rate-accuracy-complexity (RAC) trade-off byproposing a easily scalable encoder architecture, which can beadapted to edge devices with different computation capacities.III. P RELIMINARIESThe current best method in terms of rate-accuracy perfor-mance, Entropic Student [2], is a good starting point for our\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43f\u0ddc\U0001d466\u0ddc\U0001d466teachLabel \U0001d466Conv5x5, s=2GDNConv5x5, s=2GDNConv2x2, s=1Conv2x2, s=1Inv. GDNConv2x2, s=1Inv. GDNConv2x2, s=1DecoderEncoder ClassifierEntropy modelPre-trained image classifierStage 1 Stage 2(a) Training overview. GDN refers to the generalized divisible nor-malization layer [13], the entropy model is fully factorized, and theclassi\ufb01er is ResNet-50 [14]. See text for description.Edge Cloud\u0ddc\U0001d466Enc. Dec.ClassifierEntropy modelrANS rANSBitsEntropy model(b) Testing (deployment). Entropy coding is done by the range-basedvariant of asymmetric numeral systems (rANS) [15].Fig. 2: Model training (a) and deployment (b) in the EntropicStudent framework [2] for feature compression.proposed work. In this section, we brie\ufb02y summarize it toprovide the background of our method, and we refer interestedreaders to the original paper [2] for more details.The framework for classi\ufb01cation is shown in Fig. 2. Forsplit computing, a feature encoder is designed to be deployedon the edge device, and a decoder together with a classi\ufb01er areexecuted on the cloud. Speci\ufb01cally, a ResNet-50 [14] classi\ufb01eris adopted for image classi\ufb01cation.Training is done in two stages. In the \ufb01rst stage, theclassi\ufb01er is initialized using a pre-trained model and \ufb01xed(i.e., no parameter updating), and the encoder and decoder arelearned to compress images and reconstruct desired features.In the second stage, the encoder is \ufb01xed, while the decoderand the classi\ufb01er are trained to optimize the task accuracy.The training loss functions are:Lstage1 = \u03bblR + lMSE,Lstage2 = \u03bblR + 0.5 \xb7(lKL + lCE), (1)where lR is the bit rate loss estimated by a factorized entropymodel [6], lMSE is the mean squared error (MSE) betweenthe classi\ufb01er\u2019s intermediate features and the ones from a pre-trained teacher model, lKL is the KL divergence loss forknowledge distillation [16], lCE is the standard cross-entropyloss for classi\ufb01cation, and \u03bb is a scalar hyperparameter totrade-off between rate and classi\ufb01cation accuracy.The deployment scenario is shown in Fig. 2b, where theentropy model is shared between the sender (edge) and thereceiver (cloud) for entropy coding, implemented by the range-based asymmetric numeral system (rANS) algorithm [15].Despite its strong rate-accuracy performance [2], the En-tropic Student model only operates at a single encoder com-plexity and is not feasible for RAC trade-off. In the nextsection, we introduce our method, which not only improves therate-accuracy performance, but also enables the RAC trade-off.--- Page 3 ---\u0ddc\U0001d466\u0ddc\U0001d466tLabel \U0001d466Enc. Dec. ClassifierPre-trained Classifier\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43fEntropy modelLearned or fixed(a) Training overview.Conv 8x8, s=8Res. BlockRes. Block\U0001d441 blocks\U0001d467 (b) Encoder architecture.GELUConv 3x3, s=1GELUConv 3x3, s=1 (c) Residual block.GELUConvT5x5Conv 3x3, s=1GELUConv 3x3, s=1GELUConv 1x1, s=1\U0001d467 (d) Decoder architecture.Fig. 3: Summary of proposed method. In (a), we show the training strategy, where the entropy model is fully factorized, andgray blocks indicate frozen models (i.e., no parameter update during training). The classi\ufb01er can either be trained jointly withthe system, or initialized with a pre-trained model and \ufb01xed throughout training. In (b) and (c), we show our encoder anddecoder architecture, respectively. Our encoder network complexity can be tuned by adjusting the number of residual blocks,N. For the classi\ufb01er, we use a truncated version of ResNet-50, following [2].IV. O UR APPROACHInspired by the success of knowledge distillation in the En-tropic Student model, we adopt a similar framework and lossfunctions, shown in Fig. 3a. However, we make modi\ufb01cationsincluding a new training strategy and network architecture, bywhich we are able to realize the RAC trade-off.A. Single-stage TrainingThe Entropic Student method adopts a two-stage trainingstrategy, where the encoder and decoder are trained in the\ufb01rst stage, and the decoder and classi\ufb01er are \ufb01ne-tuned in thesecond stage. We note that this decoupled training strategyrequires more hyperparameter searching, and it is also sub-optimal for rate-accuracy performance since network compo-nents are trained separately. Instead, we propose a simple \ufb01x,i.e., to train the encoder, decoder, and classi\ufb01er in an end-to-end fashion. Our single-stage loss function is as follows:Lsingle = \u03bblR + lMSE + 0.5 \xb7(lKL + lCE), (2)where the terms are de\ufb01ned in Sec. III. We keep the samescaling factor for each loss term as in the Entropic Studentwithout further tuning. Later in Sec. V-D, we show thatour single-stage strategy improves the rate-accuracy trade-offdespite being simpler to implement.B. Network ArchitectureWe introduce an encoder network architecture speci\ufb01callydesigned for ef\ufb01cient and \ufb02exible feature compression, shownin Fig. 3b. We now describe our design choices in details.Downsampling layer. We \ufb01rst noticed that the 5x5 con-volutions for downsampling in previous works (e.g., in theEntropic Student [2] and learned image coders [6], [17]) arecomputationally inef\ufb01cient due to the large overlap betweenconvolutional kernels. To remove this overlap, we adopt asingle patch embedding [18] layer for downsampling, whichcan be equivalently viewed as a convolution with kernel sizeequal to its strides.Network blocks. Instead of feedforward convolutions asin previous works, we adopt residual blocks in our featureencoder, which enables easy adjustment of the encoder com-plexity simply by setting the number of residual blocks, N. Alarger N (i.e., more residual blocks in the encoder) improvesthe rate-accuracy performance while results in higher encodercomplexity. Thus by tuning N, we can trade-off between therate-accuracy performance and the encoding complexity of oursystem. We use a standard design for the residual blocks asshown in Fig. 3c. For non-linearity layers, we use the GeLUactivation [19] instead of GDN [13], as we empirically foundthat the former leads to better performance with the samecomputational complexity.Downsampling ratio. Our \ufb01nal step is to use 8\xd7downsam-pling, instead of the 4\xd7downsampling in Entropic Student,during encoding. Our choice is motivated by the observationthat 8\xd7downsampling is a common choice for various imageprocessing tasks (e.g., in traditional codecs like JPEG) andcomputer vision tasks (e.g., in image object detection [20]and semantic segmentation [21]). Note that in our featuredecoder on the cloud (Fig. 3d), we upsample the feature by2\xd7to recover spatial details of the original image. With thiscon\ufb01guration of the encoder and the decoder, we are able toreduce the encoding latency and improve the average accuracyby a large margin, as discussed later in Sec. V-D.C. Feature Compression for a Fixed Classi\ufb01erSo far, we have considered the joint training setting, wherethe classi\ufb01er and our feature coder are trained jointly. Inaddition, we also examine the setting in which a pre-trainedclassi\ufb01er is available but \ufb01xed, illustrated in Fig. 3a. Thisscenario may happen in practice where there are different edgedevices sharing a single cloud vision model. The edge devicesmay use different feature encoders/decoders according to theircomputational capacity, but the vision model on the cloudshould be \ufb01xed. In this case, we use the same loss functionas in Eq. 2, but we train the encoder and decoder networkswithout updating the classi\ufb01er. We denote the methods in thissetting by cls. \ufb01xed to distinguish them from the joint trainingsetting, where the classi\ufb01er can be trained together with theencoder and the decoder.V. E XPERIMENTSWe compare our method to previous ones for edge-cloudimage classi\ufb01cation. For fair comparison, all methods use theResNet-50 [14] (for image coding methods) or a truncated--- Page 4 ---version of ResNet-50 (for feature coding methods) as theclassi\ufb01er.A. Dataset and MetricsWe use the 1,000-class ImageNet dataset [22] for trainingand testing. Standard data augmentations, including randomresizing and cropping, are applied during training. We use theImageNet val set for testing, in which we resize and center-crop all images such that they have a resolution of 224 \xd7224.All metrics are computed on the 224 \xd7224 val set. Bit rateis measured in terms of bits per pixel (bpp). For classi\ufb01cationaccuracy, we report the top-1 accuracy of the classi\ufb01er predic-tions. For encoding complexity, we focus on the CPU latencysince it well simulates real-world scenarios. All encodinglatencies are measured using an Intel 12700K CPU, averagedover the \ufb01rst 5,000 images in the val set.We also introduce a new metric, Delta-accuracy, whichcomputes the average accuracy improvement (w.r.t. a baselinemethod) over all bit rates. By using this metric, we cansummarize each rate-accuracy curve into a scalar, thus beingable to represent the three-way RAC trade-off in a 2-D plot.Our computation of Delta-accuracy follows the convention ofBD-PSNR and BD-rate [23] except one aspect. Unlike PSNR,the unit of accuracy is percentage (not decibel), so we computeaverage accuracy improvement over the normal bit rate space(instead of the logarithmic rate scale used in BD-PSNR).B. Experiment SettingsWe consider the following baseline methods.Image coding baselines. We compress images using animage codec at the sender side, and we reconstruct images atthe receiver side using the same codec. Then, we run ResNet-50 on the reconstructed images to infer class prediction. Ourselected codecs include handcrafted (e.g., WebP and BPG [5])and learning-based methods (e.g., the Factorized model [6],and the Hyperprior model [17]).Feature coding baseline. We use the existing best method,Entropic Student [2], as a powerful baseline.As we discussed in Sec. IV-C, we group methods into twogroups so that methods within each can be fairly compared.Classi\ufb01er \ufb01xed setting: The pre-trained ResNet-50 is usedand remains \ufb01xed (i.e., no \ufb01ne-tuning). The image codingbaselines all fall in this setting. We label the methods in thissetting as cls. \ufb01xed.Joint training setting: The pre-trained ResNet-50 is jointlytrained with the feature encoder and/or decoder. The EntropicStudent model falls in this setting. We label the methods inthis setting as joint.C. Main Results for RAC Trade-offTo benchmark our method against the baselines in terms ofRAC performance, we \ufb01rst measure the rate-accuracy curvesfor all baselines and our methods. Then, we compute the Delta-accuracy of each method w.r.t. the Entropic Student baseline(whose Delta-accuracy is 0 by de\ufb01nition). We show the Delta-accuracy as a function of encoding latency in Fig. 4. Note that3 4 7 10 14 20 30 40 50Encoding latency on CPU (ms)121086420Delta-accuracy (%)RAC performance on ImageNetFeature coding: ours (joint)Feature coding: Entropic Student [2] (joint)Feature coding: ours (cls. fixed)Image coding: BPG [5] (cls. fixed)Image coding: Hyperpiror model [17] (cls. fixed)Image coding: Factorized model [6] (cls. fixed)Image coding: WebP (cls. fixed)Fig. 4: RAC trade-off, visualized by Delta-accuracy vs en-coding complexity. The Delta-accuracy is w.r.t. the EntropicStudent baseline.the Entropic Student [2] corresponds to a single point on thecomplexity vs. Delta-accuracy plot since its complexity cannotbe tuned. Similarly, the Factorized model [6] and Hyperpriormodel [17] also produce a single point on the this plot. ForWebP and BPG [5] codec, we control the encoding latency bysetting the compression speed option provided by the codecimplementation. For our method, we control the latency bychoosing N \u2208{0,4,8}, where N is the number of residualblocks in the encoder.From Fig. 4, we observe that for classi\ufb01er \ufb01xed methods,our approach clearly outperforms the image coding baselinesby a large margin, demonstrating the effectiveness of featurecoding. This improvement is expected, because our featureencoder and decoder are explicitly trained to maximize classi-\ufb01cation accuracy, instead of pixel reconstruction as in imagecoding baselines. For the joint training setting, our methodachieves higher Delta-accuracy than the Entropic Student base-line with approximately only half of the encoding latency. Inaddition, the \ufb02exible design of our encoder enables encodingcomplexity adjustment, as opposed to the Entropic Student, bywhich only a single complexity can be achieved.D. Ablation StudyTo analyze how each component of our method contributesto the RAC improvement, we start from the Entropic Studentbaseline, and we include our proposed components one ata time. At each step, we show the change of rate-accuracyperformance as well as the encoding complexity.We \ufb01rst compare our single-stage strategy (referred to ascon\ufb01guration A) with the original two-stage one in Fig. 5. InTable I, we also show the encoding complexity and Delta-accuracy. We observe that the single-stage training does notimpair the performance; instead, the average accuracy slightlyincreases by 0.13% with encoding complexity unchanged.Then, we replace the Entropic Student encoder by ourpatch embedding layer and residual blocks, and we show the--- Page 5 ---0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2Bits per pixel (bpp)57.560.062.565.067.570.072.575.0T op-1 acc. (%)Rate-accuracy trade-off on ImageNetOurs C: B + 8x downsamplingOurs B: A + improved architectureOurs A: Baseline + single-stage trainingBaseline: Entropic StudentFig. 5: Ablation study of our approach compared to the En-tropic Student baseline. Trained and tested on ImageNet [22].TABLE I: Ablation study. FLOPs and encoding latency arefor 224x224 resolution inputs. All percentages are w.r.t. to thebaseline, Entropic Student.FLOPs\u2193 Enc. Latency\u2193 Delta-acc.\u2191Baseline: Entropic Student 0.466B 13.8ms 0.0%A: Baseline + single-stage 0.466B (-0.0%) 13.8ms (-0.0%) +0.13%B: A + improved architecture 0.457B (-0.02%) 9.10ms (-34.1%) +0.20%C: B + 8x downsampling 0.489B (+0.05%) 6.70ms (-51.4%) +0.93%resulting system as con\ufb01guration B in Fig. 5 and Table I. Notethat the downsampling ratio ( 4\xd7) is unchanged. We can seethat the improved architecture reduces the encoding latencyby more than 30% and improves the average accuracy by anadditional small percentage ( 0.07%).Finally, we increase the downsampling ratio to 8\xd7 aswe discussed in Sec. IV-B, and we refer to this version ascon\ufb01guration C . By increasing the downsampling ratio, weagain reduce the encoding latency and improve the averageaccuracy by an additional large margin, after which we achievean overall latency reduction of 50% and an average accuracyimprovement of 0.93%. From Fig. 5, we notice that theimprovement is most signi\ufb01cant at lower bit rates. We con-jecture that lower bit rates naturally call for a more compactcompressed representation, which can be achieved by moreaggressive downsampling. For higher bit rate (e.g., more than0.8 bpp), our approach remains on par with the baseline.VI. C ONCLUSIONIn this paper, we present a feature coding system for imageclassi\ufb01cation in edge-cloud systems. We show that by properdesign of the network architecture, existing feature codingmethods can be improved and extended to consider the RACtrade-off. Our system outperforms all previous methods interms of RAC trade-off, which we believe is valuable formany real-world applications deployment. A limitation of oursystem is the requirement of a separate neural network modelfor every rate-complexity pair, which we leave to our futurework to solve.REFERENCES[1] H. Choi and I. V . Baji \xb4c, \u201cDeep feature compression for collaborativeobject detection,\u201d IEEE International Conference on Image Processing ,pp. 3743\u20133747, Oct. 2018.[2] Y . Matsubara, R. Yang, M. Levorato, and S. Mandt, \u201cSupervised com-pression for resource-constrained edge computing systems,\u201d IEEE/CVFWinter Conference on Applications of Computer Vision , pp. 923\u2013933,Jan. 2022.[3] S. Singh, S. Abu-El-Haija, N. Johnston, J. Ball \xb4e, A. Shrivastava, andG. Toderici, \u201cEnd-to-end learning of compressible features,\u201d IEEEInternational Conference on Image Processing , pp. 3349\u20133353, Oct2020.[4] V . Kakkad, M. Patel, and M. Shah, \u201cBiometric authentication and imageencryption for image security in cloud framework,\u201d Multiscale andMultidisciplinary Modeling, Experiments and Design , vol. 2, no. 4, pp.233\u2013248, May 2019.[5] J. Lainema, F. Bossen, W. Han, J. Min, and K. Ugur, \u201cIntra coding of thehevc standard,\u201d IEEE Transactions on Circuits and Systems for VideoTechnology, vol. 22, no. 12, pp. 1792\u20131801, Dec. 2012.[6] J. Ball \xb4e, V . Laparra, and E. Simoncelli, \u201cEnd-to-end optimizationof nonlinear transform codes for perceptual quality,\u201d Picture CodingSymposium, pp. 1\u20135, Dec. 2016.[7] D. Minnen, J. Ball \xb4e, and G. Toderici, \u201cJoint autoregressive and hier-archical priors for learned image compression,\u201d Advances in NeuralInformation Processing Systems, vol. 31, pp. 10 794\u201310 803, Dec. 2018.[8] H. Choi and I. V . Baji \xb4c, \u201cNear-lossless deep feature compression forcollaborative intelligence,\u201d IEEE International Workshop on MultimediaSignal Processing, pp. 1\u20136, Aug. 2018.[9] \u2014\u2014, \u201cDeep feature compression for collaborative object detection,\u201dIEEE International Conference on Image Processing , pp. 3743\u20133747,Oct. 2018.[10] H. Choi, R. A. Cohen, and I. V . Baji \xb4c, \u201cBack-and-forth prediction fordeep tensor compression,\u201d IEEE International Conference on Acoustics,Speech and Signal Processing , pp. 4467\u20134471, May 2020.[11] R. A. Cohen, H. Choi, and I. V . Baji \xb4c, \u201cLightweight compressionof neural network feature tensors for collaborative intelligence,\u201d IEEEInternational Conference on Multimedia and Expo , pp. 1\u20136, Jul. 2020.[12] Y . Dubois, B. Bloem-Reddy, K. Ullrich, and C. J. Maddison, \u201cLossycompression for lossless prediction,\u201d Advances in Neural InformationProcessing Systems, vol. 34, pp. 14 014\u201314 028, Dec. 2021.[13] J. Ball \xb4e, V . Laparra, and E. P. Simoncelli, \u201cDensity modeling ofimages using a generalized normalization transformation,\u201d InternationalConference on Learning Representations , Jul. 2016.[14] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for imagerecognition,\u201d Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 770\u2013778, Jun. 2016.[15] J. Duda, \u201cAsymmetric numeral systems: entropy coding combiningspeed of huffman coding with compression rate of arithmetic coding,\u201darXiv preprint arXiv:1311.2540 , Dec. 2013.[16] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neuralnetwork,\u201d https://arxiv.org/abs/1503.02531, Mar. 2015.[17] J. Ball \xb4e, D. Minnen, S. Singh, S. Hwang, and N. Johnston, \u201cVariationalimage compression with a scale hyperprior,\u201d International Conferenceon Learning Representations , Apr. 2018.[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words:Transformers for image recognition at scale,\u201d International Conferenceon Learning Representations , May. 2021.[19] D. Hendrycks and K. Gimpel, \u201cGaussian error linear units (gelus),\u201d arXivpreprint arXiv:1606.08415, Jun. 2016.[20] J. Redmon and A. Farhadi, \u201cYolov3: An incremental improvement,\u201darXiv preprint arXiv:1804.02767 , Apr. 2018.[21] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsingnetwork,\u201d Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 6230\u20136239, Jul. 2017.[22] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, \u201cImagenet: A large-scale hierarchical image database,\u201d Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition , pp. 248\u2013255, Sep. 2009.[23] G. Bjontegaard, \u201cCalculation of average psnr differences between rd-curves,\u201d Video Coding Experts Group - M33 , Apr. 2001.\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2211.09897v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 13:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #11, Phase: literature review, Feedback: 3\n--- Page 1 ---Ef\ufb01cient Feature Compression forEdge-Cloud SystemsZhihao DuanElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.duan90@purdue.eduFengqing ZhuElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.zhu0@purdue.eduAbstract\u2014Optimizing computation in an edge-cloud system isan important yet challenging problem. In this paper, we considera three way trade-off between bit rate, classi\ufb01cation accuracy,and encoding complexity in an edge-cloud image classi\ufb01cationsystem. Our method includes a new training strategy andan ef\ufb01cient encoder architecture to improve the rate-accuracyperformance. Our design can also be easily scaled accordingto different computation resources on the edge device, taking astep towards achieving rate-accuracy-complexity (RAC) trade-off.Under various settings, our feature coding system consistentlyoutperforms previous methods in terms of the RAC performance.Code is made publicly available 1.Index Terms\u2014Feature compression, Coding for machine vision,Image classi\ufb01cation, Edge-cloud systemI. I NTRODUCTIONMobile devices and wearable sensors are essential gatewaysto many autonomous systems. These edge devices are oftendeployed with vision tasks in applications involving robotics,drones, surveillance cameras, and smart home devices. How-ever, due to constraints on the device size and power supply,edge devices cannot support the real-time execution of neuralnetworks-based computer vision methods, which are typicallycomputationally expensive. A common solution is to takeadvantage of the wireless connection to a powerful cloudcomputing resource, forming an edge-cloud system [1], [2]as illustrated in Fig. 1. In such a system, we obtain data (inour context, images) from edge device sensors (cameras) andsend it to the cloud for task execution.Designing practical methods for edge-cloud systems ischallenging from several aspects. First, in most cases wherethe edge-cloud connection is band-limited, we have constraintson the transmission bit rate, requiring the captured images tobe compressed by an encoder on the edge as ef\ufb01ciently as pos-sible. Second, we also have constraints on the computationalcapacity of the edge device, and thus the encoder cannot betoo complex. Finally, for common vision tasks such as theimage classi\ufb01cation, we also desire high accuracy. Taking allthese into consideration, we face a three-way trade-off betweenencoder complexity, bit rate, and vision task accuracy, whichWe term as the rate-accuracy-complexity (RAC) trade-off.1github.com/duanzhiihao/edge-cloud-racEdge device(sender)Cloud computing(receiver)Bits \u201ccat\u201dRateEncoder complexityAccuracyFig. 1: Illustration of an edge-cloud machine vision system,where three objectives are of interest: encoder computationalcomplexity on the edge device, bit rate in transmission, andtask accuracy in the cloud. In this paper, we present a featurecompression method for the edge-cloud image classi\ufb01cation.A straightforward baseline for RAC trade-off is to com-press and transmit images using different codecs. However,image codecs are typically developed for image reconstruction,making them suboptimal for vision task execution. Recently,research focused on feature compression has gained attention,which can be optimized for different vision tasks [1], [3].Another advantage to transmit features rather than images isprivacy protection. By transmitting features, one could skippixel reconstruction on the cloud and thus reduce potentiallyexposing image contents to the attackers [4].Entropic Student [2] is a recently proposed feature com-pression framework that provides promising rate-accuracyperformance. However, it does not take into account theencoder complexity as part of the performance trade-off. Toextend the scope to the three-way RAC trade-off, we proposea \ufb02exible encoder whose complexity can be easily adjustedfor different device capacities. Together with a new trainingstrategy, we achieve better RAC performance compared toprevious methods, including image coding baselines and thestate-of-the-art Entropic Student model.Our contributions can be summarized as follows:\u2022 We expand the scope of previous feature compressionsystems to include the RAC trade-off.\u2022 We design a \ufb02exible neural network architecture forfeature coding, which is both ef\ufb01cient and achieves goodclassi\ufb01cation performance.\u2022 We conduct extensive experiments to verify the effec-tiveness of our approach, showing its advantages overprevious methods.arXiv:2211.09897v1  [eess.IV]  17 Nov 2022--- Page 2 ---II. R ELATED WORKA. Image CompressionTraditional image coders . Traditional image codecs typi-cally follow a linear transform coding paradigm, such as block-based discrete cosine transform (DCT) and discrete wavelettransform (DWT). Representative codecs include JPEG, WebP,and BPG (i.e., HEVC intra [5]). Their encoding complexityvaries depending on different factors such as the choice of theentropy coding algorithms, number of prediction modes, andhow much optimization is done during training. In general,more complex computation devoted at the encoder leads tobetter compression performance.Learned image coders . With the rise of deep learning,learned image coders (LICs) extend the traditional lineartransform coding paradigm to a non-linear fashion, which isachieved by transforming images using neural networks [6].Despite improved compression performance compared to tra-ditional codecs, LICs typically require more computationalresources due to highly parameterized neural network layersand autoregressive entropy models [7].The paradigm of image coding share a common disadvan-tage: they are designed for image reconstruction and may besuboptimal for vision tasks in most edge-cloud settings.B. Feature compressionVarious methods have been proposed to compress andtransmit features rather than images for machine vision.Hand-crafted feature coding. Earlier works have exploredcompressing deep features using hand-crafted algorithms. Choiand Baji \xb4c use a block-based intra and inter coding algorithmfor video feature compression in [8] and a HEVC-based fea-ture compression method for object detection in [9]. Followingtheir works, methods have been proposed for feature compres-sion using pruning [10], quantization [11], and bidirectionalprediction [10].Learned feature compression . Instead of manually de-signed methods, recent work has shifted attention to end-to-end learned feature compression and showed promisingresults. Singh et al. use a fully factorized entropy model toconstrain the entropy of features, which results in improvedrate-accuracy trade-off over previous methods [3]. Dubois etal. show that, given enough computational capacity, featurescan be extremely (around 1,000\xd7) compressed without affect-ing the vision task accuracy [12]. More recently, Matsubaraet al. train vision models with feature compressor jointly andshow state-of-the-art rate-accuracy performance [2].While existing methods mostly focus on improving therate-accuracy performance, their approach lacks scalabilityfor different encoder complexity. In this work, we extendthe scope to rate-accuracy-complexity (RAC) trade-off byproposing a easily scalable encoder architecture, which can beadapted to edge devices with different computation capacities.III. P RELIMINARIESThe current best method in terms of rate-accuracy perfor-mance, Entropic Student [2], is a good starting point for our\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43f\u0ddc\U0001d466\u0ddc\U0001d466teachLabel \U0001d466Conv5x5, s=2GDNConv5x5, s=2GDNConv2x2, s=1Conv2x2, s=1Inv. GDNConv2x2, s=1Inv. GDNConv2x2, s=1DecoderEncoder ClassifierEntropy modelPre-trained image classifierStage 1 Stage 2(a) Training overview. GDN refers to the generalized divisible nor-malization layer [13], the entropy model is fully factorized, and theclassi\ufb01er is ResNet-50 [14]. See text for description.Edge Cloud\u0ddc\U0001d466Enc. Dec.ClassifierEntropy modelrANS rANSBitsEntropy model(b) Testing (deployment). Entropy coding is done by the range-basedvariant of asymmetric numeral systems (rANS) [15].Fig. 2: Model training (a) and deployment (b) in the EntropicStudent framework [2] for feature compression.proposed work. In this section, we brie\ufb02y summarize it toprovide the background of our method, and we refer interestedreaders to the original paper [2] for more details.The framework for classi\ufb01cation is shown in Fig. 2. Forsplit computing, a feature encoder is designed to be deployedon the edge device, and a decoder together with a classi\ufb01er areexecuted on the cloud. Speci\ufb01cally, a ResNet-50 [14] classi\ufb01eris adopted for image classi\ufb01cation.Training is done in two stages. In the \ufb01rst stage, theclassi\ufb01er is initialized using a pre-trained model and \ufb01xed(i.e., no parameter updating), and the encoder and decoder arelearned to compress images and reconstruct desired features.In the second stage, the encoder is \ufb01xed, while the decoderand the classi\ufb01er are trained to optimize the task accuracy.The training loss functions are:Lstage1 = \u03bblR + lMSE,Lstage2 = \u03bblR + 0.5 \xb7(lKL + lCE), (1)where lR is the bit rate loss estimated by a factorized entropymodel [6], lMSE is the mean squared error (MSE) betweenthe classi\ufb01er\u2019s intermediate features and the ones from a pre-trained teacher model, lKL is the KL divergence loss forknowledge distillation [16], lCE is the standard cross-entropyloss for classi\ufb01cation, and \u03bb is a scalar hyperparameter totrade-off between rate and classi\ufb01cation accuracy.The deployment scenario is shown in Fig. 2b, where theentropy model is shared between the sender (edge) and thereceiver (cloud) for entropy coding, implemented by the range-based asymmetric numeral system (rANS) algorithm [15].Despite its strong rate-accuracy performance [2], the En-tropic Student model only operates at a single encoder com-plexity and is not feasible for RAC trade-off. In the nextsection, we introduce our method, which not only improves therate-accuracy performance, but also enables the RAC trade-off.--- Page 3 ---\u0ddc\U0001d466\u0ddc\U0001d466tLabel \U0001d466Enc. Dec. ClassifierPre-trained Classifier\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43fEntropy modelLearned or fixed(a) Training overview.Conv 8x8, s=8Res. BlockRes. Block\U0001d441 blocks\U0001d467 (b) Encoder architecture.GELUConv 3x3, s=1GELUConv 3x3, s=1 (c) Residual block.GELUConvT5x5Conv 3x3, s=1GELUConv 3x3, s=1GELUConv 1x1, s=1\U0001d467 (d) Decoder architecture.Fig. 3: Summary of proposed method. In (a), we show the training strategy, where the entropy model is fully factorized, andgray blocks indicate frozen models (i.e., no parameter update during training). The classi\ufb01er can either be trained jointly withthe system, or initialized with a pre-trained model and \ufb01xed throughout training. In (b) and (c), we show our encoder anddecoder architecture, respectively. Our encoder network complexity can be tuned by adjusting the number of residual blocks,N. For the classi\ufb01er, we use a truncated version of ResNet-50, following [2].IV. O UR APPROACHInspired by the success of knowledge distillation in the En-tropic Student model, we adopt a similar framework and lossfunctions, shown in Fig. 3a. However, we make modi\ufb01cationsincluding a new training strategy and network architecture, bywhich we are able to realize the RAC trade-off.A. Single-stage TrainingThe Entropic Student method adopts a two-stage trainingstrategy, where the encoder and decoder are trained in the\ufb01rst stage, and the decoder and classi\ufb01er are \ufb01ne-tuned in thesecond stage. We note that this decoupled training strategyrequires more hyperparameter searching, and it is also sub-optimal for rate-accuracy performance since network compo-nents are trained separately. Instead, we propose a simple \ufb01x,i.e., to train the encoder, decoder, and classi\ufb01er in an end-to-end fashion. Our single-stage loss function is as follows:Lsingle = \u03bblR + lMSE + 0.5 \xb7(lKL + lCE), (2)where the terms are de\ufb01ned in Sec. III. We keep the samescaling factor for each loss term as in the Entropic Studentwithout further tuning. Later in Sec. V-D, we show thatour single-stage strategy improves the rate-accuracy trade-offdespite being simpler to implement.B. Network ArchitectureWe introduce an encoder network architecture speci\ufb01callydesigned for ef\ufb01cient and \ufb02exible feature compression, shownin Fig. 3b. We now describe our design choices in details.Downsampling layer. We \ufb01rst noticed that the 5x5 con-volutions for downsampling in previous works (e.g., in theEntropic Student [2] and learned image coders [6], [17]) arecomputationally inef\ufb01cient due to the large overlap betweenconvolutional kernels. To remove this overlap, we adopt asingle patch embedding [18] layer for downsampling, whichcan be equivalently viewed as a convolution with kernel sizeequal to its strides.Network blocks. Instead of feedforward convolutions asin previous works, we adopt residual blocks in our featureencoder, which enables easy adjustment of the encoder com-plexity simply by setting the number of residual blocks, N. Alarger N (i.e., more residual blocks in the encoder) improvesthe rate-accuracy performance while results in higher encodercomplexity. Thus by tuning N, we can trade-off between therate-accuracy performance and the encoding complexity of oursystem. We use a standard design for the residual blocks asshown in Fig. 3c. For non-linearity layers, we use the GeLUactivation [19] instead of GDN [13], as we empirically foundthat the former leads to better performance with the samecomputational complexity.Downsampling ratio. Our \ufb01nal step is to use 8\xd7downsam-pling, instead of the 4\xd7downsampling in Entropic Student,during encoding. Our choice is motivated by the observationthat 8\xd7downsampling is a common choice for various imageprocessing tasks (e.g., in traditional codecs like JPEG) andcomputer vision tasks (e.g., in image object detection [20]and semantic segmentation [21]). Note that in our featuredecoder on the cloud (Fig. 3d), we upsample the feature by2\xd7to recover spatial details of the original image. With thiscon\ufb01guration of the encoder and the decoder, we are able toreduce the encoding latency and improve the average accuracyby a large margin, as discussed later in Sec. V-D.C. Feature Compression for a Fixed Classi\ufb01erSo far, we have considered the joint training setting, wherethe classi\ufb01er and our feature coder are trained jointly. Inaddition, we also examine the setting in which a pre-trainedclassi\ufb01er is available but \ufb01xed, illustrated in Fig. 3a. Thisscenario may happen in practice where there are different edgedevices sharing a single cloud vision model. The edge devicesmay use different feature encoders/decoders according to theircomputational capacity, but the vision model on the cloudshould be \ufb01xed. In this case, we use the same loss functionas in Eq. 2, but we train the encoder and decoder networkswithout updating the classi\ufb01er. We denote the methods in thissetting by cls. \ufb01xed to distinguish them from the joint trainingsetting, where the classi\ufb01er can be trained together with theencoder and the decoder.V. E XPERIMENTSWe compare our method to previous ones for edge-cloudimage classi\ufb01cation. For fair comparison, all methods use theResNet-50 [14] (for image coding methods) or a truncated--- Page 4 ---version of ResNet-50 (for feature coding methods) as theclassi\ufb01er.A. Dataset and MetricsWe use the 1,000-class ImageNet dataset [22] for trainingand testing. Standard data augmentations, including randomresizing and cropping, are applied during training. We use theImageNet val set for testing, in which we resize and center-crop all images such that they have a resolution of 224 \xd7224.All metrics are computed on the 224 \xd7224 val set. Bit rateis measured in terms of bits per pixel (bpp). For classi\ufb01cationaccuracy, we report the top-1 accuracy of the classi\ufb01er predic-tions. For encoding complexity, we focus on the CPU latencysince it well simulates real-world scenarios. All encodinglatencies are measured using an Intel 12700K CPU, averagedover the \ufb01rst 5,000 images in the val set.We also introduce a new metric, Delta-accuracy, whichcomputes the average accuracy improvement (w.r.t. a baselinemethod) over all bit rates. By using this metric, we cansummarize each rate-accuracy curve into a scalar, thus beingable to represent the three-way RAC trade-off in a 2-D plot.Our computation of Delta-accuracy follows the convention ofBD-PSNR and BD-rate [23] except one aspect. Unlike PSNR,the unit of accuracy is percentage (not decibel), so we computeaverage accuracy improvement over the normal bit rate space(instead of the logarithmic rate scale used in BD-PSNR).B. Experiment SettingsWe consider the following baseline methods.Image coding baselines. We compress images using animage codec at the sender side, and we reconstruct images atthe receiver side using the same codec. Then, we run ResNet-50 on the reconstructed images to infer class prediction. Ourselected codecs include handcrafted (e.g., WebP and BPG [5])and learning-based methods (e.g., the Factorized model [6],and the Hyperprior model [17]).Feature coding baseline. We use the existing best method,Entropic Student [2], as a powerful baseline.As we discussed in Sec. IV-C, we group methods into twogroups so that methods within each can be fairly compared.Classi\ufb01er \ufb01xed setting: The pre-trained ResNet-50 is usedand remains \ufb01xed (i.e., no \ufb01ne-tuning). The image codingbaselines all fall in this setting. We label the methods in thissetting as cls. \ufb01xed.Joint training setting: The pre-trained ResNet-50 is jointlytrained with the feature encoder and/or decoder. The EntropicStudent model falls in this setting. We label the methods inthis setting as joint.C. Main Results for RAC Trade-offTo benchmark our method against the baselines in terms ofRAC performance, we \ufb01rst measure the rate-accuracy curvesfor all baselines and our methods. Then, we compute the Delta-accuracy of each method w.r.t. the Entropic Student baseline(whose Delta-accuracy is 0 by de\ufb01nition). We show the Delta-accuracy as a function of encoding latency in Fig. 4. Note that3 4 7 10 14 20 30 40 50Encoding latency on CPU (ms)121086420Delta-accuracy (%)RAC performance on ImageNetFeature coding: ours (joint)Feature coding: Entropic Student [2] (joint)Feature coding: ours (cls. fixed)Image coding: BPG [5] (cls. fixed)Image coding: Hyperpiror model [17] (cls. fixed)Image coding: Factorized model [6] (cls. fixed)Image coding: WebP (cls. fixed)Fig. 4: RAC trade-off, visualized by Delta-accuracy vs en-coding complexity. The Delta-accuracy is w.r.t. the EntropicStudent baseline.the Entropic Student [2] corresponds to a single point on thecomplexity vs. Delta-accuracy plot since its complexity cannotbe tuned. Similarly, the Factorized model [6] and Hyperpriormodel [17] also produce a single point on the this plot. ForWebP and BPG [5] codec, we control the encoding latency bysetting the compression speed option provided by the codecimplementation. For our method, we control the latency bychoosing N \u2208{0,4,8}, where N is the number of residualblocks in the encoder.From Fig. 4, we observe that for classi\ufb01er \ufb01xed methods,our approach clearly outperforms the image coding baselinesby a large margin, demonstrating the effectiveness of featurecoding. This improvement is expected, because our featureencoder and decoder are explicitly trained to maximize classi-\ufb01cation accuracy, instead of pixel reconstruction as in imagecoding baselines. For the joint training setting, our methodachieves higher Delta-accuracy than the Entropic Student base-line with approximately only half of the encoding latency. Inaddition, the \ufb02exible design of our encoder enables encodingcomplexity adjustment, as opposed to the Entropic Student, bywhich only a single complexity can be achieved.D. Ablation StudyTo analyze how each component of our method contributesto the RAC improvement, we start from the Entropic Studentbaseline, and we include our proposed components one ata time. At each step, we show the change of rate-accuracyperformance as well as the encoding complexity.We \ufb01rst compare our single-stage strategy (referred to ascon\ufb01guration A) with the original two-stage one in Fig. 5. InTable I, we also show the encoding complexity and Delta-accuracy. We observe that the single-stage training does notimpair the performance; instead, the average accuracy slightlyincreases by 0.13% with encoding complexity unchanged.Then, we replace the Entropic Student encoder by ourpatch embedding layer and residual blocks, and we show the--- Page 5 ---0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2Bits per pixel (bpp)57.560.062.565.067.570.072.575.0T op-1 acc. (%)Rate-accuracy trade-off on ImageNetOurs C: B + 8x downsamplingOurs B: A + improved architectureOurs A: Baseline + single-stage trainingBaseline: Entropic StudentFig. 5: Ablation study of our approach compared to the En-tropic Student baseline. Trained and tested on ImageNet [22].TABLE I: Ablation study. FLOPs and encoding latency arefor 224x224 resolution inputs. All percentages are w.r.t. to thebaseline, Entropic Student.FLOPs\u2193 Enc. Latency\u2193 Delta-acc.\u2191Baseline: Entropic Student 0.466B 13.8ms 0.0%A: Baseline + single-stage 0.466B (-0.0%) 13.8ms (-0.0%) +0.13%B: A + improved architecture 0.457B (-0.02%) 9.10ms (-34.1%) +0.20%C: B + 8x downsampling 0.489B (+0.05%) 6.70ms (-51.4%) +0.93%resulting system as con\ufb01guration B in Fig. 5 and Table I. Notethat the downsampling ratio ( 4\xd7) is unchanged. We can seethat the improved architecture reduces the encoding latencyby more than 30% and improves the average accuracy by anadditional small percentage ( 0.07%).Finally, we increase the downsampling ratio to 8\xd7 aswe discussed in Sec. IV-B, and we refer to this version ascon\ufb01guration C . By increasing the downsampling ratio, weagain reduce the encoding latency and improve the averageaccuracy by an additional large margin, after which we achievean overall latency reduction of 50% and an average accuracyimprovement of 0.93%. From Fig. 5, we notice that theimprovement is most signi\ufb01cant at lower bit rates. We con-jecture that lower bit rates naturally call for a more compactcompressed representation, which can be achieved by moreaggressive downsampling. For higher bit rate (e.g., more than0.8 bpp), our approach remains on par with the baseline.VI. C ONCLUSIONIn this paper, we present a feature coding system for imageclassi\ufb01cation in edge-cloud systems. We show that by properdesign of the network architecture, existing feature codingmethods can be improved and extended to consider the RACtrade-off. Our system outperforms all previous methods interms of RAC trade-off, which we believe is valuable formany real-world applications deployment. A limitation of oursystem is the requirement of a separate neural network modelfor every rate-complexity pair, which we leave to our futurework to solve.REFERENCES[1] H. Choi and I. V . Baji \xb4c, \u201cDeep feature compression for collaborativeobject detection,\u201d IEEE International Conference on Image Processing ,pp. 3743\u20133747, Oct. 2018.[2] Y . Matsubara, R. Yang, M. Levorato, and S. Mandt, \u201cSupervised com-pression for resource-constrained edge computing systems,\u201d IEEE/CVFWinter Conference on Applications of Computer Vision , pp. 923\u2013933,Jan. 2022.[3] S. Singh, S. Abu-El-Haija, N. Johnston, J. Ball \xb4e, A. Shrivastava, andG. Toderici, \u201cEnd-to-end learning of compressible features,\u201d IEEEInternational Conference on Image Processing , pp. 3349\u20133353, Oct2020.[4] V . Kakkad, M. Patel, and M. Shah, \u201cBiometric authentication and imageencryption for image security in cloud framework,\u201d Multiscale andMultidisciplinary Modeling, Experiments and Design , vol. 2, no. 4, pp.233\u2013248, May 2019.[5] J. Lainema, F. Bossen, W. Han, J. Min, and K. Ugur, \u201cIntra coding of thehevc standard,\u201d IEEE Transactions on Circuits and Systems for VideoTechnology, vol. 22, no. 12, pp. 1792\u20131801, Dec. 2012.[6] J. Ball \xb4e, V . Laparra, and E. Simoncelli, \u201cEnd-to-end optimizationof nonlinear transform codes for perceptual quality,\u201d Picture CodingSymposium, pp. 1\u20135, Dec. 2016.[7] D. Minnen, J. Ball \xb4e, and G. Toderici, \u201cJoint autoregressive and hier-archical priors for learned image compression,\u201d Advances in NeuralInformation Processing Systems, vol. 31, pp. 10 794\u201310 803, Dec. 2018.[8] H. Choi and I. V . Baji \xb4c, \u201cNear-lossless deep feature compression forcollaborative intelligence,\u201d IEEE International Workshop on MultimediaSignal Processing, pp. 1\u20136, Aug. 2018.[9] \u2014\u2014, \u201cDeep feature compression for collaborative object detection,\u201dIEEE International Conference on Image Processing , pp. 3743\u20133747,Oct. 2018.[10] H. Choi, R. A. Cohen, and I. V . Baji \xb4c, \u201cBack-and-forth prediction fordeep tensor compression,\u201d IEEE International Conference on Acoustics,Speech and Signal Processing , pp. 4467\u20134471, May 2020.[11] R. A. Cohen, H. Choi, and I. V . Baji \xb4c, \u201cLightweight compressionof neural network feature tensors for collaborative intelligence,\u201d IEEEInternational Conference on Multimedia and Expo , pp. 1\u20136, Jul. 2020.[12] Y . Dubois, B. Bloem-Reddy, K. Ullrich, and C. J. Maddison, \u201cLossycompression for lossless prediction,\u201d Advances in Neural InformationProcessing Systems, vol. 34, pp. 14 014\u201314 028, Dec. 2021.[13] J. Ball \xb4e, V . Laparra, and E. P. Simoncelli, \u201cDensity modeling ofimages using a generalized normalization transformation,\u201d InternationalConference on Learning Representations , Jul. 2016.[14] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for imagerecognition,\u201d Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 770\u2013778, Jun. 2016.[15] J. Duda, \u201cAsymmetric numeral systems: entropy coding combiningspeed of huffman coding with compression rate of arithmetic coding,\u201darXiv preprint arXiv:1311.2540 , Dec. 2013.[16] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neuralnetwork,\u201d https://arxiv.org/abs/1503.02531, Mar. 2015.[17] J. Ball \xb4e, D. Minnen, S. Singh, S. Hwang, and N. Johnston, \u201cVariationalimage compression with a scale hyperprior,\u201d International Conferenceon Learning Representations , Apr. 2018.[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words:Transformers for image recognition at scale,\u201d International Conferenceon Learning Representations , May. 2021.[19] D. Hendrycks and K. Gimpel, \u201cGaussian error linear units (gelus),\u201d arXivpreprint arXiv:1606.08415, Jun. 2016.[20] J. Redmon and A. Farhadi, \u201cYolov3: An incremental improvement,\u201darXiv preprint arXiv:1804.02767 , Apr. 2018.[21] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsingnetwork,\u201d Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 6230\u20136239, Jul. 2017.[22] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, \u201cImagenet: A large-scale hierarchical image database,\u201d Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition , pp. 248\u2013255, Sep. 2009.[23] G. Bjontegaard, \u201cCalculation of average psnr differences between rd-curves,\u201d Video Coding Experts Group - M33 , Apr. 2001., Your response: ```ADD_PAPER\n2211.09897v1\nThis paper presents an efficient feature compression method for edge-cloud image classification systems, addressing the critical rate-accuracy-complexity (RAC) trade-off. The authors propose a flexible encoder architecture with adjustable residual blocks that can scale to different edge device capabilities while maintaining classification accuracy. Key innovations include a single-stage training strategy that outperforms previous two-stage approaches, and an 8\xd7 downsampling scheme that reduces encoding latency by 51.4% while improving accuracy by 0.93%. Experimental results on ImageNet demonstrate superior RAC performance compared to both traditional image codecs and state-of-the-art feature compression methods, with particular advantages at lower bit rates. The system achieves this through optimized network design choices including patch embedding layers and GeLU activations, making it particularly suitable for resource-constrained edge deployments where computational efficiency is crucial.\n```\n~~~~~~~~~~\nCurrent Step #12, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2211.09897v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2211.09897v1\nThis paper presents an efficient feature compression method for edge-cloud image classification systems, addressing the critical rate-accuracy-complexity (RAC) trade-off. The authors propose a flexible encoder architecture with adjustable residual blocks that can scale to different edge device capabilities while maintaining classification accuracy. Key innovations include a single-stage training strategy that outperforms previous two-stage approaches, and an 8\xd7 downsampling scheme that reduces encoding latency by 51.4% while improving accuracy by 0.93%. Experimental results on ImageNet demonstrate superior RAC performance compared to both traditional image codecs and state-of-the-art feature compression methods, with particular advantages at lower bit rates. The system achieves this through optimized network design choices including patch embedding layers and GeLU activations, making it particularly suitable for resource-constrained edge deployments where computational efficiency is crucial.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 14:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #11, Phase: literature review, Feedback: 3\n--- Page 1 ---Ef\ufb01cient Feature Compression forEdge-Cloud SystemsZhihao DuanElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.duan90@purdue.eduFengqing ZhuElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.zhu0@purdue.eduAbstract\u2014Optimizing computation in an edge-cloud system isan important yet challenging problem. In this paper, we considera three way trade-off between bit rate, classi\ufb01cation accuracy,and encoding complexity in an edge-cloud image classi\ufb01cationsystem. Our method includes a new training strategy andan ef\ufb01cient encoder architecture to improve the rate-accuracyperformance. Our design can also be easily scaled accordingto different computation resources on the edge device, taking astep towards achieving rate-accuracy-complexity (RAC) trade-off.Under various settings, our feature coding system consistentlyoutperforms previous methods in terms of the RAC performance.Code is made publicly available 1.Index Terms\u2014Feature compression, Coding for machine vision,Image classi\ufb01cation, Edge-cloud systemI. I NTRODUCTIONMobile devices and wearable sensors are essential gatewaysto many autonomous systems. These edge devices are oftendeployed with vision tasks in applications involving robotics,drones, surveillance cameras, and smart home devices. How-ever, due to constraints on the device size and power supply,edge devices cannot support the real-time execution of neuralnetworks-based computer vision methods, which are typicallycomputationally expensive. A common solution is to takeadvantage of the wireless connection to a powerful cloudcomputing resource, forming an edge-cloud system [1], [2]as illustrated in Fig. 1. In such a system, we obtain data (inour context, images) from edge device sensors (cameras) andsend it to the cloud for task execution.Designing practical methods for edge-cloud systems ischallenging from several aspects. First, in most cases wherethe edge-cloud connection is band-limited, we have constraintson the transmission bit rate, requiring the captured images tobe compressed by an encoder on the edge as ef\ufb01ciently as pos-sible. Second, we also have constraints on the computationalcapacity of the edge device, and thus the encoder cannot betoo complex. Finally, for common vision tasks such as theimage classi\ufb01cation, we also desire high accuracy. Taking allthese into consideration, we face a three-way trade-off betweenencoder complexity, bit rate, and vision task accuracy, whichWe term as the rate-accuracy-complexity (RAC) trade-off.1github.com/duanzhiihao/edge-cloud-racEdge device(sender)Cloud computing(receiver)Bits \u201ccat\u201dRateEncoder complexityAccuracyFig. 1: Illustration of an edge-cloud machine vision system,where three objectives are of interest: encoder computationalcomplexity on the edge device, bit rate in transmission, andtask accuracy in the cloud. In this paper, we present a featurecompression method for the edge-cloud image classi\ufb01cation.A straightforward baseline for RAC trade-off is to com-press and transmit images using different codecs. However,image codecs are typically developed for image reconstruction,making them suboptimal for vision task execution. Recently,research focused on feature compression has gained attention,which can be optimized for different vision tasks [1], [3].Another advantage to transmit features rather than images isprivacy protection. By transmitting features, one could skippixel reconstruction on the cloud and thus reduce potentiallyexposing image contents to the attackers [4].Entropic Student [2] is a recently proposed feature com-pression framework that provides promising rate-accuracyperformance. However, it does not take into account theencoder complexity as part of the performance trade-off. Toextend the scope to the three-way RAC trade-off, we proposea \ufb02exible encoder whose complexity can be easily adjustedfor different device capacities. Together with a new trainingstrategy, we achieve better RAC performance compared toprevious methods, including image coding baselines and thestate-of-the-art Entropic Student model.Our contributions can be summarized as follows:\u2022 We expand the scope of previous feature compressionsystems to include the RAC trade-off.\u2022 We design a \ufb02exible neural network architecture forfeature coding, which is both ef\ufb01cient and achieves goodclassi\ufb01cation performance.\u2022 We conduct extensive experiments to verify the effec-tiveness of our approach, showing its advantages overprevious methods.arXiv:2211.09897v1  [eess.IV]  17 Nov 2022--- Page 2 ---II. R ELATED WORKA. Image CompressionTraditional image coders . Traditional image codecs typi-cally follow a linear transform coding paradigm, such as block-based discrete cosine transform (DCT) and discrete wavelettransform (DWT). Representative codecs include JPEG, WebP,and BPG (i.e., HEVC intra [5]). Their encoding complexityvaries depending on different factors such as the choice of theentropy coding algorithms, number of prediction modes, andhow much optimization is done during training. In general,more complex computation devoted at the encoder leads tobetter compression performance.Learned image coders . With the rise of deep learning,learned image coders (LICs) extend the traditional lineartransform coding paradigm to a non-linear fashion, which isachieved by transforming images using neural networks [6].Despite improved compression performance compared to tra-ditional codecs, LICs typically require more computationalresources due to highly parameterized neural network layersand autoregressive entropy models [7].The paradigm of image coding share a common disadvan-tage: they are designed for image reconstruction and may besuboptimal for vision tasks in most edge-cloud settings.B. Feature compressionVarious methods have been proposed to compress andtransmit features rather than images for machine vision.Hand-crafted feature coding. Earlier works have exploredcompressing deep features using hand-crafted algorithms. Choiand Baji \xb4c use a block-based intra and inter coding algorithmfor video feature compression in [8] and a HEVC-based fea-ture compression method for object detection in [9]. Followingtheir works, methods have been proposed for feature compres-sion using pruning [10], quantization [11], and bidirectionalprediction [10].Learned feature compression . Instead of manually de-signed methods, recent work has shifted attention to end-to-end learned feature compression and showed promisingresults. Singh et al. use a fully factorized entropy model toconstrain the entropy of features, which results in improvedrate-accuracy trade-off over previous methods [3]. Dubois etal. show that, given enough computational capacity, featurescan be extremely (around 1,000\xd7) compressed without affect-ing the vision task accuracy [12]. More recently, Matsubaraet al. train vision models with feature compressor jointly andshow state-of-the-art rate-accuracy performance [2].While existing methods mostly focus on improving therate-accuracy performance, their approach lacks scalabilityfor different encoder complexity. In this work, we extendthe scope to rate-accuracy-complexity (RAC) trade-off byproposing a easily scalable encoder architecture, which can beadapted to edge devices with different computation capacities.III. P RELIMINARIESThe current best method in terms of rate-accuracy perfor-mance, Entropic Student [2], is a good starting point for our\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43f\u0ddc\U0001d466\u0ddc\U0001d466teachLabel \U0001d466Conv5x5, s=2GDNConv5x5, s=2GDNConv2x2, s=1Conv2x2, s=1Inv. GDNConv2x2, s=1Inv. GDNConv2x2, s=1DecoderEncoder ClassifierEntropy modelPre-trained image classifierStage 1 Stage 2(a) Training overview. GDN refers to the generalized divisible nor-malization layer [13], the entropy model is fully factorized, and theclassi\ufb01er is ResNet-50 [14]. See text for description.Edge Cloud\u0ddc\U0001d466Enc. Dec.ClassifierEntropy modelrANS rANSBitsEntropy model(b) Testing (deployment). Entropy coding is done by the range-basedvariant of asymmetric numeral systems (rANS) [15].Fig. 2: Model training (a) and deployment (b) in the EntropicStudent framework [2] for feature compression.proposed work. In this section, we brie\ufb02y summarize it toprovide the background of our method, and we refer interestedreaders to the original paper [2] for more details.The framework for classi\ufb01cation is shown in Fig. 2. Forsplit computing, a feature encoder is designed to be deployedon the edge device, and a decoder together with a classi\ufb01er areexecuted on the cloud. Speci\ufb01cally, a ResNet-50 [14] classi\ufb01eris adopted for image classi\ufb01cation.Training is done in two stages. In the \ufb01rst stage, theclassi\ufb01er is initialized using a pre-trained model and \ufb01xed(i.e., no parameter updating), and the encoder and decoder arelearned to compress images and reconstruct desired features.In the second stage, the encoder is \ufb01xed, while the decoderand the classi\ufb01er are trained to optimize the task accuracy.The training loss functions are:Lstage1 = \u03bblR + lMSE,Lstage2 = \u03bblR + 0.5 \xb7(lKL + lCE), (1)where lR is the bit rate loss estimated by a factorized entropymodel [6], lMSE is the mean squared error (MSE) betweenthe classi\ufb01er\u2019s intermediate features and the ones from a pre-trained teacher model, lKL is the KL divergence loss forknowledge distillation [16], lCE is the standard cross-entropyloss for classi\ufb01cation, and \u03bb is a scalar hyperparameter totrade-off between rate and classi\ufb01cation accuracy.The deployment scenario is shown in Fig. 2b, where theentropy model is shared between the sender (edge) and thereceiver (cloud) for entropy coding, implemented by the range-based asymmetric numeral system (rANS) algorithm [15].Despite its strong rate-accuracy performance [2], the En-tropic Student model only operates at a single encoder com-plexity and is not feasible for RAC trade-off. In the nextsection, we introduce our method, which not only improves therate-accuracy performance, but also enables the RAC trade-off.--- Page 3 ---\u0ddc\U0001d466\u0ddc\U0001d466tLabel \U0001d466Enc. Dec. ClassifierPre-trained Classifier\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43fEntropy modelLearned or fixed(a) Training overview.Conv 8x8, s=8Res. BlockRes. Block\U0001d441 blocks\U0001d467 (b) Encoder architecture.GELUConv 3x3, s=1GELUConv 3x3, s=1 (c) Residual block.GELUConvT5x5Conv 3x3, s=1GELUConv 3x3, s=1GELUConv 1x1, s=1\U0001d467 (d) Decoder architecture.Fig. 3: Summary of proposed method. In (a), we show the training strategy, where the entropy model is fully factorized, andgray blocks indicate frozen models (i.e., no parameter update during training). The classi\ufb01er can either be trained jointly withthe system, or initialized with a pre-trained model and \ufb01xed throughout training. In (b) and (c), we show our encoder anddecoder architecture, respectively. Our encoder network complexity can be tuned by adjusting the number of residual blocks,N. For the classi\ufb01er, we use a truncated version of ResNet-50, following [2].IV. O UR APPROACHInspired by the success of knowledge distillation in the En-tropic Student model, we adopt a similar framework and lossfunctions, shown in Fig. 3a. However, we make modi\ufb01cationsincluding a new training strategy and network architecture, bywhich we are able to realize the RAC trade-off.A. Single-stage TrainingThe Entropic Student method adopts a two-stage trainingstrategy, where the encoder and decoder are trained in the\ufb01rst stage, and the decoder and classi\ufb01er are \ufb01ne-tuned in thesecond stage. We note that this decoupled training strategyrequires more hyperparameter searching, and it is also sub-optimal for rate-accuracy performance since network compo-nents are trained separately. Instead, we propose a simple \ufb01x,i.e., to train the encoder, decoder, and classi\ufb01er in an end-to-end fashion. Our single-stage loss function is as follows:Lsingle = \u03bblR + lMSE + 0.5 \xb7(lKL + lCE), (2)where the terms are de\ufb01ned in Sec. III. We keep the samescaling factor for each loss term as in the Entropic Studentwithout further tuning. Later in Sec. V-D, we show thatour single-stage strategy improves the rate-accuracy trade-offdespite being simpler to implement.B. Network ArchitectureWe introduce an encoder network architecture speci\ufb01callydesigned for ef\ufb01cient and \ufb02exible feature compression, shownin Fig. 3b. We now describe our design choices in details.Downsampling layer. We \ufb01rst noticed that the 5x5 con-volutions for downsampling in previous works (e.g., in theEntropic Student [2] and learned image coders [6], [17]) arecomputationally inef\ufb01cient due to the large overlap betweenconvolutional kernels. To remove this overlap, we adopt asingle patch embedding [18] layer for downsampling, whichcan be equivalently viewed as a convolution with kernel sizeequal to its strides.Network blocks. Instead of feedforward convolutions asin previous works, we adopt residual blocks in our featureencoder, which enables easy adjustment of the encoder com-plexity simply by setting the number of residual blocks, N. Alarger N (i.e., more residual blocks in the encoder) improvesthe rate-accuracy performance while results in higher encodercomplexity. Thus by tuning N, we can trade-off between therate-accuracy performance and the encoding complexity of oursystem. We use a standard design for the residual blocks asshown in Fig. 3c. For non-linearity layers, we use the GeLUactivation [19] instead of GDN [13], as we empirically foundthat the former leads to better performance with the samecomputational complexity.Downsampling ratio. Our \ufb01nal step is to use 8\xd7downsam-pling, instead of the 4\xd7downsampling in Entropic Student,during encoding. Our choice is motivated by the observationthat 8\xd7downsampling is a common choice for various imageprocessing tasks (e.g., in traditional codecs like JPEG) andcomputer vision tasks (e.g., in image object detection [20]and semantic segmentation [21]). Note that in our featuredecoder on the cloud (Fig. 3d), we upsample the feature by2\xd7to recover spatial details of the original image. With thiscon\ufb01guration of the encoder and the decoder, we are able toreduce the encoding latency and improve the average accuracyby a large margin, as discussed later in Sec. V-D.C. Feature Compression for a Fixed Classi\ufb01erSo far, we have considered the joint training setting, wherethe classi\ufb01er and our feature coder are trained jointly. Inaddition, we also examine the setting in which a pre-trainedclassi\ufb01er is available but \ufb01xed, illustrated in Fig. 3a. Thisscenario may happen in practice where there are different edgedevices sharing a single cloud vision model. The edge devicesmay use different feature encoders/decoders according to theircomputational capacity, but the vision model on the cloudshould be \ufb01xed. In this case, we use the same loss functionas in Eq. 2, but we train the encoder and decoder networkswithout updating the classi\ufb01er. We denote the methods in thissetting by cls. \ufb01xed to distinguish them from the joint trainingsetting, where the classi\ufb01er can be trained together with theencoder and the decoder.V. E XPERIMENTSWe compare our method to previous ones for edge-cloudimage classi\ufb01cation. For fair comparison, all methods use theResNet-50 [14] (for image coding methods) or a truncated--- Page 4 ---version of ResNet-50 (for feature coding methods) as theclassi\ufb01er.A. Dataset and MetricsWe use the 1,000-class ImageNet dataset [22] for trainingand testing. Standard data augmentations, including randomresizing and cropping, are applied during training. We use theImageNet val set for testing, in which we resize and center-crop all images such that they have a resolution of 224 \xd7224.All metrics are computed on the 224 \xd7224 val set. Bit rateis measured in terms of bits per pixel (bpp). For classi\ufb01cationaccuracy, we report the top-1 accuracy of the classi\ufb01er predic-tions. For encoding complexity, we focus on the CPU latencysince it well simulates real-world scenarios. All encodinglatencies are measured using an Intel 12700K CPU, averagedover the \ufb01rst 5,000 images in the val set.We also introduce a new metric, Delta-accuracy, whichcomputes the average accuracy improvement (w.r.t. a baselinemethod) over all bit rates. By using this metric, we cansummarize each rate-accuracy curve into a scalar, thus beingable to represent the three-way RAC trade-off in a 2-D plot.Our computation of Delta-accuracy follows the convention ofBD-PSNR and BD-rate [23] except one aspect. Unlike PSNR,the unit of accuracy is percentage (not decibel), so we computeaverage accuracy improvement over the normal bit rate space(instead of the logarithmic rate scale used in BD-PSNR).B. Experiment SettingsWe consider the following baseline methods.Image coding baselines. We compress images using animage codec at the sender side, and we reconstruct images atthe receiver side using the same codec. Then, we run ResNet-50 on the reconstructed images to infer class prediction. Ourselected codecs include handcrafted (e.g., WebP and BPG [5])and learning-based methods (e.g., the Factorized model [6],and the Hyperprior model [17]).Feature coding baseline. We use the existing best method,Entropic Student [2], as a powerful baseline.As we discussed in Sec. IV-C, we group methods into twogroups so that methods within each can be fairly compared.Classi\ufb01er \ufb01xed setting: The pre-trained ResNet-50 is usedand remains \ufb01xed (i.e., no \ufb01ne-tuning). The image codingbaselines all fall in this setting. We label the methods in thissetting as cls. \ufb01xed.Joint training setting: The pre-trained ResNet-50 is jointlytrained with the feature encoder and/or decoder. The EntropicStudent model falls in this setting. We label the methods inthis setting as joint.C. Main Results for RAC Trade-offTo benchmark our method against the baselines in terms ofRAC performance, we \ufb01rst measure the rate-accuracy curvesfor all baselines and our methods. Then, we compute the Delta-accuracy of each method w.r.t. the Entropic Student baseline(whose Delta-accuracy is 0 by de\ufb01nition). We show the Delta-accuracy as a function of encoding latency in Fig. 4. Note that3 4 7 10 14 20 30 40 50Encoding latency on CPU (ms)121086420Delta-accuracy (%)RAC performance on ImageNetFeature coding: ours (joint)Feature coding: Entropic Student [2] (joint)Feature coding: ours (cls. fixed)Image coding: BPG [5] (cls. fixed)Image coding: Hyperpiror model [17] (cls. fixed)Image coding: Factorized model [6] (cls. fixed)Image coding: WebP (cls. fixed)Fig. 4: RAC trade-off, visualized by Delta-accuracy vs en-coding complexity. The Delta-accuracy is w.r.t. the EntropicStudent baseline.the Entropic Student [2] corresponds to a single point on thecomplexity vs. Delta-accuracy plot since its complexity cannotbe tuned. Similarly, the Factorized model [6] and Hyperpriormodel [17] also produce a single point on the this plot. ForWebP and BPG [5] codec, we control the encoding latency bysetting the compression speed option provided by the codecimplementation. For our method, we control the latency bychoosing N \u2208{0,4,8}, where N is the number of residualblocks in the encoder.From Fig. 4, we observe that for classi\ufb01er \ufb01xed methods,our approach clearly outperforms the image coding baselinesby a large margin, demonstrating the effectiveness of featurecoding. This improvement is expected, because our featureencoder and decoder are explicitly trained to maximize classi-\ufb01cation accuracy, instead of pixel reconstruction as in imagecoding baselines. For the joint training setting, our methodachieves higher Delta-accuracy than the Entropic Student base-line with approximately only half of the encoding latency. Inaddition, the \ufb02exible design of our encoder enables encodingcomplexity adjustment, as opposed to the Entropic Student, bywhich only a single complexity can be achieved.D. Ablation StudyTo analyze how each component of our method contributesto the RAC improvement, we start from the Entropic Studentbaseline, and we include our proposed components one ata time. At each step, we show the change of rate-accuracyperformance as well as the encoding complexity.We \ufb01rst compare our single-stage strategy (referred to ascon\ufb01guration A) with the original two-stage one in Fig. 5. InTable I, we also show the encoding complexity and Delta-accuracy. We observe that the single-stage training does notimpair the performance; instead, the average accuracy slightlyincreases by 0.13% with encoding complexity unchanged.Then, we replace the Entropic Student encoder by ourpatch embedding layer and residual blocks, and we show the--- Page 5 ---0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2Bits per pixel (bpp)57.560.062.565.067.570.072.575.0T op-1 acc. (%)Rate-accuracy trade-off on ImageNetOurs C: B + 8x downsamplingOurs B: A + improved architectureOurs A: Baseline + single-stage trainingBaseline: Entropic StudentFig. 5: Ablation study of our approach compared to the En-tropic Student baseline. Trained and tested on ImageNet [22].TABLE I: Ablation study. FLOPs and encoding latency arefor 224x224 resolution inputs. All percentages are w.r.t. to thebaseline, Entropic Student.FLOPs\u2193 Enc. Latency\u2193 Delta-acc.\u2191Baseline: Entropic Student 0.466B 13.8ms 0.0%A: Baseline + single-stage 0.466B (-0.0%) 13.8ms (-0.0%) +0.13%B: A + improved architecture 0.457B (-0.02%) 9.10ms (-34.1%) +0.20%C: B + 8x downsampling 0.489B (+0.05%) 6.70ms (-51.4%) +0.93%resulting system as con\ufb01guration B in Fig. 5 and Table I. Notethat the downsampling ratio ( 4\xd7) is unchanged. We can seethat the improved architecture reduces the encoding latencyby more than 30% and improves the average accuracy by anadditional small percentage ( 0.07%).Finally, we increase the downsampling ratio to 8\xd7 aswe discussed in Sec. IV-B, and we refer to this version ascon\ufb01guration C . By increasing the downsampling ratio, weagain reduce the encoding latency and improve the averageaccuracy by an additional large margin, after which we achievean overall latency reduction of 50% and an average accuracyimprovement of 0.93%. From Fig. 5, we notice that theimprovement is most signi\ufb01cant at lower bit rates. We con-jecture that lower bit rates naturally call for a more compactcompressed representation, which can be achieved by moreaggressive downsampling. For higher bit rate (e.g., more than0.8 bpp), our approach remains on par with the baseline.VI. C ONCLUSIONIn this paper, we present a feature coding system for imageclassi\ufb01cation in edge-cloud systems. We show that by properdesign of the network architecture, existing feature codingmethods can be improved and extended to consider the RACtrade-off. Our system outperforms all previous methods interms of RAC trade-off, which we believe is valuable formany real-world applications deployment. A limitation of oursystem is the requirement of a separate neural network modelfor every rate-complexity pair, which we leave to our futurework to solve.REFERENCES[1] H. Choi and I. V . Baji \xb4c, \u201cDeep feature compression for collaborativeobject detection,\u201d IEEE International Conference on Image Processing ,pp. 3743\u20133747, Oct. 2018.[2] Y . Matsubara, R. Yang, M. Levorato, and S. Mandt, \u201cSupervised com-pression for resource-constrained edge computing systems,\u201d IEEE/CVFWinter Conference on Applications of Computer Vision , pp. 923\u2013933,Jan. 2022.[3] S. Singh, S. Abu-El-Haija, N. Johnston, J. Ball \xb4e, A. Shrivastava, andG. Toderici, \u201cEnd-to-end learning of compressible features,\u201d IEEEInternational Conference on Image Processing , pp. 3349\u20133353, Oct2020.[4] V . Kakkad, M. Patel, and M. Shah, \u201cBiometric authentication and imageencryption for image security in cloud framework,\u201d Multiscale andMultidisciplinary Modeling, Experiments and Design , vol. 2, no. 4, pp.233\u2013248, May 2019.[5] J. Lainema, F. Bossen, W. Han, J. Min, and K. Ugur, \u201cIntra coding of thehevc standard,\u201d IEEE Transactions on Circuits and Systems for VideoTechnology, vol. 22, no. 12, pp. 1792\u20131801, Dec. 2012.[6] J. Ball \xb4e, V . Laparra, and E. Simoncelli, \u201cEnd-to-end optimizationof nonlinear transform codes for perceptual quality,\u201d Picture CodingSymposium, pp. 1\u20135, Dec. 2016.[7] D. Minnen, J. Ball \xb4e, and G. Toderici, \u201cJoint autoregressive and hier-archical priors for learned image compression,\u201d Advances in NeuralInformation Processing Systems, vol. 31, pp. 10 794\u201310 803, Dec. 2018.[8] H. Choi and I. V . Baji \xb4c, \u201cNear-lossless deep feature compression forcollaborative intelligence,\u201d IEEE International Workshop on MultimediaSignal Processing, pp. 1\u20136, Aug. 2018.[9] \u2014\u2014, \u201cDeep feature compression for collaborative object detection,\u201dIEEE International Conference on Image Processing , pp. 3743\u20133747,Oct. 2018.[10] H. Choi, R. A. Cohen, and I. V . Baji \xb4c, \u201cBack-and-forth prediction fordeep tensor compression,\u201d IEEE International Conference on Acoustics,Speech and Signal Processing , pp. 4467\u20134471, May 2020.[11] R. A. Cohen, H. Choi, and I. V . Baji \xb4c, \u201cLightweight compressionof neural network feature tensors for collaborative intelligence,\u201d IEEEInternational Conference on Multimedia and Expo , pp. 1\u20136, Jul. 2020.[12] Y . Dubois, B. Bloem-Reddy, K. Ullrich, and C. J. Maddison, \u201cLossycompression for lossless prediction,\u201d Advances in Neural InformationProcessing Systems, vol. 34, pp. 14 014\u201314 028, Dec. 2021.[13] J. Ball \xb4e, V . Laparra, and E. P. Simoncelli, \u201cDensity modeling ofimages using a generalized normalization transformation,\u201d InternationalConference on Learning Representations , Jul. 2016.[14] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for imagerecognition,\u201d Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 770\u2013778, Jun. 2016.[15] J. Duda, \u201cAsymmetric numeral systems: entropy coding combiningspeed of huffman coding with compression rate of arithmetic coding,\u201darXiv preprint arXiv:1311.2540 , Dec. 2013.[16] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neuralnetwork,\u201d https://arxiv.org/abs/1503.02531, Mar. 2015.[17] J. Ball \xb4e, D. Minnen, S. Singh, S. Hwang, and N. Johnston, \u201cVariationalimage compression with a scale hyperprior,\u201d International Conferenceon Learning Representations , Apr. 2018.[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words:Transformers for image recognition at scale,\u201d International Conferenceon Learning Representations , May. 2021.[19] D. Hendrycks and K. Gimpel, \u201cGaussian error linear units (gelus),\u201d arXivpreprint arXiv:1606.08415, Jun. 2016.[20] J. Redmon and A. Farhadi, \u201cYolov3: An incremental improvement,\u201darXiv preprint arXiv:1804.02767 , Apr. 2018.[21] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsingnetwork,\u201d Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 6230\u20136239, Jul. 2017.[22] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, \u201cImagenet: A large-scale hierarchical image database,\u201d Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition , pp. 248\u2013255, Sep. 2009.[23] G. Bjontegaard, \u201cCalculation of average psnr differences between rd-curves,\u201d Video Coding Experts Group - M33 , Apr. 2001., Your response: ```ADD_PAPER\n2211.09897v1\nThis paper presents an efficient feature compression method for edge-cloud image classification systems, addressing the critical rate-accuracy-complexity (RAC) trade-off. The authors propose a flexible encoder architecture with adjustable residual blocks that can scale to different edge device capabilities while maintaining classification accuracy. Key innovations include a single-stage training strategy that outperforms previous two-stage approaches, and an 8\xd7 downsampling scheme that reduces encoding latency by 51.4% while improving accuracy by 0.93%. Experimental results on ImageNet demonstrate superior RAC performance compared to both traditional image codecs and state-of-the-art feature compression methods, with particular advantages at lower bit rates. The system achieves this through optimized network design choices including patch embedding layers and GeLU activations, making it particularly suitable for resource-constrained edge deployments where computational efficiency is crucial.\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\n~~~~~~~~~~\nCurrent Step #13, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification spiking neural networks\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 15:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #11, Phase: literature review, Feedback: 3\n--- Page 1 ---Ef\ufb01cient Feature Compression forEdge-Cloud SystemsZhihao DuanElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.duan90@purdue.eduFengqing ZhuElmore Family School of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, Indiana, U.S.A.zhu0@purdue.eduAbstract\u2014Optimizing computation in an edge-cloud system isan important yet challenging problem. In this paper, we considera three way trade-off between bit rate, classi\ufb01cation accuracy,and encoding complexity in an edge-cloud image classi\ufb01cationsystem. Our method includes a new training strategy andan ef\ufb01cient encoder architecture to improve the rate-accuracyperformance. Our design can also be easily scaled accordingto different computation resources on the edge device, taking astep towards achieving rate-accuracy-complexity (RAC) trade-off.Under various settings, our feature coding system consistentlyoutperforms previous methods in terms of the RAC performance.Code is made publicly available 1.Index Terms\u2014Feature compression, Coding for machine vision,Image classi\ufb01cation, Edge-cloud systemI. I NTRODUCTIONMobile devices and wearable sensors are essential gatewaysto many autonomous systems. These edge devices are oftendeployed with vision tasks in applications involving robotics,drones, surveillance cameras, and smart home devices. How-ever, due to constraints on the device size and power supply,edge devices cannot support the real-time execution of neuralnetworks-based computer vision methods, which are typicallycomputationally expensive. A common solution is to takeadvantage of the wireless connection to a powerful cloudcomputing resource, forming an edge-cloud system [1], [2]as illustrated in Fig. 1. In such a system, we obtain data (inour context, images) from edge device sensors (cameras) andsend it to the cloud for task execution.Designing practical methods for edge-cloud systems ischallenging from several aspects. First, in most cases wherethe edge-cloud connection is band-limited, we have constraintson the transmission bit rate, requiring the captured images tobe compressed by an encoder on the edge as ef\ufb01ciently as pos-sible. Second, we also have constraints on the computationalcapacity of the edge device, and thus the encoder cannot betoo complex. Finally, for common vision tasks such as theimage classi\ufb01cation, we also desire high accuracy. Taking allthese into consideration, we face a three-way trade-off betweenencoder complexity, bit rate, and vision task accuracy, whichWe term as the rate-accuracy-complexity (RAC) trade-off.1github.com/duanzhiihao/edge-cloud-racEdge device(sender)Cloud computing(receiver)Bits \u201ccat\u201dRateEncoder complexityAccuracyFig. 1: Illustration of an edge-cloud machine vision system,where three objectives are of interest: encoder computationalcomplexity on the edge device, bit rate in transmission, andtask accuracy in the cloud. In this paper, we present a featurecompression method for the edge-cloud image classi\ufb01cation.A straightforward baseline for RAC trade-off is to com-press and transmit images using different codecs. However,image codecs are typically developed for image reconstruction,making them suboptimal for vision task execution. Recently,research focused on feature compression has gained attention,which can be optimized for different vision tasks [1], [3].Another advantage to transmit features rather than images isprivacy protection. By transmitting features, one could skippixel reconstruction on the cloud and thus reduce potentiallyexposing image contents to the attackers [4].Entropic Student [2] is a recently proposed feature com-pression framework that provides promising rate-accuracyperformance. However, it does not take into account theencoder complexity as part of the performance trade-off. Toextend the scope to the three-way RAC trade-off, we proposea \ufb02exible encoder whose complexity can be easily adjustedfor different device capacities. Together with a new trainingstrategy, we achieve better RAC performance compared toprevious methods, including image coding baselines and thestate-of-the-art Entropic Student model.Our contributions can be summarized as follows:\u2022 We expand the scope of previous feature compressionsystems to include the RAC trade-off.\u2022 We design a \ufb02exible neural network architecture forfeature coding, which is both ef\ufb01cient and achieves goodclassi\ufb01cation performance.\u2022 We conduct extensive experiments to verify the effec-tiveness of our approach, showing its advantages overprevious methods.arXiv:2211.09897v1  [eess.IV]  17 Nov 2022--- Page 2 ---II. R ELATED WORKA. Image CompressionTraditional image coders . Traditional image codecs typi-cally follow a linear transform coding paradigm, such as block-based discrete cosine transform (DCT) and discrete wavelettransform (DWT). Representative codecs include JPEG, WebP,and BPG (i.e., HEVC intra [5]). Their encoding complexityvaries depending on different factors such as the choice of theentropy coding algorithms, number of prediction modes, andhow much optimization is done during training. In general,more complex computation devoted at the encoder leads tobetter compression performance.Learned image coders . With the rise of deep learning,learned image coders (LICs) extend the traditional lineartransform coding paradigm to a non-linear fashion, which isachieved by transforming images using neural networks [6].Despite improved compression performance compared to tra-ditional codecs, LICs typically require more computationalresources due to highly parameterized neural network layersand autoregressive entropy models [7].The paradigm of image coding share a common disadvan-tage: they are designed for image reconstruction and may besuboptimal for vision tasks in most edge-cloud settings.B. Feature compressionVarious methods have been proposed to compress andtransmit features rather than images for machine vision.Hand-crafted feature coding. Earlier works have exploredcompressing deep features using hand-crafted algorithms. Choiand Baji \xb4c use a block-based intra and inter coding algorithmfor video feature compression in [8] and a HEVC-based fea-ture compression method for object detection in [9]. Followingtheir works, methods have been proposed for feature compres-sion using pruning [10], quantization [11], and bidirectionalprediction [10].Learned feature compression . Instead of manually de-signed methods, recent work has shifted attention to end-to-end learned feature compression and showed promisingresults. Singh et al. use a fully factorized entropy model toconstrain the entropy of features, which results in improvedrate-accuracy trade-off over previous methods [3]. Dubois etal. show that, given enough computational capacity, featurescan be extremely (around 1,000\xd7) compressed without affect-ing the vision task accuracy [12]. More recently, Matsubaraet al. train vision models with feature compressor jointly andshow state-of-the-art rate-accuracy performance [2].While existing methods mostly focus on improving therate-accuracy performance, their approach lacks scalabilityfor different encoder complexity. In this work, we extendthe scope to rate-accuracy-complexity (RAC) trade-off byproposing a easily scalable encoder architecture, which can beadapted to edge devices with different computation capacities.III. P RELIMINARIESThe current best method in terms of rate-accuracy perfor-mance, Entropic Student [2], is a good starting point for our\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43f\u0ddc\U0001d466\u0ddc\U0001d466teachLabel \U0001d466Conv5x5, s=2GDNConv5x5, s=2GDNConv2x2, s=1Conv2x2, s=1Inv. GDNConv2x2, s=1Inv. GDNConv2x2, s=1DecoderEncoder ClassifierEntropy modelPre-trained image classifierStage 1 Stage 2(a) Training overview. GDN refers to the generalized divisible nor-malization layer [13], the entropy model is fully factorized, and theclassi\ufb01er is ResNet-50 [14]. See text for description.Edge Cloud\u0ddc\U0001d466Enc. Dec.ClassifierEntropy modelrANS rANSBitsEntropy model(b) Testing (deployment). Entropy coding is done by the range-basedvariant of asymmetric numeral systems (rANS) [15].Fig. 2: Model training (a) and deployment (b) in the EntropicStudent framework [2] for feature compression.proposed work. In this section, we brie\ufb02y summarize it toprovide the background of our method, and we refer interestedreaders to the original paper [2] for more details.The framework for classi\ufb01cation is shown in Fig. 2. Forsplit computing, a feature encoder is designed to be deployedon the edge device, and a decoder together with a classi\ufb01er areexecuted on the cloud. Speci\ufb01cally, a ResNet-50 [14] classi\ufb01eris adopted for image classi\ufb01cation.Training is done in two stages. In the \ufb01rst stage, theclassi\ufb01er is initialized using a pre-trained model and \ufb01xed(i.e., no parameter updating), and the encoder and decoder arelearned to compress images and reconstruct desired features.In the second stage, the encoder is \ufb01xed, while the decoderand the classi\ufb01er are trained to optimize the task accuracy.The training loss functions are:Lstage1 = \u03bblR + lMSE,Lstage2 = \u03bblR + 0.5 \xb7(lKL + lCE), (1)where lR is the bit rate loss estimated by a factorized entropymodel [6], lMSE is the mean squared error (MSE) betweenthe classi\ufb01er\u2019s intermediate features and the ones from a pre-trained teacher model, lKL is the KL divergence loss forknowledge distillation [16], lCE is the standard cross-entropyloss for classi\ufb01cation, and \u03bb is a scalar hyperparameter totrade-off between rate and classi\ufb01cation accuracy.The deployment scenario is shown in Fig. 2b, where theentropy model is shared between the sender (edge) and thereceiver (cloud) for entropy coding, implemented by the range-based asymmetric numeral system (rANS) algorithm [15].Despite its strong rate-accuracy performance [2], the En-tropic Student model only operates at a single encoder com-plexity and is not feasible for RAC trade-off. In the nextsection, we introduce our method, which not only improves therate-accuracy performance, but also enables the RAC trade-off.--- Page 3 ---\u0ddc\U0001d466\u0ddc\U0001d466tLabel \U0001d466Enc. Dec. ClassifierPre-trained Classifier\U0001d459\U0001d445\U0001d459\U0001d436\U0001d438\U0001d459\U0001d440\U0001d446\U0001d438 \U0001d459\U0001d43e\U0001d43fEntropy modelLearned or fixed(a) Training overview.Conv 8x8, s=8Res. BlockRes. Block\U0001d441 blocks\U0001d467 (b) Encoder architecture.GELUConv 3x3, s=1GELUConv 3x3, s=1 (c) Residual block.GELUConvT5x5Conv 3x3, s=1GELUConv 3x3, s=1GELUConv 1x1, s=1\U0001d467 (d) Decoder architecture.Fig. 3: Summary of proposed method. In (a), we show the training strategy, where the entropy model is fully factorized, andgray blocks indicate frozen models (i.e., no parameter update during training). The classi\ufb01er can either be trained jointly withthe system, or initialized with a pre-trained model and \ufb01xed throughout training. In (b) and (c), we show our encoder anddecoder architecture, respectively. Our encoder network complexity can be tuned by adjusting the number of residual blocks,N. For the classi\ufb01er, we use a truncated version of ResNet-50, following [2].IV. O UR APPROACHInspired by the success of knowledge distillation in the En-tropic Student model, we adopt a similar framework and lossfunctions, shown in Fig. 3a. However, we make modi\ufb01cationsincluding a new training strategy and network architecture, bywhich we are able to realize the RAC trade-off.A. Single-stage TrainingThe Entropic Student method adopts a two-stage trainingstrategy, where the encoder and decoder are trained in the\ufb01rst stage, and the decoder and classi\ufb01er are \ufb01ne-tuned in thesecond stage. We note that this decoupled training strategyrequires more hyperparameter searching, and it is also sub-optimal for rate-accuracy performance since network compo-nents are trained separately. Instead, we propose a simple \ufb01x,i.e., to train the encoder, decoder, and classi\ufb01er in an end-to-end fashion. Our single-stage loss function is as follows:Lsingle = \u03bblR + lMSE + 0.5 \xb7(lKL + lCE), (2)where the terms are de\ufb01ned in Sec. III. We keep the samescaling factor for each loss term as in the Entropic Studentwithout further tuning. Later in Sec. V-D, we show thatour single-stage strategy improves the rate-accuracy trade-offdespite being simpler to implement.B. Network ArchitectureWe introduce an encoder network architecture speci\ufb01callydesigned for ef\ufb01cient and \ufb02exible feature compression, shownin Fig. 3b. We now describe our design choices in details.Downsampling layer. We \ufb01rst noticed that the 5x5 con-volutions for downsampling in previous works (e.g., in theEntropic Student [2] and learned image coders [6], [17]) arecomputationally inef\ufb01cient due to the large overlap betweenconvolutional kernels. To remove this overlap, we adopt asingle patch embedding [18] layer for downsampling, whichcan be equivalently viewed as a convolution with kernel sizeequal to its strides.Network blocks. Instead of feedforward convolutions asin previous works, we adopt residual blocks in our featureencoder, which enables easy adjustment of the encoder com-plexity simply by setting the number of residual blocks, N. Alarger N (i.e., more residual blocks in the encoder) improvesthe rate-accuracy performance while results in higher encodercomplexity. Thus by tuning N, we can trade-off between therate-accuracy performance and the encoding complexity of oursystem. We use a standard design for the residual blocks asshown in Fig. 3c. For non-linearity layers, we use the GeLUactivation [19] instead of GDN [13], as we empirically foundthat the former leads to better performance with the samecomputational complexity.Downsampling ratio. Our \ufb01nal step is to use 8\xd7downsam-pling, instead of the 4\xd7downsampling in Entropic Student,during encoding. Our choice is motivated by the observationthat 8\xd7downsampling is a common choice for various imageprocessing tasks (e.g., in traditional codecs like JPEG) andcomputer vision tasks (e.g., in image object detection [20]and semantic segmentation [21]). Note that in our featuredecoder on the cloud (Fig. 3d), we upsample the feature by2\xd7to recover spatial details of the original image. With thiscon\ufb01guration of the encoder and the decoder, we are able toreduce the encoding latency and improve the average accuracyby a large margin, as discussed later in Sec. V-D.C. Feature Compression for a Fixed Classi\ufb01erSo far, we have considered the joint training setting, wherethe classi\ufb01er and our feature coder are trained jointly. Inaddition, we also examine the setting in which a pre-trainedclassi\ufb01er is available but \ufb01xed, illustrated in Fig. 3a. Thisscenario may happen in practice where there are different edgedevices sharing a single cloud vision model. The edge devicesmay use different feature encoders/decoders according to theircomputational capacity, but the vision model on the cloudshould be \ufb01xed. In this case, we use the same loss functionas in Eq. 2, but we train the encoder and decoder networkswithout updating the classi\ufb01er. We denote the methods in thissetting by cls. \ufb01xed to distinguish them from the joint trainingsetting, where the classi\ufb01er can be trained together with theencoder and the decoder.V. E XPERIMENTSWe compare our method to previous ones for edge-cloudimage classi\ufb01cation. For fair comparison, all methods use theResNet-50 [14] (for image coding methods) or a truncated--- Page 4 ---version of ResNet-50 (for feature coding methods) as theclassi\ufb01er.A. Dataset and MetricsWe use the 1,000-class ImageNet dataset [22] for trainingand testing. Standard data augmentations, including randomresizing and cropping, are applied during training. We use theImageNet val set for testing, in which we resize and center-crop all images such that they have a resolution of 224 \xd7224.All metrics are computed on the 224 \xd7224 val set. Bit rateis measured in terms of bits per pixel (bpp). For classi\ufb01cationaccuracy, we report the top-1 accuracy of the classi\ufb01er predic-tions. For encoding complexity, we focus on the CPU latencysince it well simulates real-world scenarios. All encodinglatencies are measured using an Intel 12700K CPU, averagedover the \ufb01rst 5,000 images in the val set.We also introduce a new metric, Delta-accuracy, whichcomputes the average accuracy improvement (w.r.t. a baselinemethod) over all bit rates. By using this metric, we cansummarize each rate-accuracy curve into a scalar, thus beingable to represent the three-way RAC trade-off in a 2-D plot.Our computation of Delta-accuracy follows the convention ofBD-PSNR and BD-rate [23] except one aspect. Unlike PSNR,the unit of accuracy is percentage (not decibel), so we computeaverage accuracy improvement over the normal bit rate space(instead of the logarithmic rate scale used in BD-PSNR).B. Experiment SettingsWe consider the following baseline methods.Image coding baselines. We compress images using animage codec at the sender side, and we reconstruct images atthe receiver side using the same codec. Then, we run ResNet-50 on the reconstructed images to infer class prediction. Ourselected codecs include handcrafted (e.g., WebP and BPG [5])and learning-based methods (e.g., the Factorized model [6],and the Hyperprior model [17]).Feature coding baseline. We use the existing best method,Entropic Student [2], as a powerful baseline.As we discussed in Sec. IV-C, we group methods into twogroups so that methods within each can be fairly compared.Classi\ufb01er \ufb01xed setting: The pre-trained ResNet-50 is usedand remains \ufb01xed (i.e., no \ufb01ne-tuning). The image codingbaselines all fall in this setting. We label the methods in thissetting as cls. \ufb01xed.Joint training setting: The pre-trained ResNet-50 is jointlytrained with the feature encoder and/or decoder. The EntropicStudent model falls in this setting. We label the methods inthis setting as joint.C. Main Results for RAC Trade-offTo benchmark our method against the baselines in terms ofRAC performance, we \ufb01rst measure the rate-accuracy curvesfor all baselines and our methods. Then, we compute the Delta-accuracy of each method w.r.t. the Entropic Student baseline(whose Delta-accuracy is 0 by de\ufb01nition). We show the Delta-accuracy as a function of encoding latency in Fig. 4. Note that3 4 7 10 14 20 30 40 50Encoding latency on CPU (ms)121086420Delta-accuracy (%)RAC performance on ImageNetFeature coding: ours (joint)Feature coding: Entropic Student [2] (joint)Feature coding: ours (cls. fixed)Image coding: BPG [5] (cls. fixed)Image coding: Hyperpiror model [17] (cls. fixed)Image coding: Factorized model [6] (cls. fixed)Image coding: WebP (cls. fixed)Fig. 4: RAC trade-off, visualized by Delta-accuracy vs en-coding complexity. The Delta-accuracy is w.r.t. the EntropicStudent baseline.the Entropic Student [2] corresponds to a single point on thecomplexity vs. Delta-accuracy plot since its complexity cannotbe tuned. Similarly, the Factorized model [6] and Hyperpriormodel [17] also produce a single point on the this plot. ForWebP and BPG [5] codec, we control the encoding latency bysetting the compression speed option provided by the codecimplementation. For our method, we control the latency bychoosing N \u2208{0,4,8}, where N is the number of residualblocks in the encoder.From Fig. 4, we observe that for classi\ufb01er \ufb01xed methods,our approach clearly outperforms the image coding baselinesby a large margin, demonstrating the effectiveness of featurecoding. This improvement is expected, because our featureencoder and decoder are explicitly trained to maximize classi-\ufb01cation accuracy, instead of pixel reconstruction as in imagecoding baselines. For the joint training setting, our methodachieves higher Delta-accuracy than the Entropic Student base-line with approximately only half of the encoding latency. Inaddition, the \ufb02exible design of our encoder enables encodingcomplexity adjustment, as opposed to the Entropic Student, bywhich only a single complexity can be achieved.D. Ablation StudyTo analyze how each component of our method contributesto the RAC improvement, we start from the Entropic Studentbaseline, and we include our proposed components one ata time. At each step, we show the change of rate-accuracyperformance as well as the encoding complexity.We \ufb01rst compare our single-stage strategy (referred to ascon\ufb01guration A) with the original two-stage one in Fig. 5. InTable I, we also show the encoding complexity and Delta-accuracy. We observe that the single-stage training does notimpair the performance; instead, the average accuracy slightlyincreases by 0.13% with encoding complexity unchanged.Then, we replace the Entropic Student encoder by ourpatch embedding layer and residual blocks, and we show the--- Page 5 ---0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2Bits per pixel (bpp)57.560.062.565.067.570.072.575.0T op-1 acc. (%)Rate-accuracy trade-off on ImageNetOurs C: B + 8x downsamplingOurs B: A + improved architectureOurs A: Baseline + single-stage trainingBaseline: Entropic StudentFig. 5: Ablation study of our approach compared to the En-tropic Student baseline. Trained and tested on ImageNet [22].TABLE I: Ablation study. FLOPs and encoding latency arefor 224x224 resolution inputs. All percentages are w.r.t. to thebaseline, Entropic Student.FLOPs\u2193 Enc. Latency\u2193 Delta-acc.\u2191Baseline: Entropic Student 0.466B 13.8ms 0.0%A: Baseline + single-stage 0.466B (-0.0%) 13.8ms (-0.0%) +0.13%B: A + improved architecture 0.457B (-0.02%) 9.10ms (-34.1%) +0.20%C: B + 8x downsampling 0.489B (+0.05%) 6.70ms (-51.4%) +0.93%resulting system as con\ufb01guration B in Fig. 5 and Table I. Notethat the downsampling ratio ( 4\xd7) is unchanged. We can seethat the improved architecture reduces the encoding latencyby more than 30% and improves the average accuracy by anadditional small percentage ( 0.07%).Finally, we increase the downsampling ratio to 8\xd7 aswe discussed in Sec. IV-B, and we refer to this version ascon\ufb01guration C . By increasing the downsampling ratio, weagain reduce the encoding latency and improve the averageaccuracy by an additional large margin, after which we achievean overall latency reduction of 50% and an average accuracyimprovement of 0.93%. From Fig. 5, we notice that theimprovement is most signi\ufb01cant at lower bit rates. We con-jecture that lower bit rates naturally call for a more compactcompressed representation, which can be achieved by moreaggressive downsampling. For higher bit rate (e.g., more than0.8 bpp), our approach remains on par with the baseline.VI. C ONCLUSIONIn this paper, we present a feature coding system for imageclassi\ufb01cation in edge-cloud systems. We show that by properdesign of the network architecture, existing feature codingmethods can be improved and extended to consider the RACtrade-off. Our system outperforms all previous methods interms of RAC trade-off, which we believe is valuable formany real-world applications deployment. A limitation of oursystem is the requirement of a separate neural network modelfor every rate-complexity pair, which we leave to our futurework to solve.REFERENCES[1] H. Choi and I. V . Baji \xb4c, \u201cDeep feature compression for collaborativeobject detection,\u201d IEEE International Conference on Image Processing ,pp. 3743\u20133747, Oct. 2018.[2] Y . Matsubara, R. Yang, M. Levorato, and S. Mandt, \u201cSupervised com-pression for resource-constrained edge computing systems,\u201d IEEE/CVFWinter Conference on Applications of Computer Vision , pp. 923\u2013933,Jan. 2022.[3] S. Singh, S. Abu-El-Haija, N. Johnston, J. Ball \xb4e, A. Shrivastava, andG. Toderici, \u201cEnd-to-end learning of compressible features,\u201d IEEEInternational Conference on Image Processing , pp. 3349\u20133353, Oct2020.[4] V . Kakkad, M. Patel, and M. Shah, \u201cBiometric authentication and imageencryption for image security in cloud framework,\u201d Multiscale andMultidisciplinary Modeling, Experiments and Design , vol. 2, no. 4, pp.233\u2013248, May 2019.[5] J. Lainema, F. Bossen, W. Han, J. Min, and K. Ugur, \u201cIntra coding of thehevc standard,\u201d IEEE Transactions on Circuits and Systems for VideoTechnology, vol. 22, no. 12, pp. 1792\u20131801, Dec. 2012.[6] J. Ball \xb4e, V . Laparra, and E. Simoncelli, \u201cEnd-to-end optimizationof nonlinear transform codes for perceptual quality,\u201d Picture CodingSymposium, pp. 1\u20135, Dec. 2016.[7] D. Minnen, J. Ball \xb4e, and G. Toderici, \u201cJoint autoregressive and hier-archical priors for learned image compression,\u201d Advances in NeuralInformation Processing Systems, vol. 31, pp. 10 794\u201310 803, Dec. 2018.[8] H. Choi and I. V . Baji \xb4c, \u201cNear-lossless deep feature compression forcollaborative intelligence,\u201d IEEE International Workshop on MultimediaSignal Processing, pp. 1\u20136, Aug. 2018.[9] \u2014\u2014, \u201cDeep feature compression for collaborative object detection,\u201dIEEE International Conference on Image Processing , pp. 3743\u20133747,Oct. 2018.[10] H. Choi, R. A. Cohen, and I. V . Baji \xb4c, \u201cBack-and-forth prediction fordeep tensor compression,\u201d IEEE International Conference on Acoustics,Speech and Signal Processing , pp. 4467\u20134471, May 2020.[11] R. A. Cohen, H. Choi, and I. V . Baji \xb4c, \u201cLightweight compressionof neural network feature tensors for collaborative intelligence,\u201d IEEEInternational Conference on Multimedia and Expo , pp. 1\u20136, Jul. 2020.[12] Y . Dubois, B. Bloem-Reddy, K. Ullrich, and C. J. Maddison, \u201cLossycompression for lossless prediction,\u201d Advances in Neural InformationProcessing Systems, vol. 34, pp. 14 014\u201314 028, Dec. 2021.[13] J. Ball \xb4e, V . Laparra, and E. P. Simoncelli, \u201cDensity modeling ofimages using a generalized normalization transformation,\u201d InternationalConference on Learning Representations , Jul. 2016.[14] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for imagerecognition,\u201d Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 770\u2013778, Jun. 2016.[15] J. Duda, \u201cAsymmetric numeral systems: entropy coding combiningspeed of huffman coding with compression rate of arithmetic coding,\u201darXiv preprint arXiv:1311.2540 , Dec. 2013.[16] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neuralnetwork,\u201d https://arxiv.org/abs/1503.02531, Mar. 2015.[17] J. Ball \xb4e, D. Minnen, S. Singh, S. Hwang, and N. Johnston, \u201cVariationalimage compression with a scale hyperprior,\u201d International Conferenceon Learning Representations , Apr. 2018.[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words:Transformers for image recognition at scale,\u201d International Conferenceon Learning Representations , May. 2021.[19] D. Hendrycks and K. Gimpel, \u201cGaussian error linear units (gelus),\u201d arXivpreprint arXiv:1606.08415, Jun. 2016.[20] J. Redmon and A. Farhadi, \u201cYolov3: An incremental improvement,\u201darXiv preprint arXiv:1804.02767 , Apr. 2018.[21] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsingnetwork,\u201d Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 6230\u20136239, Jul. 2017.[22] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, \u201cImagenet: A large-scale hierarchical image database,\u201d Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition , pp. 248\u2013255, Sep. 2009.[23] G. Bjontegaard, \u201cCalculation of average psnr differences between rd-curves,\u201d Video Coding Experts Group - M33 , Apr. 2001., Your response: ```ADD_PAPER\n2211.09897v1\nThis paper presents an efficient feature compression method for edge-cloud image classification systems, addressing the critical rate-accuracy-complexity (RAC) trade-off. The authors propose a flexible encoder architecture with adjustable residual blocks that can scale to different edge device capabilities while maintaining classification accuracy. Key innovations include a single-stage training strategy that outperforms previous two-stage approaches, and an 8\xd7 downsampling scheme that reduces encoding latency by 51.4% while improving accuracy by 0.93%. Experimental results on ImageNet demonstrate superior RAC performance compared to both traditional image codecs and state-of-the-art feature compression methods, with particular advantages at lower bit rates. The system achieves this through optimized network design choices including patch embedding layers and GeLU activations, making it particularly suitable for resource-constrained edge deployments where computational efficiency is crucial.\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\n~~~~~~~~~~\nCurrent Step #14, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Optimized spiking neurons classify images with highaccuracy through temporal coding with two spikesChristoph St\xa8 ockl1, Wolfgang Maass1,\u2217January 27, 20211Institute of Theoretical Computer Science, Graz University of Technology,In\ufb00eldgasse 16b, Graz, Austria\u2217 To whom correspondence should be addressed; E-mail: maass@igi.tugraz.at.AbstractSpike-based neuromorphic hardware promises to reduce the energy consumptionof image classi\ufb01cation and other deep learning applications, particularly on mobilephones or other edge devices. However, direct training of deep spiking neural net-works is di\ufb03cult, and previous methods for converting trained arti\ufb01cial neural net-works to spiking neurons were ine\ufb03cient because the neurons had to emit too manyspikes. We show that a substantially more e\ufb03cient conversion arises when one op-timizes the spiking neuron model for that purpose, so that it not only matters forinformation transmission how many spikes a neuron emits, but also when it emitsthose spikes. This advances the accuracy that can be achieved for image classi\ufb01cationwith spiking neurons, and the resulting networks need on average just two spikes perneuron for classifying an image. In addition, our new conversion method improveslatency and throughput of the resulting spiking networks.Spiking neural networks (SNNs) are currently explored as possible solution for a majorimpediment of more widespread uses of modern AI in edge devices: The energy consump-tion of the large state-of-the-art arti\ufb01cial neural networks (ANNs) that are produced bydeep learning.This holds in particular for the Convolutional Neural Networks (CNNs) that are com-monly used for image classi\ufb01cation, but also other application domains. These ANNs haveto be large for achieving top performance, since they need to have a su\ufb03ciently largenumber of parameters in order to absorb enough information from the huge data setson which they have been trained, such as the 1.2 million images of the ImageNet2012dataset. Inference with standard hardware implementations of these large ANNs is inher-ently power-hungry [Garc\xb4 \u0131a-Mart\xb4 \u0131n et al., 2019].1arXiv:2002.00860v4  [cs.NE]  26 Jan 2021--- Page 2 ---Spiking neurons have been in the focus of the development of novel computing hardwarefor AI with a drastically reduced energy budget, partially because the giant SNN of thebrain \u2013consisting of about 100 billion neurons\u2013 consumes just 20W [Ling, 2001]. Spikingneurons output trains of stereotypical pulses that are called spikes. Hence their output isvery di\ufb00erent from the continuous numbers that an ANN neuron produces as output. Mostspiking neuron models that are considered for implementation in neuromorphic hardwareare inspired by simple models for spiking neurons in the brain. However, these simpleneuron models do not capture the capability of biological neurons to encode di\ufb00erentinputs by di\ufb00erent temporal spike patterns, not just by their \ufb01ring rate (see Fig. 1 for anexample).Figure 1: Encoding of di\ufb00erent input values (current steps of di\ufb00erent ampli-tudes) by temporal spike patterns in a biological neuron. Data taken from theAllen Cell Type Database1 (Layer 3 spiny neuron from the human middle temporal gyrus).While large ANNs, trained with ever more sophisticated deep learning algorithms ongiant data sets, approach \u2013and sometimes exceed\u2013 human performance in several categoriesof intelligence, the performance of the current generation of spike-based neuromorphichardware is lagging behind. There is some hope that this gap can be closed for the case ofrecurrent spiking neural networks, since those can be trained directly to achieve most ofthe performance of recurrent ANNs [Bellec et al., 2020].1 \xa9 2015 Allen Institute for Brain Science. Allen Cell Types Database. Available from:https://celltypes.brain-map.org/experiment/electrophysiology/5877702512--- Page 3 ---But the problem to produce SNNs that achieve similar performance as ANNs with fewspikes persists for feedforward networks. Feedforward CNNs that achieve really good im-age classi\ufb01cation accuracy tend to be very deep and very large, and training correspondingdeep and large feedforward SNNs has not been able to reach similar classi\ufb01cation accuracy.Problems with the timing of spikes and precision of \ufb01ring rates on higher levels of the re-sulting SNNs have been cited as possible reasons. One attractive alternative is to simplytake a well-performing trained CNN and convert it into an SNN \u2013using the same connec-tions and weights. The most common \u2013and so far best performing\u2014 conversion methodwas based on the idea of (\ufb01ring-) rate coding, where the analog output of an ANN unit isemulated by the \ufb01ring rate of a spiking neuron [Rueckauer et al., 2017]. This method hadproduced so far the best SNN results for image classi\ufb01cation. But the transmission of ananalog value through a \ufb01ring rate tends to require a fairly large number of spikes, whichreduces both latency and throughput of the network. Furthermore, the resulting SNNtends to produce so many spikes that its energy-advantage over non-spiking hardware getslost. Finally, a rate-based ANN-to-SNN conversion can not be applied to those ANNs thatcurrently achieve the highest accuracy on ImageNet, E\ufb03cientNets [Tan and Le, 2019], be-cause these employ an activation function that assumes both positive and negative values:the SiLU function [Hendrycks and Gimpel, 2016].We introduce a new ANN-to-SNN conversion that we call FS-conversion because itrequires a spiking neuron to emit just a few spikes (FS = Few Spikes). This methodis completely di\ufb00erent from rate-based conversions, and exploits the option of temporalcoding with spike patterns, where the timing of a spike transmits extra information.Most previously proposed forms of temporal coding, see e.g. [Maass and Natschl\xa8 ager,1998], [Thorpe et al., 2001], [Rueckauer et al., 2017], [Kheradpisheh and Masquelier, 2020],have turned out to be di\ufb03cult to implement e\ufb03ciently in neuromorphic hardware becausethey require to transmit \ufb01ne time-di\ufb00erences between spikes to downstream neurons. Incontrast, an FS-conversion can be implemented with just log Ndi\ufb00erent values of spiketimes and at most log Nspikes for transmitting integers between 1 and N. Practically, therequired number of spikes can be made even lower because not all N values occur equallyoften. However FS-conversion requires a modi\ufb01ed spiking neuron model, the FS-neuron,which has an internal dynamic that is optimized for emulating particular types of ANNneurons with few spikes. We demonstrate the performance of SNNs that result from FS-conversion of CNNs, on two state-of-the-art datasets for image classi\ufb01cation: ImageNet2012and CIFAR10. This optimized spiking neuron model could serve as guidance for the nextgeneration of neuromorphic hardware.Emulating an ANN neuron by a spiking neuron withfew spikesThe FS-conversion from ANNs to SNNs requires a variation of the standard spiking neuronmodel, to which we refer as FS-neuron. The computation step of a generic arti\ufb01cial neuron3--- Page 4 ---in an ANN (see Fig. 2 a) is emulated by K time steps of an FS-neuron (Fig. 2 b). Itsinternal dynamics is de\ufb01ned by \ufb01xed parameters T(t),h(t),d(t) for t= 1,...,K . These areoptimized to emulate the activation function f(x) of the given ANN neuron by a weightedsum of spikes \u2211Kt=1 d(t)z(t), where z(t) denotes the spike train that this neuron produces.More precisely: z(t) = 1 if the neuron \ufb01res at step t, else z(t) = 0. To emit a spike attime t, a neuron\u2019s membrane potential v(t) has to surpass the current value T(t) of its\ufb01ring threshold. We assume that the membrane potential v(t) has no leak, but is reset tov(t) \u2212h(t) after a spike at time t. Expressed in formulas, the membrane potential v(t)starts with value v(1) = x where x is the gate input, and evolves during the K stepsaccording tov(t+ 1) = v(t) \u2212h(t)z(t). (1)The spike output z(t) of an FS-neuron for gate input x can be de\ufb01ned compactly byz(t) = \u0398(v(t) \u2212T(t)) = \u0398((x\u2212t\u22121\u2211j=1h(j)z(j))\u2212T(t)), t = 1,...,K, (2)where \u0398 denotes the Heaviside step function. The total output \u02c6f(x) of the FS-neuron fromthese K time steps, which is collected by the FS-neurons on the next layer, can be writtenas:\u02c6f(x) =K\u2211t=1d(t)z(t). (3)An illustration of the model can be found in Fig. 2b.Figure 2: Conversion of an ANN neuron into an FS-neuron.a) A generic ANN neuron with activation function f(x) that is to be emulated.b) An FS-neuron which emulates this ANN neuron in K time steps t= 1,...,K . Its outputspike train is denoted by z(t).4--- Page 5 ---For emulating the ReLU activation function one can choose the parameters of the FS-neuron so that they de\ufb01ne a coarse-to-\ufb01ne processing strategy for all input values x thatlie below some upper bound, as described in the Methods section. For emulating the SiLUfunction of E\ufb03cientNet one achieves a better FS-conversion if the parameters are chosenin such a way that they enable iterative \u2013and thereby more precise\u2013 processing for therange of inputs between \u22122 and 2 that occur most often as gate inputs x in E\ufb03cientNet.The resulting dynamics of FS-neurons is illustrated in Fig. 3 for the case of the SiLU andsigmoid activation functions.All FS-neurons that emulate ANN neurons with the same activation function can usethe same parameters T(t), h(t), d(t), while the factor w in the weights of their outputspikes is simply lifted from the corresponding synaptic connection in the trained ANN (seeFig. 2).5--- Page 6 ---Figure 3: Internal dynamics of Few-Spikes neurons.The \ufb01rst row depicts the response of the FS-neurons to a low input value ( x= \u22120.5) andthe second row displays the response to a high input ( x = 0.5). The \ufb01rst column showsresponses of an SiLU FS-neuron, while the second column a sigmoid FS-neuron. Therelevant values of T(t) and v(t) for discrete time steps t (see Fig. 4b and d) are smoothlyinterpolated for illustration.Note that the number of neurons and connections in the network is not increasedthrough the FS-conversion. However the number of computation steps L of a feedforwardANN with Llayers is increased by the factor K. But the computations of the ANN can beemulated in a pipelined manner, where a new network input (image) is processed every 2Ktime steps by the SNN. In this case the parameters of the FS-neurons change periodically6--- Page 7 ---with a period of length K while the FS-neurons compute. These K steps are followed byK time steps during which the FS-neurons are inactive, while the FS-neurons on the nextlayer collect their spike inputs for emulating the next computation step or layer of theANN. Note that since all FS-neurons that emulate ANN neurons with the same activationfunction can use the same parameters T(t), h(t), d(t), they require only little extra memoryon a neuromorphic chip.Both the TensorFlow code and the chosen parameters of the FS-neurons are availableonline\u2217.Application to ImageNetThe ImageNet data set [Russakovsky et al., 2015] has become the most popular bench-mark for state-of-the-art image classi\ufb01cation in machine learning (we are using here theImageNet2012 version). This data set consists of 1 .281.167 training images and 50.000 testimages (both RGB images of di\ufb00erent sizes), that are labeled by 1000 di\ufb00erent categories.Classifying images from ImageNet is a nontrivial task even for a human, since this dataset contains for example 59 categories for birds of di\ufb00erent species and gender [Van Hornet al., 2015]. This may explain why a relaxed performance measurement, where one recordswhether the target class is among the top 5 classi\ufb01cations that are proposed by the neuralnetwork (\u201dTop5\u201d), is typically much higher.The recently proposed E\ufb03cientNet [Tan and Le, 2019] promises to become a new stan-dard CNN architecture due to its very high accuracy while utilizing a smaller number ofparameters than other CNN architectures. E\ufb03cientNet uses as activation function f(x)besides the SiLU function (Fig. 4) also the familiar sigmoid function, shown as the redcurve in Fig. 4 c. Note that 99 .97% of its activation functions are SiLU functions, makingthe appearance of the sigmoid function comparatively rare. The SiLU function emergedfrom preceding work on optimizing activation functions in ANNs [Zoph and Le, 2018].Another characteristic of the E\ufb03cientNet architecture is the extensive usage of depth-wiseseparated convolution layers. In between them, linear activation functions are used. Al-though it would certainly be possible to approximate linear functions using FS-coding, wesimply collapsed linear layers into the generation of the weighted sums that form the inputsto the next layers.Since the SiLU function assumes also negative values, it appears to be di\ufb03cult to convertan ANN with this activation function via rate-coding to a spiking neuron. But it is fairlyeasy to convert it to an FS-neuron. The values of the parameters T(t),h(t) and d(t) fort= 1,...,K of the FS-neuron can be obtained by training the FS-neuron model to \ufb01t theSiLU function, see Fig. 4 a and b. We used for that backpropagation through time, witha triangle-shaped pseudo derivative for the non-existing derivative of the Heaviside stepfunction.In most cases, the possible inputs to an activation function are not uniformly dis-tributed, but there exists a certain region in which most inputs lie with high probability.\u2217https://github.com/christophstoeckl/FS-neurons7--- Page 8 ---For example, most of the inputs to the SiLU functions in the E\ufb03cientNet are in the in-terval from \u22122 to 2 and therefore, achieving a high approximation accuracy in this regionis especially desirable, (see Fig. 5 a). It is possible to encourage the FS-neuron to putmore emphasis on a certain region, by assigning a high weight in the loss function to thisregion. More details about the training procedure of the FS-parameters can be found inthe Methods section.The e\ufb00ective activation function of the resulting FS-neuron is shown in Fig. 4a. Fig. 4c shows the corresponding result for the FS-conversion of an ANN neuron with the sigmoidactivation function.Figure 4: Approximations \u02c6f(x) of di\ufb00erent activation functions by FS-neuronsa) Approximation of the SiLU function with a single FS-neuron.(red: SiLU function, blue: FS-approximation with K = 16)b) Optimized internal parameters of the SiLU FS-neuron.c) Approximation of the sigmoid function with a single FS-neuron.d) Optimized internal parameters of the sigmoid FS-neuron.e) Approximation of the ReLU function with K = 10 and \u03b1= 25.8--- Page 9 ---Model ANNaccuracyaccuracy of theSNN producedby FS-conversion# params # layers # neurons # spikesImageNet2012E\ufb03cientNet-B7 85%(97.2 %)83.57%(96.7%) 66M 218 259M 554.9MResNet50 75.22%(92.4%)75.10%(92.36%) 26M 50 9.6M 14.045MCIFAR10ResNet8 87.22% 87.05% 78k 8 73k 103kResNet14 90.49% 90.39% 174k 14 131k 190kResNet20 91.58% 91.45% 271k 20 188k 261kResNet50 92.99% 92.42% 755k 50 475k 647kTable 1: Accuracy and spike numbers for classifying images from ImageNetwith FS-conversions of two state-of-the-art CNNs. The SNNs produced by FS-conversion of the ANNs achieved almost the same accuracy, and usually used at most 2spikes per neuron. Top5 accuracy is reported in parentheses. The number of spikes neededfor inference was obtained by averaging over the 1000 test images.Using these FS-neurons it is possible to emulate the E\ufb03cientNet-B7 model with spikingneurons. The accuracy of the resulting spiking CNN, using the publicly available weightsw of the trained E\ufb03cientNet, can be found in Table 1, together with the total number ofspikes that it uses for sample inferences.The FS-conversion of E\ufb03cientNet-B7 achieved an accuracy of 83 .57%. The best ac-curacy for ImageNet that had previously been reported for SNNs was 74 .6% [Rueckaueret al., 2017]. It was achieved by a rate-based conversion, which required a substantialnumber of spikes per neuron and about 550 time steps for each image classi\ufb01cation. TheSNN resulting from FS-conversion of E\ufb03cientNet-B7 used about 2 spikes per neuron forclassifying an image. The FS-neurons approximating the SiLU function used K = 16 andthe FS-neurons approximating the sigmoid function used K = 12.The layers of the CNN that use the SiLU function as activation function can be simu-lated in a pipelined manner by the SNN, processing a new image every 2 K time steps: Its\ufb01rst K time steps are spent collecting the outputs from the preceding layer of FS-neuronsduring their K time steps of activity. It then processes these collected inputs xduring thesubsequent K time steps.Hence the SNN that results from FS-conversion of E\ufb03cientNet can classify a new imageevery 2K = 32 time steps. Further implementation details can be found in the Methodssection.9--- Page 10 ---Approximating the ReLU activation functionThe ReLU activation function, see Fig. 4 d, is among the most frequently used activationfunctions, and also quite good accuracies have been achieved with it for ImageNet. Itrepresents a special case for FS-conversion, as it is possible to \ufb01nd the ideal values forh(t),T(t) and d(t) analytically, bases on the idea of computation with binary numbers.By setting the parameters of the FS-neuron to T(t) = h(t) = d(t) = 2 K\u2212t, the FS-neuron approximates the ReLU activation function f(x) with a coarse-to-\ufb01re-processingstrategy. Let us assume for simplicity that an FS-neuron receives inputs xfrom (\u2212\u221e,0] \u222a{1,2,..., 2K \u22121}. Then it reproduces with the speci\ufb01ed parameters the output ReLU( x)of the ReLU gate for any x from (\u2212\u221e,0] \u222a{1,2,..., 2K \u22121}without error. In order to beable to transmit also non-integer values xbetween 0 and some arbitrary positive constant\u03b1, one simply multiplies the given values for T(t),h(t) and d(t) with \u03b12\u2212K. Then the FS-neuron reproduces ReLU(x) for any non-negative xless than \u03b1that are multiples of \u03b12\u2212Kwithout error, and ReLU( x) is rounded down for values x in between to the next largermultiple of \u03b12\u2212K. Thus the output of the FS-neuron deviates for xin the range from \u2212\u221eto \u03b1 by at most \u03b12\u2212K from the output of the ReLU gate. The resulting approximation isplotted for \u03b1= 10 in Fig. 4 d. Several advantages arising from the simple structure of theparameters have been laid out in the Methods section.The accuracy of 75.22% for the ANN version of ResNet50 in Table 1 resulted from train-ing a variant of ResNet50 where max-pooling was replaced by average pooling, using thehyperparameters given in the TensorFlow repository\u2020. The resulting accuracy in ImageNetis close to the best published performance of 76% for ResNet50 ANNs [Tan and Le, 2019,Table 2]. The application of the FS-conversion to this variant of ResNet50 (with K = 10and \u03b1= 25) yields an SNN whose Top1 and Top5 performance is almost indistinguishablefrom that of the ANN version.Application to CIFAR10CIFAR10 [Krizhevsky et al., 2009] is a smaller and more frequently used dataset for imageclassi\ufb01cation. It consists of 60.000 colored images, each having a resolution of just 32 by32 pixels, and just 10 image classes. The results for ANN versions of ResNet that are givenin Table 1 for CIFAR10 arise from training them with the hyperparameters given in theTensorFlow models repository. They use the ReLU function as the only nonlinearity, sincewe have replaced there max-pooling by average pooling. Nevertheless, they achieve anaccuracy for CIFAR10 which is very close to the best results reported for CIFAR10 in theliterature. The best performing reported ResNet on CIFAR10 is ResNet110, where a testaccuracy of 93.57% had been achieved [He et al., 2016]. Our ResNet50 achieves 92.99%,which is similar to their accuracy of 93.03% for ResNet56.\u2020https://github.com/tensor\ufb02ow/tpu/tree/master/models/o\ufb03cial/e\ufb03cientnet10--- Page 11 ---Analysis of FS-codingOn digital neuromorphic hardware the energy consumption is proportional to the numberof spikes which are needed for a computation. The number of spikes needed for an FS-neuron to perform the approximation of the target function is depicted in Fig. 5 a and bas function of the gate input x. If one compares these numbers with the distribution ofinput values x (red curves) that typically occur during image classi\ufb01cation, one sees whyon average less than 2 spikes are used by FS-neurons for these applications.11--- Page 12 ---Figure 5: Number of spikes needed by FS-neurons for image classi\ufb01cation andin\ufb02uence of K and bit precision Q on performancea) The number of spikes used by an a FS-neuron with K = 16 to approximate the SiLU function, asfunction of its input value x. The red Gaussian models the probability that the FS-neuron will receive thisinput value in the E\ufb03cientNet-B7 model (mean = \u22120.112, variance = 1.99). b) The number of spikes usedby an FS-neuron to approximate the ReLU function with K = 6 and \u03b1 = 10. The red Gaussian modelsthe probability that the FS-neuron will receive this input value in the ResNet50 model in an application toimages from ImageNet (mean \u22120.36970, variance = 2.19). c) Test Accuracy of the ResNet50 model onCIFAR10 with FS-neurons, in dependence on K. The red cross indicates the chosen value of K for ourresults. d) Mean squared error (MSE) of a SiLU approximation by FS-neurons with di\ufb00erent values ofK. The red cross indicates the chosen value of K in the given context. e) MSE of a SiLU approximationby FS-neurons with K = 16 as function of the bit precision Q of its parameters. f) MSE of a sigmoidapproximation by FS-neurons with K = 12 as function of the bit precision Q of its parameters.12--- Page 13 ---The most important speci\ufb01cation of an FS-neuron is the number K of time steps thatit uses. Fig. 5c, d provide insight into the nature of the trade-o\ufb00 between the size of Kand the approximation quality of the FS-neuron.Furthermore, it is of interest to consider scenarios where only a certain number of bitsare available for the FS-neuron parameters. To analyze the impact of that we considera setting where the parameters of the FS-neurons can only take on discrete values in therange from [\u22128,8]. The possible values are equally spaced and the number of values can bewritten as 2Q, where Qrefers to the number of bits which are available for each parameterT(t), h(t), d(t) of the FS-neuron. Fig. 5e, f depict the impact of such quantization on themean squared error of the approximation of the activation function.Expected implementation cost on neuromorphic hardwareWe distinguish three types of neuromorphic hardware\u2022 Digital, but hardware not constrained to a particular neuron model (example: SpiN-Naker)\u2022 Digital, but hardware is constrained to a particular neuron model (example: Loihi)\u2022 Mixed digital analog (examples: IBM research chip with memristors and BrainScaleS-2)SpiNNakerThe SpiNNaker platform [Furber et al., 2014] provides a \ufb02exible environment which is notconstrained to a speci\ufb01c neuron model. SpiNNaker allows to compute all products d(t)won the chip, which reduces the additional memory consumption to to a small constantvalue. All parameters T(t), h(t) and d(t) only need to be stored in memory once, as theycan be shared across all neurons which approximate the same activation function. Theadditional computational complexity of the FS-neuron model also has a very small impact,as computing the updated weight d(t)w can be done with a single instruction cycle.LoihiLoihi [Davies et al., 2018] also promises to be an interesting target platform for FS-neurons.Especially FS-neurons approximating the ReLU activation function could be ported verye\ufb03ciently to this hardware platform. As the chip is based on \ufb01xed-point arithmetic, onecan implement T(t), h(t) and d(t) for ReLu using a single parameter, namely the sharedweight exponent. This is be possible due to the fact that at every time step t all FS-parameters have the same value, which is always a power of 2. Therefore, the additionalmemory consumption does not grow with K. It is also possible to use other activationfunctions besides ReLU on Loihi, however, in this case it would be necessary to store allproducts d(t)w on the chip, as computing the updated weight in an online fashion is notpossible. In this case, an increase in memory consumption of O(K) is expected.13--- Page 14 ---IBM research chip with memristorsIBM has presented an in-memory chip architecture supporting both ANNs and SNNs in theSupplementary Material S3 of the article [Wo\xb4 zniak et al., 2020]. This architecture employsa memristor crossbar array for fast (time complexity O(1)) and energy-e\ufb03cient multipli-cation of the outputs of one layer l of neurons with the weights of synaptic connectionsto neurons on the next layer l+ 1. One can replace all spikes (i.e., values 1) that emergefrom layer l at time t of the K-step cycle in the emulation of the neurons on layer l by acommon value value d(t) that is centrally stored. Since the values d(t) and 0 can be useddirectly as inputs to the memristor array, no signi\ufb01cant extra cost is expected. The neuronmodels are implemented in the digital part of this neuromorphic chip architecture of IBM,using very fast digital logic and SRAM for storing parameters. Since all neurons on a layerl of our FS networks use the same parameters T(t) and h(t), they can be stored in a localSRAM for all neurons on layer l, in a similar fashion as on SpiNNaker. A neuron modelthat goes already one step in the direction from LIF to FS-neurons has actually alreadybeen implemented on this architecture: The soft spiking neural unit (sSNU), that emitsanalog instead of binary values and subtracts a corresponding value from the membranepotential [Wo\xb4 zniak et al., 2020].BrainScaleS-2This neuromorphic chip [Billaudelle et al., 2020] is also a mixed analog digital architecturewhere a digital plasticity processor allows fast changes of synaptic weights, but also centralmemory storage and application of the time-varying parameters T(t), h(t) and d(t) ofthe neuron dynamics. Like on SpiNNaker, the parameters only have to be stored once inmemory and can be shared across many neurons. The leak term of the membrane voltageof the analog neuron models can be switched o\ufb00, so that the analog part can be used fore\ufb03cient matrix multiplication in a similar manner as on the IBM chip.MethodsIn this section various details necessary to reproduce our results have been listed. Ad-ditionally, to aid the interpretation of the results, a comparison to previous conversionapproaches has been added.When training the parameters of the FS-neurons it is important to specify an intervalin which the approximation should be very good. Ideally, most of the inputs to the ANNneuron should fall into this interval to guarantee a good performance. In our experiments,the FS-neurons have been trained to approximate the interval from [ \u22128,12] for the SiLUfunction and [ \u221210,10] for the sigmoid function. The resulting FS-neuron approximatesthe SiLU function with a mean squared error of 0 .0023 inside the main region [ \u22122,2] and0.0064 in the region outside, which can be written as [ \u22128,\u22122] \u222a[2,12]. As a result of our14--- Page 15 ---\ufb01ne-tuning the values for T(t), d(t) and h(t) stay for most time steps t within the mainregion [\u22122,2] as can be seen in Fig. 4b.To reduce the complexity of the converted CNN, we decided not to emulate the mul-tiplication operation by FS-neurons, which occurs in the CNN if the squeeze and excita-tion optimization [Hu et al., 2018] is being used. In many neuromorphic chips, such asSpiNNaker and Loihi, the on-chip digital processor could carry out these multiplications.Otherwise one can approximate multiplication in a similar manner as the SiLU functionwith a suitably optimized FS-neuron, see [St\xa8 ockl and Maass, 2019]. Alternatively one cancompute multiplication with a small circuit of threshold gates, i.e., very simple types ofspiking neurons, of depth 2 or 3. A recent summary of such results is provided in section3 of [Parekh et al., 2018].Due to the simple structure of the parameters of the ReLU FS-neurons several advan-tages arise. In particular when approximating the ReLU function with an FS-neuron it ispossible to calculate the changes of parameters for t= 1,...,K by simply using a bit shiftoperation, possibly providing a very e\ufb03cient implementation on neuromorphic hardware.The resulting SNN can be used in a pipelined manner, processing a new network inputevery 2K = 20 time steps, analogously as for the SiLU function.Further properties of FS-coding that are relevant for neuromor-phic hardwareOne of the major advantages of using FS-neurons in neuromorphic hardware is the smalleramount of time steps and spikes required to approximate arti\ufb01cial neurons. For the case ofthe ReLU activation function, a rate coded spiking neuron requires N time steps to encodeN di\ufb00erent values. FS-neurons improve upon this unary coding scheme by utilizing thetime dimension to implement a binary coding scheme. Therefore, the number of time stepsrequired to encode N di\ufb00erent values can be reduced to just log2(N).To underline the binary coding nature of FS-neurons, in the case of the ReLU activationfunction, the corresponding FS-neurons will show a spiking pattern equivalent to of theoutput of the ReLU function, written as a binary number. The same logarithmic relationholds not only for the number of time steps required but also for the number of spikes neededto transmit a value. More sophisticated codes could be used to make the computationrobust to noise in spike transmission.Note, that most of the inputs to the FS-neurons have a value close to 0, as shown inFig. 5 a and b. Consequently, the FS-neurons usually require only a few spikes to transmitthe output values, making the the coding scheme even more sparse in practice.Comparison with previous methodsThe idea of converting a pre-trained ANN to a SNN has received a fair amount of attentionin the recent years. The most popular conversion approaches are rate-based, meaning they15--- Page 16 ---translate the continuous output of an arti\ufb01cial ReLU neuron into a \ufb01ring rate of a spikingneuron. Unfortunately there are some drawbacks associated with rate-coding. Due to itsunary coding nature, rate-codes are sub-optimal in the sense that they do not make gooduse of the time dimension. Usually a large amount of time steps is required to achieve asu\ufb03ciently accurate approximation. Furthermore, rate-based conversions are only capableof converting simple activation functions like ReLU, but fail to convert more sophisticatedfunctions like SiLU.Another popular conversion approach uses time to \ufb01rst spike (TTFS) coding [Rueckauerand Liu, 2018]. This approach encodes the continuous outputs of the corresponding ReLUANN neurons in the length of the time interval until the \ufb01rst spike, resulting in a verysparse spiking activity. However, this method seems to not scale easily to large modelsand has, to the best of our knowledge, not been tested on large data sets like ImageNet.The idea of using single spike temporal coding has \ufb01rst been explored in [Maass, 1997] andit has been shown to have a variety of applications, like implementing an e\ufb03cient k-NNalgorithm on neuromorphic hardware [Frady et al., 2020].Furthermore, a new conversion method has been proposed, in which the spiking neuronscan approximate the ReLU function using a hysteresis quantization method [Yousefzadehet al., 2019]. This approach waits to be tested on larger networks and datasets.One property that all previously mentioned conversion methods have in common is thatthey only consider transforming arti\ufb01cial ReLU neurons to spiking neurons, and thereforecannot convert more sophisticated activation functions, which are used in network archi-tecture like the E\ufb03cientNets.A detailed summary comparing FS-coding to previous results can be found in theExtended Data Table 1.It is worth noting, that the throughput using FS-coding is substantially better thanthat of SNNs which result from rate-based ANN-to-SNN conversions of ANNs with theReLU function, as proposed for example in [Rueckauer et al., 2017, Sengupta et al., 2019].The Inception-v3 model in [Rueckauer et al., 2017] was reported to yield a SNN that needed550 time steps to classify an image. Under the assumption that rate-based models pro\ufb01tonly very little from pipelining, it is reasonable to estimate that the throughput of an SNNthat results from FS-conversion of ReLU gates with K = 10 is roughly 25 times higher.The SNN resulting from the rate-based conversion of the ResNet34 model discussedin [Sengupta et al., 2019] has been reported to use 2500 time steps for a classi\ufb01cation.Therefore we estimate that the throughput is increased here by a factor around 125 throughFS-conversion.Spiking versions of ResNet20 have already been previously explored [Sengupta et al.,2019]. Using a rate-based conversion scheme an accuracy of 87.46% was reported.FS-conversion of ResNet20 yields a substantially higher accuracy of 91.45%, using just80 to 500 time steps for each image -depending on the model depth- instead of 2000,thereby signi\ufb01cantly reducing latency. In addition, the throughput is drastically improved.Also the number of spikes that the SNN uses for classifying an image from CIFAR10is signi\ufb01cantly reduced when one moves from a rate-based conversion to an FS conversion.A converted ResNet11 has been reported to use more than 8 million spikes to classify16--- Page 17 ---a single test example [Lee et al., 2020]. Comparing this to an FS-converted ResNet14we \ufb01nd that the latter uses 40 times fewer spikes despite being a slightly larger model.Using direct training of SNNs instead of a conversion scheme has been reported to resultin a lower amount of spikes needed to perform a single classi\ufb01cation. However, even adirectly trained SNN version of ResNet11 uses 7 times more spikes than an FS-conversionof ResNet14 [Lee et al., 2020, Table 8].In [Rathi et al., 2020] the authors present a novel approach for obtaining high perfor-mance SNNs by combining a rate-based conversion scheme with a subsequent gradient-based \ufb01ne-tuning procedure. They report the highest accuracy for an SNN on CIFAR10,which was achieved by converting a very performant ANN. They also show results forImageNet, where they achieve an accuracy of 65 .1% on their ResNet-34. Deeper models,like the ResNet-50, were not considered in this work. On ImageNet FS-conversion of theResNet-50 model improves their accuracy by 10% and FS-conversion of the E\ufb03cientNet-B7surpasses their performance by 18.47%.DiscussionWe have presented a new approach for generating SNNs that are very close to ANNs interms of classi\ufb01cation accuracy for images, while working in the energetically most attrac-tive regime with very sparse \ufb01ring activity. Besides substantially improved classi\ufb01cationaccuracy, they exhibit drastically improved latency and throughput compared with rate-based ANN-to-SNN conversions. o\ufb00 the shelf. One can argue that this is exactly theway which evolution has chosen for the design of neurons in living organism. Not onlyneurons with particular information processing tasks in the smaller nervous systems ofinsects, but also neurons in the neocortex of mammals exhibit an astounding diversity ofgenetically encoded response properties [ [Sterling and Laughlin, 2015], [Gouwens et al.,2019], [Bakken et al., 2020]]. In particular, the probability of producing a spike dependsin diverse ways on the recent stimulation history of the neuron, see [Gerstner et al., 2014]for some standard models. In other words, the excitability of di\ufb00erent types of biologicalneurons increases or decreases in complex ways in response to their previous \ufb01ring. As aresult, the temporal structure of a train of spikes that is produced by a biological neuroncontains additional information about the neuron input that can not be captured by its\ufb01ring rate. Similary, FS-neurons that are optimized for high accuracy image classi\ufb01cationwith few spikes exhibit history-dependent changes -encoded through their functions T(t)and h(t) according to equ. (2)- in their propensity to \ufb01re, see Fig. 4b and 4e. Furthermorethe function d(t) enables subsequent neurons to decode their spikes in a timing-sensitivemanner. In these regards an FS-conversion from ANNs to SNNs captures more of the func-tional capabilities of spiking neurons than previously considered rate-based conversions toan o\ufb00-the-shelf spiking neuron model.It is well known that spikes from the same neurons in the brain can transmit di\ufb00erentinformation to downstream neurons depending on the timing of the spike, see e.g. phaseprecession in the hipppocampus [Harris et al., 2002]. Hence is is conceivable that down-17--- Page 18 ---stream neurons give di\ufb00erent weights to these spikes, in dependence of the \ufb01ring time ofthe presynaptic neuron. In fact, it is well known that the large repertoire of pre- andpostsynaptic synaptic dynamics found in di\ufb00erent synapses of the brain [Markram et al.,2004, Kopanitsa et al., 2018] enables postsynaptic neurons to modulate the amplitude ofpostsynaptic responses in dependence of the timing of presynaptic spikes relative to an un-derlying rhythm. This can be viewed as a biological counterpart of the timing-dependentweights d(t) in our model. Altogether we believe that FS-neurons provide a \ufb01rst step inexploring new uses of SNNs where information is not encoded by the timing of single spikesor \ufb01ring rates, but by temporal spike patterns.Important for applications of FS-coding in neuromorphic hardware is that it is ap-plicable to virtually any activation function, in particular to that activation function forANN neurons that currently provides the highest accuracy on ImageNet, the SiLU func-tion. Rate-based conversion can not be readily applied to the SiLU function because itassumes both positive and negative output values. When approximating the more com-monly used ReLU function, FS-neurons approach the information theoretic minimum ofspikes for spike-based communication.In fact, FS-neurons that emulate ANN gates with the ReLU activation function produce1.5 spikes on average for classifying an image, while those for the Switch activation functionproduce 2 spikes on average. As the number of spikes required for inference by an SNNis directly related to its energy consumption in spike-based neuromorphic hardware, theenergy consumption of FS-converted SNNs appears to be close to the theoretical optimumfor SNNs. Since FS-conversion provides a tight bound on the number K of time stepsduring which a spiking neuron is occupied, it can also be used for converting recurrentlyconnected ANNs to SNNs.The proposed method for generating highly performant SNNs for image classi\ufb01cationthrough FS-conversion of trained CNNs o\ufb00ers an opportunity to combine the computa-tionally more e\ufb03cient and functionally more powerful training of ANNs with the superiorenergy-e\ufb03ciency of SNNs for inference. Note that one can also use the resulting SNN asinitialization for further training of the SNN, e.g., for a more speci\ufb01c task.Altogether our results suggest that spike-based hardware may gain an edge in thecompetition for the development of drastically more energy-e\ufb03cient hardware for AI if onedoes not forgot to optimize the spiking neuron model in the hardware for its intendedrange of applications. In contrast to energy e\ufb03cient digital hardware that is optimizedfor speci\ufb01c ANN architectures and activation functions, see e.g. [Zhang et al., 2019] for areview, a spike-based neuromorphic chip that is able to emulate FS-neurons can carry outinference for all possible ANN architectures. It can also emulate ANNs with previouslynot considered activation functions, since a change of the activation function just requiresreprogramming of the digitally stored parameters of FS-neurons. Hence such spike-basedchip will be substantially more versatile than common digital hardware accelerators for AI.18--- Page 19 ---AcknowledgementsWe would like to thank Franz Scherr for helpful discussions. We thank Thomas Bohnst-ingl, Evangelos Eleftheriou, Steve Furber, Christian Pehle, Philipp Plank and JohannesSchemmell for advice regarding implementation aspects of FS-neurons in various typesof neuromorphic hardware. This research was partially supported by the Human BrainProject of the European Union (Grant agreement number 785907). We also thank ouranonymous reviewers for very constructive comments.Competing InterestsWe are not aware of competing interests.Data availabilityBoth ImageNet [Deng et al., 2009] and CIFAR10 [Krizhevsky et al., 2009] are publiclyavailable datasets. No additional datasets were generated or analysed during the cur-rent study. The data for the spike response depiced in \ufb01gure 1 has been publishedbytheAllen Institute for Brain Science in 2015 (Allen Cell Types Database). Available from:https://celltypes.brain-map.org/experiment/electrophysiology/587770251Author contributionsCS conceived the main idea, CS and WM designed the model and planned the experiments,CS carried out the experiments, CS and WM wrote the paper.Code availabilityThe code this work is based on is publicly available at: https://github.com/christophstoeckl/FS-neurons (DOI: 10.5281/zenodo.4326749). Additionally, the code is also available in a CodeOcean compute capsule: https://codeocean.com/capsule/7743810/treeReferences[Bakken et al., 2020] Bakken, T. E., Jorstad, N. L., Hu, Q., Lake, B. B., Tian, W., Kalm-bach, B. E., Crow, M., Hodge, R. D., Krienen, F. M., Sorensen, S. A., et al. (2020).Evolution of cellular diversity in primary motor cortex of human, marmoset monkey,and mouse. bioRxiv.19--- Page 20 ---[Bellec et al., 2020] Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legen-stein, R., and Maass, W. (2020). A solution to the learning dilemma for recurrentnetworks of spiking neurons. Nature Communications, 11(1):1\u201315.[Billaudelle et al., 2020] Billaudelle, S., Stradmann, Y., Schreiber, K., Cramer, B., Baum-bach, A., Dold, D., G\xa8 oltz, J., Kungl, A. F., Wunderlich, T. C., Hartel, A., M\xa8 uller, E.,Breitwieser, O., Mauch, C., Kleider, M., Gr\xa8 ubl, A., St\xa8 ockel, D., Pehle, C., Heimbrecht,A., Spilger, P., Kiene, G., Karasenko, V., Senn, W., Petrovici, M. A., Schemmel, J., andMeier, K. (2020). Versatile emulation of spiking neural networks on an accelerated neu-romorphic substrate. In 2020 IEEE International Symposium on Circuits and Systems(ISCAS), pages 1\u20135.[Davies et al., 2018] Davies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y., Choday,S. H., Dimou, G., Joshi, P., Imam, N., Jain, S., et al. (2018). Loihi: A neuromorphicmanycore processor with on-chip learning. IEEE Micro, 38(1):82\u201399.[Deng et al., 2009] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on com-puter vision and pattern recognition , pages 248\u2013255. Ieee.[Frady et al., 2020] Frady, E. P., Orchard, G., Florey, D., Imam, N., Liu, R., Mishra, J.,Tse, J., Wild, A., Sommer, F. T., and Davies, M. (2020). Neuromorphic nearest neighborsearch using intel\u2019s pohoiki springs. In Okandan, M. and Aimone, J. B., editors, NICE\u201920: Neuro-inspired Computational Elements Workshop, Heidelberg, Germany, March17-20, 2020, pages 23:1\u201323:10. ACM.[Furber et al., 2014] Furber, S. B., Galluppi, F., Temple, S., and Plana, L. A. (2014). Thespinnaker project. Proceedings of the IEEE, 102(5):652\u2013665.[Garc\xb4 \u0131a-Mart\xb4 \u0131n et al., 2019] Garc\xb4 \u0131a-Mart\xb4 \u0131n, E., Rodrigues, C. F., Riley, G., and Grahn, H.(2019). Estimation of energy consumption in machine learning. Journal of Parallel andDistributed Computing, 134:75\u201388.[Gerstner et al., 2014] Gerstner, W., Kistler, W. M., Naud, R., and Paninski, L. (2014).Neuronal dynamics: From single neurons to networks and models of cognition. Cam-bridge University Press .[Gouwens et al., 2019] Gouwens, N. W., Sorensen, S. A., Berg, J., Lee, C., Jarsky, T., Ting,J., Sunkin, S. M., Feng, D., Anastassiou, C. A., Barkan, E., et al. (2019). Classi\ufb01cation ofelectrophysiological and morphological neuron types in the mouse visual cortex. Natureneuroscience, 22(7):1182\u20131195.[Han et al., 2020] Han, B., Srinivasan, G., and Roy, K. (2020). Rmp-snn: Residual mem-brane potential neuron for enabling deeper high-accuracy and low-latency spiking neuralnetwork. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 13558\u201313567.20--- Page 21 ---[Harris et al., 2002] Harris, K. D., Henze, D. A., Hirase, H., Leinekugel, X., Dragoi, G.,Czurk\xb4 o, A., and Buzs\xb4 aki, G. (2002). Spike train dynamics predicts theta-related phaseprecession in hippocampal pyramidal cells. Nature, 417(6890):738\u2013741.[He et al., 2016] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning forimage recognition. Proceedings of the IEEE Computer Society Conference on ComputerVision and Pattern Recognition, 2016-Decem:770\u2013778.[Hendrycks and Gimpel, 2016] Hendrycks, D. and Gimpel, K. (2016). Gaussian error linearunits (gelus). arXiv preprint arXiv:1606.08415 .[Hu et al., 2018] Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-Excitation Networks.Proceedings of the IEEE Computer Society Conference on Computer Vision and PatternRecognition, pages 7132\u20137141.[Kheradpisheh and Masquelier, 2020] Kheradpisheh, S. R. and Masquelier, T. (2020).S4nn: temporal backpropagation for spiking neural networks with one spike per neuron.International Journal of Neural Systems , 30(6):2050027.[Kopanitsa et al., 2018] Kopanitsa, M. V., van de Lagemaat, L. N., A\ufb01nowi, N. O.,Strathdee, D. J., Strathdee, K. E., Fricker, D. G., Tuck, E. J., Elsegood, K. A., Croning,M. D., Komiyama, N. H., et al. (2018). A combinatorial postsynaptic molecular mech-anism converts patterns of nerve impulses into the behavioral repertoire. BioRxiv, page500447.[Krizhevsky et al., 2009] Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layersof features from tiny images. Technical report.[Lee et al., 2020] Lee, C., Sarwar, S. S., Panda, P., Srinivasan, G., and Roy, K. (2020).Enabling spike-based backpropagation for training deep neural network architectures.Frontiers in Neuroscience, 14:119.[Ling, 2001] Ling, J. (2001). https://hypertextbook.com/facts/2001/JacquelineLing.shtml.[Maass, 1997] Maass, W. (1997). Fast sigmoidal networks via spiking neurons. NeuralComputation, 9(2):279\u2013304.[Maass and Natschl\xa8 ager, 1998] Maass, W. and Natschl\xa8 ager, T. (1998). Emulation of Hop-\ufb01eld networks with spiking neurons in temporal coding. In Computational Neuroscience,pages 221\u2013226. Springer.[Markram et al., 2004] Markram, H., Toledo-Rodriguez, M., Wang, Y., Gupta, A., Silber-berg, G., and Wu, C. (2004). Interneurons of the neocortical inhibitory system. Naturereviews neuroscience, 5(10):793\u2013807.21--- Page 22 ---[Parekh et al., 2018] Parekh, O., Phillips, C. A., James, C. D., and Aimone, J. B. (2018).Constant-depth and subcubic-size threshold circuits for matrix multiplication. In Pro-ceedings of the 30th on Symposium on Parallelism in Algorithms and Architectures, pages67\u201376.[Rathi et al., 2020] Rathi, N., Srinivasan, G., Panda, P., and Roy, K. (2020). Enablingdeep spiking neural networks with hybrid conversion and spike timing dependent back-propagation. In International Conference on Learning Representations.[Rueckauer and Liu, 2018] Rueckauer, B. and Liu, S.-C. (2018). Conversion of analog tospiking neural networks using sparse temporal coding. In 2018 IEEE InternationalSymposium on Circuits and Systems (ISCAS) , pages 1\u20135. IEEE.[Rueckauer et al., 2017] Rueckauer, B., Lungu, I. A., Hu, Y., Pfei\ufb00er, M., and Liu, S. C.(2017). Conversion of continuous-valued deep networks to e\ufb03cient event-driven networksfor image classi\ufb01cation. Frontiers in Neuroscience, 11(DEC):1\u201312.[Russakovsky et al., 2015] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei,L. (2015). ImageNet Large Scale Visual Recognition Challenge. International Journalof Computer Vision , 115(3):211\u2013252.[Sengupta et al., 2019] Sengupta, A., Ye, Y., Wang, R., Liu, C., and Roy, K. (2019).Going Deeper in Spiking Neural Networks: VGG and Residual Architectures. Frontiersin Neuroscience, 13(1998):1\u201316.[Sterling and Laughlin, 2015] Sterling, P. and Laughlin, S. (2015). Principles of neuraldesign. MIT Press.[St\xa8 ockl and Maass, 2019] St\xa8 ockl, C. and Maass, W. (2019). Recognizing images with atmost one spike per neuron. arXiv preprint arXiv:2001.01682 .[Tan and Le, 2019] Tan, M. and Le, Q. (2019). E\ufb03cientNet: Rethinking model scalingfor convolutional neural networks. In Chaudhuri, K. and Salakhutdinov, R., editors,Proceedings of the 36th International Conference on Machine Learning , volume 97 ofProceedings of Machine Learning Research , pages 6105\u20136114, Long Beach, California,USA. PMLR.[Thorpe et al., 2001] Thorpe, S., Delorme, A., and Rullen, R. (2001). Spike-based strate-gies for rapid processing. Neural networks : the o\ufb03cial journal of the InternationalNeural Network Society, 14:715\u201325.[Van Horn et al., 2015] Van Horn, G., Branson, S., Farrell, R., Haber, S., Barry, J., Ipeiro-tis, P., Perona, P., and Belongie, S. (2015). Building a bird recognition app and largescale dataset with citizen scientists: The \ufb01ne print in \ufb01ne-grained dataset collection.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,pages 595\u2013604.22--- Page 23 ---[Wo\xb4 zniak et al., 2020] Wo\xb4 zniak, S., P\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2002.00860v4\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 16:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #14, Phase: literature review, Feedback: 3\n--- Page 1 ---Optimized spiking neurons classify images with highaccuracy through temporal coding with two spikesChristoph St\xa8 ockl1, Wolfgang Maass1,\u2217January 27, 20211Institute of Theoretical Computer Science, Graz University of Technology,In\ufb00eldgasse 16b, Graz, Austria\u2217 To whom correspondence should be addressed; E-mail: maass@igi.tugraz.at.AbstractSpike-based neuromorphic hardware promises to reduce the energy consumptionof image classi\ufb01cation and other deep learning applications, particularly on mobilephones or other edge devices. However, direct training of deep spiking neural net-works is di\ufb03cult, and previous methods for converting trained arti\ufb01cial neural net-works to spiking neurons were ine\ufb03cient because the neurons had to emit too manyspikes. We show that a substantially more e\ufb03cient conversion arises when one op-timizes the spiking neuron model for that purpose, so that it not only matters forinformation transmission how many spikes a neuron emits, but also when it emitsthose spikes. This advances the accuracy that can be achieved for image classi\ufb01cationwith spiking neurons, and the resulting networks need on average just two spikes perneuron for classifying an image. In addition, our new conversion method improveslatency and throughput of the resulting spiking networks.Spiking neural networks (SNNs) are currently explored as possible solution for a majorimpediment of more widespread uses of modern AI in edge devices: The energy consump-tion of the large state-of-the-art arti\ufb01cial neural networks (ANNs) that are produced bydeep learning.This holds in particular for the Convolutional Neural Networks (CNNs) that are com-monly used for image classi\ufb01cation, but also other application domains. These ANNs haveto be large for achieving top performance, since they need to have a su\ufb03ciently largenumber of parameters in order to absorb enough information from the huge data setson which they have been trained, such as the 1.2 million images of the ImageNet2012dataset. Inference with standard hardware implementations of these large ANNs is inher-ently power-hungry [Garc\xb4 \u0131a-Mart\xb4 \u0131n et al., 2019].1arXiv:2002.00860v4  [cs.NE]  26 Jan 2021--- Page 2 ---Spiking neurons have been in the focus of the development of novel computing hardwarefor AI with a drastically reduced energy budget, partially because the giant SNN of thebrain \u2013consisting of about 100 billion neurons\u2013 consumes just 20W [Ling, 2001]. Spikingneurons output trains of stereotypical pulses that are called spikes. Hence their output isvery di\ufb00erent from the continuous numbers that an ANN neuron produces as output. Mostspiking neuron models that are considered for implementation in neuromorphic hardwareare inspired by simple models for spiking neurons in the brain. However, these simpleneuron models do not capture the capability of biological neurons to encode di\ufb00erentinputs by di\ufb00erent temporal spike patterns, not just by their \ufb01ring rate (see Fig. 1 for anexample).Figure 1: Encoding of di\ufb00erent input values (current steps of di\ufb00erent ampli-tudes) by temporal spike patterns in a biological neuron. Data taken from theAllen Cell Type Database1 (Layer 3 spiny neuron from the human middle temporal gyrus).While large ANNs, trained with ever more sophisticated deep learning algorithms ongiant data sets, approach \u2013and sometimes exceed\u2013 human performance in several categoriesof intelligence, the performance of the current generation of spike-based neuromorphichardware is lagging behind. There is some hope that this gap can be closed for the case ofrecurrent spiking neural networks, since those can be trained directly to achieve most ofthe performance of recurrent ANNs [Bellec et al., 2020].1 \xa9 2015 Allen Institute for Brain Science. Allen Cell Types Database. Available from:https://celltypes.brain-map.org/experiment/electrophysiology/5877702512--- Page 3 ---But the problem to produce SNNs that achieve similar performance as ANNs with fewspikes persists for feedforward networks. Feedforward CNNs that achieve really good im-age classi\ufb01cation accuracy tend to be very deep and very large, and training correspondingdeep and large feedforward SNNs has not been able to reach similar classi\ufb01cation accuracy.Problems with the timing of spikes and precision of \ufb01ring rates on higher levels of the re-sulting SNNs have been cited as possible reasons. One attractive alternative is to simplytake a well-performing trained CNN and convert it into an SNN \u2013using the same connec-tions and weights. The most common \u2013and so far best performing\u2014 conversion methodwas based on the idea of (\ufb01ring-) rate coding, where the analog output of an ANN unit isemulated by the \ufb01ring rate of a spiking neuron [Rueckauer et al., 2017]. This method hadproduced so far the best SNN results for image classi\ufb01cation. But the transmission of ananalog value through a \ufb01ring rate tends to require a fairly large number of spikes, whichreduces both latency and throughput of the network. Furthermore, the resulting SNNtends to produce so many spikes that its energy-advantage over non-spiking hardware getslost. Finally, a rate-based ANN-to-SNN conversion can not be applied to those ANNs thatcurrently achieve the highest accuracy on ImageNet, E\ufb03cientNets [Tan and Le, 2019], be-cause these employ an activation function that assumes both positive and negative values:the SiLU function [Hendrycks and Gimpel, 2016].We introduce a new ANN-to-SNN conversion that we call FS-conversion because itrequires a spiking neuron to emit just a few spikes (FS = Few Spikes). This methodis completely di\ufb00erent from rate-based conversions, and exploits the option of temporalcoding with spike patterns, where the timing of a spike transmits extra information.Most previously proposed forms of temporal coding, see e.g. [Maass and Natschl\xa8 ager,1998], [Thorpe et al., 2001], [Rueckauer et al., 2017], [Kheradpisheh and Masquelier, 2020],have turned out to be di\ufb03cult to implement e\ufb03ciently in neuromorphic hardware becausethey require to transmit \ufb01ne time-di\ufb00erences between spikes to downstream neurons. Incontrast, an FS-conversion can be implemented with just log Ndi\ufb00erent values of spiketimes and at most log Nspikes for transmitting integers between 1 and N. Practically, therequired number of spikes can be made even lower because not all N values occur equallyoften. However FS-conversion requires a modi\ufb01ed spiking neuron model, the FS-neuron,which has an internal dynamic that is optimized for emulating particular types of ANNneurons with few spikes. We demonstrate the performance of SNNs that result from FS-conversion of CNNs, on two state-of-the-art datasets for image classi\ufb01cation: ImageNet2012and CIFAR10. This optimized spiking neuron model could serve as guidance for the nextgeneration of neuromorphic hardware.Emulating an ANN neuron by a spiking neuron withfew spikesThe FS-conversion from ANNs to SNNs requires a variation of the standard spiking neuronmodel, to which we refer as FS-neuron. The computation step of a generic arti\ufb01cial neuron3--- Page 4 ---in an ANN (see Fig. 2 a) is emulated by K time steps of an FS-neuron (Fig. 2 b). Itsinternal dynamics is de\ufb01ned by \ufb01xed parameters T(t),h(t),d(t) for t= 1,...,K . These areoptimized to emulate the activation function f(x) of the given ANN neuron by a weightedsum of spikes \u2211Kt=1 d(t)z(t), where z(t) denotes the spike train that this neuron produces.More precisely: z(t) = 1 if the neuron \ufb01res at step t, else z(t) = 0. To emit a spike attime t, a neuron\u2019s membrane potential v(t) has to surpass the current value T(t) of its\ufb01ring threshold. We assume that the membrane potential v(t) has no leak, but is reset tov(t) \u2212h(t) after a spike at time t. Expressed in formulas, the membrane potential v(t)starts with value v(1) = x where x is the gate input, and evolves during the K stepsaccording tov(t+ 1) = v(t) \u2212h(t)z(t). (1)The spike output z(t) of an FS-neuron for gate input x can be de\ufb01ned compactly byz(t) = \u0398(v(t) \u2212T(t)) = \u0398((x\u2212t\u22121\u2211j=1h(j)z(j))\u2212T(t)), t = 1,...,K, (2)where \u0398 denotes the Heaviside step function. The total output \u02c6f(x) of the FS-neuron fromthese K time steps, which is collected by the FS-neurons on the next layer, can be writtenas:\u02c6f(x) =K\u2211t=1d(t)z(t). (3)An illustration of the model can be found in Fig. 2b.Figure 2: Conversion of an ANN neuron into an FS-neuron.a) A generic ANN neuron with activation function f(x) that is to be emulated.b) An FS-neuron which emulates this ANN neuron in K time steps t= 1,...,K . Its outputspike train is denoted by z(t).4--- Page 5 ---For emulating the ReLU activation function one can choose the parameters of the FS-neuron so that they de\ufb01ne a coarse-to-\ufb01ne processing strategy for all input values x thatlie below some upper bound, as described in the Methods section. For emulating the SiLUfunction of E\ufb03cientNet one achieves a better FS-conversion if the parameters are chosenin such a way that they enable iterative \u2013and thereby more precise\u2013 processing for therange of inputs between \u22122 and 2 that occur most often as gate inputs x in E\ufb03cientNet.The resulting dynamics of FS-neurons is illustrated in Fig. 3 for the case of the SiLU andsigmoid activation functions.All FS-neurons that emulate ANN neurons with the same activation function can usethe same parameters T(t), h(t), d(t), while the factor w in the weights of their outputspikes is simply lifted from the corresponding synaptic connection in the trained ANN (seeFig. 2).5--- Page 6 ---Figure 3: Internal dynamics of Few-Spikes neurons.The \ufb01rst row depicts the response of the FS-neurons to a low input value ( x= \u22120.5) andthe second row displays the response to a high input ( x = 0.5). The \ufb01rst column showsresponses of an SiLU FS-neuron, while the second column a sigmoid FS-neuron. Therelevant values of T(t) and v(t) for discrete time steps t (see Fig. 4b and d) are smoothlyinterpolated for illustration.Note that the number of neurons and connections in the network is not increasedthrough the FS-conversion. However the number of computation steps L of a feedforwardANN with Llayers is increased by the factor K. But the computations of the ANN can beemulated in a pipelined manner, where a new network input (image) is processed every 2Ktime steps by the SNN. In this case the parameters of the FS-neurons change periodically6--- Page 7 ---with a period of length K while the FS-neurons compute. These K steps are followed byK time steps during which the FS-neurons are inactive, while the FS-neurons on the nextlayer collect their spike inputs for emulating the next computation step or layer of theANN. Note that since all FS-neurons that emulate ANN neurons with the same activationfunction can use the same parameters T(t), h(t), d(t), they require only little extra memoryon a neuromorphic chip.Both the TensorFlow code and the chosen parameters of the FS-neurons are availableonline\u2217.Application to ImageNetThe ImageNet data set [Russakovsky et al., 2015] has become the most popular bench-mark for state-of-the-art image classi\ufb01cation in machine learning (we are using here theImageNet2012 version). This data set consists of 1 .281.167 training images and 50.000 testimages (both RGB images of di\ufb00erent sizes), that are labeled by 1000 di\ufb00erent categories.Classifying images from ImageNet is a nontrivial task even for a human, since this dataset contains for example 59 categories for birds of di\ufb00erent species and gender [Van Hornet al., 2015]. This may explain why a relaxed performance measurement, where one recordswhether the target class is among the top 5 classi\ufb01cations that are proposed by the neuralnetwork (\u201dTop5\u201d), is typically much higher.The recently proposed E\ufb03cientNet [Tan and Le, 2019] promises to become a new stan-dard CNN architecture due to its very high accuracy while utilizing a smaller number ofparameters than other CNN architectures. E\ufb03cientNet uses as activation function f(x)besides the SiLU function (Fig. 4) also the familiar sigmoid function, shown as the redcurve in Fig. 4 c. Note that 99 .97% of its activation functions are SiLU functions, makingthe appearance of the sigmoid function comparatively rare. The SiLU function emergedfrom preceding work on optimizing activation functions in ANNs [Zoph and Le, 2018].Another characteristic of the E\ufb03cientNet architecture is the extensive usage of depth-wiseseparated convolution layers. In between them, linear activation functions are used. Al-though it would certainly be possible to approximate linear functions using FS-coding, wesimply collapsed linear layers into the generation of the weighted sums that form the inputsto the next layers.Since the SiLU function assumes also negative values, it appears to be di\ufb03cult to convertan ANN with this activation function via rate-coding to a spiking neuron. But it is fairlyeasy to convert it to an FS-neuron. The values of the parameters T(t),h(t) and d(t) fort= 1,...,K of the FS-neuron can be obtained by training the FS-neuron model to \ufb01t theSiLU function, see Fig. 4 a and b. We used for that backpropagation through time, witha triangle-shaped pseudo derivative for the non-existing derivative of the Heaviside stepfunction.In most cases, the possible inputs to an activation function are not uniformly dis-tributed, but there exists a certain region in which most inputs lie with high probability.\u2217https://github.com/christophstoeckl/FS-neurons7--- Page 8 ---For example, most of the inputs to the SiLU functions in the E\ufb03cientNet are in the in-terval from \u22122 to 2 and therefore, achieving a high approximation accuracy in this regionis especially desirable, (see Fig. 5 a). It is possible to encourage the FS-neuron to putmore emphasis on a certain region, by assigning a high weight in the loss function to thisregion. More details about the training procedure of the FS-parameters can be found inthe Methods section.The e\ufb00ective activation function of the resulting FS-neuron is shown in Fig. 4a. Fig. 4c shows the corresponding result for the FS-conversion of an ANN neuron with the sigmoidactivation function.Figure 4: Approximations \u02c6f(x) of di\ufb00erent activation functions by FS-neuronsa) Approximation of the SiLU function with a single FS-neuron.(red: SiLU function, blue: FS-approximation with K = 16)b) Optimized internal parameters of the SiLU FS-neuron.c) Approximation of the sigmoid function with a single FS-neuron.d) Optimized internal parameters of the sigmoid FS-neuron.e) Approximation of the ReLU function with K = 10 and \u03b1= 25.8--- Page 9 ---Model ANNaccuracyaccuracy of theSNN producedby FS-conversion# params # layers # neurons # spikesImageNet2012E\ufb03cientNet-B7 85%(97.2 %)83.57%(96.7%) 66M 218 259M 554.9MResNet50 75.22%(92.4%)75.10%(92.36%) 26M 50 9.6M 14.045MCIFAR10ResNet8 87.22% 87.05% 78k 8 73k 103kResNet14 90.49% 90.39% 174k 14 131k 190kResNet20 91.58% 91.45% 271k 20 188k 261kResNet50 92.99% 92.42% 755k 50 475k 647kTable 1: Accuracy and spike numbers for classifying images from ImageNetwith FS-conversions of two state-of-the-art CNNs. The SNNs produced by FS-conversion of the ANNs achieved almost the same accuracy, and usually used at most 2spikes per neuron. Top5 accuracy is reported in parentheses. The number of spikes neededfor inference was obtained by averaging over the 1000 test images.Using these FS-neurons it is possible to emulate the E\ufb03cientNet-B7 model with spikingneurons. The accuracy of the resulting spiking CNN, using the publicly available weightsw of the trained E\ufb03cientNet, can be found in Table 1, together with the total number ofspikes that it uses for sample inferences.The FS-conversion of E\ufb03cientNet-B7 achieved an accuracy of 83 .57%. The best ac-curacy for ImageNet that had previously been reported for SNNs was 74 .6% [Rueckaueret al., 2017]. It was achieved by a rate-based conversion, which required a substantialnumber of spikes per neuron and about 550 time steps for each image classi\ufb01cation. TheSNN resulting from FS-conversion of E\ufb03cientNet-B7 used about 2 spikes per neuron forclassifying an image. The FS-neurons approximating the SiLU function used K = 16 andthe FS-neurons approximating the sigmoid function used K = 12.The layers of the CNN that use the SiLU function as activation function can be simu-lated in a pipelined manner by the SNN, processing a new image every 2 K time steps: Its\ufb01rst K time steps are spent collecting the outputs from the preceding layer of FS-neuronsduring their K time steps of activity. It then processes these collected inputs xduring thesubsequent K time steps.Hence the SNN that results from FS-conversion of E\ufb03cientNet can classify a new imageevery 2K = 32 time steps. Further implementation details can be found in the Methodssection.9--- Page 10 ---Approximating the ReLU activation functionThe ReLU activation function, see Fig. 4 d, is among the most frequently used activationfunctions, and also quite good accuracies have been achieved with it for ImageNet. Itrepresents a special case for FS-conversion, as it is possible to \ufb01nd the ideal values forh(t),T(t) and d(t) analytically, bases on the idea of computation with binary numbers.By setting the parameters of the FS-neuron to T(t) = h(t) = d(t) = 2 K\u2212t, the FS-neuron approximates the ReLU activation function f(x) with a coarse-to-\ufb01re-processingstrategy. Let us assume for simplicity that an FS-neuron receives inputs xfrom (\u2212\u221e,0] \u222a{1,2,..., 2K \u22121}. Then it reproduces with the speci\ufb01ed parameters the output ReLU( x)of the ReLU gate for any x from (\u2212\u221e,0] \u222a{1,2,..., 2K \u22121}without error. In order to beable to transmit also non-integer values xbetween 0 and some arbitrary positive constant\u03b1, one simply multiplies the given values for T(t),h(t) and d(t) with \u03b12\u2212K. Then the FS-neuron reproduces ReLU(x) for any non-negative xless than \u03b1that are multiples of \u03b12\u2212Kwithout error, and ReLU( x) is rounded down for values x in between to the next largermultiple of \u03b12\u2212K. Thus the output of the FS-neuron deviates for xin the range from \u2212\u221eto \u03b1 by at most \u03b12\u2212K from the output of the ReLU gate. The resulting approximation isplotted for \u03b1= 10 in Fig. 4 d. Several advantages arising from the simple structure of theparameters have been laid out in the Methods section.The accuracy of 75.22% for the ANN version of ResNet50 in Table 1 resulted from train-ing a variant of ResNet50 where max-pooling was replaced by average pooling, using thehyperparameters given in the TensorFlow repository\u2020. The resulting accuracy in ImageNetis close to the best published performance of 76% for ResNet50 ANNs [Tan and Le, 2019,Table 2]. The application of the FS-conversion to this variant of ResNet50 (with K = 10and \u03b1= 25) yields an SNN whose Top1 and Top5 performance is almost indistinguishablefrom that of the ANN version.Application to CIFAR10CIFAR10 [Krizhevsky et al., 2009] is a smaller and more frequently used dataset for imageclassi\ufb01cation. It consists of 60.000 colored images, each having a resolution of just 32 by32 pixels, and just 10 image classes. The results for ANN versions of ResNet that are givenin Table 1 for CIFAR10 arise from training them with the hyperparameters given in theTensorFlow models repository. They use the ReLU function as the only nonlinearity, sincewe have replaced there max-pooling by average pooling. Nevertheless, they achieve anaccuracy for CIFAR10 which is very close to the best results reported for CIFAR10 in theliterature. The best performing reported ResNet on CIFAR10 is ResNet110, where a testaccuracy of 93.57% had been achieved [He et al., 2016]. Our ResNet50 achieves 92.99%,which is similar to their accuracy of 93.03% for ResNet56.\u2020https://github.com/tensor\ufb02ow/tpu/tree/master/models/o\ufb03cial/e\ufb03cientnet10--- Page 11 ---Analysis of FS-codingOn digital neuromorphic hardware the energy consumption is proportional to the numberof spikes which are needed for a computation. The number of spikes needed for an FS-neuron to perform the approximation of the target function is depicted in Fig. 5 a and bas function of the gate input x. If one compares these numbers with the distribution ofinput values x (red curves) that typically occur during image classi\ufb01cation, one sees whyon average less than 2 spikes are used by FS-neurons for these applications.11--- Page 12 ---Figure 5: Number of spikes needed by FS-neurons for image classi\ufb01cation andin\ufb02uence of K and bit precision Q on performancea) The number of spikes used by an a FS-neuron with K = 16 to approximate the SiLU function, asfunction of its input value x. The red Gaussian models the probability that the FS-neuron will receive thisinput value in the E\ufb03cientNet-B7 model (mean = \u22120.112, variance = 1.99). b) The number of spikes usedby an FS-neuron to approximate the ReLU function with K = 6 and \u03b1 = 10. The red Gaussian modelsthe probability that the FS-neuron will receive this input value in the ResNet50 model in an application toimages from ImageNet (mean \u22120.36970, variance = 2.19). c) Test Accuracy of the ResNet50 model onCIFAR10 with FS-neurons, in dependence on K. The red cross indicates the chosen value of K for ourresults. d) Mean squared error (MSE) of a SiLU approximation by FS-neurons with di\ufb00erent values ofK. The red cross indicates the chosen value of K in the given context. e) MSE of a SiLU approximationby FS-neurons with K = 16 as function of the bit precision Q of its parameters. f) MSE of a sigmoidapproximation by FS-neurons with K = 12 as function of the bit precision Q of its parameters.12--- Page 13 ---The most important speci\ufb01cation of an FS-neuron is the number K of time steps thatit uses. Fig. 5c, d provide insight into the nature of the trade-o\ufb00 between the size of Kand the approximation quality of the FS-neuron.Furthermore, it is of interest to consider scenarios where only a certain number of bitsare available for the FS-neuron parameters. To analyze the impact of that we considera setting where the parameters of the FS-neurons can only take on discrete values in therange from [\u22128,8]. The possible values are equally spaced and the number of values can bewritten as 2Q, where Qrefers to the number of bits which are available for each parameterT(t), h(t), d(t) of the FS-neuron. Fig. 5e, f depict the impact of such quantization on themean squared error of the approximation of the activation function.Expected implementation cost on neuromorphic hardwareWe distinguish three types of neuromorphic hardware\u2022 Digital, but hardware not constrained to a particular neuron model (example: SpiN-Naker)\u2022 Digital, but hardware is constrained to a particular neuron model (example: Loihi)\u2022 Mixed digital analog (examples: IBM research chip with memristors and BrainScaleS-2)SpiNNakerThe SpiNNaker platform [Furber et al., 2014] provides a \ufb02exible environment which is notconstrained to a speci\ufb01c neuron model. SpiNNaker allows to compute all products d(t)won the chip, which reduces the additional memory consumption to to a small constantvalue. All parameters T(t), h(t) and d(t) only need to be stored in memory once, as theycan be shared across all neurons which approximate the same activation function. Theadditional computational complexity of the FS-neuron model also has a very small impact,as computing the updated weight d(t)w can be done with a single instruction cycle.LoihiLoihi [Davies et al., 2018] also promises to be an interesting target platform for FS-neurons.Especially FS-neurons approximating the ReLU activation function could be ported verye\ufb03ciently to this hardware platform. As the chip is based on \ufb01xed-point arithmetic, onecan implement T(t), h(t) and d(t) for ReLu using a single parameter, namely the sharedweight exponent. This is be possible due to the fact that at every time step t all FS-parameters have the same value, which is always a power of 2. Therefore, the additionalmemory consumption does not grow with K. It is also possible to use other activationfunctions besides ReLU on Loihi, however, in this case it would be necessary to store allproducts d(t)w on the chip, as computing the updated weight in an online fashion is notpossible. In this case, an increase in memory consumption of O(K) is expected.13--- Page 14 ---IBM research chip with memristorsIBM has presented an in-memory chip architecture supporting both ANNs and SNNs in theSupplementary Material S3 of the article [Wo\xb4 zniak et al., 2020]. This architecture employsa memristor crossbar array for fast (time complexity O(1)) and energy-e\ufb03cient multipli-cation of the outputs of one layer l of neurons with the weights of synaptic connectionsto neurons on the next layer l+ 1. One can replace all spikes (i.e., values 1) that emergefrom layer l at time t of the K-step cycle in the emulation of the neurons on layer l by acommon value value d(t) that is centrally stored. Since the values d(t) and 0 can be useddirectly as inputs to the memristor array, no signi\ufb01cant extra cost is expected. The neuronmodels are implemented in the digital part of this neuromorphic chip architecture of IBM,using very fast digital logic and SRAM for storing parameters. Since all neurons on a layerl of our FS networks use the same parameters T(t) and h(t), they can be stored in a localSRAM for all neurons on layer l, in a similar fashion as on SpiNNaker. A neuron modelthat goes already one step in the direction from LIF to FS-neurons has actually alreadybeen implemented on this architecture: The soft spiking neural unit (sSNU), that emitsanalog instead of binary values and subtracts a corresponding value from the membranepotential [Wo\xb4 zniak et al., 2020].BrainScaleS-2This neuromorphic chip [Billaudelle et al., 2020] is also a mixed analog digital architecturewhere a digital plasticity processor allows fast changes of synaptic weights, but also centralmemory storage and application of the time-varying parameters T(t), h(t) and d(t) ofthe neuron dynamics. Like on SpiNNaker, the parameters only have to be stored once inmemory and can be shared across many neurons. The leak term of the membrane voltageof the analog neuron models can be switched o\ufb00, so that the analog part can be used fore\ufb03cient matrix multiplication in a similar manner as on the IBM chip.MethodsIn this section various details necessary to reproduce our results have been listed. Ad-ditionally, to aid the interpretation of the results, a comparison to previous conversionapproaches has been added.When training the parameters of the FS-neurons it is important to specify an intervalin which the approximation should be very good. Ideally, most of the inputs to the ANNneuron should fall into this interval to guarantee a good performance. In our experiments,the FS-neurons have been trained to approximate the interval from [ \u22128,12] for the SiLUfunction and [ \u221210,10] for the sigmoid function. The resulting FS-neuron approximatesthe SiLU function with a mean squared error of 0 .0023 inside the main region [ \u22122,2] and0.0064 in the region outside, which can be written as [ \u22128,\u22122] \u222a[2,12]. As a result of our14--- Page 15 ---\ufb01ne-tuning the values for T(t), d(t) and h(t) stay for most time steps t within the mainregion [\u22122,2] as can be seen in Fig. 4b.To reduce the complexity of the converted CNN, we decided not to emulate the mul-tiplication operation by FS-neurons, which occurs in the CNN if the squeeze and excita-tion optimization [Hu et al., 2018] is being used. In many neuromorphic chips, such asSpiNNaker and Loihi, the on-chip digital processor could carry out these multiplications.Otherwise one can approximate multiplication in a similar manner as the SiLU functionwith a suitably optimized FS-neuron, see [St\xa8 ockl and Maass, 2019]. Alternatively one cancompute multiplication with a small circuit of threshold gates, i.e., very simple types ofspiking neurons, of depth 2 or 3. A recent summary of such results is provided in section3 of [Parekh et al., 2018].Due to the simple structure of the parameters of the ReLU FS-neurons several advan-tages arise. In particular when approximating the ReLU function with an FS-neuron it ispossible to calculate the changes of parameters for t= 1,...,K by simply using a bit shiftoperation, possibly providing a very e\ufb03cient implementation on neuromorphic hardware.The resulting SNN can be used in a pipelined manner, processing a new network inputevery 2K = 20 time steps, analogously as for the SiLU function.Further properties of FS-coding that are relevant for neuromor-phic hardwareOne of the major advantages of using FS-neurons in neuromorphic hardware is the smalleramount of time steps and spikes required to approximate arti\ufb01cial neurons. For the case ofthe ReLU activation function, a rate coded spiking neuron requires N time steps to encodeN di\ufb00erent values. FS-neurons improve upon this unary coding scheme by utilizing thetime dimension to implement a binary coding scheme. Therefore, the number of time stepsrequired to encode N di\ufb00erent values can be reduced to just log2(N).To underline the binary coding nature of FS-neurons, in the case of the ReLU activationfunction, the corresponding FS-neurons will show a spiking pattern equivalent to of theoutput of the ReLU function, written as a binary number. The same logarithmic relationholds not only for the number of time steps required but also for the number of spikes neededto transmit a value. More sophisticated codes could be used to make the computationrobust to noise in spike transmission.Note, that most of the inputs to the FS-neurons have a value close to 0, as shown inFig. 5 a and b. Consequently, the FS-neurons usually require only a few spikes to transmitthe output values, making the the coding scheme even more sparse in practice.Comparison with previous methodsThe idea of converting a pre-trained ANN to a SNN has received a fair amount of attentionin the recent years. The most popular conversion approaches are rate-based, meaning they15--- Page 16 ---translate the continuous output of an arti\ufb01cial ReLU neuron into a \ufb01ring rate of a spikingneuron. Unfortunately there are some drawbacks associated with rate-coding. Due to itsunary coding nature, rate-codes are sub-optimal in the sense that they do not make gooduse of the time dimension. Usually a large amount of time steps is required to achieve asu\ufb03ciently accurate approximation. Furthermore, rate-based conversions are only capableof converting simple activation functions like ReLU, but fail to convert more sophisticatedfunctions like SiLU.Another popular conversion approach uses time to \ufb01rst spike (TTFS) coding [Rueckauerand Liu, 2018]. This approach encodes the continuous outputs of the corresponding ReLUANN neurons in the length of the time interval until the \ufb01rst spike, resulting in a verysparse spiking activity. However, this method seems to not scale easily to large modelsand has, to the best of our knowledge, not been tested on large data sets like ImageNet.The idea of using single spike temporal coding has \ufb01rst been explored in [Maass, 1997] andit has been shown to have a variety of applications, like implementing an e\ufb03cient k-NNalgorithm on neuromorphic hardware [Frady et al., 2020].Furthermore, a new conversion method has been proposed, in which the spiking neuronscan approximate the ReLU function using a hysteresis quantization method [Yousefzadehet al., 2019]. This approach waits to be tested on larger networks and datasets.One property that all previously mentioned conversion methods have in common is thatthey only consider transforming arti\ufb01cial ReLU neurons to spiking neurons, and thereforecannot convert more sophisticated activation functions, which are used in network archi-tecture like the E\ufb03cientNets.A detailed summary comparing FS-coding to previous results can be found in theExtended Data Table 1.It is worth noting, that the throughput using FS-coding is substantially better thanthat of SNNs which result from rate-based ANN-to-SNN conversions of ANNs with theReLU function, as proposed for example in [Rueckauer et al., 2017, Sengupta et al., 2019].The Inception-v3 model in [Rueckauer et al., 2017] was reported to yield a SNN that needed550 time steps to classify an image. Under the assumption that rate-based models pro\ufb01tonly very little from pipelining, it is reasonable to estimate that the throughput of an SNNthat results from FS-conversion of ReLU gates with K = 10 is roughly 25 times higher.The SNN resulting from the rate-based conversion of the ResNet34 model discussedin [Sengupta et al., 2019] has been reported to use 2500 time steps for a classi\ufb01cation.Therefore we estimate that the throughput is increased here by a factor around 125 throughFS-conversion.Spiking versions of ResNet20 have already been previously explored [Sengupta et al.,2019]. Using a rate-based conversion scheme an accuracy of 87.46% was reported.FS-conversion of ResNet20 yields a substantially higher accuracy of 91.45%, using just80 to 500 time steps for each image -depending on the model depth- instead of 2000,thereby signi\ufb01cantly reducing latency. In addition, the throughput is drastically improved.Also the number of spikes that the SNN uses for classifying an image from CIFAR10is signi\ufb01cantly reduced when one moves from a rate-based conversion to an FS conversion.A converted ResNet11 has been reported to use more than 8 million spikes to classify16--- Page 17 ---a single test example [Lee et al., 2020]. Comparing this to an FS-converted ResNet14we \ufb01nd that the latter uses 40 times fewer spikes despite being a slightly larger model.Using direct training of SNNs instead of a conversion scheme has been reported to resultin a lower amount of spikes needed to perform a single classi\ufb01cation. However, even adirectly trained SNN version of ResNet11 uses 7 times more spikes than an FS-conversionof ResNet14 [Lee et al., 2020, Table 8].In [Rathi et al., 2020] the authors present a novel approach for obtaining high perfor-mance SNNs by combining a rate-based conversion scheme with a subsequent gradient-based \ufb01ne-tuning procedure. They report the highest accuracy for an SNN on CIFAR10,which was achieved by converting a very performant ANN. They also show results forImageNet, where they achieve an accuracy of 65 .1% on their ResNet-34. Deeper models,like the ResNet-50, were not considered in this work. On ImageNet FS-conversion of theResNet-50 model improves their accuracy by 10% and FS-conversion of the E\ufb03cientNet-B7surpasses their performance by 18.47%.DiscussionWe have presented a new approach for generating SNNs that are very close to ANNs interms of classi\ufb01cation accuracy for images, while working in the energetically most attrac-tive regime with very sparse \ufb01ring activity. Besides substantially improved classi\ufb01cationaccuracy, they exhibit drastically improved latency and throughput compared with rate-based ANN-to-SNN conversions. o\ufb00 the shelf. One can argue that this is exactly theway which evolution has chosen for the design of neurons in living organism. Not onlyneurons with particular information processing tasks in the smaller nervous systems ofinsects, but also neurons in the neocortex of mammals exhibit an astounding diversity ofgenetically encoded response properties [ [Sterling and Laughlin, 2015], [Gouwens et al.,2019], [Bakken et al., 2020]]. In particular, the probability of producing a spike dependsin diverse ways on the recent stimulation history of the neuron, see [Gerstner et al., 2014]for some standard models. In other words, the excitability of di\ufb00erent types of biologicalneurons increases or decreases in complex ways in response to their previous \ufb01ring. As aresult, the temporal structure of a train of spikes that is produced by a biological neuroncontains additional information about the neuron input that can not be captured by its\ufb01ring rate. Similary, FS-neurons that are optimized for high accuracy image classi\ufb01cationwith few spikes exhibit history-dependent changes -encoded through their functions T(t)and h(t) according to equ. (2)- in their propensity to \ufb01re, see Fig. 4b and 4e. Furthermorethe function d(t) enables subsequent neurons to decode their spikes in a timing-sensitivemanner. In these regards an FS-conversion from ANNs to SNNs captures more of the func-tional capabilities of spiking neurons than previously considered rate-based conversions toan o\ufb00-the-shelf spiking neuron model.It is well known that spikes from the same neurons in the brain can transmit di\ufb00erentinformation to downstream neurons depending on the timing of the spike, see e.g. phaseprecession in the hipppocampus [Harris et al., 2002]. Hence is is conceivable that down-17--- Page 18 ---stream neurons give di\ufb00erent weights to these spikes, in dependence of the \ufb01ring time ofthe presynaptic neuron. In fact, it is well known that the large repertoire of pre- andpostsynaptic synaptic dynamics found in di\ufb00erent synapses of the brain [Markram et al.,2004, Kopanitsa et al., 2018] enables postsynaptic neurons to modulate the amplitude ofpostsynaptic responses in dependence of the timing of presynaptic spikes relative to an un-derlying rhythm. This can be viewed as a biological counterpart of the timing-dependentweights d(t) in our model. Altogether we believe that FS-neurons provide a \ufb01rst step inexploring new uses of SNNs where information is not encoded by the timing of single spikesor \ufb01ring rates, but by temporal spike patterns.Important for applications of FS-coding in neuromorphic hardware is that it is ap-plicable to virtually any activation function, in particular to that activation function forANN neurons that currently provides the highest accuracy on ImageNet, the SiLU func-tion. Rate-based conversion can not be readily applied to the SiLU function because itassumes both positive and negative output values. When approximating the more com-monly used ReLU function, FS-neurons approach the information theoretic minimum ofspikes for spike-based communication.In fact, FS-neurons that emulate ANN gates with the ReLU activation function produce1.5 spikes on average for classifying an image, while those for the Switch activation functionproduce 2 spikes on average. As the number of spikes required for inference by an SNNis directly related to its energy consumption in spike-based neuromorphic hardware, theenergy consumption of FS-converted SNNs appears to be close to the theoretical optimumfor SNNs. Since FS-conversion provides a tight bound on the number K of time stepsduring which a spiking neuron is occupied, it can also be used for converting recurrentlyconnected ANNs to SNNs.The proposed method for generating highly performant SNNs for image classi\ufb01cationthrough FS-conversion of trained CNNs o\ufb00ers an opportunity to combine the computa-tionally more e\ufb03cient and functionally more powerful training of ANNs with the superiorenergy-e\ufb03ciency of SNNs for inference. Note that one can also use the resulting SNN asinitialization for further training of the SNN, e.g., for a more speci\ufb01c task.Altogether our results suggest that spike-based hardware may gain an edge in thecompetition for the development of drastically more energy-e\ufb03cient hardware for AI if onedoes not forgot to optimize the spiking neuron model in the hardware for its intendedrange of applications. In contrast to energy e\ufb03cient digital hardware that is optimizedfor speci\ufb01c ANN architectures and activation functions, see e.g. [Zhang et al., 2019] for areview, a spike-based neuromorphic chip that is able to emulate FS-neurons can carry outinference for all possible ANN architectures. It can also emulate ANNs with previouslynot considered activation functions, since a change of the activation function just requiresreprogramming of the digitally stored parameters of FS-neurons. Hence such spike-basedchip will be substantially more versatile than common digital hardware accelerators for AI.18--- Page 19 ---AcknowledgementsWe would like to thank Franz Scherr for helpful discussions. We thank Thomas Bohnst-ingl, Evangelos Eleftheriou, Steve Furber, Christian Pehle, Philipp Plank and JohannesSchemmell for advice regarding implementation aspects of FS-neurons in various typesof neuromorphic hardware. This research was partially supported by the Human BrainProject of the European Union (Grant agreement number 785907). We also thank ouranonymous reviewers for very constructive comments.Competing InterestsWe are not aware of competing interests.Data availabilityBoth ImageNet [Deng et al., 2009] and CIFAR10 [Krizhevsky et al., 2009] are publiclyavailable datasets. No additional datasets were generated or analysed during the cur-rent study. The data for the spike response depiced in \ufb01gure 1 has been publishedbytheAllen Institute for Brain Science in 2015 (Allen Cell Types Database). Available from:https://celltypes.brain-map.org/experiment/electrophysiology/587770251Author contributionsCS conceived the main idea, CS and WM designed the model and planned the experiments,CS carried out the experiments, CS and WM wrote the paper.Code availabilityThe code this work is based on is publicly available at: https://github.com/christophstoeckl/FS-neurons (DOI: 10.5281/zenodo.4326749). Additionally, the code is also available in a CodeOcean compute capsule: https://codeocean.com/capsule/7743810/treeReferences[Bakken et al., 2020] Bakken, T. E., Jorstad, N. L., Hu, Q., Lake, B. B., Tian, W., Kalm-bach, B. E., Crow, M., Hodge, R. D., Krienen, F. M., Sorensen, S. A., et al. (2020).Evolution of cellular diversity in primary motor cortex of human, marmoset monkey,and mouse. bioRxiv.19--- Page 20 ---[Bellec et al., 2020] Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legen-stein, R., and Maass, W. (2020). A solution to the learning dilemma for recurrentnetworks of spiking neurons. Nature Communications, 11(1):1\u201315.[Billaudelle et al., 2020] Billaudelle, S., Stradmann, Y., Schreiber, K., Cramer, B., Baum-bach, A., Dold, D., G\xa8 oltz, J., Kungl, A. F., Wunderlich, T. C., Hartel, A., M\xa8 uller, E.,Breitwieser, O., Mauch, C., Kleider, M., Gr\xa8 ubl, A., St\xa8 ockel, D., Pehle, C., Heimbrecht,A., Spilger, P., Kiene, G., Karasenko, V., Senn, W., Petrovici, M. A., Schemmel, J., andMeier, K. (2020). Versatile emulation of spiking neural networks on an accelerated neu-romorphic substrate. In 2020 IEEE International Symposium on Circuits and Systems(ISCAS), pages 1\u20135.[Davies et al., 2018] Davies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y., Choday,S. H., Dimou, G., Joshi, P., Imam, N., Jain, S., et al. (2018). Loihi: A neuromorphicmanycore processor with on-chip learning. IEEE Micro, 38(1):82\u201399.[Deng et al., 2009] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on com-puter vision and pattern recognition , pages 248\u2013255. Ieee.[Frady et al., 2020] Frady, E. P., Orchard, G., Florey, D., Imam, N., Liu, R., Mishra, J.,Tse, J., Wild, A., Sommer, F. T., and Davies, M. (2020). Neuromorphic nearest neighborsearch using intel\u2019s pohoiki springs. In Okandan, M. and Aimone, J. B., editors, NICE\u201920: Neuro-inspired Computational Elements Workshop, Heidelberg, Germany, March17-20, 2020, pages 23:1\u201323:10. ACM.[Furber et al., 2014] Furber, S. B., Galluppi, F., Temple, S., and Plana, L. A. (2014). Thespinnaker project. Proceedings of the IEEE, 102(5):652\u2013665.[Garc\xb4 \u0131a-Mart\xb4 \u0131n et al., 2019] Garc\xb4 \u0131a-Mart\xb4 \u0131n, E., Rodrigues, C. F., Riley, G., and Grahn, H.(2019). Estimation of energy consumption in machine learning. Journal of Parallel andDistributed Computing, 134:75\u201388.[Gerstner et al., 2014] Gerstner, W., Kistler, W. M., Naud, R., and Paninski, L. (2014).Neuronal dynamics: From single neurons to networks and models of cognition. Cam-bridge University Press .[Gouwens et al., 2019] Gouwens, N. W., Sorensen, S. A., Berg, J., Lee, C., Jarsky, T., Ting,J., Sunkin, S. M., Feng, D., Anastassiou, C. A., Barkan, E., et al. (2019). Classi\ufb01cation ofelectrophysiological and morphological neuron types in the mouse visual cortex. Natureneuroscience, 22(7):1182\u20131195.[Han et al., 2020] Han, B., Srinivasan, G., and Roy, K. (2020). Rmp-snn: Residual mem-brane potential neuron for enabling deeper high-accuracy and low-latency spiking neuralnetwork. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 13558\u201313567.20--- Page 21 ---[Harris et al., 2002] Harris, K. D., Henze, D. A., Hirase, H., Leinekugel, X., Dragoi, G.,Czurk\xb4 o, A., and Buzs\xb4 aki, G. (2002). Spike train dynamics predicts theta-related phaseprecession in hippocampal pyramidal cells. Nature, 417(6890):738\u2013741.[He et al., 2016] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning forimage recognition. Proceedings of the IEEE Computer Society Conference on ComputerVision and Pattern Recognition, 2016-Decem:770\u2013778.[Hendrycks and Gimpel, 2016] Hendrycks, D. and Gimpel, K. (2016). Gaussian error linearunits (gelus). arXiv preprint arXiv:1606.08415 .[Hu et al., 2018] Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-Excitation Networks.Proceedings of the IEEE Computer Society Conference on Computer Vision and PatternRecognition, pages 7132\u20137141.[Kheradpisheh and Masquelier, 2020] Kheradpisheh, S. R. and Masquelier, T. (2020).S4nn: temporal backpropagation for spiking neural networks with one spike per neuron.International Journal of Neural Systems , 30(6):2050027.[Kopanitsa et al., 2018] Kopanitsa, M. V., van de Lagemaat, L. N., A\ufb01nowi, N. O.,Strathdee, D. J., Strathdee, K. E., Fricker, D. G., Tuck, E. J., Elsegood, K. A., Croning,M. D., Komiyama, N. H., et al. (2018). A combinatorial postsynaptic molecular mech-anism converts patterns of nerve impulses into the behavioral repertoire. BioRxiv, page500447.[Krizhevsky et al., 2009] Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layersof features from tiny images. Technical report.[Lee et al., 2020] Lee, C., Sarwar, S. S., Panda, P., Srinivasan, G., and Roy, K. (2020).Enabling spike-based backpropagation for training deep neural network architectures.Frontiers in Neuroscience, 14:119.[Ling, 2001] Ling, J. (2001). https://hypertextbook.com/facts/2001/JacquelineLing.shtml.[Maass, 1997] Maass, W. (1997). Fast sigmoidal networks via spiking neurons. NeuralComputation, 9(2):279\u2013304.[Maass and Natschl\xa8 ager, 1998] Maass, W. and Natschl\xa8 ager, T. (1998). Emulation of Hop-\ufb01eld networks with spiking neurons in temporal coding. In Computational Neuroscience,pages 221\u2013226. Springer.[Markram et al., 2004] Markram, H., Toledo-Rodriguez, M., Wang, Y., Gupta, A., Silber-berg, G., and Wu, C. (2004). Interneurons of the neocortical inhibitory system. Naturereviews neuroscience, 5(10):793\u2013807.21--- Page 22 ---[Parekh et al., 2018] Parekh, O., Phillips, C. A., James, C. D., and Aimone, J. B. (2018).Constant-depth and subcubic-size threshold circuits for matrix multiplication. In Pro-ceedings of the 30th on Symposium on Parallelism in Algorithms and Architectures, pages67\u201376.[Rathi et al., 2020] Rathi, N., Srinivasan, G., Panda, P., and Roy, K. (2020). Enablingdeep spiking neural networks with hybrid conversion and spike timing dependent back-propagation. In International Conference on Learning Representations.[Rueckauer and Liu, 2018] Rueckauer, B. and Liu, S.-C. (2018). Conversion of analog tospiking neural networks using sparse temporal coding. In 2018 IEEE InternationalSymposium on Circuits and Systems (ISCAS) , pages 1\u20135. IEEE.[Rueckauer et al., 2017] Rueckauer, B., Lungu, I. A., Hu, Y., Pfei\ufb00er, M., and Liu, S. C.(2017). Conversion of continuous-valued deep networks to e\ufb03cient event-driven networksfor image classi\ufb01cation. Frontiers in Neuroscience, 11(DEC):1\u201312.[Russakovsky et al., 2015] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei,L. (2015). ImageNet Large Scale Visual Recognition Challenge. International Journalof Computer Vision , 115(3):211\u2013252.[Sengupta et al., 2019] Sengupta, A., Ye, Y., Wang, R., Liu, C., and Roy, K. (2019).Going Deeper in Spiking Neural Networks: VGG and Residual Architectures. Frontiersin Neuroscience, 13(1998):1\u201316.[Sterling and Laughlin, 2015] Sterling, P. and Laughlin, S. (2015). Principles of neuraldesign. MIT Press.[St\xa8 ockl and Maass, 2019] St\xa8 ockl, C. and Maass, W. (2019). Recognizing images with atmost one spike per neuron. arXiv preprint arXiv:2001.01682 .[Tan and Le, 2019] Tan, M. and Le, Q. (2019). E\ufb03cientNet: Rethinking model scalingfor convolutional neural networks. In Chaudhuri, K. and Salakhutdinov, R., editors,Proceedings of the 36th International Conference on Machine Learning , volume 97 ofProceedings of Machine Learning Research , pages 6105\u20136114, Long Beach, California,USA. PMLR.[Thorpe et al., 2001] Thorpe, S., Delorme, A., and Rullen, R. (2001). Spike-based strate-gies for rapid processing. Neural networks : the o\ufb03cial journal of the InternationalNeural Network Society, 14:715\u201325.[Van Horn et al., 2015] Van Horn, G., Branson, S., Farrell, R., Haber, S., Barry, J., Ipeiro-tis, P., Perona, P., and Belongie, S. (2015). Building a bird recognition app and largescale dataset with citizen scientists: The \ufb01ne print in \ufb01ne-grained dataset collection.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,pages 595\u2013604.22--- Page 23 ---[Wo\xb4 zniak et al., 2020] Wo\xb4 zniak, S., P, Your response: ```ADD_PAPER\n2002.00860v4\nThis paper introduces an optimized spiking neuron model for efficient image classification on edge devices through temporal coding with just two spikes per neuron. The authors propose a novel ANN-to-SNN conversion method called FS-conversion that significantly improves upon traditional rate-based approaches by encoding information in spike timing rather than just spike counts. Key innovations include: 1) A modified spiking neuron model (FS-neuron) with optimized internal dynamics for emulating ANN activation functions with minimal spikes, 2) Demonstration on ImageNet and CIFAR10 showing near-identical accuracy to original ANNs (83.57% on ImageNet with EfficientNet-B7) while using only ~2 spikes per neuron, 3) Substantial improvements in latency and throughput compared to rate-based conversions, enabling new image classification every 32 time steps. The method is particularly effective for edge deployment as it achieves biological-level energy efficiency while maintaining high accuracy, making it suitable for mobile and embedded vision applications.\n```\n~~~~~~~~~~\nCurrent Step #15, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2002.00860v4\nNotes: \nYour previous command was: ```ADD_PAPER\n2002.00860v4\nThis paper introduces an optimized spiking neuron model for efficient image classification on edge devices through temporal coding with just two spikes per neuron. The authors propose a novel ANN-to-SNN conversion method called FS-conversion that significantly improves upon traditional rate-based approaches by encoding information in spike timing rather than just spike counts. Key innovations include: 1) A modified spiking neuron model (FS-neuron) with optimized internal dynamics for emulating ANN activation functions with minimal spikes, 2) Demonstration on ImageNet and CIFAR10 showing near-identical accuracy to original ANNs (83.57% on ImageNet with EfficientNet-B7) while using only ~2 spikes per neuron, 3) Substantial improvements in latency and throughput compared to rate-based conversions, enabling new image classification every 32 time steps. The method is particularly effective for edge deployment as it achieves biological-level energy efficiency while maintaining high accuracy, making it suitable for mobile and embedded vision applications.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 17:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #14, Phase: literature review, Feedback: 3\n--- Page 1 ---Optimized spiking neurons classify images with highaccuracy through temporal coding with two spikesChristoph St\xa8 ockl1, Wolfgang Maass1,\u2217January 27, 20211Institute of Theoretical Computer Science, Graz University of Technology,In\ufb00eldgasse 16b, Graz, Austria\u2217 To whom correspondence should be addressed; E-mail: maass@igi.tugraz.at.AbstractSpike-based neuromorphic hardware promises to reduce the energy consumptionof image classi\ufb01cation and other deep learning applications, particularly on mobilephones or other edge devices. However, direct training of deep spiking neural net-works is di\ufb03cult, and previous methods for converting trained arti\ufb01cial neural net-works to spiking neurons were ine\ufb03cient because the neurons had to emit too manyspikes. We show that a substantially more e\ufb03cient conversion arises when one op-timizes the spiking neuron model for that purpose, so that it not only matters forinformation transmission how many spikes a neuron emits, but also when it emitsthose spikes. This advances the accuracy that can be achieved for image classi\ufb01cationwith spiking neurons, and the resulting networks need on average just two spikes perneuron for classifying an image. In addition, our new conversion method improveslatency and throughput of the resulting spiking networks.Spiking neural networks (SNNs) are currently explored as possible solution for a majorimpediment of more widespread uses of modern AI in edge devices: The energy consump-tion of the large state-of-the-art arti\ufb01cial neural networks (ANNs) that are produced bydeep learning.This holds in particular for the Convolutional Neural Networks (CNNs) that are com-monly used for image classi\ufb01cation, but also other application domains. These ANNs haveto be large for achieving top performance, since they need to have a su\ufb03ciently largenumber of parameters in order to absorb enough information from the huge data setson which they have been trained, such as the 1.2 million images of the ImageNet2012dataset. Inference with standard hardware implementations of these large ANNs is inher-ently power-hungry [Garc\xb4 \u0131a-Mart\xb4 \u0131n et al., 2019].1arXiv:2002.00860v4  [cs.NE]  26 Jan 2021--- Page 2 ---Spiking neurons have been in the focus of the development of novel computing hardwarefor AI with a drastically reduced energy budget, partially because the giant SNN of thebrain \u2013consisting of about 100 billion neurons\u2013 consumes just 20W [Ling, 2001]. Spikingneurons output trains of stereotypical pulses that are called spikes. Hence their output isvery di\ufb00erent from the continuous numbers that an ANN neuron produces as output. Mostspiking neuron models that are considered for implementation in neuromorphic hardwareare inspired by simple models for spiking neurons in the brain. However, these simpleneuron models do not capture the capability of biological neurons to encode di\ufb00erentinputs by di\ufb00erent temporal spike patterns, not just by their \ufb01ring rate (see Fig. 1 for anexample).Figure 1: Encoding of di\ufb00erent input values (current steps of di\ufb00erent ampli-tudes) by temporal spike patterns in a biological neuron. Data taken from theAllen Cell Type Database1 (Layer 3 spiny neuron from the human middle temporal gyrus).While large ANNs, trained with ever more sophisticated deep learning algorithms ongiant data sets, approach \u2013and sometimes exceed\u2013 human performance in several categoriesof intelligence, the performance of the current generation of spike-based neuromorphichardware is lagging behind. There is some hope that this gap can be closed for the case ofrecurrent spiking neural networks, since those can be trained directly to achieve most ofthe performance of recurrent ANNs [Bellec et al., 2020].1 \xa9 2015 Allen Institute for Brain Science. Allen Cell Types Database. Available from:https://celltypes.brain-map.org/experiment/electrophysiology/5877702512--- Page 3 ---But the problem to produce SNNs that achieve similar performance as ANNs with fewspikes persists for feedforward networks. Feedforward CNNs that achieve really good im-age classi\ufb01cation accuracy tend to be very deep and very large, and training correspondingdeep and large feedforward SNNs has not been able to reach similar classi\ufb01cation accuracy.Problems with the timing of spikes and precision of \ufb01ring rates on higher levels of the re-sulting SNNs have been cited as possible reasons. One attractive alternative is to simplytake a well-performing trained CNN and convert it into an SNN \u2013using the same connec-tions and weights. The most common \u2013and so far best performing\u2014 conversion methodwas based on the idea of (\ufb01ring-) rate coding, where the analog output of an ANN unit isemulated by the \ufb01ring rate of a spiking neuron [Rueckauer et al., 2017]. This method hadproduced so far the best SNN results for image classi\ufb01cation. But the transmission of ananalog value through a \ufb01ring rate tends to require a fairly large number of spikes, whichreduces both latency and throughput of the network. Furthermore, the resulting SNNtends to produce so many spikes that its energy-advantage over non-spiking hardware getslost. Finally, a rate-based ANN-to-SNN conversion can not be applied to those ANNs thatcurrently achieve the highest accuracy on ImageNet, E\ufb03cientNets [Tan and Le, 2019], be-cause these employ an activation function that assumes both positive and negative values:the SiLU function [Hendrycks and Gimpel, 2016].We introduce a new ANN-to-SNN conversion that we call FS-conversion because itrequires a spiking neuron to emit just a few spikes (FS = Few Spikes). This methodis completely di\ufb00erent from rate-based conversions, and exploits the option of temporalcoding with spike patterns, where the timing of a spike transmits extra information.Most previously proposed forms of temporal coding, see e.g. [Maass and Natschl\xa8 ager,1998], [Thorpe et al., 2001], [Rueckauer et al., 2017], [Kheradpisheh and Masquelier, 2020],have turned out to be di\ufb03cult to implement e\ufb03ciently in neuromorphic hardware becausethey require to transmit \ufb01ne time-di\ufb00erences between spikes to downstream neurons. Incontrast, an FS-conversion can be implemented with just log Ndi\ufb00erent values of spiketimes and at most log Nspikes for transmitting integers between 1 and N. Practically, therequired number of spikes can be made even lower because not all N values occur equallyoften. However FS-conversion requires a modi\ufb01ed spiking neuron model, the FS-neuron,which has an internal dynamic that is optimized for emulating particular types of ANNneurons with few spikes. We demonstrate the performance of SNNs that result from FS-conversion of CNNs, on two state-of-the-art datasets for image classi\ufb01cation: ImageNet2012and CIFAR10. This optimized spiking neuron model could serve as guidance for the nextgeneration of neuromorphic hardware.Emulating an ANN neuron by a spiking neuron withfew spikesThe FS-conversion from ANNs to SNNs requires a variation of the standard spiking neuronmodel, to which we refer as FS-neuron. The computation step of a generic arti\ufb01cial neuron3--- Page 4 ---in an ANN (see Fig. 2 a) is emulated by K time steps of an FS-neuron (Fig. 2 b). Itsinternal dynamics is de\ufb01ned by \ufb01xed parameters T(t),h(t),d(t) for t= 1,...,K . These areoptimized to emulate the activation function f(x) of the given ANN neuron by a weightedsum of spikes \u2211Kt=1 d(t)z(t), where z(t) denotes the spike train that this neuron produces.More precisely: z(t) = 1 if the neuron \ufb01res at step t, else z(t) = 0. To emit a spike attime t, a neuron\u2019s membrane potential v(t) has to surpass the current value T(t) of its\ufb01ring threshold. We assume that the membrane potential v(t) has no leak, but is reset tov(t) \u2212h(t) after a spike at time t. Expressed in formulas, the membrane potential v(t)starts with value v(1) = x where x is the gate input, and evolves during the K stepsaccording tov(t+ 1) = v(t) \u2212h(t)z(t). (1)The spike output z(t) of an FS-neuron for gate input x can be de\ufb01ned compactly byz(t) = \u0398(v(t) \u2212T(t)) = \u0398((x\u2212t\u22121\u2211j=1h(j)z(j))\u2212T(t)), t = 1,...,K, (2)where \u0398 denotes the Heaviside step function. The total output \u02c6f(x) of the FS-neuron fromthese K time steps, which is collected by the FS-neurons on the next layer, can be writtenas:\u02c6f(x) =K\u2211t=1d(t)z(t). (3)An illustration of the model can be found in Fig. 2b.Figure 2: Conversion of an ANN neuron into an FS-neuron.a) A generic ANN neuron with activation function f(x) that is to be emulated.b) An FS-neuron which emulates this ANN neuron in K time steps t= 1,...,K . Its outputspike train is denoted by z(t).4--- Page 5 ---For emulating the ReLU activation function one can choose the parameters of the FS-neuron so that they de\ufb01ne a coarse-to-\ufb01ne processing strategy for all input values x thatlie below some upper bound, as described in the Methods section. For emulating the SiLUfunction of E\ufb03cientNet one achieves a better FS-conversion if the parameters are chosenin such a way that they enable iterative \u2013and thereby more precise\u2013 processing for therange of inputs between \u22122 and 2 that occur most often as gate inputs x in E\ufb03cientNet.The resulting dynamics of FS-neurons is illustrated in Fig. 3 for the case of the SiLU andsigmoid activation functions.All FS-neurons that emulate ANN neurons with the same activation function can usethe same parameters T(t), h(t), d(t), while the factor w in the weights of their outputspikes is simply lifted from the corresponding synaptic connection in the trained ANN (seeFig. 2).5--- Page 6 ---Figure 3: Internal dynamics of Few-Spikes neurons.The \ufb01rst row depicts the response of the FS-neurons to a low input value ( x= \u22120.5) andthe second row displays the response to a high input ( x = 0.5). The \ufb01rst column showsresponses of an SiLU FS-neuron, while the second column a sigmoid FS-neuron. Therelevant values of T(t) and v(t) for discrete time steps t (see Fig. 4b and d) are smoothlyinterpolated for illustration.Note that the number of neurons and connections in the network is not increasedthrough the FS-conversion. However the number of computation steps L of a feedforwardANN with Llayers is increased by the factor K. But the computations of the ANN can beemulated in a pipelined manner, where a new network input (image) is processed every 2Ktime steps by the SNN. In this case the parameters of the FS-neurons change periodically6--- Page 7 ---with a period of length K while the FS-neurons compute. These K steps are followed byK time steps during which the FS-neurons are inactive, while the FS-neurons on the nextlayer collect their spike inputs for emulating the next computation step or layer of theANN. Note that since all FS-neurons that emulate ANN neurons with the same activationfunction can use the same parameters T(t), h(t), d(t), they require only little extra memoryon a neuromorphic chip.Both the TensorFlow code and the chosen parameters of the FS-neurons are availableonline\u2217.Application to ImageNetThe ImageNet data set [Russakovsky et al., 2015] has become the most popular bench-mark for state-of-the-art image classi\ufb01cation in machine learning (we are using here theImageNet2012 version). This data set consists of 1 .281.167 training images and 50.000 testimages (both RGB images of di\ufb00erent sizes), that are labeled by 1000 di\ufb00erent categories.Classifying images from ImageNet is a nontrivial task even for a human, since this dataset contains for example 59 categories for birds of di\ufb00erent species and gender [Van Hornet al., 2015]. This may explain why a relaxed performance measurement, where one recordswhether the target class is among the top 5 classi\ufb01cations that are proposed by the neuralnetwork (\u201dTop5\u201d), is typically much higher.The recently proposed E\ufb03cientNet [Tan and Le, 2019] promises to become a new stan-dard CNN architecture due to its very high accuracy while utilizing a smaller number ofparameters than other CNN architectures. E\ufb03cientNet uses as activation function f(x)besides the SiLU function (Fig. 4) also the familiar sigmoid function, shown as the redcurve in Fig. 4 c. Note that 99 .97% of its activation functions are SiLU functions, makingthe appearance of the sigmoid function comparatively rare. The SiLU function emergedfrom preceding work on optimizing activation functions in ANNs [Zoph and Le, 2018].Another characteristic of the E\ufb03cientNet architecture is the extensive usage of depth-wiseseparated convolution layers. In between them, linear activation functions are used. Al-though it would certainly be possible to approximate linear functions using FS-coding, wesimply collapsed linear layers into the generation of the weighted sums that form the inputsto the next layers.Since the SiLU function assumes also negative values, it appears to be di\ufb03cult to convertan ANN with this activation function via rate-coding to a spiking neuron. But it is fairlyeasy to convert it to an FS-neuron. The values of the parameters T(t),h(t) and d(t) fort= 1,...,K of the FS-neuron can be obtained by training the FS-neuron model to \ufb01t theSiLU function, see Fig. 4 a and b. We used for that backpropagation through time, witha triangle-shaped pseudo derivative for the non-existing derivative of the Heaviside stepfunction.In most cases, the possible inputs to an activation function are not uniformly dis-tributed, but there exists a certain region in which most inputs lie with high probability.\u2217https://github.com/christophstoeckl/FS-neurons7--- Page 8 ---For example, most of the inputs to the SiLU functions in the E\ufb03cientNet are in the in-terval from \u22122 to 2 and therefore, achieving a high approximation accuracy in this regionis especially desirable, (see Fig. 5 a). It is possible to encourage the FS-neuron to putmore emphasis on a certain region, by assigning a high weight in the loss function to thisregion. More details about the training procedure of the FS-parameters can be found inthe Methods section.The e\ufb00ective activation function of the resulting FS-neuron is shown in Fig. 4a. Fig. 4c shows the corresponding result for the FS-conversion of an ANN neuron with the sigmoidactivation function.Figure 4: Approximations \u02c6f(x) of di\ufb00erent activation functions by FS-neuronsa) Approximation of the SiLU function with a single FS-neuron.(red: SiLU function, blue: FS-approximation with K = 16)b) Optimized internal parameters of the SiLU FS-neuron.c) Approximation of the sigmoid function with a single FS-neuron.d) Optimized internal parameters of the sigmoid FS-neuron.e) Approximation of the ReLU function with K = 10 and \u03b1= 25.8--- Page 9 ---Model ANNaccuracyaccuracy of theSNN producedby FS-conversion# params # layers # neurons # spikesImageNet2012E\ufb03cientNet-B7 85%(97.2 %)83.57%(96.7%) 66M 218 259M 554.9MResNet50 75.22%(92.4%)75.10%(92.36%) 26M 50 9.6M 14.045MCIFAR10ResNet8 87.22% 87.05% 78k 8 73k 103kResNet14 90.49% 90.39% 174k 14 131k 190kResNet20 91.58% 91.45% 271k 20 188k 261kResNet50 92.99% 92.42% 755k 50 475k 647kTable 1: Accuracy and spike numbers for classifying images from ImageNetwith FS-conversions of two state-of-the-art CNNs. The SNNs produced by FS-conversion of the ANNs achieved almost the same accuracy, and usually used at most 2spikes per neuron. Top5 accuracy is reported in parentheses. The number of spikes neededfor inference was obtained by averaging over the 1000 test images.Using these FS-neurons it is possible to emulate the E\ufb03cientNet-B7 model with spikingneurons. The accuracy of the resulting spiking CNN, using the publicly available weightsw of the trained E\ufb03cientNet, can be found in Table 1, together with the total number ofspikes that it uses for sample inferences.The FS-conversion of E\ufb03cientNet-B7 achieved an accuracy of 83 .57%. The best ac-curacy for ImageNet that had previously been reported for SNNs was 74 .6% [Rueckaueret al., 2017]. It was achieved by a rate-based conversion, which required a substantialnumber of spikes per neuron and about 550 time steps for each image classi\ufb01cation. TheSNN resulting from FS-conversion of E\ufb03cientNet-B7 used about 2 spikes per neuron forclassifying an image. The FS-neurons approximating the SiLU function used K = 16 andthe FS-neurons approximating the sigmoid function used K = 12.The layers of the CNN that use the SiLU function as activation function can be simu-lated in a pipelined manner by the SNN, processing a new image every 2 K time steps: Its\ufb01rst K time steps are spent collecting the outputs from the preceding layer of FS-neuronsduring their K time steps of activity. It then processes these collected inputs xduring thesubsequent K time steps.Hence the SNN that results from FS-conversion of E\ufb03cientNet can classify a new imageevery 2K = 32 time steps. Further implementation details can be found in the Methodssection.9--- Page 10 ---Approximating the ReLU activation functionThe ReLU activation function, see Fig. 4 d, is among the most frequently used activationfunctions, and also quite good accuracies have been achieved with it for ImageNet. Itrepresents a special case for FS-conversion, as it is possible to \ufb01nd the ideal values forh(t),T(t) and d(t) analytically, bases on the idea of computation with binary numbers.By setting the parameters of the FS-neuron to T(t) = h(t) = d(t) = 2 K\u2212t, the FS-neuron approximates the ReLU activation function f(x) with a coarse-to-\ufb01re-processingstrategy. Let us assume for simplicity that an FS-neuron receives inputs xfrom (\u2212\u221e,0] \u222a{1,2,..., 2K \u22121}. Then it reproduces with the speci\ufb01ed parameters the output ReLU( x)of the ReLU gate for any x from (\u2212\u221e,0] \u222a{1,2,..., 2K \u22121}without error. In order to beable to transmit also non-integer values xbetween 0 and some arbitrary positive constant\u03b1, one simply multiplies the given values for T(t),h(t) and d(t) with \u03b12\u2212K. Then the FS-neuron reproduces ReLU(x) for any non-negative xless than \u03b1that are multiples of \u03b12\u2212Kwithout error, and ReLU( x) is rounded down for values x in between to the next largermultiple of \u03b12\u2212K. Thus the output of the FS-neuron deviates for xin the range from \u2212\u221eto \u03b1 by at most \u03b12\u2212K from the output of the ReLU gate. The resulting approximation isplotted for \u03b1= 10 in Fig. 4 d. Several advantages arising from the simple structure of theparameters have been laid out in the Methods section.The accuracy of 75.22% for the ANN version of ResNet50 in Table 1 resulted from train-ing a variant of ResNet50 where max-pooling was replaced by average pooling, using thehyperparameters given in the TensorFlow repository\u2020. The resulting accuracy in ImageNetis close to the best published performance of 76% for ResNet50 ANNs [Tan and Le, 2019,Table 2]. The application of the FS-conversion to this variant of ResNet50 (with K = 10and \u03b1= 25) yields an SNN whose Top1 and Top5 performance is almost indistinguishablefrom that of the ANN version.Application to CIFAR10CIFAR10 [Krizhevsky et al., 2009] is a smaller and more frequently used dataset for imageclassi\ufb01cation. It consists of 60.000 colored images, each having a resolution of just 32 by32 pixels, and just 10 image classes. The results for ANN versions of ResNet that are givenin Table 1 for CIFAR10 arise from training them with the hyperparameters given in theTensorFlow models repository. They use the ReLU function as the only nonlinearity, sincewe have replaced there max-pooling by average pooling. Nevertheless, they achieve anaccuracy for CIFAR10 which is very close to the best results reported for CIFAR10 in theliterature. The best performing reported ResNet on CIFAR10 is ResNet110, where a testaccuracy of 93.57% had been achieved [He et al., 2016]. Our ResNet50 achieves 92.99%,which is similar to their accuracy of 93.03% for ResNet56.\u2020https://github.com/tensor\ufb02ow/tpu/tree/master/models/o\ufb03cial/e\ufb03cientnet10--- Page 11 ---Analysis of FS-codingOn digital neuromorphic hardware the energy consumption is proportional to the numberof spikes which are needed for a computation. The number of spikes needed for an FS-neuron to perform the approximation of the target function is depicted in Fig. 5 a and bas function of the gate input x. If one compares these numbers with the distribution ofinput values x (red curves) that typically occur during image classi\ufb01cation, one sees whyon average less than 2 spikes are used by FS-neurons for these applications.11--- Page 12 ---Figure 5: Number of spikes needed by FS-neurons for image classi\ufb01cation andin\ufb02uence of K and bit precision Q on performancea) The number of spikes used by an a FS-neuron with K = 16 to approximate the SiLU function, asfunction of its input value x. The red Gaussian models the probability that the FS-neuron will receive thisinput value in the E\ufb03cientNet-B7 model (mean = \u22120.112, variance = 1.99). b) The number of spikes usedby an FS-neuron to approximate the ReLU function with K = 6 and \u03b1 = 10. The red Gaussian modelsthe probability that the FS-neuron will receive this input value in the ResNet50 model in an application toimages from ImageNet (mean \u22120.36970, variance = 2.19). c) Test Accuracy of the ResNet50 model onCIFAR10 with FS-neurons, in dependence on K. The red cross indicates the chosen value of K for ourresults. d) Mean squared error (MSE) of a SiLU approximation by FS-neurons with di\ufb00erent values ofK. The red cross indicates the chosen value of K in the given context. e) MSE of a SiLU approximationby FS-neurons with K = 16 as function of the bit precision Q of its parameters. f) MSE of a sigmoidapproximation by FS-neurons with K = 12 as function of the bit precision Q of its parameters.12--- Page 13 ---The most important speci\ufb01cation of an FS-neuron is the number K of time steps thatit uses. Fig. 5c, d provide insight into the nature of the trade-o\ufb00 between the size of Kand the approximation quality of the FS-neuron.Furthermore, it is of interest to consider scenarios where only a certain number of bitsare available for the FS-neuron parameters. To analyze the impact of that we considera setting where the parameters of the FS-neurons can only take on discrete values in therange from [\u22128,8]. The possible values are equally spaced and the number of values can bewritten as 2Q, where Qrefers to the number of bits which are available for each parameterT(t), h(t), d(t) of the FS-neuron. Fig. 5e, f depict the impact of such quantization on themean squared error of the approximation of the activation function.Expected implementation cost on neuromorphic hardwareWe distinguish three types of neuromorphic hardware\u2022 Digital, but hardware not constrained to a particular neuron model (example: SpiN-Naker)\u2022 Digital, but hardware is constrained to a particular neuron model (example: Loihi)\u2022 Mixed digital analog (examples: IBM research chip with memristors and BrainScaleS-2)SpiNNakerThe SpiNNaker platform [Furber et al., 2014] provides a \ufb02exible environment which is notconstrained to a speci\ufb01c neuron model. SpiNNaker allows to compute all products d(t)won the chip, which reduces the additional memory consumption to to a small constantvalue. All parameters T(t), h(t) and d(t) only need to be stored in memory once, as theycan be shared across all neurons which approximate the same activation function. Theadditional computational complexity of the FS-neuron model also has a very small impact,as computing the updated weight d(t)w can be done with a single instruction cycle.LoihiLoihi [Davies et al., 2018] also promises to be an interesting target platform for FS-neurons.Especially FS-neurons approximating the ReLU activation function could be ported verye\ufb03ciently to this hardware platform. As the chip is based on \ufb01xed-point arithmetic, onecan implement T(t), h(t) and d(t) for ReLu using a single parameter, namely the sharedweight exponent. This is be possible due to the fact that at every time step t all FS-parameters have the same value, which is always a power of 2. Therefore, the additionalmemory consumption does not grow with K. It is also possible to use other activationfunctions besides ReLU on Loihi, however, in this case it would be necessary to store allproducts d(t)w on the chip, as computing the updated weight in an online fashion is notpossible. In this case, an increase in memory consumption of O(K) is expected.13--- Page 14 ---IBM research chip with memristorsIBM has presented an in-memory chip architecture supporting both ANNs and SNNs in theSupplementary Material S3 of the article [Wo\xb4 zniak et al., 2020]. This architecture employsa memristor crossbar array for fast (time complexity O(1)) and energy-e\ufb03cient multipli-cation of the outputs of one layer l of neurons with the weights of synaptic connectionsto neurons on the next layer l+ 1. One can replace all spikes (i.e., values 1) that emergefrom layer l at time t of the K-step cycle in the emulation of the neurons on layer l by acommon value value d(t) that is centrally stored. Since the values d(t) and 0 can be useddirectly as inputs to the memristor array, no signi\ufb01cant extra cost is expected. The neuronmodels are implemented in the digital part of this neuromorphic chip architecture of IBM,using very fast digital logic and SRAM for storing parameters. Since all neurons on a layerl of our FS networks use the same parameters T(t) and h(t), they can be stored in a localSRAM for all neurons on layer l, in a similar fashion as on SpiNNaker. A neuron modelthat goes already one step in the direction from LIF to FS-neurons has actually alreadybeen implemented on this architecture: The soft spiking neural unit (sSNU), that emitsanalog instead of binary values and subtracts a corresponding value from the membranepotential [Wo\xb4 zniak et al., 2020].BrainScaleS-2This neuromorphic chip [Billaudelle et al., 2020] is also a mixed analog digital architecturewhere a digital plasticity processor allows fast changes of synaptic weights, but also centralmemory storage and application of the time-varying parameters T(t), h(t) and d(t) ofthe neuron dynamics. Like on SpiNNaker, the parameters only have to be stored once inmemory and can be shared across many neurons. The leak term of the membrane voltageof the analog neuron models can be switched o\ufb00, so that the analog part can be used fore\ufb03cient matrix multiplication in a similar manner as on the IBM chip.MethodsIn this section various details necessary to reproduce our results have been listed. Ad-ditionally, to aid the interpretation of the results, a comparison to previous conversionapproaches has been added.When training the parameters of the FS-neurons it is important to specify an intervalin which the approximation should be very good. Ideally, most of the inputs to the ANNneuron should fall into this interval to guarantee a good performance. In our experiments,the FS-neurons have been trained to approximate the interval from [ \u22128,12] for the SiLUfunction and [ \u221210,10] for the sigmoid function. The resulting FS-neuron approximatesthe SiLU function with a mean squared error of 0 .0023 inside the main region [ \u22122,2] and0.0064 in the region outside, which can be written as [ \u22128,\u22122] \u222a[2,12]. As a result of our14--- Page 15 ---\ufb01ne-tuning the values for T(t), d(t) and h(t) stay for most time steps t within the mainregion [\u22122,2] as can be seen in Fig. 4b.To reduce the complexity of the converted CNN, we decided not to emulate the mul-tiplication operation by FS-neurons, which occurs in the CNN if the squeeze and excita-tion optimization [Hu et al., 2018] is being used. In many neuromorphic chips, such asSpiNNaker and Loihi, the on-chip digital processor could carry out these multiplications.Otherwise one can approximate multiplication in a similar manner as the SiLU functionwith a suitably optimized FS-neuron, see [St\xa8 ockl and Maass, 2019]. Alternatively one cancompute multiplication with a small circuit of threshold gates, i.e., very simple types ofspiking neurons, of depth 2 or 3. A recent summary of such results is provided in section3 of [Parekh et al., 2018].Due to the simple structure of the parameters of the ReLU FS-neurons several advan-tages arise. In particular when approximating the ReLU function with an FS-neuron it ispossible to calculate the changes of parameters for t= 1,...,K by simply using a bit shiftoperation, possibly providing a very e\ufb03cient implementation on neuromorphic hardware.The resulting SNN can be used in a pipelined manner, processing a new network inputevery 2K = 20 time steps, analogously as for the SiLU function.Further properties of FS-coding that are relevant for neuromor-phic hardwareOne of the major advantages of using FS-neurons in neuromorphic hardware is the smalleramount of time steps and spikes required to approximate arti\ufb01cial neurons. For the case ofthe ReLU activation function, a rate coded spiking neuron requires N time steps to encodeN di\ufb00erent values. FS-neurons improve upon this unary coding scheme by utilizing thetime dimension to implement a binary coding scheme. Therefore, the number of time stepsrequired to encode N di\ufb00erent values can be reduced to just log2(N).To underline the binary coding nature of FS-neurons, in the case of the ReLU activationfunction, the corresponding FS-neurons will show a spiking pattern equivalent to of theoutput of the ReLU function, written as a binary number. The same logarithmic relationholds not only for the number of time steps required but also for the number of spikes neededto transmit a value. More sophisticated codes could be used to make the computationrobust to noise in spike transmission.Note, that most of the inputs to the FS-neurons have a value close to 0, as shown inFig. 5 a and b. Consequently, the FS-neurons usually require only a few spikes to transmitthe output values, making the the coding scheme even more sparse in practice.Comparison with previous methodsThe idea of converting a pre-trained ANN to a SNN has received a fair amount of attentionin the recent years. The most popular conversion approaches are rate-based, meaning they15--- Page 16 ---translate the continuous output of an arti\ufb01cial ReLU neuron into a \ufb01ring rate of a spikingneuron. Unfortunately there are some drawbacks associated with rate-coding. Due to itsunary coding nature, rate-codes are sub-optimal in the sense that they do not make gooduse of the time dimension. Usually a large amount of time steps is required to achieve asu\ufb03ciently accurate approximation. Furthermore, rate-based conversions are only capableof converting simple activation functions like ReLU, but fail to convert more sophisticatedfunctions like SiLU.Another popular conversion approach uses time to \ufb01rst spike (TTFS) coding [Rueckauerand Liu, 2018]. This approach encodes the continuous outputs of the corresponding ReLUANN neurons in the length of the time interval until the \ufb01rst spike, resulting in a verysparse spiking activity. However, this method seems to not scale easily to large modelsand has, to the best of our knowledge, not been tested on large data sets like ImageNet.The idea of using single spike temporal coding has \ufb01rst been explored in [Maass, 1997] andit has been shown to have a variety of applications, like implementing an e\ufb03cient k-NNalgorithm on neuromorphic hardware [Frady et al., 2020].Furthermore, a new conversion method has been proposed, in which the spiking neuronscan approximate the ReLU function using a hysteresis quantization method [Yousefzadehet al., 2019]. This approach waits to be tested on larger networks and datasets.One property that all previously mentioned conversion methods have in common is thatthey only consider transforming arti\ufb01cial ReLU neurons to spiking neurons, and thereforecannot convert more sophisticated activation functions, which are used in network archi-tecture like the E\ufb03cientNets.A detailed summary comparing FS-coding to previous results can be found in theExtended Data Table 1.It is worth noting, that the throughput using FS-coding is substantially better thanthat of SNNs which result from rate-based ANN-to-SNN conversions of ANNs with theReLU function, as proposed for example in [Rueckauer et al., 2017, Sengupta et al., 2019].The Inception-v3 model in [Rueckauer et al., 2017] was reported to yield a SNN that needed550 time steps to classify an image. Under the assumption that rate-based models pro\ufb01tonly very little from pipelining, it is reasonable to estimate that the throughput of an SNNthat results from FS-conversion of ReLU gates with K = 10 is roughly 25 times higher.The SNN resulting from the rate-based conversion of the ResNet34 model discussedin [Sengupta et al., 2019] has been reported to use 2500 time steps for a classi\ufb01cation.Therefore we estimate that the throughput is increased here by a factor around 125 throughFS-conversion.Spiking versions of ResNet20 have already been previously explored [Sengupta et al.,2019]. Using a rate-based conversion scheme an accuracy of 87.46% was reported.FS-conversion of ResNet20 yields a substantially higher accuracy of 91.45%, using just80 to 500 time steps for each image -depending on the model depth- instead of 2000,thereby signi\ufb01cantly reducing latency. In addition, the throughput is drastically improved.Also the number of spikes that the SNN uses for classifying an image from CIFAR10is signi\ufb01cantly reduced when one moves from a rate-based conversion to an FS conversion.A converted ResNet11 has been reported to use more than 8 million spikes to classify16--- Page 17 ---a single test example [Lee et al., 2020]. Comparing this to an FS-converted ResNet14we \ufb01nd that the latter uses 40 times fewer spikes despite being a slightly larger model.Using direct training of SNNs instead of a conversion scheme has been reported to resultin a lower amount of spikes needed to perform a single classi\ufb01cation. However, even adirectly trained SNN version of ResNet11 uses 7 times more spikes than an FS-conversionof ResNet14 [Lee et al., 2020, Table 8].In [Rathi et al., 2020] the authors present a novel approach for obtaining high perfor-mance SNNs by combining a rate-based conversion scheme with a subsequent gradient-based \ufb01ne-tuning procedure. They report the highest accuracy for an SNN on CIFAR10,which was achieved by converting a very performant ANN. They also show results forImageNet, where they achieve an accuracy of 65 .1% on their ResNet-34. Deeper models,like the ResNet-50, were not considered in this work. On ImageNet FS-conversion of theResNet-50 model improves their accuracy by 10% and FS-conversion of the E\ufb03cientNet-B7surpasses their performance by 18.47%.DiscussionWe have presented a new approach for generating SNNs that are very close to ANNs interms of classi\ufb01cation accuracy for images, while working in the energetically most attrac-tive regime with very sparse \ufb01ring activity. Besides substantially improved classi\ufb01cationaccuracy, they exhibit drastically improved latency and throughput compared with rate-based ANN-to-SNN conversions. o\ufb00 the shelf. One can argue that this is exactly theway which evolution has chosen for the design of neurons in living organism. Not onlyneurons with particular information processing tasks in the smaller nervous systems ofinsects, but also neurons in the neocortex of mammals exhibit an astounding diversity ofgenetically encoded response properties [ [Sterling and Laughlin, 2015], [Gouwens et al.,2019], [Bakken et al., 2020]]. In particular, the probability of producing a spike dependsin diverse ways on the recent stimulation history of the neuron, see [Gerstner et al., 2014]for some standard models. In other words, the excitability of di\ufb00erent types of biologicalneurons increases or decreases in complex ways in response to their previous \ufb01ring. As aresult, the temporal structure of a train of spikes that is produced by a biological neuroncontains additional information about the neuron input that can not be captured by its\ufb01ring rate. Similary, FS-neurons that are optimized for high accuracy image classi\ufb01cationwith few spikes exhibit history-dependent changes -encoded through their functions T(t)and h(t) according to equ. (2)- in their propensity to \ufb01re, see Fig. 4b and 4e. Furthermorethe function d(t) enables subsequent neurons to decode their spikes in a timing-sensitivemanner. In these regards an FS-conversion from ANNs to SNNs captures more of the func-tional capabilities of spiking neurons than previously considered rate-based conversions toan o\ufb00-the-shelf spiking neuron model.It is well known that spikes from the same neurons in the brain can transmit di\ufb00erentinformation to downstream neurons depending on the timing of the spike, see e.g. phaseprecession in the hipppocampus [Harris et al., 2002]. Hence is is conceivable that down-17--- Page 18 ---stream neurons give di\ufb00erent weights to these spikes, in dependence of the \ufb01ring time ofthe presynaptic neuron. In fact, it is well known that the large repertoire of pre- andpostsynaptic synaptic dynamics found in di\ufb00erent synapses of the brain [Markram et al.,2004, Kopanitsa et al., 2018] enables postsynaptic neurons to modulate the amplitude ofpostsynaptic responses in dependence of the timing of presynaptic spikes relative to an un-derlying rhythm. This can be viewed as a biological counterpart of the timing-dependentweights d(t) in our model. Altogether we believe that FS-neurons provide a \ufb01rst step inexploring new uses of SNNs where information is not encoded by the timing of single spikesor \ufb01ring rates, but by temporal spike patterns.Important for applications of FS-coding in neuromorphic hardware is that it is ap-plicable to virtually any activation function, in particular to that activation function forANN neurons that currently provides the highest accuracy on ImageNet, the SiLU func-tion. Rate-based conversion can not be readily applied to the SiLU function because itassumes both positive and negative output values. When approximating the more com-monly used ReLU function, FS-neurons approach the information theoretic minimum ofspikes for spike-based communication.In fact, FS-neurons that emulate ANN gates with the ReLU activation function produce1.5 spikes on average for classifying an image, while those for the Switch activation functionproduce 2 spikes on average. As the number of spikes required for inference by an SNNis directly related to its energy consumption in spike-based neuromorphic hardware, theenergy consumption of FS-converted SNNs appears to be close to the theoretical optimumfor SNNs. Since FS-conversion provides a tight bound on the number K of time stepsduring which a spiking neuron is occupied, it can also be used for converting recurrentlyconnected ANNs to SNNs.The proposed method for generating highly performant SNNs for image classi\ufb01cationthrough FS-conversion of trained CNNs o\ufb00ers an opportunity to combine the computa-tionally more e\ufb03cient and functionally more powerful training of ANNs with the superiorenergy-e\ufb03ciency of SNNs for inference. Note that one can also use the resulting SNN asinitialization for further training of the SNN, e.g., for a more speci\ufb01c task.Altogether our results suggest that spike-based hardware may gain an edge in thecompetition for the development of drastically more energy-e\ufb03cient hardware for AI if onedoes not forgot to optimize the spiking neuron model in the hardware for its intendedrange of applications. In contrast to energy e\ufb03cient digital hardware that is optimizedfor speci\ufb01c ANN architectures and activation functions, see e.g. [Zhang et al., 2019] for areview, a spike-based neuromorphic chip that is able to emulate FS-neurons can carry outinference for all possible ANN architectures. It can also emulate ANNs with previouslynot considered activation functions, since a change of the activation function just requiresreprogramming of the digitally stored parameters of FS-neurons. Hence such spike-basedchip will be substantially more versatile than common digital hardware accelerators for AI.18--- Page 19 ---AcknowledgementsWe would like to thank Franz Scherr for helpful discussions. We thank Thomas Bohnst-ingl, Evangelos Eleftheriou, Steve Furber, Christian Pehle, Philipp Plank and JohannesSchemmell for advice regarding implementation aspects of FS-neurons in various typesof neuromorphic hardware. This research was partially supported by the Human BrainProject of the European Union (Grant agreement number 785907). We also thank ouranonymous reviewers for very constructive comments.Competing InterestsWe are not aware of competing interests.Data availabilityBoth ImageNet [Deng et al., 2009] and CIFAR10 [Krizhevsky et al., 2009] are publiclyavailable datasets. No additional datasets were generated or analysed during the cur-rent study. The data for the spike response depiced in \ufb01gure 1 has been publishedbytheAllen Institute for Brain Science in 2015 (Allen Cell Types Database). Available from:https://celltypes.brain-map.org/experiment/electrophysiology/587770251Author contributionsCS conceived the main idea, CS and WM designed the model and planned the experiments,CS carried out the experiments, CS and WM wrote the paper.Code availabilityThe code this work is based on is publicly available at: https://github.com/christophstoeckl/FS-neurons (DOI: 10.5281/zenodo.4326749). Additionally, the code is also available in a CodeOcean compute capsule: https://codeocean.com/capsule/7743810/treeReferences[Bakken et al., 2020] Bakken, T. E., Jorstad, N. L., Hu, Q., Lake, B. B., Tian, W., Kalm-bach, B. E., Crow, M., Hodge, R. D., Krienen, F. M., Sorensen, S. A., et al. (2020).Evolution of cellular diversity in primary motor cortex of human, marmoset monkey,and mouse. bioRxiv.19--- Page 20 ---[Bellec et al., 2020] Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legen-stein, R., and Maass, W. (2020). A solution to the learning dilemma for recurrentnetworks of spiking neurons. Nature Communications, 11(1):1\u201315.[Billaudelle et al., 2020] Billaudelle, S., Stradmann, Y., Schreiber, K., Cramer, B., Baum-bach, A., Dold, D., G\xa8 oltz, J., Kungl, A. F., Wunderlich, T. C., Hartel, A., M\xa8 uller, E.,Breitwieser, O., Mauch, C., Kleider, M., Gr\xa8 ubl, A., St\xa8 ockel, D., Pehle, C., Heimbrecht,A., Spilger, P., Kiene, G., Karasenko, V., Senn, W., Petrovici, M. A., Schemmel, J., andMeier, K. (2020). Versatile emulation of spiking neural networks on an accelerated neu-romorphic substrate. In 2020 IEEE International Symposium on Circuits and Systems(ISCAS), pages 1\u20135.[Davies et al., 2018] Davies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y., Choday,S. H., Dimou, G., Joshi, P., Imam, N., Jain, S., et al. (2018). Loihi: A neuromorphicmanycore processor with on-chip learning. IEEE Micro, 38(1):82\u201399.[Deng et al., 2009] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on com-puter vision and pattern recognition , pages 248\u2013255. Ieee.[Frady et al., 2020] Frady, E. P., Orchard, G., Florey, D., Imam, N., Liu, R., Mishra, J.,Tse, J., Wild, A., Sommer, F. T., and Davies, M. (2020). Neuromorphic nearest neighborsearch using intel\u2019s pohoiki springs. In Okandan, M. and Aimone, J. B., editors, NICE\u201920: Neuro-inspired Computational Elements Workshop, Heidelberg, Germany, March17-20, 2020, pages 23:1\u201323:10. ACM.[Furber et al., 2014] Furber, S. B., Galluppi, F., Temple, S., and Plana, L. A. (2014). Thespinnaker project. Proceedings of the IEEE, 102(5):652\u2013665.[Garc\xb4 \u0131a-Mart\xb4 \u0131n et al., 2019] Garc\xb4 \u0131a-Mart\xb4 \u0131n, E., Rodrigues, C. F., Riley, G., and Grahn, H.(2019). Estimation of energy consumption in machine learning. Journal of Parallel andDistributed Computing, 134:75\u201388.[Gerstner et al., 2014] Gerstner, W., Kistler, W. M., Naud, R., and Paninski, L. (2014).Neuronal dynamics: From single neurons to networks and models of cognition. Cam-bridge University Press .[Gouwens et al., 2019] Gouwens, N. W., Sorensen, S. A., Berg, J., Lee, C., Jarsky, T., Ting,J., Sunkin, S. M., Feng, D., Anastassiou, C. A., Barkan, E., et al. (2019). Classi\ufb01cation ofelectrophysiological and morphological neuron types in the mouse visual cortex. Natureneuroscience, 22(7):1182\u20131195.[Han et al., 2020] Han, B., Srinivasan, G., and Roy, K. (2020). Rmp-snn: Residual mem-brane potential neuron for enabling deeper high-accuracy and low-latency spiking neuralnetwork. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 13558\u201313567.20--- Page 21 ---[Harris et al., 2002] Harris, K. D., Henze, D. A., Hirase, H., Leinekugel, X., Dragoi, G.,Czurk\xb4 o, A., and Buzs\xb4 aki, G. (2002). Spike train dynamics predicts theta-related phaseprecession in hippocampal pyramidal cells. Nature, 417(6890):738\u2013741.[He et al., 2016] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning forimage recognition. Proceedings of the IEEE Computer Society Conference on ComputerVision and Pattern Recognition, 2016-Decem:770\u2013778.[Hendrycks and Gimpel, 2016] Hendrycks, D. and Gimpel, K. (2016). Gaussian error linearunits (gelus). arXiv preprint arXiv:1606.08415 .[Hu et al., 2018] Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-Excitation Networks.Proceedings of the IEEE Computer Society Conference on Computer Vision and PatternRecognition, pages 7132\u20137141.[Kheradpisheh and Masquelier, 2020] Kheradpisheh, S. R. and Masquelier, T. (2020).S4nn: temporal backpropagation for spiking neural networks with one spike per neuron.International Journal of Neural Systems , 30(6):2050027.[Kopanitsa et al., 2018] Kopanitsa, M. V., van de Lagemaat, L. N., A\ufb01nowi, N. O.,Strathdee, D. J., Strathdee, K. E., Fricker, D. G., Tuck, E. J., Elsegood, K. A., Croning,M. D., Komiyama, N. H., et al. (2018). A combinatorial postsynaptic molecular mech-anism converts patterns of nerve impulses into the behavioral repertoire. BioRxiv, page500447.[Krizhevsky et al., 2009] Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layersof features from tiny images. Technical report.[Lee et al., 2020] Lee, C., Sarwar, S. S., Panda, P., Srinivasan, G., and Roy, K. (2020).Enabling spike-based backpropagation for training deep neural network architectures.Frontiers in Neuroscience, 14:119.[Ling, 2001] Ling, J. (2001). https://hypertextbook.com/facts/2001/JacquelineLing.shtml.[Maass, 1997] Maass, W. (1997). Fast sigmoidal networks via spiking neurons. NeuralComputation, 9(2):279\u2013304.[Maass and Natschl\xa8 ager, 1998] Maass, W. and Natschl\xa8 ager, T. (1998). Emulation of Hop-\ufb01eld networks with spiking neurons in temporal coding. In Computational Neuroscience,pages 221\u2013226. Springer.[Markram et al., 2004] Markram, H., Toledo-Rodriguez, M., Wang, Y., Gupta, A., Silber-berg, G., and Wu, C. (2004). Interneurons of the neocortical inhibitory system. Naturereviews neuroscience, 5(10):793\u2013807.21--- Page 22 ---[Parekh et al., 2018] Parekh, O., Phillips, C. A., James, C. D., and Aimone, J. B. (2018).Constant-depth and subcubic-size threshold circuits for matrix multiplication. In Pro-ceedings of the 30th on Symposium on Parallelism in Algorithms and Architectures, pages67\u201376.[Rathi et al., 2020] Rathi, N., Srinivasan, G., Panda, P., and Roy, K. (2020). Enablingdeep spiking neural networks with hybrid conversion and spike timing dependent back-propagation. In International Conference on Learning Representations.[Rueckauer and Liu, 2018] Rueckauer, B. and Liu, S.-C. (2018). Conversion of analog tospiking neural networks using sparse temporal coding. In 2018 IEEE InternationalSymposium on Circuits and Systems (ISCAS) , pages 1\u20135. IEEE.[Rueckauer et al., 2017] Rueckauer, B., Lungu, I. A., Hu, Y., Pfei\ufb00er, M., and Liu, S. C.(2017). Conversion of continuous-valued deep networks to e\ufb03cient event-driven networksfor image classi\ufb01cation. Frontiers in Neuroscience, 11(DEC):1\u201312.[Russakovsky et al., 2015] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei,L. (2015). ImageNet Large Scale Visual Recognition Challenge. International Journalof Computer Vision , 115(3):211\u2013252.[Sengupta et al., 2019] Sengupta, A., Ye, Y., Wang, R., Liu, C., and Roy, K. (2019).Going Deeper in Spiking Neural Networks: VGG and Residual Architectures. Frontiersin Neuroscience, 13(1998):1\u201316.[Sterling and Laughlin, 2015] Sterling, P. and Laughlin, S. (2015). Principles of neuraldesign. MIT Press.[St\xa8 ockl and Maass, 2019] St\xa8 ockl, C. and Maass, W. (2019). Recognizing images with atmost one spike per neuron. arXiv preprint arXiv:2001.01682 .[Tan and Le, 2019] Tan, M. and Le, Q. (2019). E\ufb03cientNet: Rethinking model scalingfor convolutional neural networks. In Chaudhuri, K. and Salakhutdinov, R., editors,Proceedings of the 36th International Conference on Machine Learning , volume 97 ofProceedings of Machine Learning Research , pages 6105\u20136114, Long Beach, California,USA. PMLR.[Thorpe et al., 2001] Thorpe, S., Delorme, A., and Rullen, R. (2001). Spike-based strate-gies for rapid processing. Neural networks : the o\ufb03cial journal of the InternationalNeural Network Society, 14:715\u201325.[Van Horn et al., 2015] Van Horn, G., Branson, S., Farrell, R., Haber, S., Barry, J., Ipeiro-tis, P., Perona, P., and Belongie, S. (2015). Building a bird recognition app and largescale dataset with citizen scientists: The \ufb01ne print in \ufb01ne-grained dataset collection.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,pages 595\u2013604.22--- Page 23 ---[Wo\xb4 zniak et al., 2020] Wo\xb4 zniak, S., P, Your response: ```ADD_PAPER\n2002.00860v4\nThis paper introduces an optimized spiking neuron model for efficient image classification on edge devices through temporal coding with just two spikes per neuron. The authors propose a novel ANN-to-SNN conversion method called FS-conversion that significantly improves upon traditional rate-based approaches by encoding information in spike timing rather than just spike counts. Key innovations include: 1) A modified spiking neuron model (FS-neuron) with optimized internal dynamics for emulating ANN activation functions with minimal spikes, 2) Demonstration on ImageNet and CIFAR10 showing near-identical accuracy to original ANNs (83.57% on ImageNet with EfficientNet-B7) while using only ~2 spikes per neuron, 3) Substantial improvements in latency and throughput compared to rate-based conversions, enabling new image classification every 32 time steps. The method is particularly effective for edge deployment as it achieves biological-level energy efficiency while maintaining high accuracy, making it suitable for mobile and embedded vision applications.\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\n~~~~~~~~~~\nCurrent Step #16, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification model compression techniques\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 18:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #14, Phase: literature review, Feedback: 3\n--- Page 1 ---Optimized spiking neurons classify images with highaccuracy through temporal coding with two spikesChristoph St\xa8 ockl1, Wolfgang Maass1,\u2217January 27, 20211Institute of Theoretical Computer Science, Graz University of Technology,In\ufb00eldgasse 16b, Graz, Austria\u2217 To whom correspondence should be addressed; E-mail: maass@igi.tugraz.at.AbstractSpike-based neuromorphic hardware promises to reduce the energy consumptionof image classi\ufb01cation and other deep learning applications, particularly on mobilephones or other edge devices. However, direct training of deep spiking neural net-works is di\ufb03cult, and previous methods for converting trained arti\ufb01cial neural net-works to spiking neurons were ine\ufb03cient because the neurons had to emit too manyspikes. We show that a substantially more e\ufb03cient conversion arises when one op-timizes the spiking neuron model for that purpose, so that it not only matters forinformation transmission how many spikes a neuron emits, but also when it emitsthose spikes. This advances the accuracy that can be achieved for image classi\ufb01cationwith spiking neurons, and the resulting networks need on average just two spikes perneuron for classifying an image. In addition, our new conversion method improveslatency and throughput of the resulting spiking networks.Spiking neural networks (SNNs) are currently explored as possible solution for a majorimpediment of more widespread uses of modern AI in edge devices: The energy consump-tion of the large state-of-the-art arti\ufb01cial neural networks (ANNs) that are produced bydeep learning.This holds in particular for the Convolutional Neural Networks (CNNs) that are com-monly used for image classi\ufb01cation, but also other application domains. These ANNs haveto be large for achieving top performance, since they need to have a su\ufb03ciently largenumber of parameters in order to absorb enough information from the huge data setson which they have been trained, such as the 1.2 million images of the ImageNet2012dataset. Inference with standard hardware implementations of these large ANNs is inher-ently power-hungry [Garc\xb4 \u0131a-Mart\xb4 \u0131n et al., 2019].1arXiv:2002.00860v4  [cs.NE]  26 Jan 2021--- Page 2 ---Spiking neurons have been in the focus of the development of novel computing hardwarefor AI with a drastically reduced energy budget, partially because the giant SNN of thebrain \u2013consisting of about 100 billion neurons\u2013 consumes just 20W [Ling, 2001]. Spikingneurons output trains of stereotypical pulses that are called spikes. Hence their output isvery di\ufb00erent from the continuous numbers that an ANN neuron produces as output. Mostspiking neuron models that are considered for implementation in neuromorphic hardwareare inspired by simple models for spiking neurons in the brain. However, these simpleneuron models do not capture the capability of biological neurons to encode di\ufb00erentinputs by di\ufb00erent temporal spike patterns, not just by their \ufb01ring rate (see Fig. 1 for anexample).Figure 1: Encoding of di\ufb00erent input values (current steps of di\ufb00erent ampli-tudes) by temporal spike patterns in a biological neuron. Data taken from theAllen Cell Type Database1 (Layer 3 spiny neuron from the human middle temporal gyrus).While large ANNs, trained with ever more sophisticated deep learning algorithms ongiant data sets, approach \u2013and sometimes exceed\u2013 human performance in several categoriesof intelligence, the performance of the current generation of spike-based neuromorphichardware is lagging behind. There is some hope that this gap can be closed for the case ofrecurrent spiking neural networks, since those can be trained directly to achieve most ofthe performance of recurrent ANNs [Bellec et al., 2020].1 \xa9 2015 Allen Institute for Brain Science. Allen Cell Types Database. Available from:https://celltypes.brain-map.org/experiment/electrophysiology/5877702512--- Page 3 ---But the problem to produce SNNs that achieve similar performance as ANNs with fewspikes persists for feedforward networks. Feedforward CNNs that achieve really good im-age classi\ufb01cation accuracy tend to be very deep and very large, and training correspondingdeep and large feedforward SNNs has not been able to reach similar classi\ufb01cation accuracy.Problems with the timing of spikes and precision of \ufb01ring rates on higher levels of the re-sulting SNNs have been cited as possible reasons. One attractive alternative is to simplytake a well-performing trained CNN and convert it into an SNN \u2013using the same connec-tions and weights. The most common \u2013and so far best performing\u2014 conversion methodwas based on the idea of (\ufb01ring-) rate coding, where the analog output of an ANN unit isemulated by the \ufb01ring rate of a spiking neuron [Rueckauer et al., 2017]. This method hadproduced so far the best SNN results for image classi\ufb01cation. But the transmission of ananalog value through a \ufb01ring rate tends to require a fairly large number of spikes, whichreduces both latency and throughput of the network. Furthermore, the resulting SNNtends to produce so many spikes that its energy-advantage over non-spiking hardware getslost. Finally, a rate-based ANN-to-SNN conversion can not be applied to those ANNs thatcurrently achieve the highest accuracy on ImageNet, E\ufb03cientNets [Tan and Le, 2019], be-cause these employ an activation function that assumes both positive and negative values:the SiLU function [Hendrycks and Gimpel, 2016].We introduce a new ANN-to-SNN conversion that we call FS-conversion because itrequires a spiking neuron to emit just a few spikes (FS = Few Spikes). This methodis completely di\ufb00erent from rate-based conversions, and exploits the option of temporalcoding with spike patterns, where the timing of a spike transmits extra information.Most previously proposed forms of temporal coding, see e.g. [Maass and Natschl\xa8 ager,1998], [Thorpe et al., 2001], [Rueckauer et al., 2017], [Kheradpisheh and Masquelier, 2020],have turned out to be di\ufb03cult to implement e\ufb03ciently in neuromorphic hardware becausethey require to transmit \ufb01ne time-di\ufb00erences between spikes to downstream neurons. Incontrast, an FS-conversion can be implemented with just log Ndi\ufb00erent values of spiketimes and at most log Nspikes for transmitting integers between 1 and N. Practically, therequired number of spikes can be made even lower because not all N values occur equallyoften. However FS-conversion requires a modi\ufb01ed spiking neuron model, the FS-neuron,which has an internal dynamic that is optimized for emulating particular types of ANNneurons with few spikes. We demonstrate the performance of SNNs that result from FS-conversion of CNNs, on two state-of-the-art datasets for image classi\ufb01cation: ImageNet2012and CIFAR10. This optimized spiking neuron model could serve as guidance for the nextgeneration of neuromorphic hardware.Emulating an ANN neuron by a spiking neuron withfew spikesThe FS-conversion from ANNs to SNNs requires a variation of the standard spiking neuronmodel, to which we refer as FS-neuron. The computation step of a generic arti\ufb01cial neuron3--- Page 4 ---in an ANN (see Fig. 2 a) is emulated by K time steps of an FS-neuron (Fig. 2 b). Itsinternal dynamics is de\ufb01ned by \ufb01xed parameters T(t),h(t),d(t) for t= 1,...,K . These areoptimized to emulate the activation function f(x) of the given ANN neuron by a weightedsum of spikes \u2211Kt=1 d(t)z(t), where z(t) denotes the spike train that this neuron produces.More precisely: z(t) = 1 if the neuron \ufb01res at step t, else z(t) = 0. To emit a spike attime t, a neuron\u2019s membrane potential v(t) has to surpass the current value T(t) of its\ufb01ring threshold. We assume that the membrane potential v(t) has no leak, but is reset tov(t) \u2212h(t) after a spike at time t. Expressed in formulas, the membrane potential v(t)starts with value v(1) = x where x is the gate input, and evolves during the K stepsaccording tov(t+ 1) = v(t) \u2212h(t)z(t). (1)The spike output z(t) of an FS-neuron for gate input x can be de\ufb01ned compactly byz(t) = \u0398(v(t) \u2212T(t)) = \u0398((x\u2212t\u22121\u2211j=1h(j)z(j))\u2212T(t)), t = 1,...,K, (2)where \u0398 denotes the Heaviside step function. The total output \u02c6f(x) of the FS-neuron fromthese K time steps, which is collected by the FS-neurons on the next layer, can be writtenas:\u02c6f(x) =K\u2211t=1d(t)z(t). (3)An illustration of the model can be found in Fig. 2b.Figure 2: Conversion of an ANN neuron into an FS-neuron.a) A generic ANN neuron with activation function f(x) that is to be emulated.b) An FS-neuron which emulates this ANN neuron in K time steps t= 1,...,K . Its outputspike train is denoted by z(t).4--- Page 5 ---For emulating the ReLU activation function one can choose the parameters of the FS-neuron so that they de\ufb01ne a coarse-to-\ufb01ne processing strategy for all input values x thatlie below some upper bound, as described in the Methods section. For emulating the SiLUfunction of E\ufb03cientNet one achieves a better FS-conversion if the parameters are chosenin such a way that they enable iterative \u2013and thereby more precise\u2013 processing for therange of inputs between \u22122 and 2 that occur most often as gate inputs x in E\ufb03cientNet.The resulting dynamics of FS-neurons is illustrated in Fig. 3 for the case of the SiLU andsigmoid activation functions.All FS-neurons that emulate ANN neurons with the same activation function can usethe same parameters T(t), h(t), d(t), while the factor w in the weights of their outputspikes is simply lifted from the corresponding synaptic connection in the trained ANN (seeFig. 2).5--- Page 6 ---Figure 3: Internal dynamics of Few-Spikes neurons.The \ufb01rst row depicts the response of the FS-neurons to a low input value ( x= \u22120.5) andthe second row displays the response to a high input ( x = 0.5). The \ufb01rst column showsresponses of an SiLU FS-neuron, while the second column a sigmoid FS-neuron. Therelevant values of T(t) and v(t) for discrete time steps t (see Fig. 4b and d) are smoothlyinterpolated for illustration.Note that the number of neurons and connections in the network is not increasedthrough the FS-conversion. However the number of computation steps L of a feedforwardANN with Llayers is increased by the factor K. But the computations of the ANN can beemulated in a pipelined manner, where a new network input (image) is processed every 2Ktime steps by the SNN. In this case the parameters of the FS-neurons change periodically6--- Page 7 ---with a period of length K while the FS-neurons compute. These K steps are followed byK time steps during which the FS-neurons are inactive, while the FS-neurons on the nextlayer collect their spike inputs for emulating the next computation step or layer of theANN. Note that since all FS-neurons that emulate ANN neurons with the same activationfunction can use the same parameters T(t), h(t), d(t), they require only little extra memoryon a neuromorphic chip.Both the TensorFlow code and the chosen parameters of the FS-neurons are availableonline\u2217.Application to ImageNetThe ImageNet data set [Russakovsky et al., 2015] has become the most popular bench-mark for state-of-the-art image classi\ufb01cation in machine learning (we are using here theImageNet2012 version). This data set consists of 1 .281.167 training images and 50.000 testimages (both RGB images of di\ufb00erent sizes), that are labeled by 1000 di\ufb00erent categories.Classifying images from ImageNet is a nontrivial task even for a human, since this dataset contains for example 59 categories for birds of di\ufb00erent species and gender [Van Hornet al., 2015]. This may explain why a relaxed performance measurement, where one recordswhether the target class is among the top 5 classi\ufb01cations that are proposed by the neuralnetwork (\u201dTop5\u201d), is typically much higher.The recently proposed E\ufb03cientNet [Tan and Le, 2019] promises to become a new stan-dard CNN architecture due to its very high accuracy while utilizing a smaller number ofparameters than other CNN architectures. E\ufb03cientNet uses as activation function f(x)besides the SiLU function (Fig. 4) also the familiar sigmoid function, shown as the redcurve in Fig. 4 c. Note that 99 .97% of its activation functions are SiLU functions, makingthe appearance of the sigmoid function comparatively rare. The SiLU function emergedfrom preceding work on optimizing activation functions in ANNs [Zoph and Le, 2018].Another characteristic of the E\ufb03cientNet architecture is the extensive usage of depth-wiseseparated convolution layers. In between them, linear activation functions are used. Al-though it would certainly be possible to approximate linear functions using FS-coding, wesimply collapsed linear layers into the generation of the weighted sums that form the inputsto the next layers.Since the SiLU function assumes also negative values, it appears to be di\ufb03cult to convertan ANN with this activation function via rate-coding to a spiking neuron. But it is fairlyeasy to convert it to an FS-neuron. The values of the parameters T(t),h(t) and d(t) fort= 1,...,K of the FS-neuron can be obtained by training the FS-neuron model to \ufb01t theSiLU function, see Fig. 4 a and b. We used for that backpropagation through time, witha triangle-shaped pseudo derivative for the non-existing derivative of the Heaviside stepfunction.In most cases, the possible inputs to an activation function are not uniformly dis-tributed, but there exists a certain region in which most inputs lie with high probability.\u2217https://github.com/christophstoeckl/FS-neurons7--- Page 8 ---For example, most of the inputs to the SiLU functions in the E\ufb03cientNet are in the in-terval from \u22122 to 2 and therefore, achieving a high approximation accuracy in this regionis especially desirable, (see Fig. 5 a). It is possible to encourage the FS-neuron to putmore emphasis on a certain region, by assigning a high weight in the loss function to thisregion. More details about the training procedure of the FS-parameters can be found inthe Methods section.The e\ufb00ective activation function of the resulting FS-neuron is shown in Fig. 4a. Fig. 4c shows the corresponding result for the FS-conversion of an ANN neuron with the sigmoidactivation function.Figure 4: Approximations \u02c6f(x) of di\ufb00erent activation functions by FS-neuronsa) Approximation of the SiLU function with a single FS-neuron.(red: SiLU function, blue: FS-approximation with K = 16)b) Optimized internal parameters of the SiLU FS-neuron.c) Approximation of the sigmoid function with a single FS-neuron.d) Optimized internal parameters of the sigmoid FS-neuron.e) Approximation of the ReLU function with K = 10 and \u03b1= 25.8--- Page 9 ---Model ANNaccuracyaccuracy of theSNN producedby FS-conversion# params # layers # neurons # spikesImageNet2012E\ufb03cientNet-B7 85%(97.2 %)83.57%(96.7%) 66M 218 259M 554.9MResNet50 75.22%(92.4%)75.10%(92.36%) 26M 50 9.6M 14.045MCIFAR10ResNet8 87.22% 87.05% 78k 8 73k 103kResNet14 90.49% 90.39% 174k 14 131k 190kResNet20 91.58% 91.45% 271k 20 188k 261kResNet50 92.99% 92.42% 755k 50 475k 647kTable 1: Accuracy and spike numbers for classifying images from ImageNetwith FS-conversions of two state-of-the-art CNNs. The SNNs produced by FS-conversion of the ANNs achieved almost the same accuracy, and usually used at most 2spikes per neuron. Top5 accuracy is reported in parentheses. The number of spikes neededfor inference was obtained by averaging over the 1000 test images.Using these FS-neurons it is possible to emulate the E\ufb03cientNet-B7 model with spikingneurons. The accuracy of the resulting spiking CNN, using the publicly available weightsw of the trained E\ufb03cientNet, can be found in Table 1, together with the total number ofspikes that it uses for sample inferences.The FS-conversion of E\ufb03cientNet-B7 achieved an accuracy of 83 .57%. The best ac-curacy for ImageNet that had previously been reported for SNNs was 74 .6% [Rueckaueret al., 2017]. It was achieved by a rate-based conversion, which required a substantialnumber of spikes per neuron and about 550 time steps for each image classi\ufb01cation. TheSNN resulting from FS-conversion of E\ufb03cientNet-B7 used about 2 spikes per neuron forclassifying an image. The FS-neurons approximating the SiLU function used K = 16 andthe FS-neurons approximating the sigmoid function used K = 12.The layers of the CNN that use the SiLU function as activation function can be simu-lated in a pipelined manner by the SNN, processing a new image every 2 K time steps: Its\ufb01rst K time steps are spent collecting the outputs from the preceding layer of FS-neuronsduring their K time steps of activity. It then processes these collected inputs xduring thesubsequent K time steps.Hence the SNN that results from FS-conversion of E\ufb03cientNet can classify a new imageevery 2K = 32 time steps. Further implementation details can be found in the Methodssection.9--- Page 10 ---Approximating the ReLU activation functionThe ReLU activation function, see Fig. 4 d, is among the most frequently used activationfunctions, and also quite good accuracies have been achieved with it for ImageNet. Itrepresents a special case for FS-conversion, as it is possible to \ufb01nd the ideal values forh(t),T(t) and d(t) analytically, bases on the idea of computation with binary numbers.By setting the parameters of the FS-neuron to T(t) = h(t) = d(t) = 2 K\u2212t, the FS-neuron approximates the ReLU activation function f(x) with a coarse-to-\ufb01re-processingstrategy. Let us assume for simplicity that an FS-neuron receives inputs xfrom (\u2212\u221e,0] \u222a{1,2,..., 2K \u22121}. Then it reproduces with the speci\ufb01ed parameters the output ReLU( x)of the ReLU gate for any x from (\u2212\u221e,0] \u222a{1,2,..., 2K \u22121}without error. In order to beable to transmit also non-integer values xbetween 0 and some arbitrary positive constant\u03b1, one simply multiplies the given values for T(t),h(t) and d(t) with \u03b12\u2212K. Then the FS-neuron reproduces ReLU(x) for any non-negative xless than \u03b1that are multiples of \u03b12\u2212Kwithout error, and ReLU( x) is rounded down for values x in between to the next largermultiple of \u03b12\u2212K. Thus the output of the FS-neuron deviates for xin the range from \u2212\u221eto \u03b1 by at most \u03b12\u2212K from the output of the ReLU gate. The resulting approximation isplotted for \u03b1= 10 in Fig. 4 d. Several advantages arising from the simple structure of theparameters have been laid out in the Methods section.The accuracy of 75.22% for the ANN version of ResNet50 in Table 1 resulted from train-ing a variant of ResNet50 where max-pooling was replaced by average pooling, using thehyperparameters given in the TensorFlow repository\u2020. The resulting accuracy in ImageNetis close to the best published performance of 76% for ResNet50 ANNs [Tan and Le, 2019,Table 2]. The application of the FS-conversion to this variant of ResNet50 (with K = 10and \u03b1= 25) yields an SNN whose Top1 and Top5 performance is almost indistinguishablefrom that of the ANN version.Application to CIFAR10CIFAR10 [Krizhevsky et al., 2009] is a smaller and more frequently used dataset for imageclassi\ufb01cation. It consists of 60.000 colored images, each having a resolution of just 32 by32 pixels, and just 10 image classes. The results for ANN versions of ResNet that are givenin Table 1 for CIFAR10 arise from training them with the hyperparameters given in theTensorFlow models repository. They use the ReLU function as the only nonlinearity, sincewe have replaced there max-pooling by average pooling. Nevertheless, they achieve anaccuracy for CIFAR10 which is very close to the best results reported for CIFAR10 in theliterature. The best performing reported ResNet on CIFAR10 is ResNet110, where a testaccuracy of 93.57% had been achieved [He et al., 2016]. Our ResNet50 achieves 92.99%,which is similar to their accuracy of 93.03% for ResNet56.\u2020https://github.com/tensor\ufb02ow/tpu/tree/master/models/o\ufb03cial/e\ufb03cientnet10--- Page 11 ---Analysis of FS-codingOn digital neuromorphic hardware the energy consumption is proportional to the numberof spikes which are needed for a computation. The number of spikes needed for an FS-neuron to perform the approximation of the target function is depicted in Fig. 5 a and bas function of the gate input x. If one compares these numbers with the distribution ofinput values x (red curves) that typically occur during image classi\ufb01cation, one sees whyon average less than 2 spikes are used by FS-neurons for these applications.11--- Page 12 ---Figure 5: Number of spikes needed by FS-neurons for image classi\ufb01cation andin\ufb02uence of K and bit precision Q on performancea) The number of spikes used by an a FS-neuron with K = 16 to approximate the SiLU function, asfunction of its input value x. The red Gaussian models the probability that the FS-neuron will receive thisinput value in the E\ufb03cientNet-B7 model (mean = \u22120.112, variance = 1.99). b) The number of spikes usedby an FS-neuron to approximate the ReLU function with K = 6 and \u03b1 = 10. The red Gaussian modelsthe probability that the FS-neuron will receive this input value in the ResNet50 model in an application toimages from ImageNet (mean \u22120.36970, variance = 2.19). c) Test Accuracy of the ResNet50 model onCIFAR10 with FS-neurons, in dependence on K. The red cross indicates the chosen value of K for ourresults. d) Mean squared error (MSE) of a SiLU approximation by FS-neurons with di\ufb00erent values ofK. The red cross indicates the chosen value of K in the given context. e) MSE of a SiLU approximationby FS-neurons with K = 16 as function of the bit precision Q of its parameters. f) MSE of a sigmoidapproximation by FS-neurons with K = 12 as function of the bit precision Q of its parameters.12--- Page 13 ---The most important speci\ufb01cation of an FS-neuron is the number K of time steps thatit uses. Fig. 5c, d provide insight into the nature of the trade-o\ufb00 between the size of Kand the approximation quality of the FS-neuron.Furthermore, it is of interest to consider scenarios where only a certain number of bitsare available for the FS-neuron parameters. To analyze the impact of that we considera setting where the parameters of the FS-neurons can only take on discrete values in therange from [\u22128,8]. The possible values are equally spaced and the number of values can bewritten as 2Q, where Qrefers to the number of bits which are available for each parameterT(t), h(t), d(t) of the FS-neuron. Fig. 5e, f depict the impact of such quantization on themean squared error of the approximation of the activation function.Expected implementation cost on neuromorphic hardwareWe distinguish three types of neuromorphic hardware\u2022 Digital, but hardware not constrained to a particular neuron model (example: SpiN-Naker)\u2022 Digital, but hardware is constrained to a particular neuron model (example: Loihi)\u2022 Mixed digital analog (examples: IBM research chip with memristors and BrainScaleS-2)SpiNNakerThe SpiNNaker platform [Furber et al., 2014] provides a \ufb02exible environment which is notconstrained to a speci\ufb01c neuron model. SpiNNaker allows to compute all products d(t)won the chip, which reduces the additional memory consumption to to a small constantvalue. All parameters T(t), h(t) and d(t) only need to be stored in memory once, as theycan be shared across all neurons which approximate the same activation function. Theadditional computational complexity of the FS-neuron model also has a very small impact,as computing the updated weight d(t)w can be done with a single instruction cycle.LoihiLoihi [Davies et al., 2018] also promises to be an interesting target platform for FS-neurons.Especially FS-neurons approximating the ReLU activation function could be ported verye\ufb03ciently to this hardware platform. As the chip is based on \ufb01xed-point arithmetic, onecan implement T(t), h(t) and d(t) for ReLu using a single parameter, namely the sharedweight exponent. This is be possible due to the fact that at every time step t all FS-parameters have the same value, which is always a power of 2. Therefore, the additionalmemory consumption does not grow with K. It is also possible to use other activationfunctions besides ReLU on Loihi, however, in this case it would be necessary to store allproducts d(t)w on the chip, as computing the updated weight in an online fashion is notpossible. In this case, an increase in memory consumption of O(K) is expected.13--- Page 14 ---IBM research chip with memristorsIBM has presented an in-memory chip architecture supporting both ANNs and SNNs in theSupplementary Material S3 of the article [Wo\xb4 zniak et al., 2020]. This architecture employsa memristor crossbar array for fast (time complexity O(1)) and energy-e\ufb03cient multipli-cation of the outputs of one layer l of neurons with the weights of synaptic connectionsto neurons on the next layer l+ 1. One can replace all spikes (i.e., values 1) that emergefrom layer l at time t of the K-step cycle in the emulation of the neurons on layer l by acommon value value d(t) that is centrally stored. Since the values d(t) and 0 can be useddirectly as inputs to the memristor array, no signi\ufb01cant extra cost is expected. The neuronmodels are implemented in the digital part of this neuromorphic chip architecture of IBM,using very fast digital logic and SRAM for storing parameters. Since all neurons on a layerl of our FS networks use the same parameters T(t) and h(t), they can be stored in a localSRAM for all neurons on layer l, in a similar fashion as on SpiNNaker. A neuron modelthat goes already one step in the direction from LIF to FS-neurons has actually alreadybeen implemented on this architecture: The soft spiking neural unit (sSNU), that emitsanalog instead of binary values and subtracts a corresponding value from the membranepotential [Wo\xb4 zniak et al., 2020].BrainScaleS-2This neuromorphic chip [Billaudelle et al., 2020] is also a mixed analog digital architecturewhere a digital plasticity processor allows fast changes of synaptic weights, but also centralmemory storage and application of the time-varying parameters T(t), h(t) and d(t) ofthe neuron dynamics. Like on SpiNNaker, the parameters only have to be stored once inmemory and can be shared across many neurons. The leak term of the membrane voltageof the analog neuron models can be switched o\ufb00, so that the analog part can be used fore\ufb03cient matrix multiplication in a similar manner as on the IBM chip.MethodsIn this section various details necessary to reproduce our results have been listed. Ad-ditionally, to aid the interpretation of the results, a comparison to previous conversionapproaches has been added.When training the parameters of the FS-neurons it is important to specify an intervalin which the approximation should be very good. Ideally, most of the inputs to the ANNneuron should fall into this interval to guarantee a good performance. In our experiments,the FS-neurons have been trained to approximate the interval from [ \u22128,12] for the SiLUfunction and [ \u221210,10] for the sigmoid function. The resulting FS-neuron approximatesthe SiLU function with a mean squared error of 0 .0023 inside the main region [ \u22122,2] and0.0064 in the region outside, which can be written as [ \u22128,\u22122] \u222a[2,12]. As a result of our14--- Page 15 ---\ufb01ne-tuning the values for T(t), d(t) and h(t) stay for most time steps t within the mainregion [\u22122,2] as can be seen in Fig. 4b.To reduce the complexity of the converted CNN, we decided not to emulate the mul-tiplication operation by FS-neurons, which occurs in the CNN if the squeeze and excita-tion optimization [Hu et al., 2018] is being used. In many neuromorphic chips, such asSpiNNaker and Loihi, the on-chip digital processor could carry out these multiplications.Otherwise one can approximate multiplication in a similar manner as the SiLU functionwith a suitably optimized FS-neuron, see [St\xa8 ockl and Maass, 2019]. Alternatively one cancompute multiplication with a small circuit of threshold gates, i.e., very simple types ofspiking neurons, of depth 2 or 3. A recent summary of such results is provided in section3 of [Parekh et al., 2018].Due to the simple structure of the parameters of the ReLU FS-neurons several advan-tages arise. In particular when approximating the ReLU function with an FS-neuron it ispossible to calculate the changes of parameters for t= 1,...,K by simply using a bit shiftoperation, possibly providing a very e\ufb03cient implementation on neuromorphic hardware.The resulting SNN can be used in a pipelined manner, processing a new network inputevery 2K = 20 time steps, analogously as for the SiLU function.Further properties of FS-coding that are relevant for neuromor-phic hardwareOne of the major advantages of using FS-neurons in neuromorphic hardware is the smalleramount of time steps and spikes required to approximate arti\ufb01cial neurons. For the case ofthe ReLU activation function, a rate coded spiking neuron requires N time steps to encodeN di\ufb00erent values. FS-neurons improve upon this unary coding scheme by utilizing thetime dimension to implement a binary coding scheme. Therefore, the number of time stepsrequired to encode N di\ufb00erent values can be reduced to just log2(N).To underline the binary coding nature of FS-neurons, in the case of the ReLU activationfunction, the corresponding FS-neurons will show a spiking pattern equivalent to of theoutput of the ReLU function, written as a binary number. The same logarithmic relationholds not only for the number of time steps required but also for the number of spikes neededto transmit a value. More sophisticated codes could be used to make the computationrobust to noise in spike transmission.Note, that most of the inputs to the FS-neurons have a value close to 0, as shown inFig. 5 a and b. Consequently, the FS-neurons usually require only a few spikes to transmitthe output values, making the the coding scheme even more sparse in practice.Comparison with previous methodsThe idea of converting a pre-trained ANN to a SNN has received a fair amount of attentionin the recent years. The most popular conversion approaches are rate-based, meaning they15--- Page 16 ---translate the continuous output of an arti\ufb01cial ReLU neuron into a \ufb01ring rate of a spikingneuron. Unfortunately there are some drawbacks associated with rate-coding. Due to itsunary coding nature, rate-codes are sub-optimal in the sense that they do not make gooduse of the time dimension. Usually a large amount of time steps is required to achieve asu\ufb03ciently accurate approximation. Furthermore, rate-based conversions are only capableof converting simple activation functions like ReLU, but fail to convert more sophisticatedfunctions like SiLU.Another popular conversion approach uses time to \ufb01rst spike (TTFS) coding [Rueckauerand Liu, 2018]. This approach encodes the continuous outputs of the corresponding ReLUANN neurons in the length of the time interval until the \ufb01rst spike, resulting in a verysparse spiking activity. However, this method seems to not scale easily to large modelsand has, to the best of our knowledge, not been tested on large data sets like ImageNet.The idea of using single spike temporal coding has \ufb01rst been explored in [Maass, 1997] andit has been shown to have a variety of applications, like implementing an e\ufb03cient k-NNalgorithm on neuromorphic hardware [Frady et al., 2020].Furthermore, a new conversion method has been proposed, in which the spiking neuronscan approximate the ReLU function using a hysteresis quantization method [Yousefzadehet al., 2019]. This approach waits to be tested on larger networks and datasets.One property that all previously mentioned conversion methods have in common is thatthey only consider transforming arti\ufb01cial ReLU neurons to spiking neurons, and thereforecannot convert more sophisticated activation functions, which are used in network archi-tecture like the E\ufb03cientNets.A detailed summary comparing FS-coding to previous results can be found in theExtended Data Table 1.It is worth noting, that the throughput using FS-coding is substantially better thanthat of SNNs which result from rate-based ANN-to-SNN conversions of ANNs with theReLU function, as proposed for example in [Rueckauer et al., 2017, Sengupta et al., 2019].The Inception-v3 model in [Rueckauer et al., 2017] was reported to yield a SNN that needed550 time steps to classify an image. Under the assumption that rate-based models pro\ufb01tonly very little from pipelining, it is reasonable to estimate that the throughput of an SNNthat results from FS-conversion of ReLU gates with K = 10 is roughly 25 times higher.The SNN resulting from the rate-based conversion of the ResNet34 model discussedin [Sengupta et al., 2019] has been reported to use 2500 time steps for a classi\ufb01cation.Therefore we estimate that the throughput is increased here by a factor around 125 throughFS-conversion.Spiking versions of ResNet20 have already been previously explored [Sengupta et al.,2019]. Using a rate-based conversion scheme an accuracy of 87.46% was reported.FS-conversion of ResNet20 yields a substantially higher accuracy of 91.45%, using just80 to 500 time steps for each image -depending on the model depth- instead of 2000,thereby signi\ufb01cantly reducing latency. In addition, the throughput is drastically improved.Also the number of spikes that the SNN uses for classifying an image from CIFAR10is signi\ufb01cantly reduced when one moves from a rate-based conversion to an FS conversion.A converted ResNet11 has been reported to use more than 8 million spikes to classify16--- Page 17 ---a single test example [Lee et al., 2020]. Comparing this to an FS-converted ResNet14we \ufb01nd that the latter uses 40 times fewer spikes despite being a slightly larger model.Using direct training of SNNs instead of a conversion scheme has been reported to resultin a lower amount of spikes needed to perform a single classi\ufb01cation. However, even adirectly trained SNN version of ResNet11 uses 7 times more spikes than an FS-conversionof ResNet14 [Lee et al., 2020, Table 8].In [Rathi et al., 2020] the authors present a novel approach for obtaining high perfor-mance SNNs by combining a rate-based conversion scheme with a subsequent gradient-based \ufb01ne-tuning procedure. They report the highest accuracy for an SNN on CIFAR10,which was achieved by converting a very performant ANN. They also show results forImageNet, where they achieve an accuracy of 65 .1% on their ResNet-34. Deeper models,like the ResNet-50, were not considered in this work. On ImageNet FS-conversion of theResNet-50 model improves their accuracy by 10% and FS-conversion of the E\ufb03cientNet-B7surpasses their performance by 18.47%.DiscussionWe have presented a new approach for generating SNNs that are very close to ANNs interms of classi\ufb01cation accuracy for images, while working in the energetically most attrac-tive regime with very sparse \ufb01ring activity. Besides substantially improved classi\ufb01cationaccuracy, they exhibit drastically improved latency and throughput compared with rate-based ANN-to-SNN conversions. o\ufb00 the shelf. One can argue that this is exactly theway which evolution has chosen for the design of neurons in living organism. Not onlyneurons with particular information processing tasks in the smaller nervous systems ofinsects, but also neurons in the neocortex of mammals exhibit an astounding diversity ofgenetically encoded response properties [ [Sterling and Laughlin, 2015], [Gouwens et al.,2019], [Bakken et al., 2020]]. In particular, the probability of producing a spike dependsin diverse ways on the recent stimulation history of the neuron, see [Gerstner et al., 2014]for some standard models. In other words, the excitability of di\ufb00erent types of biologicalneurons increases or decreases in complex ways in response to their previous \ufb01ring. As aresult, the temporal structure of a train of spikes that is produced by a biological neuroncontains additional information about the neuron input that can not be captured by its\ufb01ring rate. Similary, FS-neurons that are optimized for high accuracy image classi\ufb01cationwith few spikes exhibit history-dependent changes -encoded through their functions T(t)and h(t) according to equ. (2)- in their propensity to \ufb01re, see Fig. 4b and 4e. Furthermorethe function d(t) enables subsequent neurons to decode their spikes in a timing-sensitivemanner. In these regards an FS-conversion from ANNs to SNNs captures more of the func-tional capabilities of spiking neurons than previously considered rate-based conversions toan o\ufb00-the-shelf spiking neuron model.It is well known that spikes from the same neurons in the brain can transmit di\ufb00erentinformation to downstream neurons depending on the timing of the spike, see e.g. phaseprecession in the hipppocampus [Harris et al., 2002]. Hence is is conceivable that down-17--- Page 18 ---stream neurons give di\ufb00erent weights to these spikes, in dependence of the \ufb01ring time ofthe presynaptic neuron. In fact, it is well known that the large repertoire of pre- andpostsynaptic synaptic dynamics found in di\ufb00erent synapses of the brain [Markram et al.,2004, Kopanitsa et al., 2018] enables postsynaptic neurons to modulate the amplitude ofpostsynaptic responses in dependence of the timing of presynaptic spikes relative to an un-derlying rhythm. This can be viewed as a biological counterpart of the timing-dependentweights d(t) in our model. Altogether we believe that FS-neurons provide a \ufb01rst step inexploring new uses of SNNs where information is not encoded by the timing of single spikesor \ufb01ring rates, but by temporal spike patterns.Important for applications of FS-coding in neuromorphic hardware is that it is ap-plicable to virtually any activation function, in particular to that activation function forANN neurons that currently provides the highest accuracy on ImageNet, the SiLU func-tion. Rate-based conversion can not be readily applied to the SiLU function because itassumes both positive and negative output values. When approximating the more com-monly used ReLU function, FS-neurons approach the information theoretic minimum ofspikes for spike-based communication.In fact, FS-neurons that emulate ANN gates with the ReLU activation function produce1.5 spikes on average for classifying an image, while those for the Switch activation functionproduce 2 spikes on average. As the number of spikes required for inference by an SNNis directly related to its energy consumption in spike-based neuromorphic hardware, theenergy consumption of FS-converted SNNs appears to be close to the theoretical optimumfor SNNs. Since FS-conversion provides a tight bound on the number K of time stepsduring which a spiking neuron is occupied, it can also be used for converting recurrentlyconnected ANNs to SNNs.The proposed method for generating highly performant SNNs for image classi\ufb01cationthrough FS-conversion of trained CNNs o\ufb00ers an opportunity to combine the computa-tionally more e\ufb03cient and functionally more powerful training of ANNs with the superiorenergy-e\ufb03ciency of SNNs for inference. Note that one can also use the resulting SNN asinitialization for further training of the SNN, e.g., for a more speci\ufb01c task.Altogether our results suggest that spike-based hardware may gain an edge in thecompetition for the development of drastically more energy-e\ufb03cient hardware for AI if onedoes not forgot to optimize the spiking neuron model in the hardware for its intendedrange of applications. In contrast to energy e\ufb03cient digital hardware that is optimizedfor speci\ufb01c ANN architectures and activation functions, see e.g. [Zhang et al., 2019] for areview, a spike-based neuromorphic chip that is able to emulate FS-neurons can carry outinference for all possible ANN architectures. It can also emulate ANNs with previouslynot considered activation functions, since a change of the activation function just requiresreprogramming of the digitally stored parameters of FS-neurons. Hence such spike-basedchip will be substantially more versatile than common digital hardware accelerators for AI.18--- Page 19 ---AcknowledgementsWe would like to thank Franz Scherr for helpful discussions. We thank Thomas Bohnst-ingl, Evangelos Eleftheriou, Steve Furber, Christian Pehle, Philipp Plank and JohannesSchemmell for advice regarding implementation aspects of FS-neurons in various typesof neuromorphic hardware. This research was partially supported by the Human BrainProject of the European Union (Grant agreement number 785907). We also thank ouranonymous reviewers for very constructive comments.Competing InterestsWe are not aware of competing interests.Data availabilityBoth ImageNet [Deng et al., 2009] and CIFAR10 [Krizhevsky et al., 2009] are publiclyavailable datasets. No additional datasets were generated or analysed during the cur-rent study. The data for the spike response depiced in \ufb01gure 1 has been publishedbytheAllen Institute for Brain Science in 2015 (Allen Cell Types Database). Available from:https://celltypes.brain-map.org/experiment/electrophysiology/587770251Author contributionsCS conceived the main idea, CS and WM designed the model and planned the experiments,CS carried out the experiments, CS and WM wrote the paper.Code availabilityThe code this work is based on is publicly available at: https://github.com/christophstoeckl/FS-neurons (DOI: 10.5281/zenodo.4326749). Additionally, the code is also available in a CodeOcean compute capsule: https://codeocean.com/capsule/7743810/treeReferences[Bakken et al., 2020] Bakken, T. E., Jorstad, N. L., Hu, Q., Lake, B. B., Tian, W., Kalm-bach, B. E., Crow, M., Hodge, R. D., Krienen, F. M., Sorensen, S. A., et al. (2020).Evolution of cellular diversity in primary motor cortex of human, marmoset monkey,and mouse. bioRxiv.19--- Page 20 ---[Bellec et al., 2020] Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legen-stein, R., and Maass, W. (2020). A solution to the learning dilemma for recurrentnetworks of spiking neurons. Nature Communications, 11(1):1\u201315.[Billaudelle et al., 2020] Billaudelle, S., Stradmann, Y., Schreiber, K., Cramer, B., Baum-bach, A., Dold, D., G\xa8 oltz, J., Kungl, A. F., Wunderlich, T. C., Hartel, A., M\xa8 uller, E.,Breitwieser, O., Mauch, C., Kleider, M., Gr\xa8 ubl, A., St\xa8 ockel, D., Pehle, C., Heimbrecht,A., Spilger, P., Kiene, G., Karasenko, V., Senn, W., Petrovici, M. A., Schemmel, J., andMeier, K. (2020). Versatile emulation of spiking neural networks on an accelerated neu-romorphic substrate. In 2020 IEEE International Symposium on Circuits and Systems(ISCAS), pages 1\u20135.[Davies et al., 2018] Davies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y., Choday,S. H., Dimou, G., Joshi, P., Imam, N., Jain, S., et al. (2018). Loihi: A neuromorphicmanycore processor with on-chip learning. IEEE Micro, 38(1):82\u201399.[Deng et al., 2009] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on com-puter vision and pattern recognition , pages 248\u2013255. Ieee.[Frady et al., 2020] Frady, E. P., Orchard, G., Florey, D., Imam, N., Liu, R., Mishra, J.,Tse, J., Wild, A., Sommer, F. T., and Davies, M. (2020). Neuromorphic nearest neighborsearch using intel\u2019s pohoiki springs. In Okandan, M. and Aimone, J. B., editors, NICE\u201920: Neuro-inspired Computational Elements Workshop, Heidelberg, Germany, March17-20, 2020, pages 23:1\u201323:10. ACM.[Furber et al., 2014] Furber, S. B., Galluppi, F., Temple, S., and Plana, L. A. (2014). Thespinnaker project. Proceedings of the IEEE, 102(5):652\u2013665.[Garc\xb4 \u0131a-Mart\xb4 \u0131n et al., 2019] Garc\xb4 \u0131a-Mart\xb4 \u0131n, E., Rodrigues, C. F., Riley, G., and Grahn, H.(2019). Estimation of energy consumption in machine learning. Journal of Parallel andDistributed Computing, 134:75\u201388.[Gerstner et al., 2014] Gerstner, W., Kistler, W. M., Naud, R., and Paninski, L. (2014).Neuronal dynamics: From single neurons to networks and models of cognition. Cam-bridge University Press .[Gouwens et al., 2019] Gouwens, N. W., Sorensen, S. A., Berg, J., Lee, C., Jarsky, T., Ting,J., Sunkin, S. M., Feng, D., Anastassiou, C. A., Barkan, E., et al. (2019). Classi\ufb01cation ofelectrophysiological and morphological neuron types in the mouse visual cortex. Natureneuroscience, 22(7):1182\u20131195.[Han et al., 2020] Han, B., Srinivasan, G., and Roy, K. (2020). Rmp-snn: Residual mem-brane potential neuron for enabling deeper high-accuracy and low-latency spiking neuralnetwork. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 13558\u201313567.20--- Page 21 ---[Harris et al., 2002] Harris, K. D., Henze, D. A., Hirase, H., Leinekugel, X., Dragoi, G.,Czurk\xb4 o, A., and Buzs\xb4 aki, G. (2002). Spike train dynamics predicts theta-related phaseprecession in hippocampal pyramidal cells. Nature, 417(6890):738\u2013741.[He et al., 2016] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning forimage recognition. Proceedings of the IEEE Computer Society Conference on ComputerVision and Pattern Recognition, 2016-Decem:770\u2013778.[Hendrycks and Gimpel, 2016] Hendrycks, D. and Gimpel, K. (2016). Gaussian error linearunits (gelus). arXiv preprint arXiv:1606.08415 .[Hu et al., 2018] Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-Excitation Networks.Proceedings of the IEEE Computer Society Conference on Computer Vision and PatternRecognition, pages 7132\u20137141.[Kheradpisheh and Masquelier, 2020] Kheradpisheh, S. R. and Masquelier, T. (2020).S4nn: temporal backpropagation for spiking neural networks with one spike per neuron.International Journal of Neural Systems , 30(6):2050027.[Kopanitsa et al., 2018] Kopanitsa, M. V., van de Lagemaat, L. N., A\ufb01nowi, N. O.,Strathdee, D. J., Strathdee, K. E., Fricker, D. G., Tuck, E. J., Elsegood, K. A., Croning,M. D., Komiyama, N. H., et al. (2018). A combinatorial postsynaptic molecular mech-anism converts patterns of nerve impulses into the behavioral repertoire. BioRxiv, page500447.[Krizhevsky et al., 2009] Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layersof features from tiny images. Technical report.[Lee et al., 2020] Lee, C., Sarwar, S. S., Panda, P., Srinivasan, G., and Roy, K. (2020).Enabling spike-based backpropagation for training deep neural network architectures.Frontiers in Neuroscience, 14:119.[Ling, 2001] Ling, J. (2001). https://hypertextbook.com/facts/2001/JacquelineLing.shtml.[Maass, 1997] Maass, W. (1997). Fast sigmoidal networks via spiking neurons. NeuralComputation, 9(2):279\u2013304.[Maass and Natschl\xa8 ager, 1998] Maass, W. and Natschl\xa8 ager, T. (1998). Emulation of Hop-\ufb01eld networks with spiking neurons in temporal coding. In Computational Neuroscience,pages 221\u2013226. Springer.[Markram et al., 2004] Markram, H., Toledo-Rodriguez, M., Wang, Y., Gupta, A., Silber-berg, G., and Wu, C. (2004). Interneurons of the neocortical inhibitory system. Naturereviews neuroscience, 5(10):793\u2013807.21--- Page 22 ---[Parekh et al., 2018] Parekh, O., Phillips, C. A., James, C. D., and Aimone, J. B. (2018).Constant-depth and subcubic-size threshold circuits for matrix multiplication. In Pro-ceedings of the 30th on Symposium on Parallelism in Algorithms and Architectures, pages67\u201376.[Rathi et al., 2020] Rathi, N., Srinivasan, G., Panda, P., and Roy, K. (2020). Enablingdeep spiking neural networks with hybrid conversion and spike timing dependent back-propagation. In International Conference on Learning Representations.[Rueckauer and Liu, 2018] Rueckauer, B. and Liu, S.-C. (2018). Conversion of analog tospiking neural networks using sparse temporal coding. In 2018 IEEE InternationalSymposium on Circuits and Systems (ISCAS) , pages 1\u20135. IEEE.[Rueckauer et al., 2017] Rueckauer, B., Lungu, I. A., Hu, Y., Pfei\ufb00er, M., and Liu, S. C.(2017). Conversion of continuous-valued deep networks to e\ufb03cient event-driven networksfor image classi\ufb01cation. Frontiers in Neuroscience, 11(DEC):1\u201312.[Russakovsky et al., 2015] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei,L. (2015). ImageNet Large Scale Visual Recognition Challenge. International Journalof Computer Vision , 115(3):211\u2013252.[Sengupta et al., 2019] Sengupta, A., Ye, Y., Wang, R., Liu, C., and Roy, K. (2019).Going Deeper in Spiking Neural Networks: VGG and Residual Architectures. Frontiersin Neuroscience, 13(1998):1\u201316.[Sterling and Laughlin, 2015] Sterling, P. and Laughlin, S. (2015). Principles of neuraldesign. MIT Press.[St\xa8 ockl and Maass, 2019] St\xa8 ockl, C. and Maass, W. (2019). Recognizing images with atmost one spike per neuron. arXiv preprint arXiv:2001.01682 .[Tan and Le, 2019] Tan, M. and Le, Q. (2019). E\ufb03cientNet: Rethinking model scalingfor convolutional neural networks. In Chaudhuri, K. and Salakhutdinov, R., editors,Proceedings of the 36th International Conference on Machine Learning , volume 97 ofProceedings of Machine Learning Research , pages 6105\u20136114, Long Beach, California,USA. PMLR.[Thorpe et al., 2001] Thorpe, S., Delorme, A., and Rullen, R. (2001). Spike-based strate-gies for rapid processing. Neural networks : the o\ufb03cial journal of the InternationalNeural Network Society, 14:715\u201325.[Van Horn et al., 2015] Van Horn, G., Branson, S., Farrell, R., Haber, S., Barry, J., Ipeiro-tis, P., Perona, P., and Belongie, S. (2015). Building a bird recognition app and largescale dataset with citizen scientists: The \ufb01ne print in \ufb01ne-grained dataset collection.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,pages 595\u2013604.22--- Page 23 ---[Wo\xb4 zniak et al., 2020] Wo\xb4 zniak, S., P, Your response: ```ADD_PAPER\n2002.00860v4\nThis paper introduces an optimized spiking neuron model for efficient image classification on edge devices through temporal coding with just two spikes per neuron. The authors propose a novel ANN-to-SNN conversion method called FS-conversion that significantly improves upon traditional rate-based approaches by encoding information in spike timing rather than just spike counts. Key innovations include: 1) A modified spiking neuron model (FS-neuron) with optimized internal dynamics for emulating ANN activation functions with minimal spikes, 2) Demonstration on ImageNet and CIFAR10 showing near-identical accuracy to original ANNs (83.57% on ImageNet with EfficientNet-B7) while using only ~2 spikes per neuron, 3) Substantial improvements in latency and throughput compared to rate-based conversions, enabling new image classification every 32 time steps. The method is particularly effective for edge deployment as it achieves biological-level energy efficiency while maintaining high accuracy, making it suitable for mobile and embedded vision applications.\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\n~~~~~~~~~~\nCurrent Step #17, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification quantization techniques\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 19:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\n~~~~~~~~~~\nCurrent Step #18, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification hardware-aware optimization\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 20:
\n~~~~~~~~~~\nHistory: Step #0, Phase: literature review, Feedback: , Your response: ```SUMMARY\nreal-time edge image classification optimization\n```\nStep #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\n~~~~~~~~~~\nCurrent Step #19, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 21:
\n~~~~~~~~~~\nHistory: Step #1, Phase: literature review, Feedback: You requested arXiv papers related to the query real-time edge image classification optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```FULL_TEXT\n2409.08943v1\n```\nStep #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\n~~~~~~~~~~\nCurrent Step #20, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 22:
\n~~~~~~~~~~\nHistory: Step #3, Phase: literature review, Feedback: Successfully added paper 2409.08943v1, Your response: ```SUMMARY\nedge device image classification NAS optimization\n```\nStep #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\n~~~~~~~~~~\nCurrent Step #21, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGAAli Jahanshahiajaha004@ucr.eduUniversity of California, RiversideAbstractIn recent years, Convolutional Neural Network (CNN)based methods have achieved great success in a large num-ber of applications and have been among the most power-ful and widely used techniques in computer vision. How-ever, CNN-based methods are computational-intensive andresource-consuming, and thus are hard to be integrated intoembedded systems such as smart phones, smart glasses, androbots. FPGA is one of the most promising platforms for ac-celerating CNN, but the limited on-chip memory size limitthe performance of FPGA accelerator for CNN.In this paper, we propose a framework for designingCNN accelerator on embedded FPGA for image classi\ufb01ca-tion. The proposed framework provides a tool for FPGAresource-aware design space exploration of CNNs and au-tomatically generates the hardware description of the CNNto be programmed on a target FPGA. The framework con-sists of three main backends; software, hardware gener-ation, and simulation/precision adjustment. The softwarebackend serves as an API to the designer to design the CNNand train it according to the hardware resources that areavailable. Using the CNN model, hardware backend gen-erates the necessary hardware components and integratesthem to generate the hardware description of the CNN. Fi-naly, Simulation/precision adjustment backend adjusts theinter-layer precision units to minimize the classi\ufb01cation er-ror.We used 16-bit \ufb01xed-point data in a CNN accelerator(FPGA) and compared it to the exactly similar software ver-sion running on an ARM processor (32-bit \ufb02oating pointdata). We encounter about 3% accuracy loss in classi\ufb01ca-tion of the accelerated (FPGA) version. In return, we gotup to 15.75\xd7 speedup by classifying with the acceleratedversion on the FPGA.1. IntroductionThe exponential growth of big data during the last decademotivates for innovative methods to extract high semanticinformation from raw sensor data such as videos, imagesand speech sequences. Among the proposed methods, Con-volutional Neural Networks (CNNs) have become the de-facto standard by delivering near-human accuracy in manyapplications related to machine vision. While CNNs havebeen known to researchers for decades, they were popu-larized after demonstrating high accuracy at the 2012 Ima-geNet recognition challenge [1]. Subsequently, CNNs havebecome the state-of-the-art for image classi\ufb01cation, detec-tion, and localization tasks. Research in CNNs and otherareas of deep learning continues at a rapid pace, with hun-dreds of new papers published each year introducing newmodels and techniques.The CNN\u2019s high performance (at classi\ufb01cation, detec-tion, and localization) comes at the price of a large com-putational cost as they require tens of GOP/s to classify asingle frame. Thus, one challenge to the widespread de-ployment of CNNs is their signi\ufb01cant demands for com-putation and storage capacity. Therefore, dedicated hard-ware is required to accelerate their execution. GraphicsProcessing Units (GPUs), are the most widely used plat-form to implement CNNs as they offer the best performancein terms of pure computational throughput, reaching up to11 TFLOP/s. Nevertheless, in terms of power consump-tion, Field-Programmable Gate Array (FPGA) solutions areknown to be more energy ef\ufb01cient (vs GPUs). Recent workby Microsoft has even explored cost-effective accelerationof deep learning on FPGAs at datacenter scale [2, 3]. Thereare also efforts in the academic community on FPGA-basedCNN accelerators [4,5] as well as tools for generating themautomatically [6, 7].We observe two trends which may help overcome im-plementing CNN on FPGAs. The \ufb01rst is a series of recentpapers in the machine learning community regarding very-low-precision CNNs. Networks with binary weights [8], orbinary weights and activations [9, 10] have in certain casesdemonstrated accuracy comparable to full precision nets.Such binarized neural net-works (BNNs) may be the keyto ef\ufb01cient deep learning on FPGA. Binarization reducesstorage and memory bandwidth requirements, and replaceFP operations with binary operations which can be very ef-\ufb01ciently performed on the LUT-based FPGA fabric. Con-1arXiv:1911.06777v1  [cs.LG]  15 Nov 2019--- Page 2 ---cerning the cost and effort of FPGA implementation,we seea steady improvement in FPGA design automation toolsover the past decade. High-level synthesis (HLS) toolssuch as Xilinx Vivado HLS [11] and LegUp [12] enable auser to write code in a high-level programming language,then algorithmically compile that code down to a register-transfer level (RTL) design speci\ufb01cation. More recent toolssuch as Intel FPGA SDK for OpenCL [13] and Xilinx SD-SoC [14] offer further automation features for generatingthe hardware-software interface and on-chip memory net-work. In the context of deep learning, these tools have thepotential to critically reduce time-to-market on new acceler-ator designs and thus reduce the aforementioned innovationgap.In this project, we aim to design and implement a CNNaccelerator on an embedded FPGA for image classi\ufb01cation.We use [15] as our reference that presents the design of aBNN accelerator for FPGAs. In contrast to our reference, inthis work, the aim is to provide designers a general frame-work to enable them design their CNNeasily and use it as anaccelerator on an FPGA. Thus, we do not use BNN-basedCNN in this project.In addition to generality of the framework, we also targetembedded systems that have hardware resource limitation.Compressing the CNN model is a good choice to addressthe resource limitations of the hardware. A straight forwardway to compress a network is to reduce the bit-width forcomputing. This utilizes the \ufb02exibility of FPGA or ASICdesign compared with GPU or CPU. It is proved to be aneffective way to use 16-bits \ufb01xed-point operations in [16]with small loss in accuracy. In our framework, we use 16-bits operation and we show that the accuracy loss is verysmall.This work proposes a general framework for design andimplementation of tiny modular CNN accelerators on em-bedded FPGAs. Our framework automatically generatesthe hardware code for the designed CNN and trained withthe provided software backend. The framework consistsof three main components that correspond to our contribu-tions. They are as follows:\u2022 Software backend: Using Python, we provide a toolfor designing and training a CNN architecture. Themodel (weights) of the trained network and otherCNN architecture parameters are used in the hardwareframework to generate the hardware description lan-guage of the CNN, which is going to be implementedon the FPGA. This back-end also provides the in-formation needed for Simulation/precision adjustmentbackend as well as checking the hardware resourcesfor the designed CNN model.\u2022 Hardware backend: Using CHISEL [17], we performautomatic generation and integration of different hard-ware components needed for the CNN architecture de-signed in the software backend. The CNN model ispassed to this back-end to be used in HDL generationof the CNN. The output of this backend is the HDLcode that is ready to be synthesized and programmedon an FPGA.\u2022 Simulation/precision adjustment backend: UsingScala testing libraries and the data passed to this back-end, inter-layer precision of the generated CNN hard-ware will be adjusted. The CNN output error intro-duced by varying integer and fractional part of eachlayer\u2019s data is minimized by this backend.The rest of this paper is organized as follows; we \ufb01rst de-scribe the framework overall structure, then we go in de-tails for each component of the framework. Finally, we pro-vide the results discussing different aspects of the generatedCNN for our target FPGA.2. Tiny CNNCNN is a machine learning classi\ufb01er that typically takesin an image and produces the probabilities of that imagebelonging to each output class. A typical CNN consists ofa pipeline of connected layers. Each layer takes as inputa set of feature maps ( fmaps), performs some computationon them, and produces a new set of fmaps to be fed intothe next layer. The input fmaps of the \ufb01rst layer is the in-put image. Layers may require con\ufb01guration values knownas parameters, which must \ufb01rst be determined by trainingthe CNN of\ufb02ine on pre-classi\ufb01ed data. Once the parame-ters are \ufb01nalized, the CNN can be deployed for inferencethe classi\ufb01cation of new data points. For most practical ma-chine learning applications, the \ufb01rst-class concerns are theaccuracy and execution time of online classi\ufb01cation. Thisproject will thus focus on accelerating the inference taskwithout compromising accuracy. The aim of this project isto provide a framework that automates implementation ofCNNs on an embedded FPGA for image classi\ufb01cation. Fig-ure 1 shows the framework we proposed for this purpose. Inthe following subsections, we are going to elaborate moreon each component of our framework.2.1. Software backendThe software backend is an API by which users are capa-ble of designing their CNN in the software level. The usercan tune their CNN with the train and test data sets, and\ufb01nally, export their model for further processings. We usedKeras library as the underlying deep learning framework ofthe software backend. Keras is a high-level neural networksAPI, written in Python and capable of running on top ofTensorFlow, CNTK, or Theano. It was developed with afocus on enabling fast experimentation. We address threechallenges of designing a CNN by Software backend:2--- Page 3 ---Figure 1: Tiny CNN framework.CNN designing, training, and model generation: In ourframework, using Keras API, we provide a templatePythonclass to the designers to design their CNN. Then, the de-signer\u2019s CNN is trained and the model is saved in order tobe used by the hardware backend. Essentially, the model isthe CNN weights that are going to be used as initial valuesof ROMs.Veri\ufb01cation data: The software backend also inputs somerandom data from test dataset to CNN and captures the out-put of all layers of CNN with respect to that input. Thecollected dataset is called veri\ufb01cation data and is used inhardware backend for precision adjustment and veri\ufb01cationpurposes.Hardware resource check: As mentioned before, we aretargeting embedded FPGA in this work. Thus, we want tosqueez the whole CNN model in FPGA BRAMs. Thus,one step for designing the CNN would be checking if theCNN model (weights) \ufb01t in FPGA BRAMs. Software back-end uses the target FPGA spec to check if the model \ufb01ts inFPGA or not. In case the model does not \ufb01t on the tar-get FPGA, the software backend throws an exception thatshows not enough space for the model. In such cases, thedesigner should play around the CNN hyper parameters andmake it smaller to \ufb01t on the FPGA. Fully-connected layers,due to their high number of parameters contributes to themost of the memory (BRAM) usage.2.2. Hardware backendHardware backend generates the hardware of the CNNwhich is designed in the software backend according tothe inputs that are provided to this backend. We imple-mented all basic hardware components that are needed forCNNs. The developed hardware components are modularand highly con\ufb01gurable. They can be con\ufb01gured based onthe CNN speci\ufb01cations including: data bit-width, shared orexclusive components, and number of DSPs available forcomponents. All of the implemented hardware componentsare modular that means we can simply attach them togetherwith few lines of code just like we do in designing CNNs insoftware.2.2.1 Convolution unitThis unit is the most critical component of the accelera-tor, as it will take up the vast majority of the runtime. Theunit must maintain high throughput and resource ef\ufb01ciency.Convolution operation has multiplication and addition in itscore. Since multiplication is a very expensive operation,we used the built-in DSPs on FPGA board that are spe-cialized for digital signal processing purposing \u2013 mainly in-clude multiplication and addition.The challenge with using the FPGA DSPs is that thereare a limited number of them on every FPGA board. Theimplemented Convolution unit hardware is con\ufb01gurable insuch a way that the designer determines the number ofDSPsfor this layer, and the hardware generator framework gener-ates the convolver state machine in a way that it uses onlythe speci\ufb01ed number of DSPs. Apparently, the more DSPswe allot to this hardware unit, the more throughput we get.The maximum number of DSPs is the input image pixels,and the minimum is one.Since we are targeting embedded systems, in our frame-work, this unit can be used by the designer of the CNN intwo modes:\u2022 Shared mode: in the this mode, the convolution unitis shared among all layers of CNN. Sharing this unitsresults in using less hardware resources of the FPGA,but it introduces throughput degradation. If this modeis chosen by the designer, a wrapper is generated au-tomatically by the hardware generator backend. Thewrapper acts as a resource manager for this unit by ar-bitrating different layers requesting to use convolutionunit.\u2022 Exclusive mode: in the exclusive mode, for each con-volution layer in the CNN a convolution unit hardwareis generated, which results in higher throughput andFPGA resource usage.3--- Page 4 ---Figure 2: FeedForward unit architecture.2.2.2 FeedForward unitThe input to convolution unit hardware is three lines of theinput fmap and the \ufb01lter that is going to be applied on thefmaps. Also, the output of the convolution unit hardware isone line that its size equal to the input fmap size and it goesthrough activation, max pooling, and precision adjustmentlayer to get to the next layer. Such inter-layer data shouldbe handled and stored in a hardware unit.FeedForward unit is responsible for handling accumula-tion of the input fmaps in an inter-layer RAM by a state ma-chine (SM), and output them to the convolution unit threeline at a time in conjunction with the \ufb01lter that should beapplied to the fmap. Figure 2 shows the structure of thishardware unit.As it is shown in Figure 2, one state machine (SM) isresponsible for buffering the feature maps ( fmap) that areinput to this layer. This state machine is generated for eachlayer according to the size and number of fmaps that areinputted to the layer. On the output side, another state ma-chine is responsible for feeding three lines of fmaps to theconvolution unit as well as the \ufb01lter that is going to appliedto that. The state machine is also generated speci\ufb01c to thelayer. Also, the RAM and ROM in Figure 2 are generatedaccording to the fmaps that are going to buffered in thislayer and number of that layer \ufb01lters. ROMs are initializedwith the CNN model (weights) provided by the Softwarebackend.2.2.3 Activation and Max pooling unitsIn our framework, we implemented a max pooling unit andan activation hardware unit. The activation unit performsa Recti\ufb01ed Linear ReLU function. The max pooling unitis con\ufb01gurable and is generated to perform M \xd7 M maxpooling in which M is speci\ufb01ed by the designer.2.2.4 Inter-layer precision adjustment unitAs we mentioned before, we are using \ufb01xed-point numbersand operations in our framework. Using \ufb01xed-point insteadof \ufb02oating-point introduced error to the CNN. This errorpropagates through the network and affects the \ufb01nal resultsadversely. Since the error propagated through the early lay-ers has more negative effects on the output anf the rangeof the data propagated through the network varies from onelayer to the other, we need to adjust the proportion of in-teger part and fractional part at the end of each layer ofthe network. For doing so, we inser an Inter-layer preci-sion adjustment unit at the end of each layer. This unit isresponsible for adjusting the integer and fractional part ofthe output data of each layer before propagating to the restof the network. The adjustment of this unit is done by theSimulation/precision adjustment backend.2.2.5 Dense or fully-connected (FC) unitDense or fully-connected is also implemented as a con\ufb01g-urable hardware unit. Since this unit mainly consists ofmultiplication and addition, it needs DSP resources. In ourframework, the number of DSPs and the size of the FC unitis adjustable according to the designer needs. The weightsfor this layer are stored in ROMs. So, they are generatedbased on the weights provided by Software backend andused in the synthesis phase of the generate hardware.2.3. Simulation and Precision Adjustment BackendIn order to adjust the \ufb01xed-point precision of the CNN,we need to propagate several test inputs to the CNN im-plemented in the hardware. The test points are generated bysoftware backend based on the user\u2019s input dataset for train-ing the CNN. The data consists of the input to the CNN andthe output of all layers that are produced for that image. Thesimulation backend uses Scala language testing libraries toinput the veri\ufb01cation data to the generated hardware. Thenall the data from the output of each layer of the generatedCNN hardware is collected, and the difference of the col-lected data and the data provided by the software backendare calculated. Then, this backend adjusts the bit-width ofthe Inter-layer precision adjustment unit of each layer basedto minimize the error generated by each layer.This sequence of propagating data, collecting data, cal-culating error, and adjusting the precision units is performeduntil the network merges to a minimum output error. At theend, the Simulation and precision adjustment backend ad-justs all integer and fractional parts of the layers based onthe veri\ufb01cation data that are provided by the Software back-end.3. Experimental ResultsWe evaluate our design on a PYNQ board [18] that is anopen-source project from Xilinx that makes it easy to designembedded systems withXilinx Zynq SoC. PYNQ uses a low-cost Xilinx Zynq-7000 SoC containing an XC7Z020 FPGAalongside an ARM Cortex-A9 embedded processor. On the4--- Page 5 ---Table 1: The CNN architecture generated by the framework.Layer type Output size Output fmaps Param #Conv (32, 32) 32 320Activation (32, 32) 32 0Max pool (16, 16) 32 0Conv (16, 16) 64 18496Activation (16, 16) 64 0Max pool (8, 8) 64 0Conv (8, 8) 128 73856Activation (8, 8) 128 0Max pool (4, 4) 128 0Conv (4, 4) 128 147584Activation (4, 4) 128 0Max pool (2, 2) 128 0Dense (1, 100) 51300Activation (1, 100) 0Dense (1, 10) 1010Activation (1, 10) 0Total 292,566software side of PYQN, using the Python language and li-braries, designers can exploit the bene\ufb01ts of programmablelogic and microprocessors in Zynq to build more capableand exciting embedded systems. We make use of XilinxSDSoC 2016.4 as the primary design tool, which leveragesVivado HLS and Vivado to perform the actual synthesis andprogramming the FPGA implementation. We used CIFAR-10 dataset to train and test the CNN. We converted the im-ages to grey to have one-channel input due to our a smallFPGA board.Using the software backend we designed and trained aCNN. Table 1 shows the architecture of the CNN we tar-geted for classifying CIFAR-10 dataset. We used3\xd73 \ufb01ltersfor convolutions. The reason behind choosing such a smallCNN is that we are targeting a small FPGA as our hardwareplatform and any larger CNN would not pass the checkingresource limitation phase of the Software backend. Also,we wanted to show the accuracy difference between the ac-celerated version on FPGA and the software version.In order to compare the software version and the ver-sion implemented on FPGA, we classi\ufb01ed CIFAR-10 testdataset for both software model on ARM Cortex-A9 whichis the embedded processor of our FPGA and on the Tiny-CNN accelerator with two modes. Table 2 shows the com-parison averaged results per image. SW is the Python-based software classi\ufb01er ran on ARM processor of the board(650MHz). HW-SM is the CNN accelerator generated inshared mode (sharing one convolution unit among all lay-ers), and HW-EM is the CNN accelerator generated in ex-clusive mode (one convolution for each layer).We see that the effect of using 16-bit \ufb01xed-point dataFigure 3: Hardware resource utilization of the Table 1 CNNimplemented on FPGA.Table 2: Software vs. TinyCNN accelerator.version Accuracy(%)Runtime(ms)Data typeSW 65.54 42.54 32-b \ufb02oatingHW-SM 62.28 8.12 16-b \ufb01xedHW-EM 62.28 2.7 16-b \ufb01xedon the precision of the classi\ufb01cation is negligible. We canalso see that using exclusive convolution hardware units im-proves the runtime by avoiding stalls due to access con-tention for using convolution layer by different layers.Figure 3 shows the resource usage of the CNN hardwareimplemented in PYNQ board. As we expected, memory(BRAM) is the hardware resource limitation that we en-counter while implementing a CNN on an embedded FPGA.In the software backend, the CNN architecture is designedin such a way that it utilized the FPGA BRAM to the high-est.4. ConclusionIn this paper, we proposed a framework that enables thedesigners to design a CNN accelerator for an embeddedFPGA fast. The framework provides a software API thatmakes designers capable of exploring the CNN design spaceconsidering the hardware resource limitations. Then, theCNN hardware is generated and tuned to have a low accu-racy loss by inter-layer precision adjustment. Our resultsshow that we can reach up to 15.75% speedup compared tothe software implementation with 16-bit \ufb01xed-point data.As future work, we are going to automate the softwareCNN design space exploration part in the software backendto make the CNN design even more easily. Since the onlysupported activation function is ReLU, we are going to addmore activation units to the framework.5--- Page 6 ---References[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cIm-agenet classi\ufb01cation with deep convolutional neuralnetworks,\u201d inAdvances in neural information process-ing systems, pp. 1097\u20131105, 2012.[2] K. Ovtcharov, O. Ruwase, J.-Y . Kim, J. Fowers,K. Strauss, and E. S. Chung, \u201cAccelerating deep con-volutional neural networks using specialized hard-ware,\u201dMicrosoft Research Whitepaper, vol. 2, no. 11,pp. 1\u20134, 2015.[3] A. Jahanshahi, M. K. Taram, and N. Eskandari,\u201cBlokus duo game on fpga,\u201d in The 17th CSI Interna-tional Symposium on Computer Architecture & Digi-tal Systems (CADS 2013), pp. 149\u2013152, IEEE, 2013.[4] C. Zhang, P. Li, G. Sun, Y . Guan, B. Xiao, andJ. Cong, \u201cOptimizing fpga-based accelerator designfor deep convolutional neural networks,\u201d in Proceed-ings of the 2015 ACM/SIGDA International Sympo-sium on Field-Programmable Gate Arrays , pp. 161\u2013170, ACM, 2015.[5] J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu,T. Tang, N. Xu, S. Song, et al., \u201cGoing deeper withembedded fpga platform for convolutional neural net-work,\u201d in Proceedings of the 2016 ACM/SIGDA In-ternational Symposium on Field-Programmable GateArrays, pp. 26\u201335, ACM, 2016.[6] N. Suda, V . Chandra, G. Dasika, A. Mohanty, Y . Ma,S. Vrudhula, J.-s. Seo, and Y . Cao, \u201cThroughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks,\u201d in Proceedingsof the 2016 ACM/SIGDA International Symposium onField-Programmable Gate Arrays, pp. 16\u201325, ACM,2016.[7] Y . Wang, J. Xu, Y . Han, H. Li, and X. Li, \u201cDeepburn-ing: automatic generation of fpga-based learning ac-celerators for the neural network family,\u201d in Proceed-ings of the 53rd Annual Design Automation Confer-ence, p. 110, ACM, 2016.[8] M. Courbariaux, Y . Bengio, and J.-P. David, \u201cBina-ryconnect: Training deep neural networks with binaryweights during propagations,\u201d in Advances in neuralinformation processing systems, pp. 3123\u20133131, 2015.[9] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv,and Y . Bengio, \u201cBinarized neural networks: Train-ing deep neural networks with weights and ac-tivations constrained to+ 1 or-1,\u201d arXiv preprintarXiv:1602.02830, 2016.[10] M. Rastegari, V . Ordonez, J. Redmon, and A. Farhadi,\u201cXnor-net: Imagenet classi\ufb01cation using binary con-volutional neural networks,\u201d in European Conferenceon Computer Vision, pp. 525\u2013542, Springer, 2016.[11] J. Cong, B. Liu, S. Neuendorffer, J. Noguera, K. Vis-sers, and Z. Zhang, \u201cHigh-level synthesis for fpgas:From prototyping to deployment,\u201d IEEE Transactionson Computer-Aided Design of Integrated Circuits andSystems, vol. 30, no. 4, pp. 473\u2013491, 2011.[12] A. Canis, J. Choi, M. Aldham, V . Zhang, A. Kam-moona, T. Czajkowski, S. D. Brown, and J. H. An-derson, \u201cLegup: An open-source high-level synthe-sis tool for fpga-based processor/accelerator systems,\u201dACM Transactions on Embedded Computing Systems(TECS), vol. 13, no. 2, p. 24, 2013.[13] T. S. Czajkowski, U. Aydonat, D. Denisenko, J. Free-man, M. Kinsner, D. Neto, J. Wong, P. Yiannacouras,and D. P. Singh, \u201cFrom opencl to high-performancehardware on fpgas,\u201d in 22nd international conferenceon \ufb01eld programmable logic and applications (FPL) ,pp. 531\u2013534, IEEE, 2012.[14] V . Kathail, J. Hwang, W. Sun, Y . Chobe, T. Shui, andJ. Carrillo, \u201cSdsoc: A higher-level programming envi-ronment for zynq soc and ultrascale+ mpsoc,\u201d in Pro-ceedings of the 2016 ACM/SIGDA International Sym-posium on Field-Programmable Gate Arrays, pp. 4\u20134,ACM, 2016.[15] R. Zhao, W. Song, W. Zhang, T. Xing, J.-H. Lin,M. Srivastava, R. Gupta, and Z. Zhang, \u201cAccel-erating Binarized Convolutional Neural Networkswith Software-Programmable FPGAs,\u201dInt\u2019l Symp. onField-Programmable Gate Arrays (FPGA), Feb 2017.[16] L. Cavigelli and L. Benini, \u201cOrigami: A 803-gop/s/wconvolutional network accelerator,\u201d IEEE Transac-tions on Circuits and Systems for Video Technology ,vol. 27, no. 11, pp. 2461\u20132475, 2016.[17] J. Bachrach, H. V o, B. Richards, Y . Lee, A. Water-man, R. Avi \u02c7zienis, J. Wawrzynek, and K. Asanovi \xb4c,\u201cChisel: constructing hardware in a scala embeddedlanguage,\u201d in DAC Design Automation Conference2012, pp. 1212\u20131221, IEEE, 2012.[18] \u201cPynq: Python productivity for zynq.\u201d http://www.pynq.io/. Accessed: 04/21/2019.6\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n1911.06777v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 23:
\n~~~~~~~~~~\nHistory: Step #4, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification NAS optimization, here was the response\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge\nSummary: Neural Architecture Search (NAS) is quickly becoming the go-to approach to\noptimize the structure of Deep Learning (DL) models for complex tasks such as\nImage Classification or Object Detection. However, many other relevant\napplications of DL, especially at the edge, are based on time-series processing\nand require models with unique features, for which NAS is less explored. This\nwork focuses in particular on Temporal Convolutional Networks (TCNs), a\nconvolutional model for time-series processing that has recently emerged as a\npromising alternative to more complex recurrent architectures. We propose the\nfirst NAS tool that explicitly targets the optimization of the most peculiar\narchitectural parameters of TCNs, namely dilation, receptive-field and number\nof features in each layer. The proposed approach searches for networks that\noffer good trade-offs between accuracy and number of parameters/operations,\nenabling an efficient deployment on embedded platforms. We test the proposed\nNAS on four real-world, edge-relevant tasks, involving audio and bio-signals.\nResults show that, starting from a single seed network, our method is capable\nof obtaining a rich collection of Pareto optimal architectures, among which we\nobtain models with the same accuracy as the seed, and 15.9-152x fewer\nparameters. Compared to three state-of-the-art NAS tools, ProxylessNAS,\nMorphNet and FBNetV2, our method explores a larger search space for TCNs (up to\n10^12x) and obtains superior solutions, while requiring low GPU memory and\nsearch time. We deploy our NAS outputs on two distinct edge devices, the\nmulticore GreenWaves Technology GAP8 IoT processor and the single-core\nSTMicroelectronics STM32H7 microcontroller. With respect to the\nstate-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5x\nand 3.8x on the two targets respectively, without any accuracy loss.\nPublication Date: 2023-01-24\narXiv paper ID: 2301.10281v1\n\nTitle: Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment\nSummary: As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.\nPublication Date: 2024-10-29\narXiv paper ID: 2411.00838v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #21, Phase: literature review, Feedback: 3\n--- Page 1 ---TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGAAli Jahanshahiajaha004@ucr.eduUniversity of California, RiversideAbstractIn recent years, Convolutional Neural Network (CNN)based methods have achieved great success in a large num-ber of applications and have been among the most power-ful and widely used techniques in computer vision. How-ever, CNN-based methods are computational-intensive andresource-consuming, and thus are hard to be integrated intoembedded systems such as smart phones, smart glasses, androbots. FPGA is one of the most promising platforms for ac-celerating CNN, but the limited on-chip memory size limitthe performance of FPGA accelerator for CNN.In this paper, we propose a framework for designingCNN accelerator on embedded FPGA for image classi\ufb01ca-tion. The proposed framework provides a tool for FPGAresource-aware design space exploration of CNNs and au-tomatically generates the hardware description of the CNNto be programmed on a target FPGA. The framework con-sists of three main backends; software, hardware gener-ation, and simulation/precision adjustment. The softwarebackend serves as an API to the designer to design the CNNand train it according to the hardware resources that areavailable. Using the CNN model, hardware backend gen-erates the necessary hardware components and integratesthem to generate the hardware description of the CNN. Fi-naly, Simulation/precision adjustment backend adjusts theinter-layer precision units to minimize the classi\ufb01cation er-ror.We used 16-bit \ufb01xed-point data in a CNN accelerator(FPGA) and compared it to the exactly similar software ver-sion running on an ARM processor (32-bit \ufb02oating pointdata). We encounter about 3% accuracy loss in classi\ufb01ca-tion of the accelerated (FPGA) version. In return, we gotup to 15.75\xd7 speedup by classifying with the acceleratedversion on the FPGA.1. IntroductionThe exponential growth of big data during the last decademotivates for innovative methods to extract high semanticinformation from raw sensor data such as videos, imagesand speech sequences. Among the proposed methods, Con-volutional Neural Networks (CNNs) have become the de-facto standard by delivering near-human accuracy in manyapplications related to machine vision. While CNNs havebeen known to researchers for decades, they were popu-larized after demonstrating high accuracy at the 2012 Ima-geNet recognition challenge [1]. Subsequently, CNNs havebecome the state-of-the-art for image classi\ufb01cation, detec-tion, and localization tasks. Research in CNNs and otherareas of deep learning continues at a rapid pace, with hun-dreds of new papers published each year introducing newmodels and techniques.The CNN\u2019s high performance (at classi\ufb01cation, detec-tion, and localization) comes at the price of a large com-putational cost as they require tens of GOP/s to classify asingle frame. Thus, one challenge to the widespread de-ployment of CNNs is their signi\ufb01cant demands for com-putation and storage capacity. Therefore, dedicated hard-ware is required to accelerate their execution. GraphicsProcessing Units (GPUs), are the most widely used plat-form to implement CNNs as they offer the best performancein terms of pure computational throughput, reaching up to11 TFLOP/s. Nevertheless, in terms of power consump-tion, Field-Programmable Gate Array (FPGA) solutions areknown to be more energy ef\ufb01cient (vs GPUs). Recent workby Microsoft has even explored cost-effective accelerationof deep learning on FPGAs at datacenter scale [2, 3]. Thereare also efforts in the academic community on FPGA-basedCNN accelerators [4,5] as well as tools for generating themautomatically [6, 7].We observe two trends which may help overcome im-plementing CNN on FPGAs. The \ufb01rst is a series of recentpapers in the machine learning community regarding very-low-precision CNNs. Networks with binary weights [8], orbinary weights and activations [9, 10] have in certain casesdemonstrated accuracy comparable to full precision nets.Such binarized neural net-works (BNNs) may be the keyto ef\ufb01cient deep learning on FPGA. Binarization reducesstorage and memory bandwidth requirements, and replaceFP operations with binary operations which can be very ef-\ufb01ciently performed on the LUT-based FPGA fabric. Con-1arXiv:1911.06777v1  [cs.LG]  15 Nov 2019--- Page 2 ---cerning the cost and effort of FPGA implementation,we seea steady improvement in FPGA design automation toolsover the past decade. High-level synthesis (HLS) toolssuch as Xilinx Vivado HLS [11] and LegUp [12] enable auser to write code in a high-level programming language,then algorithmically compile that code down to a register-transfer level (RTL) design speci\ufb01cation. More recent toolssuch as Intel FPGA SDK for OpenCL [13] and Xilinx SD-SoC [14] offer further automation features for generatingthe hardware-software interface and on-chip memory net-work. In the context of deep learning, these tools have thepotential to critically reduce time-to-market on new acceler-ator designs and thus reduce the aforementioned innovationgap.In this project, we aim to design and implement a CNNaccelerator on an embedded FPGA for image classi\ufb01cation.We use [15] as our reference that presents the design of aBNN accelerator for FPGAs. In contrast to our reference, inthis work, the aim is to provide designers a general frame-work to enable them design their CNNeasily and use it as anaccelerator on an FPGA. Thus, we do not use BNN-basedCNN in this project.In addition to generality of the framework, we also targetembedded systems that have hardware resource limitation.Compressing the CNN model is a good choice to addressthe resource limitations of the hardware. A straight forwardway to compress a network is to reduce the bit-width forcomputing. This utilizes the \ufb02exibility of FPGA or ASICdesign compared with GPU or CPU. It is proved to be aneffective way to use 16-bits \ufb01xed-point operations in [16]with small loss in accuracy. In our framework, we use 16-bits operation and we show that the accuracy loss is verysmall.This work proposes a general framework for design andimplementation of tiny modular CNN accelerators on em-bedded FPGAs. Our framework automatically generatesthe hardware code for the designed CNN and trained withthe provided software backend. The framework consistsof three main components that correspond to our contribu-tions. They are as follows:\u2022 Software backend: Using Python, we provide a toolfor designing and training a CNN architecture. Themodel (weights) of the trained network and otherCNN architecture parameters are used in the hardwareframework to generate the hardware description lan-guage of the CNN, which is going to be implementedon the FPGA. This back-end also provides the in-formation needed for Simulation/precision adjustmentbackend as well as checking the hardware resourcesfor the designed CNN model.\u2022 Hardware backend: Using CHISEL [17], we performautomatic generation and integration of different hard-ware components needed for the CNN architecture de-signed in the software backend. The CNN model ispassed to this back-end to be used in HDL generationof the CNN. The output of this backend is the HDLcode that is ready to be synthesized and programmedon an FPGA.\u2022 Simulation/precision adjustment backend: UsingScala testing libraries and the data passed to this back-end, inter-layer precision of the generated CNN hard-ware will be adjusted. The CNN output error intro-duced by varying integer and fractional part of eachlayer\u2019s data is minimized by this backend.The rest of this paper is organized as follows; we \ufb01rst de-scribe the framework overall structure, then we go in de-tails for each component of the framework. Finally, we pro-vide the results discussing different aspects of the generatedCNN for our target FPGA.2. Tiny CNNCNN is a machine learning classi\ufb01er that typically takesin an image and produces the probabilities of that imagebelonging to each output class. A typical CNN consists ofa pipeline of connected layers. Each layer takes as inputa set of feature maps ( fmaps), performs some computationon them, and produces a new set of fmaps to be fed intothe next layer. The input fmaps of the \ufb01rst layer is the in-put image. Layers may require con\ufb01guration values knownas parameters, which must \ufb01rst be determined by trainingthe CNN of\ufb02ine on pre-classi\ufb01ed data. Once the parame-ters are \ufb01nalized, the CNN can be deployed for inferencethe classi\ufb01cation of new data points. For most practical ma-chine learning applications, the \ufb01rst-class concerns are theaccuracy and execution time of online classi\ufb01cation. Thisproject will thus focus on accelerating the inference taskwithout compromising accuracy. The aim of this project isto provide a framework that automates implementation ofCNNs on an embedded FPGA for image classi\ufb01cation. Fig-ure 1 shows the framework we proposed for this purpose. Inthe following subsections, we are going to elaborate moreon each component of our framework.2.1. Software backendThe software backend is an API by which users are capa-ble of designing their CNN in the software level. The usercan tune their CNN with the train and test data sets, and\ufb01nally, export their model for further processings. We usedKeras library as the underlying deep learning framework ofthe software backend. Keras is a high-level neural networksAPI, written in Python and capable of running on top ofTensorFlow, CNTK, or Theano. It was developed with afocus on enabling fast experimentation. We address threechallenges of designing a CNN by Software backend:2--- Page 3 ---Figure 1: Tiny CNN framework.CNN designing, training, and model generation: In ourframework, using Keras API, we provide a templatePythonclass to the designers to design their CNN. Then, the de-signer\u2019s CNN is trained and the model is saved in order tobe used by the hardware backend. Essentially, the model isthe CNN weights that are going to be used as initial valuesof ROMs.Veri\ufb01cation data: The software backend also inputs somerandom data from test dataset to CNN and captures the out-put of all layers of CNN with respect to that input. Thecollected dataset is called veri\ufb01cation data and is used inhardware backend for precision adjustment and veri\ufb01cationpurposes.Hardware resource check: As mentioned before, we aretargeting embedded FPGA in this work. Thus, we want tosqueez the whole CNN model in FPGA BRAMs. Thus,one step for designing the CNN would be checking if theCNN model (weights) \ufb01t in FPGA BRAMs. Software back-end uses the target FPGA spec to check if the model \ufb01ts inFPGA or not. In case the model does not \ufb01t on the tar-get FPGA, the software backend throws an exception thatshows not enough space for the model. In such cases, thedesigner should play around the CNN hyper parameters andmake it smaller to \ufb01t on the FPGA. Fully-connected layers,due to their high number of parameters contributes to themost of the memory (BRAM) usage.2.2. Hardware backendHardware backend generates the hardware of the CNNwhich is designed in the software backend according tothe inputs that are provided to this backend. We imple-mented all basic hardware components that are needed forCNNs. The developed hardware components are modularand highly con\ufb01gurable. They can be con\ufb01gured based onthe CNN speci\ufb01cations including: data bit-width, shared orexclusive components, and number of DSPs available forcomponents. All of the implemented hardware componentsare modular that means we can simply attach them togetherwith few lines of code just like we do in designing CNNs insoftware.2.2.1 Convolution unitThis unit is the most critical component of the accelera-tor, as it will take up the vast majority of the runtime. Theunit must maintain high throughput and resource ef\ufb01ciency.Convolution operation has multiplication and addition in itscore. Since multiplication is a very expensive operation,we used the built-in DSPs on FPGA board that are spe-cialized for digital signal processing purposing \u2013 mainly in-clude multiplication and addition.The challenge with using the FPGA DSPs is that thereare a limited number of them on every FPGA board. Theimplemented Convolution unit hardware is con\ufb01gurable insuch a way that the designer determines the number ofDSPsfor this layer, and the hardware generator framework gener-ates the convolver state machine in a way that it uses onlythe speci\ufb01ed number of DSPs. Apparently, the more DSPswe allot to this hardware unit, the more throughput we get.The maximum number of DSPs is the input image pixels,and the minimum is one.Since we are targeting embedded systems, in our frame-work, this unit can be used by the designer of the CNN intwo modes:\u2022 Shared mode: in the this mode, the convolution unitis shared among all layers of CNN. Sharing this unitsresults in using less hardware resources of the FPGA,but it introduces throughput degradation. If this modeis chosen by the designer, a wrapper is generated au-tomatically by the hardware generator backend. Thewrapper acts as a resource manager for this unit by ar-bitrating different layers requesting to use convolutionunit.\u2022 Exclusive mode: in the exclusive mode, for each con-volution layer in the CNN a convolution unit hardwareis generated, which results in higher throughput andFPGA resource usage.3--- Page 4 ---Figure 2: FeedForward unit architecture.2.2.2 FeedForward unitThe input to convolution unit hardware is three lines of theinput fmap and the \ufb01lter that is going to be applied on thefmaps. Also, the output of the convolution unit hardware isone line that its size equal to the input fmap size and it goesthrough activation, max pooling, and precision adjustmentlayer to get to the next layer. Such inter-layer data shouldbe handled and stored in a hardware unit.FeedForward unit is responsible for handling accumula-tion of the input fmaps in an inter-layer RAM by a state ma-chine (SM), and output them to the convolution unit threeline at a time in conjunction with the \ufb01lter that should beapplied to the fmap. Figure 2 shows the structure of thishardware unit.As it is shown in Figure 2, one state machine (SM) isresponsible for buffering the feature maps ( fmap) that areinput to this layer. This state machine is generated for eachlayer according to the size and number of fmaps that areinputted to the layer. On the output side, another state ma-chine is responsible for feeding three lines of fmaps to theconvolution unit as well as the \ufb01lter that is going to appliedto that. The state machine is also generated speci\ufb01c to thelayer. Also, the RAM and ROM in Figure 2 are generatedaccording to the fmaps that are going to buffered in thislayer and number of that layer \ufb01lters. ROMs are initializedwith the CNN model (weights) provided by the Softwarebackend.2.2.3 Activation and Max pooling unitsIn our framework, we implemented a max pooling unit andan activation hardware unit. The activation unit performsa Recti\ufb01ed Linear ReLU function. The max pooling unitis con\ufb01gurable and is generated to perform M \xd7 M maxpooling in which M is speci\ufb01ed by the designer.2.2.4 Inter-layer precision adjustment unitAs we mentioned before, we are using \ufb01xed-point numbersand operations in our framework. Using \ufb01xed-point insteadof \ufb02oating-point introduced error to the CNN. This errorpropagates through the network and affects the \ufb01nal resultsadversely. Since the error propagated through the early lay-ers has more negative effects on the output anf the rangeof the data propagated through the network varies from onelayer to the other, we need to adjust the proportion of in-teger part and fractional part at the end of each layer ofthe network. For doing so, we inser an Inter-layer preci-sion adjustment unit at the end of each layer. This unit isresponsible for adjusting the integer and fractional part ofthe output data of each layer before propagating to the restof the network. The adjustment of this unit is done by theSimulation/precision adjustment backend.2.2.5 Dense or fully-connected (FC) unitDense or fully-connected is also implemented as a con\ufb01g-urable hardware unit. Since this unit mainly consists ofmultiplication and addition, it needs DSP resources. In ourframework, the number of DSPs and the size of the FC unitis adjustable according to the designer needs. The weightsfor this layer are stored in ROMs. So, they are generatedbased on the weights provided by Software backend andused in the synthesis phase of the generate hardware.2.3. Simulation and Precision Adjustment BackendIn order to adjust the \ufb01xed-point precision of the CNN,we need to propagate several test inputs to the CNN im-plemented in the hardware. The test points are generated bysoftware backend based on the user\u2019s input dataset for train-ing the CNN. The data consists of the input to the CNN andthe output of all layers that are produced for that image. Thesimulation backend uses Scala language testing libraries toinput the veri\ufb01cation data to the generated hardware. Thenall the data from the output of each layer of the generatedCNN hardware is collected, and the difference of the col-lected data and the data provided by the software backendare calculated. Then, this backend adjusts the bit-width ofthe Inter-layer precision adjustment unit of each layer basedto minimize the error generated by each layer.This sequence of propagating data, collecting data, cal-culating error, and adjusting the precision units is performeduntil the network merges to a minimum output error. At theend, the Simulation and precision adjustment backend ad-justs all integer and fractional parts of the layers based onthe veri\ufb01cation data that are provided by the Software back-end.3. Experimental ResultsWe evaluate our design on a PYNQ board [18] that is anopen-source project from Xilinx that makes it easy to designembedded systems withXilinx Zynq SoC. PYNQ uses a low-cost Xilinx Zynq-7000 SoC containing an XC7Z020 FPGAalongside an ARM Cortex-A9 embedded processor. On the4--- Page 5 ---Table 1: The CNN architecture generated by the framework.Layer type Output size Output fmaps Param #Conv (32, 32) 32 320Activation (32, 32) 32 0Max pool (16, 16) 32 0Conv (16, 16) 64 18496Activation (16, 16) 64 0Max pool (8, 8) 64 0Conv (8, 8) 128 73856Activation (8, 8) 128 0Max pool (4, 4) 128 0Conv (4, 4) 128 147584Activation (4, 4) 128 0Max pool (2, 2) 128 0Dense (1, 100) 51300Activation (1, 100) 0Dense (1, 10) 1010Activation (1, 10) 0Total 292,566software side of PYQN, using the Python language and li-braries, designers can exploit the bene\ufb01ts of programmablelogic and microprocessors in Zynq to build more capableand exciting embedded systems. We make use of XilinxSDSoC 2016.4 as the primary design tool, which leveragesVivado HLS and Vivado to perform the actual synthesis andprogramming the FPGA implementation. We used CIFAR-10 dataset to train and test the CNN. We converted the im-ages to grey to have one-channel input due to our a smallFPGA board.Using the software backend we designed and trained aCNN. Table 1 shows the architecture of the CNN we tar-geted for classifying CIFAR-10 dataset. We used3\xd73 \ufb01ltersfor convolutions. The reason behind choosing such a smallCNN is that we are targeting a small FPGA as our hardwareplatform and any larger CNN would not pass the checkingresource limitation phase of the Software backend. Also,we wanted to show the accuracy difference between the ac-celerated version on FPGA and the software version.In order to compare the software version and the ver-sion implemented on FPGA, we classi\ufb01ed CIFAR-10 testdataset for both software model on ARM Cortex-A9 whichis the embedded processor of our FPGA and on the Tiny-CNN accelerator with two modes. Table 2 shows the com-parison averaged results per image. SW is the Python-based software classi\ufb01er ran on ARM processor of the board(650MHz). HW-SM is the CNN accelerator generated inshared mode (sharing one convolution unit among all lay-ers), and HW-EM is the CNN accelerator generated in ex-clusive mode (one convolution for each layer).We see that the effect of using 16-bit \ufb01xed-point dataFigure 3: Hardware resource utilization of the Table 1 CNNimplemented on FPGA.Table 2: Software vs. TinyCNN accelerator.version Accuracy(%)Runtime(ms)Data typeSW 65.54 42.54 32-b \ufb02oatingHW-SM 62.28 8.12 16-b \ufb01xedHW-EM 62.28 2.7 16-b \ufb01xedon the precision of the classi\ufb01cation is negligible. We canalso see that using exclusive convolution hardware units im-proves the runtime by avoiding stalls due to access con-tention for using convolution layer by different layers.Figure 3 shows the resource usage of the CNN hardwareimplemented in PYNQ board. As we expected, memory(BRAM) is the hardware resource limitation that we en-counter while implementing a CNN on an embedded FPGA.In the software backend, the CNN architecture is designedin such a way that it utilized the FPGA BRAM to the high-est.4. ConclusionIn this paper, we proposed a framework that enables thedesigners to design a CNN accelerator for an embeddedFPGA fast. The framework provides a software API thatmakes designers capable of exploring the CNN design spaceconsidering the hardware resource limitations. Then, theCNN hardware is generated and tuned to have a low accu-racy loss by inter-layer precision adjustment. Our resultsshow that we can reach up to 15.75% speedup compared tothe software implementation with 16-bit \ufb01xed-point data.As future work, we are going to automate the softwareCNN design space exploration part in the software backendto make the CNN design even more easily. Since the onlysupported activation function is ReLU, we are going to addmore activation units to the framework.5--- Page 6 ---References[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cIm-agenet classi\ufb01cation with deep convolutional neuralnetworks,\u201d inAdvances in neural information process-ing systems, pp. 1097\u20131105, 2012.[2] K. Ovtcharov, O. Ruwase, J.-Y . Kim, J. Fowers,K. Strauss, and E. S. Chung, \u201cAccelerating deep con-volutional neural networks using specialized hard-ware,\u201dMicrosoft Research Whitepaper, vol. 2, no. 11,pp. 1\u20134, 2015.[3] A. Jahanshahi, M. K. Taram, and N. Eskandari,\u201cBlokus duo game on fpga,\u201d in The 17th CSI Interna-tional Symposium on Computer Architecture & Digi-tal Systems (CADS 2013), pp. 149\u2013152, IEEE, 2013.[4] C. Zhang, P. Li, G. Sun, Y . Guan, B. Xiao, andJ. Cong, \u201cOptimizing fpga-based accelerator designfor deep convolutional neural networks,\u201d in Proceed-ings of the 2015 ACM/SIGDA International Sympo-sium on Field-Programmable Gate Arrays , pp. 161\u2013170, ACM, 2015.[5] J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu,T. Tang, N. Xu, S. Song, et al., \u201cGoing deeper withembedded fpga platform for convolutional neural net-work,\u201d in Proceedings of the 2016 ACM/SIGDA In-ternational Symposium on Field-Programmable GateArrays, pp. 26\u201335, ACM, 2016.[6] N. Suda, V . Chandra, G. Dasika, A. Mohanty, Y . Ma,S. Vrudhula, J.-s. Seo, and Y . Cao, \u201cThroughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks,\u201d in Proceedingsof the 2016 ACM/SIGDA International Symposium onField-Programmable Gate Arrays, pp. 16\u201325, ACM,2016.[7] Y . Wang, J. Xu, Y . Han, H. Li, and X. Li, \u201cDeepburn-ing: automatic generation of fpga-based learning ac-celerators for the neural network family,\u201d in Proceed-ings of the 53rd Annual Design Automation Confer-ence, p. 110, ACM, 2016.[8] M. Courbariaux, Y . Bengio, and J.-P. David, \u201cBina-ryconnect: Training deep neural networks with binaryweights during propagations,\u201d in Advances in neuralinformation processing systems, pp. 3123\u20133131, 2015.[9] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv,and Y . Bengio, \u201cBinarized neural networks: Train-ing deep neural networks with weights and ac-tivations constrained to+ 1 or-1,\u201d arXiv preprintarXiv:1602.02830, 2016.[10] M. Rastegari, V . Ordonez, J. Redmon, and A. Farhadi,\u201cXnor-net: Imagenet classi\ufb01cation using binary con-volutional neural networks,\u201d in European Conferenceon Computer Vision, pp. 525\u2013542, Springer, 2016.[11] J. Cong, B. Liu, S. Neuendorffer, J. Noguera, K. Vis-sers, and Z. Zhang, \u201cHigh-level synthesis for fpgas:From prototyping to deployment,\u201d IEEE Transactionson Computer-Aided Design of Integrated Circuits andSystems, vol. 30, no. 4, pp. 473\u2013491, 2011.[12] A. Canis, J. Choi, M. Aldham, V . Zhang, A. Kam-moona, T. Czajkowski, S. D. Brown, and J. H. An-derson, \u201cLegup: An open-source high-level synthe-sis tool for fpga-based processor/accelerator systems,\u201dACM Transactions on Embedded Computing Systems(TECS), vol. 13, no. 2, p. 24, 2013.[13] T. S. Czajkowski, U. Aydonat, D. Denisenko, J. Free-man, M. Kinsner, D. Neto, J. Wong, P. Yiannacouras,and D. P. Singh, \u201cFrom opencl to high-performancehardware on fpgas,\u201d in 22nd international conferenceon \ufb01eld programmable logic and applications (FPL) ,pp. 531\u2013534, IEEE, 2012.[14] V . Kathail, J. Hwang, W. Sun, Y . Chobe, T. Shui, andJ. Carrillo, \u201cSdsoc: A higher-level programming envi-ronment for zynq soc and ultrascale+ mpsoc,\u201d in Pro-ceedings of the 2016 ACM/SIGDA International Sym-posium on Field-Programmable Gate Arrays, pp. 4\u20134,ACM, 2016.[15] R. Zhao, W. Song, W. Zhang, T. Xing, J.-H. Lin,M. Srivastava, R. Gupta, and Z. Zhang, \u201cAccel-erating Binarized Convolutional Neural Networkswith Software-Programmable FPGAs,\u201dInt\u2019l Symp. onField-Programmable Gate Arrays (FPGA), Feb 2017.[16] L. Cavigelli and L. Benini, \u201cOrigami: A 803-gop/s/wconvolutional network accelerator,\u201d IEEE Transac-tions on Circuits and Systems for Video Technology ,vol. 27, no. 11, pp. 2461\u20132475, 2016.[17] J. Bachrach, H. V o, B. Richards, Y . Lee, A. Water-man, R. Avi \u02c7zienis, J. Wawrzynek, and K. Asanovi \xb4c,\u201cChisel: constructing hardware in a scala embeddedlanguage,\u201d in DAC Design Automation Conference2012, pp. 1212\u20131221, IEEE, 2012.[18] \u201cPynq: Python productivity for zynq.\u201d http://www.pynq.io/. Accessed: 04/21/2019.6, Your response: ```ADD_PAPER\n1911.06777v1\nThis paper presents TinyCNN, a framework for designing CNN accelerators on embedded FPGAs for image classification. The framework provides automated hardware generation from a software-designed CNN, with three key components: (1) a Python-based software backend for CNN design/training, (2) a CHISEL-based hardware generator, and (3) a precision adjustment backend. The system uses 16-bit fixed-point data, achieving only 3% accuracy loss compared to 32-bit floating-point while providing up to 15.75x speedup on a Xilinx Zynq-7000 FPGA. The framework addresses resource constraints through configurable convolution units (shared or exclusive modes), inter-layer precision adjustment, and automatic hardware resource checking. Experimental results on CIFAR-10 show the FPGA implementation achieves 62.28% accuracy (vs 65.54% software) with 2.7ms inference time (vs 42.54ms on ARM CPU). The work demonstrates effective FPGA acceleration for CNNs while maintaining accuracy through precision optimization techniques.\n```\n~~~~~~~~~~\nCurrent Step #22, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 1911.06777v1\nNotes: \nYour previous command was: ```ADD_PAPER\n1911.06777v1\nThis paper presents TinyCNN, a framework for designing CNN accelerators on embedded FPGAs for image classification. The framework provides automated hardware generation from a software-designed CNN, with three key components: (1) a Python-based software backend for CNN design/training, (2) a CHISEL-based hardware generator, and (3) a precision adjustment backend. The system uses 16-bit fixed-point data, achieving only 3% accuracy loss compared to 32-bit floating-point while providing up to 15.75x speedup on a Xilinx Zynq-7000 FPGA. The framework addresses resource constraints through configurable convolution units (shared or exclusive modes), inter-layer precision adjustment, and automatic hardware resource checking. Experimental results on CIFAR-10 show the FPGA implementation achieves 62.28% accuracy (vs 65.54% software) with 2.7ms inference time (vs 42.54ms on ARM CPU). The work demonstrates effective FPGA acceleration for CNNs while maintaining accuracy through precision optimization techniques.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 24:
\n~~~~~~~~~~\nHistory: Step #6, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge device image classification latency optimization\n```\nStep #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #21, Phase: literature review, Feedback: 3\n--- Page 1 ---TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGAAli Jahanshahiajaha004@ucr.eduUniversity of California, RiversideAbstractIn recent years, Convolutional Neural Network (CNN)based methods have achieved great success in a large num-ber of applications and have been among the most power-ful and widely used techniques in computer vision. How-ever, CNN-based methods are computational-intensive andresource-consuming, and thus are hard to be integrated intoembedded systems such as smart phones, smart glasses, androbots. FPGA is one of the most promising platforms for ac-celerating CNN, but the limited on-chip memory size limitthe performance of FPGA accelerator for CNN.In this paper, we propose a framework for designingCNN accelerator on embedded FPGA for image classi\ufb01ca-tion. The proposed framework provides a tool for FPGAresource-aware design space exploration of CNNs and au-tomatically generates the hardware description of the CNNto be programmed on a target FPGA. The framework con-sists of three main backends; software, hardware gener-ation, and simulation/precision adjustment. The softwarebackend serves as an API to the designer to design the CNNand train it according to the hardware resources that areavailable. Using the CNN model, hardware backend gen-erates the necessary hardware components and integratesthem to generate the hardware description of the CNN. Fi-naly, Simulation/precision adjustment backend adjusts theinter-layer precision units to minimize the classi\ufb01cation er-ror.We used 16-bit \ufb01xed-point data in a CNN accelerator(FPGA) and compared it to the exactly similar software ver-sion running on an ARM processor (32-bit \ufb02oating pointdata). We encounter about 3% accuracy loss in classi\ufb01ca-tion of the accelerated (FPGA) version. In return, we gotup to 15.75\xd7 speedup by classifying with the acceleratedversion on the FPGA.1. IntroductionThe exponential growth of big data during the last decademotivates for innovative methods to extract high semanticinformation from raw sensor data such as videos, imagesand speech sequences. Among the proposed methods, Con-volutional Neural Networks (CNNs) have become the de-facto standard by delivering near-human accuracy in manyapplications related to machine vision. While CNNs havebeen known to researchers for decades, they were popu-larized after demonstrating high accuracy at the 2012 Ima-geNet recognition challenge [1]. Subsequently, CNNs havebecome the state-of-the-art for image classi\ufb01cation, detec-tion, and localization tasks. Research in CNNs and otherareas of deep learning continues at a rapid pace, with hun-dreds of new papers published each year introducing newmodels and techniques.The CNN\u2019s high performance (at classi\ufb01cation, detec-tion, and localization) comes at the price of a large com-putational cost as they require tens of GOP/s to classify asingle frame. Thus, one challenge to the widespread de-ployment of CNNs is their signi\ufb01cant demands for com-putation and storage capacity. Therefore, dedicated hard-ware is required to accelerate their execution. GraphicsProcessing Units (GPUs), are the most widely used plat-form to implement CNNs as they offer the best performancein terms of pure computational throughput, reaching up to11 TFLOP/s. Nevertheless, in terms of power consump-tion, Field-Programmable Gate Array (FPGA) solutions areknown to be more energy ef\ufb01cient (vs GPUs). Recent workby Microsoft has even explored cost-effective accelerationof deep learning on FPGAs at datacenter scale [2, 3]. Thereare also efforts in the academic community on FPGA-basedCNN accelerators [4,5] as well as tools for generating themautomatically [6, 7].We observe two trends which may help overcome im-plementing CNN on FPGAs. The \ufb01rst is a series of recentpapers in the machine learning community regarding very-low-precision CNNs. Networks with binary weights [8], orbinary weights and activations [9, 10] have in certain casesdemonstrated accuracy comparable to full precision nets.Such binarized neural net-works (BNNs) may be the keyto ef\ufb01cient deep learning on FPGA. Binarization reducesstorage and memory bandwidth requirements, and replaceFP operations with binary operations which can be very ef-\ufb01ciently performed on the LUT-based FPGA fabric. Con-1arXiv:1911.06777v1  [cs.LG]  15 Nov 2019--- Page 2 ---cerning the cost and effort of FPGA implementation,we seea steady improvement in FPGA design automation toolsover the past decade. High-level synthesis (HLS) toolssuch as Xilinx Vivado HLS [11] and LegUp [12] enable auser to write code in a high-level programming language,then algorithmically compile that code down to a register-transfer level (RTL) design speci\ufb01cation. More recent toolssuch as Intel FPGA SDK for OpenCL [13] and Xilinx SD-SoC [14] offer further automation features for generatingthe hardware-software interface and on-chip memory net-work. In the context of deep learning, these tools have thepotential to critically reduce time-to-market on new acceler-ator designs and thus reduce the aforementioned innovationgap.In this project, we aim to design and implement a CNNaccelerator on an embedded FPGA for image classi\ufb01cation.We use [15] as our reference that presents the design of aBNN accelerator for FPGAs. In contrast to our reference, inthis work, the aim is to provide designers a general frame-work to enable them design their CNNeasily and use it as anaccelerator on an FPGA. Thus, we do not use BNN-basedCNN in this project.In addition to generality of the framework, we also targetembedded systems that have hardware resource limitation.Compressing the CNN model is a good choice to addressthe resource limitations of the hardware. A straight forwardway to compress a network is to reduce the bit-width forcomputing. This utilizes the \ufb02exibility of FPGA or ASICdesign compared with GPU or CPU. It is proved to be aneffective way to use 16-bits \ufb01xed-point operations in [16]with small loss in accuracy. In our framework, we use 16-bits operation and we show that the accuracy loss is verysmall.This work proposes a general framework for design andimplementation of tiny modular CNN accelerators on em-bedded FPGAs. Our framework automatically generatesthe hardware code for the designed CNN and trained withthe provided software backend. The framework consistsof three main components that correspond to our contribu-tions. They are as follows:\u2022 Software backend: Using Python, we provide a toolfor designing and training a CNN architecture. Themodel (weights) of the trained network and otherCNN architecture parameters are used in the hardwareframework to generate the hardware description lan-guage of the CNN, which is going to be implementedon the FPGA. This back-end also provides the in-formation needed for Simulation/precision adjustmentbackend as well as checking the hardware resourcesfor the designed CNN model.\u2022 Hardware backend: Using CHISEL [17], we performautomatic generation and integration of different hard-ware components needed for the CNN architecture de-signed in the software backend. The CNN model ispassed to this back-end to be used in HDL generationof the CNN. The output of this backend is the HDLcode that is ready to be synthesized and programmedon an FPGA.\u2022 Simulation/precision adjustment backend: UsingScala testing libraries and the data passed to this back-end, inter-layer precision of the generated CNN hard-ware will be adjusted. The CNN output error intro-duced by varying integer and fractional part of eachlayer\u2019s data is minimized by this backend.The rest of this paper is organized as follows; we \ufb01rst de-scribe the framework overall structure, then we go in de-tails for each component of the framework. Finally, we pro-vide the results discussing different aspects of the generatedCNN for our target FPGA.2. Tiny CNNCNN is a machine learning classi\ufb01er that typically takesin an image and produces the probabilities of that imagebelonging to each output class. A typical CNN consists ofa pipeline of connected layers. Each layer takes as inputa set of feature maps ( fmaps), performs some computationon them, and produces a new set of fmaps to be fed intothe next layer. The input fmaps of the \ufb01rst layer is the in-put image. Layers may require con\ufb01guration values knownas parameters, which must \ufb01rst be determined by trainingthe CNN of\ufb02ine on pre-classi\ufb01ed data. Once the parame-ters are \ufb01nalized, the CNN can be deployed for inferencethe classi\ufb01cation of new data points. For most practical ma-chine learning applications, the \ufb01rst-class concerns are theaccuracy and execution time of online classi\ufb01cation. Thisproject will thus focus on accelerating the inference taskwithout compromising accuracy. The aim of this project isto provide a framework that automates implementation ofCNNs on an embedded FPGA for image classi\ufb01cation. Fig-ure 1 shows the framework we proposed for this purpose. Inthe following subsections, we are going to elaborate moreon each component of our framework.2.1. Software backendThe software backend is an API by which users are capa-ble of designing their CNN in the software level. The usercan tune their CNN with the train and test data sets, and\ufb01nally, export their model for further processings. We usedKeras library as the underlying deep learning framework ofthe software backend. Keras is a high-level neural networksAPI, written in Python and capable of running on top ofTensorFlow, CNTK, or Theano. It was developed with afocus on enabling fast experimentation. We address threechallenges of designing a CNN by Software backend:2--- Page 3 ---Figure 1: Tiny CNN framework.CNN designing, training, and model generation: In ourframework, using Keras API, we provide a templatePythonclass to the designers to design their CNN. Then, the de-signer\u2019s CNN is trained and the model is saved in order tobe used by the hardware backend. Essentially, the model isthe CNN weights that are going to be used as initial valuesof ROMs.Veri\ufb01cation data: The software backend also inputs somerandom data from test dataset to CNN and captures the out-put of all layers of CNN with respect to that input. Thecollected dataset is called veri\ufb01cation data and is used inhardware backend for precision adjustment and veri\ufb01cationpurposes.Hardware resource check: As mentioned before, we aretargeting embedded FPGA in this work. Thus, we want tosqueez the whole CNN model in FPGA BRAMs. Thus,one step for designing the CNN would be checking if theCNN model (weights) \ufb01t in FPGA BRAMs. Software back-end uses the target FPGA spec to check if the model \ufb01ts inFPGA or not. In case the model does not \ufb01t on the tar-get FPGA, the software backend throws an exception thatshows not enough space for the model. In such cases, thedesigner should play around the CNN hyper parameters andmake it smaller to \ufb01t on the FPGA. Fully-connected layers,due to their high number of parameters contributes to themost of the memory (BRAM) usage.2.2. Hardware backendHardware backend generates the hardware of the CNNwhich is designed in the software backend according tothe inputs that are provided to this backend. We imple-mented all basic hardware components that are needed forCNNs. The developed hardware components are modularand highly con\ufb01gurable. They can be con\ufb01gured based onthe CNN speci\ufb01cations including: data bit-width, shared orexclusive components, and number of DSPs available forcomponents. All of the implemented hardware componentsare modular that means we can simply attach them togetherwith few lines of code just like we do in designing CNNs insoftware.2.2.1 Convolution unitThis unit is the most critical component of the accelera-tor, as it will take up the vast majority of the runtime. Theunit must maintain high throughput and resource ef\ufb01ciency.Convolution operation has multiplication and addition in itscore. Since multiplication is a very expensive operation,we used the built-in DSPs on FPGA board that are spe-cialized for digital signal processing purposing \u2013 mainly in-clude multiplication and addition.The challenge with using the FPGA DSPs is that thereare a limited number of them on every FPGA board. Theimplemented Convolution unit hardware is con\ufb01gurable insuch a way that the designer determines the number ofDSPsfor this layer, and the hardware generator framework gener-ates the convolver state machine in a way that it uses onlythe speci\ufb01ed number of DSPs. Apparently, the more DSPswe allot to this hardware unit, the more throughput we get.The maximum number of DSPs is the input image pixels,and the minimum is one.Since we are targeting embedded systems, in our frame-work, this unit can be used by the designer of the CNN intwo modes:\u2022 Shared mode: in the this mode, the convolution unitis shared among all layers of CNN. Sharing this unitsresults in using less hardware resources of the FPGA,but it introduces throughput degradation. If this modeis chosen by the designer, a wrapper is generated au-tomatically by the hardware generator backend. Thewrapper acts as a resource manager for this unit by ar-bitrating different layers requesting to use convolutionunit.\u2022 Exclusive mode: in the exclusive mode, for each con-volution layer in the CNN a convolution unit hardwareis generated, which results in higher throughput andFPGA resource usage.3--- Page 4 ---Figure 2: FeedForward unit architecture.2.2.2 FeedForward unitThe input to convolution unit hardware is three lines of theinput fmap and the \ufb01lter that is going to be applied on thefmaps. Also, the output of the convolution unit hardware isone line that its size equal to the input fmap size and it goesthrough activation, max pooling, and precision adjustmentlayer to get to the next layer. Such inter-layer data shouldbe handled and stored in a hardware unit.FeedForward unit is responsible for handling accumula-tion of the input fmaps in an inter-layer RAM by a state ma-chine (SM), and output them to the convolution unit threeline at a time in conjunction with the \ufb01lter that should beapplied to the fmap. Figure 2 shows the structure of thishardware unit.As it is shown in Figure 2, one state machine (SM) isresponsible for buffering the feature maps ( fmap) that areinput to this layer. This state machine is generated for eachlayer according to the size and number of fmaps that areinputted to the layer. On the output side, another state ma-chine is responsible for feeding three lines of fmaps to theconvolution unit as well as the \ufb01lter that is going to appliedto that. The state machine is also generated speci\ufb01c to thelayer. Also, the RAM and ROM in Figure 2 are generatedaccording to the fmaps that are going to buffered in thislayer and number of that layer \ufb01lters. ROMs are initializedwith the CNN model (weights) provided by the Softwarebackend.2.2.3 Activation and Max pooling unitsIn our framework, we implemented a max pooling unit andan activation hardware unit. The activation unit performsa Recti\ufb01ed Linear ReLU function. The max pooling unitis con\ufb01gurable and is generated to perform M \xd7 M maxpooling in which M is speci\ufb01ed by the designer.2.2.4 Inter-layer precision adjustment unitAs we mentioned before, we are using \ufb01xed-point numbersand operations in our framework. Using \ufb01xed-point insteadof \ufb02oating-point introduced error to the CNN. This errorpropagates through the network and affects the \ufb01nal resultsadversely. Since the error propagated through the early lay-ers has more negative effects on the output anf the rangeof the data propagated through the network varies from onelayer to the other, we need to adjust the proportion of in-teger part and fractional part at the end of each layer ofthe network. For doing so, we inser an Inter-layer preci-sion adjustment unit at the end of each layer. This unit isresponsible for adjusting the integer and fractional part ofthe output data of each layer before propagating to the restof the network. The adjustment of this unit is done by theSimulation/precision adjustment backend.2.2.5 Dense or fully-connected (FC) unitDense or fully-connected is also implemented as a con\ufb01g-urable hardware unit. Since this unit mainly consists ofmultiplication and addition, it needs DSP resources. In ourframework, the number of DSPs and the size of the FC unitis adjustable according to the designer needs. The weightsfor this layer are stored in ROMs. So, they are generatedbased on the weights provided by Software backend andused in the synthesis phase of the generate hardware.2.3. Simulation and Precision Adjustment BackendIn order to adjust the \ufb01xed-point precision of the CNN,we need to propagate several test inputs to the CNN im-plemented in the hardware. The test points are generated bysoftware backend based on the user\u2019s input dataset for train-ing the CNN. The data consists of the input to the CNN andthe output of all layers that are produced for that image. Thesimulation backend uses Scala language testing libraries toinput the veri\ufb01cation data to the generated hardware. Thenall the data from the output of each layer of the generatedCNN hardware is collected, and the difference of the col-lected data and the data provided by the software backendare calculated. Then, this backend adjusts the bit-width ofthe Inter-layer precision adjustment unit of each layer basedto minimize the error generated by each layer.This sequence of propagating data, collecting data, cal-culating error, and adjusting the precision units is performeduntil the network merges to a minimum output error. At theend, the Simulation and precision adjustment backend ad-justs all integer and fractional parts of the layers based onthe veri\ufb01cation data that are provided by the Software back-end.3. Experimental ResultsWe evaluate our design on a PYNQ board [18] that is anopen-source project from Xilinx that makes it easy to designembedded systems withXilinx Zynq SoC. PYNQ uses a low-cost Xilinx Zynq-7000 SoC containing an XC7Z020 FPGAalongside an ARM Cortex-A9 embedded processor. On the4--- Page 5 ---Table 1: The CNN architecture generated by the framework.Layer type Output size Output fmaps Param #Conv (32, 32) 32 320Activation (32, 32) 32 0Max pool (16, 16) 32 0Conv (16, 16) 64 18496Activation (16, 16) 64 0Max pool (8, 8) 64 0Conv (8, 8) 128 73856Activation (8, 8) 128 0Max pool (4, 4) 128 0Conv (4, 4) 128 147584Activation (4, 4) 128 0Max pool (2, 2) 128 0Dense (1, 100) 51300Activation (1, 100) 0Dense (1, 10) 1010Activation (1, 10) 0Total 292,566software side of PYQN, using the Python language and li-braries, designers can exploit the bene\ufb01ts of programmablelogic and microprocessors in Zynq to build more capableand exciting embedded systems. We make use of XilinxSDSoC 2016.4 as the primary design tool, which leveragesVivado HLS and Vivado to perform the actual synthesis andprogramming the FPGA implementation. We used CIFAR-10 dataset to train and test the CNN. We converted the im-ages to grey to have one-channel input due to our a smallFPGA board.Using the software backend we designed and trained aCNN. Table 1 shows the architecture of the CNN we tar-geted for classifying CIFAR-10 dataset. We used3\xd73 \ufb01ltersfor convolutions. The reason behind choosing such a smallCNN is that we are targeting a small FPGA as our hardwareplatform and any larger CNN would not pass the checkingresource limitation phase of the Software backend. Also,we wanted to show the accuracy difference between the ac-celerated version on FPGA and the software version.In order to compare the software version and the ver-sion implemented on FPGA, we classi\ufb01ed CIFAR-10 testdataset for both software model on ARM Cortex-A9 whichis the embedded processor of our FPGA and on the Tiny-CNN accelerator with two modes. Table 2 shows the com-parison averaged results per image. SW is the Python-based software classi\ufb01er ran on ARM processor of the board(650MHz). HW-SM is the CNN accelerator generated inshared mode (sharing one convolution unit among all lay-ers), and HW-EM is the CNN accelerator generated in ex-clusive mode (one convolution for each layer).We see that the effect of using 16-bit \ufb01xed-point dataFigure 3: Hardware resource utilization of the Table 1 CNNimplemented on FPGA.Table 2: Software vs. TinyCNN accelerator.version Accuracy(%)Runtime(ms)Data typeSW 65.54 42.54 32-b \ufb02oatingHW-SM 62.28 8.12 16-b \ufb01xedHW-EM 62.28 2.7 16-b \ufb01xedon the precision of the classi\ufb01cation is negligible. We canalso see that using exclusive convolution hardware units im-proves the runtime by avoiding stalls due to access con-tention for using convolution layer by different layers.Figure 3 shows the resource usage of the CNN hardwareimplemented in PYNQ board. As we expected, memory(BRAM) is the hardware resource limitation that we en-counter while implementing a CNN on an embedded FPGA.In the software backend, the CNN architecture is designedin such a way that it utilized the FPGA BRAM to the high-est.4. ConclusionIn this paper, we proposed a framework that enables thedesigners to design a CNN accelerator for an embeddedFPGA fast. The framework provides a software API thatmakes designers capable of exploring the CNN design spaceconsidering the hardware resource limitations. Then, theCNN hardware is generated and tuned to have a low accu-racy loss by inter-layer precision adjustment. Our resultsshow that we can reach up to 15.75% speedup compared tothe software implementation with 16-bit \ufb01xed-point data.As future work, we are going to automate the softwareCNN design space exploration part in the software backendto make the CNN design even more easily. Since the onlysupported activation function is ReLU, we are going to addmore activation units to the framework.5--- Page 6 ---References[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cIm-agenet classi\ufb01cation with deep convolutional neuralnetworks,\u201d inAdvances in neural information process-ing systems, pp. 1097\u20131105, 2012.[2] K. Ovtcharov, O. Ruwase, J.-Y . Kim, J. Fowers,K. Strauss, and E. S. Chung, \u201cAccelerating deep con-volutional neural networks using specialized hard-ware,\u201dMicrosoft Research Whitepaper, vol. 2, no. 11,pp. 1\u20134, 2015.[3] A. Jahanshahi, M. K. Taram, and N. Eskandari,\u201cBlokus duo game on fpga,\u201d in The 17th CSI Interna-tional Symposium on Computer Architecture & Digi-tal Systems (CADS 2013), pp. 149\u2013152, IEEE, 2013.[4] C. Zhang, P. Li, G. Sun, Y . Guan, B. Xiao, andJ. Cong, \u201cOptimizing fpga-based accelerator designfor deep convolutional neural networks,\u201d in Proceed-ings of the 2015 ACM/SIGDA International Sympo-sium on Field-Programmable Gate Arrays , pp. 161\u2013170, ACM, 2015.[5] J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu,T. Tang, N. Xu, S. Song, et al., \u201cGoing deeper withembedded fpga platform for convolutional neural net-work,\u201d in Proceedings of the 2016 ACM/SIGDA In-ternational Symposium on Field-Programmable GateArrays, pp. 26\u201335, ACM, 2016.[6] N. Suda, V . Chandra, G. Dasika, A. Mohanty, Y . Ma,S. Vrudhula, J.-s. Seo, and Y . Cao, \u201cThroughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks,\u201d in Proceedingsof the 2016 ACM/SIGDA International Symposium onField-Programmable Gate Arrays, pp. 16\u201325, ACM,2016.[7] Y . Wang, J. Xu, Y . Han, H. Li, and X. Li, \u201cDeepburn-ing: automatic generation of fpga-based learning ac-celerators for the neural network family,\u201d in Proceed-ings of the 53rd Annual Design Automation Confer-ence, p. 110, ACM, 2016.[8] M. Courbariaux, Y . Bengio, and J.-P. David, \u201cBina-ryconnect: Training deep neural networks with binaryweights during propagations,\u201d in Advances in neuralinformation processing systems, pp. 3123\u20133131, 2015.[9] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv,and Y . Bengio, \u201cBinarized neural networks: Train-ing deep neural networks with weights and ac-tivations constrained to+ 1 or-1,\u201d arXiv preprintarXiv:1602.02830, 2016.[10] M. Rastegari, V . Ordonez, J. Redmon, and A. Farhadi,\u201cXnor-net: Imagenet classi\ufb01cation using binary con-volutional neural networks,\u201d in European Conferenceon Computer Vision, pp. 525\u2013542, Springer, 2016.[11] J. Cong, B. Liu, S. Neuendorffer, J. Noguera, K. Vis-sers, and Z. Zhang, \u201cHigh-level synthesis for fpgas:From prototyping to deployment,\u201d IEEE Transactionson Computer-Aided Design of Integrated Circuits andSystems, vol. 30, no. 4, pp. 473\u2013491, 2011.[12] A. Canis, J. Choi, M. Aldham, V . Zhang, A. Kam-moona, T. Czajkowski, S. D. Brown, and J. H. An-derson, \u201cLegup: An open-source high-level synthe-sis tool for fpga-based processor/accelerator systems,\u201dACM Transactions on Embedded Computing Systems(TECS), vol. 13, no. 2, p. 24, 2013.[13] T. S. Czajkowski, U. Aydonat, D. Denisenko, J. Free-man, M. Kinsner, D. Neto, J. Wong, P. Yiannacouras,and D. P. Singh, \u201cFrom opencl to high-performancehardware on fpgas,\u201d in 22nd international conferenceon \ufb01eld programmable logic and applications (FPL) ,pp. 531\u2013534, IEEE, 2012.[14] V . Kathail, J. Hwang, W. Sun, Y . Chobe, T. Shui, andJ. Carrillo, \u201cSdsoc: A higher-level programming envi-ronment for zynq soc and ultrascale+ mpsoc,\u201d in Pro-ceedings of the 2016 ACM/SIGDA International Sym-posium on Field-Programmable Gate Arrays, pp. 4\u20134,ACM, 2016.[15] R. Zhao, W. Song, W. Zhang, T. Xing, J.-H. Lin,M. Srivastava, R. Gupta, and Z. Zhang, \u201cAccel-erating Binarized Convolutional Neural Networkswith Software-Programmable FPGAs,\u201dInt\u2019l Symp. onField-Programmable Gate Arrays (FPGA), Feb 2017.[16] L. Cavigelli and L. Benini, \u201cOrigami: A 803-gop/s/wconvolutional network accelerator,\u201d IEEE Transac-tions on Circuits and Systems for Video Technology ,vol. 27, no. 11, pp. 2461\u20132475, 2016.[17] J. Bachrach, H. V o, B. Richards, Y . Lee, A. Water-man, R. Avi \u02c7zienis, J. Wawrzynek, and K. Asanovi \xb4c,\u201cChisel: constructing hardware in a scala embeddedlanguage,\u201d in DAC Design Automation Conference2012, pp. 1212\u20131221, IEEE, 2012.[18] \u201cPynq: Python productivity for zynq.\u201d http://www.pynq.io/. Accessed: 04/21/2019.6, Your response: ```ADD_PAPER\n1911.06777v1\nThis paper presents TinyCNN, a framework for designing CNN accelerators on embedded FPGAs for image classification. The framework provides automated hardware generation from a software-designed CNN, with three key components: (1) a Python-based software backend for CNN design/training, (2) a CHISEL-based hardware generator, and (3) a precision adjustment backend. The system uses 16-bit fixed-point data, achieving only 3% accuracy loss compared to 32-bit floating-point while providing up to 15.75x speedup on a Xilinx Zynq-7000 FPGA. The framework addresses resource constraints through configurable convolution units (shared or exclusive modes), inter-layer precision adjustment, and automatic hardware resource checking. Experimental results on CIFAR-10 show the FPGA implementation achieves 62.28% accuracy (vs 65.54% software) with 2.7ms inference time (vs 42.54ms on ARM CPU). The work demonstrates effective FPGA acceleration for CNNs while maintaining accuracy through precision optimization techniques.\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\n~~~~~~~~~~\nCurrent Step #23, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification model compression NAS\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 25:
\n~~~~~~~~~~\nHistory: Step #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #21, Phase: literature review, Feedback: 3\n--- Page 1 ---TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGAAli Jahanshahiajaha004@ucr.eduUniversity of California, RiversideAbstractIn recent years, Convolutional Neural Network (CNN)based methods have achieved great success in a large num-ber of applications and have been among the most power-ful and widely used techniques in computer vision. How-ever, CNN-based methods are computational-intensive andresource-consuming, and thus are hard to be integrated intoembedded systems such as smart phones, smart glasses, androbots. FPGA is one of the most promising platforms for ac-celerating CNN, but the limited on-chip memory size limitthe performance of FPGA accelerator for CNN.In this paper, we propose a framework for designingCNN accelerator on embedded FPGA for image classi\ufb01ca-tion. The proposed framework provides a tool for FPGAresource-aware design space exploration of CNNs and au-tomatically generates the hardware description of the CNNto be programmed on a target FPGA. The framework con-sists of three main backends; software, hardware gener-ation, and simulation/precision adjustment. The softwarebackend serves as an API to the designer to design the CNNand train it according to the hardware resources that areavailable. Using the CNN model, hardware backend gen-erates the necessary hardware components and integratesthem to generate the hardware description of the CNN. Fi-naly, Simulation/precision adjustment backend adjusts theinter-layer precision units to minimize the classi\ufb01cation er-ror.We used 16-bit \ufb01xed-point data in a CNN accelerator(FPGA) and compared it to the exactly similar software ver-sion running on an ARM processor (32-bit \ufb02oating pointdata). We encounter about 3% accuracy loss in classi\ufb01ca-tion of the accelerated (FPGA) version. In return, we gotup to 15.75\xd7 speedup by classifying with the acceleratedversion on the FPGA.1. IntroductionThe exponential growth of big data during the last decademotivates for innovative methods to extract high semanticinformation from raw sensor data such as videos, imagesand speech sequences. Among the proposed methods, Con-volutional Neural Networks (CNNs) have become the de-facto standard by delivering near-human accuracy in manyapplications related to machine vision. While CNNs havebeen known to researchers for decades, they were popu-larized after demonstrating high accuracy at the 2012 Ima-geNet recognition challenge [1]. Subsequently, CNNs havebecome the state-of-the-art for image classi\ufb01cation, detec-tion, and localization tasks. Research in CNNs and otherareas of deep learning continues at a rapid pace, with hun-dreds of new papers published each year introducing newmodels and techniques.The CNN\u2019s high performance (at classi\ufb01cation, detec-tion, and localization) comes at the price of a large com-putational cost as they require tens of GOP/s to classify asingle frame. Thus, one challenge to the widespread de-ployment of CNNs is their signi\ufb01cant demands for com-putation and storage capacity. Therefore, dedicated hard-ware is required to accelerate their execution. GraphicsProcessing Units (GPUs), are the most widely used plat-form to implement CNNs as they offer the best performancein terms of pure computational throughput, reaching up to11 TFLOP/s. Nevertheless, in terms of power consump-tion, Field-Programmable Gate Array (FPGA) solutions areknown to be more energy ef\ufb01cient (vs GPUs). Recent workby Microsoft has even explored cost-effective accelerationof deep learning on FPGAs at datacenter scale [2, 3]. Thereare also efforts in the academic community on FPGA-basedCNN accelerators [4,5] as well as tools for generating themautomatically [6, 7].We observe two trends which may help overcome im-plementing CNN on FPGAs. The \ufb01rst is a series of recentpapers in the machine learning community regarding very-low-precision CNNs. Networks with binary weights [8], orbinary weights and activations [9, 10] have in certain casesdemonstrated accuracy comparable to full precision nets.Such binarized neural net-works (BNNs) may be the keyto ef\ufb01cient deep learning on FPGA. Binarization reducesstorage and memory bandwidth requirements, and replaceFP operations with binary operations which can be very ef-\ufb01ciently performed on the LUT-based FPGA fabric. Con-1arXiv:1911.06777v1  [cs.LG]  15 Nov 2019--- Page 2 ---cerning the cost and effort of FPGA implementation,we seea steady improvement in FPGA design automation toolsover the past decade. High-level synthesis (HLS) toolssuch as Xilinx Vivado HLS [11] and LegUp [12] enable auser to write code in a high-level programming language,then algorithmically compile that code down to a register-transfer level (RTL) design speci\ufb01cation. More recent toolssuch as Intel FPGA SDK for OpenCL [13] and Xilinx SD-SoC [14] offer further automation features for generatingthe hardware-software interface and on-chip memory net-work. In the context of deep learning, these tools have thepotential to critically reduce time-to-market on new acceler-ator designs and thus reduce the aforementioned innovationgap.In this project, we aim to design and implement a CNNaccelerator on an embedded FPGA for image classi\ufb01cation.We use [15] as our reference that presents the design of aBNN accelerator for FPGAs. In contrast to our reference, inthis work, the aim is to provide designers a general frame-work to enable them design their CNNeasily and use it as anaccelerator on an FPGA. Thus, we do not use BNN-basedCNN in this project.In addition to generality of the framework, we also targetembedded systems that have hardware resource limitation.Compressing the CNN model is a good choice to addressthe resource limitations of the hardware. A straight forwardway to compress a network is to reduce the bit-width forcomputing. This utilizes the \ufb02exibility of FPGA or ASICdesign compared with GPU or CPU. It is proved to be aneffective way to use 16-bits \ufb01xed-point operations in [16]with small loss in accuracy. In our framework, we use 16-bits operation and we show that the accuracy loss is verysmall.This work proposes a general framework for design andimplementation of tiny modular CNN accelerators on em-bedded FPGAs. Our framework automatically generatesthe hardware code for the designed CNN and trained withthe provided software backend. The framework consistsof three main components that correspond to our contribu-tions. They are as follows:\u2022 Software backend: Using Python, we provide a toolfor designing and training a CNN architecture. Themodel (weights) of the trained network and otherCNN architecture parameters are used in the hardwareframework to generate the hardware description lan-guage of the CNN, which is going to be implementedon the FPGA. This back-end also provides the in-formation needed for Simulation/precision adjustmentbackend as well as checking the hardware resourcesfor the designed CNN model.\u2022 Hardware backend: Using CHISEL [17], we performautomatic generation and integration of different hard-ware components needed for the CNN architecture de-signed in the software backend. The CNN model ispassed to this back-end to be used in HDL generationof the CNN. The output of this backend is the HDLcode that is ready to be synthesized and programmedon an FPGA.\u2022 Simulation/precision adjustment backend: UsingScala testing libraries and the data passed to this back-end, inter-layer precision of the generated CNN hard-ware will be adjusted. The CNN output error intro-duced by varying integer and fractional part of eachlayer\u2019s data is minimized by this backend.The rest of this paper is organized as follows; we \ufb01rst de-scribe the framework overall structure, then we go in de-tails for each component of the framework. Finally, we pro-vide the results discussing different aspects of the generatedCNN for our target FPGA.2. Tiny CNNCNN is a machine learning classi\ufb01er that typically takesin an image and produces the probabilities of that imagebelonging to each output class. A typical CNN consists ofa pipeline of connected layers. Each layer takes as inputa set of feature maps ( fmaps), performs some computationon them, and produces a new set of fmaps to be fed intothe next layer. The input fmaps of the \ufb01rst layer is the in-put image. Layers may require con\ufb01guration values knownas parameters, which must \ufb01rst be determined by trainingthe CNN of\ufb02ine on pre-classi\ufb01ed data. Once the parame-ters are \ufb01nalized, the CNN can be deployed for inferencethe classi\ufb01cation of new data points. For most practical ma-chine learning applications, the \ufb01rst-class concerns are theaccuracy and execution time of online classi\ufb01cation. Thisproject will thus focus on accelerating the inference taskwithout compromising accuracy. The aim of this project isto provide a framework that automates implementation ofCNNs on an embedded FPGA for image classi\ufb01cation. Fig-ure 1 shows the framework we proposed for this purpose. Inthe following subsections, we are going to elaborate moreon each component of our framework.2.1. Software backendThe software backend is an API by which users are capa-ble of designing their CNN in the software level. The usercan tune their CNN with the train and test data sets, and\ufb01nally, export their model for further processings. We usedKeras library as the underlying deep learning framework ofthe software backend. Keras is a high-level neural networksAPI, written in Python and capable of running on top ofTensorFlow, CNTK, or Theano. It was developed with afocus on enabling fast experimentation. We address threechallenges of designing a CNN by Software backend:2--- Page 3 ---Figure 1: Tiny CNN framework.CNN designing, training, and model generation: In ourframework, using Keras API, we provide a templatePythonclass to the designers to design their CNN. Then, the de-signer\u2019s CNN is trained and the model is saved in order tobe used by the hardware backend. Essentially, the model isthe CNN weights that are going to be used as initial valuesof ROMs.Veri\ufb01cation data: The software backend also inputs somerandom data from test dataset to CNN and captures the out-put of all layers of CNN with respect to that input. Thecollected dataset is called veri\ufb01cation data and is used inhardware backend for precision adjustment and veri\ufb01cationpurposes.Hardware resource check: As mentioned before, we aretargeting embedded FPGA in this work. Thus, we want tosqueez the whole CNN model in FPGA BRAMs. Thus,one step for designing the CNN would be checking if theCNN model (weights) \ufb01t in FPGA BRAMs. Software back-end uses the target FPGA spec to check if the model \ufb01ts inFPGA or not. In case the model does not \ufb01t on the tar-get FPGA, the software backend throws an exception thatshows not enough space for the model. In such cases, thedesigner should play around the CNN hyper parameters andmake it smaller to \ufb01t on the FPGA. Fully-connected layers,due to their high number of parameters contributes to themost of the memory (BRAM) usage.2.2. Hardware backendHardware backend generates the hardware of the CNNwhich is designed in the software backend according tothe inputs that are provided to this backend. We imple-mented all basic hardware components that are needed forCNNs. The developed hardware components are modularand highly con\ufb01gurable. They can be con\ufb01gured based onthe CNN speci\ufb01cations including: data bit-width, shared orexclusive components, and number of DSPs available forcomponents. All of the implemented hardware componentsare modular that means we can simply attach them togetherwith few lines of code just like we do in designing CNNs insoftware.2.2.1 Convolution unitThis unit is the most critical component of the accelera-tor, as it will take up the vast majority of the runtime. Theunit must maintain high throughput and resource ef\ufb01ciency.Convolution operation has multiplication and addition in itscore. Since multiplication is a very expensive operation,we used the built-in DSPs on FPGA board that are spe-cialized for digital signal processing purposing \u2013 mainly in-clude multiplication and addition.The challenge with using the FPGA DSPs is that thereare a limited number of them on every FPGA board. Theimplemented Convolution unit hardware is con\ufb01gurable insuch a way that the designer determines the number ofDSPsfor this layer, and the hardware generator framework gener-ates the convolver state machine in a way that it uses onlythe speci\ufb01ed number of DSPs. Apparently, the more DSPswe allot to this hardware unit, the more throughput we get.The maximum number of DSPs is the input image pixels,and the minimum is one.Since we are targeting embedded systems, in our frame-work, this unit can be used by the designer of the CNN intwo modes:\u2022 Shared mode: in the this mode, the convolution unitis shared among all layers of CNN. Sharing this unitsresults in using less hardware resources of the FPGA,but it introduces throughput degradation. If this modeis chosen by the designer, a wrapper is generated au-tomatically by the hardware generator backend. Thewrapper acts as a resource manager for this unit by ar-bitrating different layers requesting to use convolutionunit.\u2022 Exclusive mode: in the exclusive mode, for each con-volution layer in the CNN a convolution unit hardwareis generated, which results in higher throughput andFPGA resource usage.3--- Page 4 ---Figure 2: FeedForward unit architecture.2.2.2 FeedForward unitThe input to convolution unit hardware is three lines of theinput fmap and the \ufb01lter that is going to be applied on thefmaps. Also, the output of the convolution unit hardware isone line that its size equal to the input fmap size and it goesthrough activation, max pooling, and precision adjustmentlayer to get to the next layer. Such inter-layer data shouldbe handled and stored in a hardware unit.FeedForward unit is responsible for handling accumula-tion of the input fmaps in an inter-layer RAM by a state ma-chine (SM), and output them to the convolution unit threeline at a time in conjunction with the \ufb01lter that should beapplied to the fmap. Figure 2 shows the structure of thishardware unit.As it is shown in Figure 2, one state machine (SM) isresponsible for buffering the feature maps ( fmap) that areinput to this layer. This state machine is generated for eachlayer according to the size and number of fmaps that areinputted to the layer. On the output side, another state ma-chine is responsible for feeding three lines of fmaps to theconvolution unit as well as the \ufb01lter that is going to appliedto that. The state machine is also generated speci\ufb01c to thelayer. Also, the RAM and ROM in Figure 2 are generatedaccording to the fmaps that are going to buffered in thislayer and number of that layer \ufb01lters. ROMs are initializedwith the CNN model (weights) provided by the Softwarebackend.2.2.3 Activation and Max pooling unitsIn our framework, we implemented a max pooling unit andan activation hardware unit. The activation unit performsa Recti\ufb01ed Linear ReLU function. The max pooling unitis con\ufb01gurable and is generated to perform M \xd7 M maxpooling in which M is speci\ufb01ed by the designer.2.2.4 Inter-layer precision adjustment unitAs we mentioned before, we are using \ufb01xed-point numbersand operations in our framework. Using \ufb01xed-point insteadof \ufb02oating-point introduced error to the CNN. This errorpropagates through the network and affects the \ufb01nal resultsadversely. Since the error propagated through the early lay-ers has more negative effects on the output anf the rangeof the data propagated through the network varies from onelayer to the other, we need to adjust the proportion of in-teger part and fractional part at the end of each layer ofthe network. For doing so, we inser an Inter-layer preci-sion adjustment unit at the end of each layer. This unit isresponsible for adjusting the integer and fractional part ofthe output data of each layer before propagating to the restof the network. The adjustment of this unit is done by theSimulation/precision adjustment backend.2.2.5 Dense or fully-connected (FC) unitDense or fully-connected is also implemented as a con\ufb01g-urable hardware unit. Since this unit mainly consists ofmultiplication and addition, it needs DSP resources. In ourframework, the number of DSPs and the size of the FC unitis adjustable according to the designer needs. The weightsfor this layer are stored in ROMs. So, they are generatedbased on the weights provided by Software backend andused in the synthesis phase of the generate hardware.2.3. Simulation and Precision Adjustment BackendIn order to adjust the \ufb01xed-point precision of the CNN,we need to propagate several test inputs to the CNN im-plemented in the hardware. The test points are generated bysoftware backend based on the user\u2019s input dataset for train-ing the CNN. The data consists of the input to the CNN andthe output of all layers that are produced for that image. Thesimulation backend uses Scala language testing libraries toinput the veri\ufb01cation data to the generated hardware. Thenall the data from the output of each layer of the generatedCNN hardware is collected, and the difference of the col-lected data and the data provided by the software backendare calculated. Then, this backend adjusts the bit-width ofthe Inter-layer precision adjustment unit of each layer basedto minimize the error generated by each layer.This sequence of propagating data, collecting data, cal-culating error, and adjusting the precision units is performeduntil the network merges to a minimum output error. At theend, the Simulation and precision adjustment backend ad-justs all integer and fractional parts of the layers based onthe veri\ufb01cation data that are provided by the Software back-end.3. Experimental ResultsWe evaluate our design on a PYNQ board [18] that is anopen-source project from Xilinx that makes it easy to designembedded systems withXilinx Zynq SoC. PYNQ uses a low-cost Xilinx Zynq-7000 SoC containing an XC7Z020 FPGAalongside an ARM Cortex-A9 embedded processor. On the4--- Page 5 ---Table 1: The CNN architecture generated by the framework.Layer type Output size Output fmaps Param #Conv (32, 32) 32 320Activation (32, 32) 32 0Max pool (16, 16) 32 0Conv (16, 16) 64 18496Activation (16, 16) 64 0Max pool (8, 8) 64 0Conv (8, 8) 128 73856Activation (8, 8) 128 0Max pool (4, 4) 128 0Conv (4, 4) 128 147584Activation (4, 4) 128 0Max pool (2, 2) 128 0Dense (1, 100) 51300Activation (1, 100) 0Dense (1, 10) 1010Activation (1, 10) 0Total 292,566software side of PYQN, using the Python language and li-braries, designers can exploit the bene\ufb01ts of programmablelogic and microprocessors in Zynq to build more capableand exciting embedded systems. We make use of XilinxSDSoC 2016.4 as the primary design tool, which leveragesVivado HLS and Vivado to perform the actual synthesis andprogramming the FPGA implementation. We used CIFAR-10 dataset to train and test the CNN. We converted the im-ages to grey to have one-channel input due to our a smallFPGA board.Using the software backend we designed and trained aCNN. Table 1 shows the architecture of the CNN we tar-geted for classifying CIFAR-10 dataset. We used3\xd73 \ufb01ltersfor convolutions. The reason behind choosing such a smallCNN is that we are targeting a small FPGA as our hardwareplatform and any larger CNN would not pass the checkingresource limitation phase of the Software backend. Also,we wanted to show the accuracy difference between the ac-celerated version on FPGA and the software version.In order to compare the software version and the ver-sion implemented on FPGA, we classi\ufb01ed CIFAR-10 testdataset for both software model on ARM Cortex-A9 whichis the embedded processor of our FPGA and on the Tiny-CNN accelerator with two modes. Table 2 shows the com-parison averaged results per image. SW is the Python-based software classi\ufb01er ran on ARM processor of the board(650MHz). HW-SM is the CNN accelerator generated inshared mode (sharing one convolution unit among all lay-ers), and HW-EM is the CNN accelerator generated in ex-clusive mode (one convolution for each layer).We see that the effect of using 16-bit \ufb01xed-point dataFigure 3: Hardware resource utilization of the Table 1 CNNimplemented on FPGA.Table 2: Software vs. TinyCNN accelerator.version Accuracy(%)Runtime(ms)Data typeSW 65.54 42.54 32-b \ufb02oatingHW-SM 62.28 8.12 16-b \ufb01xedHW-EM 62.28 2.7 16-b \ufb01xedon the precision of the classi\ufb01cation is negligible. We canalso see that using exclusive convolution hardware units im-proves the runtime by avoiding stalls due to access con-tention for using convolution layer by different layers.Figure 3 shows the resource usage of the CNN hardwareimplemented in PYNQ board. As we expected, memory(BRAM) is the hardware resource limitation that we en-counter while implementing a CNN on an embedded FPGA.In the software backend, the CNN architecture is designedin such a way that it utilized the FPGA BRAM to the high-est.4. ConclusionIn this paper, we proposed a framework that enables thedesigners to design a CNN accelerator for an embeddedFPGA fast. The framework provides a software API thatmakes designers capable of exploring the CNN design spaceconsidering the hardware resource limitations. Then, theCNN hardware is generated and tuned to have a low accu-racy loss by inter-layer precision adjustment. Our resultsshow that we can reach up to 15.75% speedup compared tothe software implementation with 16-bit \ufb01xed-point data.As future work, we are going to automate the softwareCNN design space exploration part in the software backendto make the CNN design even more easily. Since the onlysupported activation function is ReLU, we are going to addmore activation units to the framework.5--- Page 6 ---References[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cIm-agenet classi\ufb01cation with deep convolutional neuralnetworks,\u201d inAdvances in neural information process-ing systems, pp. 1097\u20131105, 2012.[2] K. Ovtcharov, O. Ruwase, J.-Y . Kim, J. Fowers,K. Strauss, and E. S. Chung, \u201cAccelerating deep con-volutional neural networks using specialized hard-ware,\u201dMicrosoft Research Whitepaper, vol. 2, no. 11,pp. 1\u20134, 2015.[3] A. Jahanshahi, M. K. Taram, and N. Eskandari,\u201cBlokus duo game on fpga,\u201d in The 17th CSI Interna-tional Symposium on Computer Architecture & Digi-tal Systems (CADS 2013), pp. 149\u2013152, IEEE, 2013.[4] C. Zhang, P. Li, G. Sun, Y . Guan, B. Xiao, andJ. Cong, \u201cOptimizing fpga-based accelerator designfor deep convolutional neural networks,\u201d in Proceed-ings of the 2015 ACM/SIGDA International Sympo-sium on Field-Programmable Gate Arrays , pp. 161\u2013170, ACM, 2015.[5] J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu,T. Tang, N. Xu, S. Song, et al., \u201cGoing deeper withembedded fpga platform for convolutional neural net-work,\u201d in Proceedings of the 2016 ACM/SIGDA In-ternational Symposium on Field-Programmable GateArrays, pp. 26\u201335, ACM, 2016.[6] N. Suda, V . Chandra, G. Dasika, A. Mohanty, Y . Ma,S. Vrudhula, J.-s. Seo, and Y . Cao, \u201cThroughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks,\u201d in Proceedingsof the 2016 ACM/SIGDA International Symposium onField-Programmable Gate Arrays, pp. 16\u201325, ACM,2016.[7] Y . Wang, J. Xu, Y . Han, H. Li, and X. Li, \u201cDeepburn-ing: automatic generation of fpga-based learning ac-celerators for the neural network family,\u201d in Proceed-ings of the 53rd Annual Design Automation Confer-ence, p. 110, ACM, 2016.[8] M. Courbariaux, Y . Bengio, and J.-P. David, \u201cBina-ryconnect: Training deep neural networks with binaryweights during propagations,\u201d in Advances in neuralinformation processing systems, pp. 3123\u20133131, 2015.[9] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv,and Y . Bengio, \u201cBinarized neural networks: Train-ing deep neural networks with weights and ac-tivations constrained to+ 1 or-1,\u201d arXiv preprintarXiv:1602.02830, 2016.[10] M. Rastegari, V . Ordonez, J. Redmon, and A. Farhadi,\u201cXnor-net: Imagenet classi\ufb01cation using binary con-volutional neural networks,\u201d in European Conferenceon Computer Vision, pp. 525\u2013542, Springer, 2016.[11] J. Cong, B. Liu, S. Neuendorffer, J. Noguera, K. Vis-sers, and Z. Zhang, \u201cHigh-level synthesis for fpgas:From prototyping to deployment,\u201d IEEE Transactionson Computer-Aided Design of Integrated Circuits andSystems, vol. 30, no. 4, pp. 473\u2013491, 2011.[12] A. Canis, J. Choi, M. Aldham, V . Zhang, A. Kam-moona, T. Czajkowski, S. D. Brown, and J. H. An-derson, \u201cLegup: An open-source high-level synthe-sis tool for fpga-based processor/accelerator systems,\u201dACM Transactions on Embedded Computing Systems(TECS), vol. 13, no. 2, p. 24, 2013.[13] T. S. Czajkowski, U. Aydonat, D. Denisenko, J. Free-man, M. Kinsner, D. Neto, J. Wong, P. Yiannacouras,and D. P. Singh, \u201cFrom opencl to high-performancehardware on fpgas,\u201d in 22nd international conferenceon \ufb01eld programmable logic and applications (FPL) ,pp. 531\u2013534, IEEE, 2012.[14] V . Kathail, J. Hwang, W. Sun, Y . Chobe, T. Shui, andJ. Carrillo, \u201cSdsoc: A higher-level programming envi-ronment for zynq soc and ultrascale+ mpsoc,\u201d in Pro-ceedings of the 2016 ACM/SIGDA International Sym-posium on Field-Programmable Gate Arrays, pp. 4\u20134,ACM, 2016.[15] R. Zhao, W. Song, W. Zhang, T. Xing, J.-H. Lin,M. Srivastava, R. Gupta, and Z. Zhang, \u201cAccel-erating Binarized Convolutional Neural Networkswith Software-Programmable FPGAs,\u201dInt\u2019l Symp. onField-Programmable Gate Arrays (FPGA), Feb 2017.[16] L. Cavigelli and L. Benini, \u201cOrigami: A 803-gop/s/wconvolutional network accelerator,\u201d IEEE Transac-tions on Circuits and Systems for Video Technology ,vol. 27, no. 11, pp. 2461\u20132475, 2016.[17] J. Bachrach, H. V o, B. Richards, Y . Lee, A. Water-man, R. Avi \u02c7zienis, J. Wawrzynek, and K. Asanovi \xb4c,\u201cChisel: constructing hardware in a scala embeddedlanguage,\u201d in DAC Design Automation Conference2012, pp. 1212\u20131221, IEEE, 2012.[18] \u201cPynq: Python productivity for zynq.\u201d http://www.pynq.io/. Accessed: 04/21/2019.6, Your response: ```ADD_PAPER\n1911.06777v1\nThis paper presents TinyCNN, a framework for designing CNN accelerators on embedded FPGAs for image classification. The framework provides automated hardware generation from a software-designed CNN, with three key components: (1) a Python-based software backend for CNN design/training, (2) a CHISEL-based hardware generator, and (3) a precision adjustment backend. The system uses 16-bit fixed-point data, achieving only 3% accuracy loss compared to 32-bit floating-point while providing up to 15.75x speedup on a Xilinx Zynq-7000 FPGA. The framework addresses resource constraints through configurable convolution units (shared or exclusive modes), inter-layer precision adjustment, and automatic hardware resource checking. Experimental results on CIFAR-10 show the FPGA implementation achieves 62.28% accuracy (vs 65.54% software) with 2.7ms inference time (vs 42.54ms on ARM CPU). The work demonstrates effective FPGA acceleration for CNNs while maintaining accuracy through precision optimization techniques.\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\n~~~~~~~~~~\nCurrent Step #24, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance.\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2307.04443v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 26:
\n~~~~~~~~~~\nHistory: Step #7, Phase: literature review, Feedback: You requested arXiv papers related to the query edge device image classification latency optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Latency-aware Spatial-wise Dynamic Networks\nSummary: Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.\nPublication Date: 2022-10-12\narXiv paper ID: 2210.06223v1\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Real-Time Edge Classification: Optimal Offloading under Token Bucket Constraints\nSummary: To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.\nPublication Date: 2020-10-26\narXiv paper ID: 2010.13737v2\n\nTitle: Adaptive Edge Offloading for Image Classification Under Rate Limit\nSummary: This paper considers a setting where embedded devices are used to acquire and\nclassify images. Because of limited computing capacity, embedded devices rely\non a parsimonious classification model with uneven accuracy. When local\nclassification is deemed inaccurate, devices can decide to offload the image to\nan edge server with a more accurate but resource-intensive model. Resource\nconstraints, e.g., network bandwidth, however, require regulating such\ntransmissions to avoid congestion and high latency. The paper investigates this\noffloading problem when transmissions regulation is through a token bucket, a\nmechanism commonly used for such purposes. The goal is to devise a lightweight,\nonline offloading policy that optimizes an application-specific metric (e.g.,\nclassification accuracy) under the constraints of the token bucket. The paper\ndevelops a policy based on a Deep Q-Network (DQN), and demonstrates both its\nefficacy and the feasibility of its deployment on embedded devices. Of note is\nthe fact that the policy can handle complex input patterns, including\ncorrelation in image arrivals and classification accuracy. The evaluation is\ncarried out by performing image classification over a local testbed using\nsynthetic traces generated from the ImageNet image classification benchmark.\nImplementation of this work is available at\nhttps://github.com/qiujiaming315/edgeml-dqn.\nPublication Date: 2022-07-31\narXiv paper ID: 2208.00485v1\n, Your response: ```FULL_TEXT\n2210.06223v1\n```\nStep #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #24, Phase: literature review, Feedback: 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance., Your response: ```ADD_PAPER\n2307.04443v1\nThis paper presents DCA-NAS, a device constraints-aware neural architecture search framework for optimizing deep learning models for edge devices. The method incorporates hardware constraints (memory and FLOPs) directly into the NAS objective function through a constrained optimization formulation. Key innovations include: 1) A differentiable resource constraint using a Lagrange multiplier, 2) Weight sharing and channel bottleneck techniques to reduce search time (4-17x faster than prior hardware-aware NAS), and 3) A lookup graph mechanism to handle differences between search and evaluation architectures. Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 10-15x smaller than manual architectures with comparable accuracy, and 3x smaller than DARTS variants. The approach achieves 97.2% accuracy on CIFAR-10 with only 3.4M parameters, and maintains performance when transferred across datasets, demonstrating its effectiveness for edge deployment scenarios.\n```\n~~~~~~~~~~\nCurrent Step #25, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2307.04443v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2307.04443v1\nThis paper presents DCA-NAS, a device constraints-aware neural architecture search framework for optimizing deep learning models for edge devices. The method incorporates hardware constraints (memory and FLOPs) directly into the NAS objective function through a constrained optimization formulation. Key innovations include: 1) A differentiable resource constraint using a Lagrange multiplier, 2) Weight sharing and channel bottleneck techniques to reduce search time (4-17x faster than prior hardware-aware NAS), and 3) A lookup graph mechanism to handle differences between search and evaluation architectures. Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 10-15x smaller than manual architectures with comparable accuracy, and 3x smaller than DARTS variants. The approach achieves 97.2% accuracy on CIFAR-10 with only 3.4M parameters, and maintains performance when transferred across datasets, demonstrating its effectiveness for edge deployment scenarios.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 27:
\n~~~~~~~~~~\nHistory: Step #9, Phase: literature review, Feedback: Successfully added paper 2210.06223v1, Your response: ```SUMMARY\nedge image classification energy efficiency optimization\n```\nStep #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #24, Phase: literature review, Feedback: 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance., Your response: ```ADD_PAPER\n2307.04443v1\nThis paper presents DCA-NAS, a device constraints-aware neural architecture search framework for optimizing deep learning models for edge devices. The method incorporates hardware constraints (memory and FLOPs) directly into the NAS objective function through a constrained optimization formulation. Key innovations include: 1) A differentiable resource constraint using a Lagrange multiplier, 2) Weight sharing and channel bottleneck techniques to reduce search time (4-17x faster than prior hardware-aware NAS), and 3) A lookup graph mechanism to handle differences between search and evaluation architectures. Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 10-15x smaller than manual architectures with comparable accuracy, and 3x smaller than DARTS variants. The approach achieves 97.2% accuracy on CIFAR-10 with only 3.4M parameters, and maintains performance when transferred across datasets, demonstrating its effectiveness for edge deployment scenarios.\n```\nStep #25, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge image classification dynamic computation optimization\n```\n~~~~~~~~~~\nCurrent Step #26, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification dynamic computation optimization, here was the response\nTitle: Immiscible Color Flows in Optimal Transport Networks for Image Classification\nSummary: In classification tasks, it is crucial to meaningfully exploit the\ninformation contained in data. While much of the work in addressing these tasks\nis devoted to building complex algorithmic infrastructures to process inputs in\na black-box fashion, less is known about how to exploit the various facets of\nthe data, before inputting this into an algorithm. Here, we focus on this\nlatter perspective, by proposing a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nThe resulting optimal flows can then be fed into standard classifiers to\ndistinguish images in different classes. We show how our method can outperform\ncompeting approaches on image classification tasks in datasets where color\ninformation matters.\nPublication Date: 2022-05-04\narXiv paper ID: 2205.02938v2\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)\nSummary: Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\nPublication Date: 2022-04-10\narXiv paper ID: 2204.04704v1\n\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification dynamic computation optimization\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 28:
\n~~~~~~~~~~\nHistory: Step #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #24, Phase: literature review, Feedback: 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance., Your response: ```ADD_PAPER\n2307.04443v1\nThis paper presents DCA-NAS, a device constraints-aware neural architecture search framework for optimizing deep learning models for edge devices. The method incorporates hardware constraints (memory and FLOPs) directly into the NAS objective function through a constrained optimization formulation. Key innovations include: 1) A differentiable resource constraint using a Lagrange multiplier, 2) Weight sharing and channel bottleneck techniques to reduce search time (4-17x faster than prior hardware-aware NAS), and 3) A lookup graph mechanism to handle differences between search and evaluation architectures. Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 10-15x smaller than manual architectures with comparable accuracy, and 3x smaller than DARTS variants. The approach achieves 97.2% accuracy on CIFAR-10 with only 3.4M parameters, and maintains performance when transferred across datasets, demonstrating its effectiveness for edge deployment scenarios.\n```\nStep #25, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge image classification dynamic computation optimization\n```\nStep #26, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification dynamic computation optimization, here was the response\nTitle: Immiscible Color Flows in Optimal Transport Networks for Image Classification\nSummary: In classification tasks, it is crucial to meaningfully exploit the\ninformation contained in data. While much of the work in addressing these tasks\nis devoted to building complex algorithmic infrastructures to process inputs in\na black-box fashion, less is known about how to exploit the various facets of\nthe data, before inputting this into an algorithm. Here, we focus on this\nlatter perspective, by proposing a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nThe resulting optimal flows can then be fed into standard classifiers to\ndistinguish images in different classes. We show how our method can outperform\ncompeting approaches on image classification tasks in datasets where color\ninformation matters.\nPublication Date: 2022-05-04\narXiv paper ID: 2205.02938v2\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)\nSummary: Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\nPublication Date: 2022-04-10\narXiv paper ID: 2204.04704v1\n\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n, Your response: ```SUMMARY\nedge image classification adaptive inference techniques\n```\n~~~~~~~~~~\nCurrent Step #27, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification adaptive inference techniques, here was the response\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection to Suppress Adversarial Perturbation\nSummary: This paper presents a novel context-aware image denoising algorithm that\ncombines an adaptive image smoothing technique and color reduction techniques\nto remove perturbation from adversarial images. Adaptive image smoothing is\nachieved using auto-threshold canny edge detection to produce an accurate edge\nmap used to produce a blurred image that preserves more edge features. The\nproposed algorithm then uses color reduction techniques to reconstruct the\nimage using only a few representative colors. Through this technique, the\nalgorithm can reduce the effects of adversarial perturbations on images. We\nalso discuss experimental data on classification accuracy. Our results showed\nthat the proposed approach reduces adversarial perturbation in adversarial\nattacks and increases the robustness of the deep convolutional neural network\nmodels.\nPublication Date: 2021-01-14\narXiv paper ID: 2101.05833v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification adaptive inference techniques\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 29:
\n~~~~~~~~~~\nHistory: Step #10, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy efficiency optimization, here was the response\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference\nSummary: This paper presents AppealNet, a novel edge/cloud collaborative architecture\nthat runs deep learning (DL) tasks more efficiently than state-of-the-art\nsolutions. For a given input, AppealNet accurately predicts on-the-fly whether\nit can be successfully processed by the DL model deployed on the\nresource-constrained edge device, and if not, appeals to the more powerful DL\nmodel deployed at the cloud. This is achieved by employing a two-head neural\nnetwork architecture that explicitly takes inference difficulty into\nconsideration and optimizes the tradeoff between accuracy and\ncomputation/communication cost of the edge/cloud collaborative architecture.\nExperimental results on several image classification datasets show up to more\nthan 40% energy savings compared to existing techniques without sacrificing\naccuracy.\nPublication Date: 2021-05-10\narXiv paper ID: 2105.04104v3\n\nTitle: Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory\nSummary: Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental "memory wall": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.\nPublication Date: 2021-08-17\narXiv paper ID: 2108.07879v1\n, Your response: ```FULL_TEXT\n2211.09897v1\n```\nStep #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #25, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge image classification dynamic computation optimization\n```\nStep #26, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification dynamic computation optimization, here was the response\nTitle: Immiscible Color Flows in Optimal Transport Networks for Image Classification\nSummary: In classification tasks, it is crucial to meaningfully exploit the\ninformation contained in data. While much of the work in addressing these tasks\nis devoted to building complex algorithmic infrastructures to process inputs in\na black-box fashion, less is known about how to exploit the various facets of\nthe data, before inputting this into an algorithm. Here, we focus on this\nlatter perspective, by proposing a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nThe resulting optimal flows can then be fed into standard classifiers to\ndistinguish images in different classes. We show how our method can outperform\ncompeting approaches on image classification tasks in datasets where color\ninformation matters.\nPublication Date: 2022-05-04\narXiv paper ID: 2205.02938v2\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)\nSummary: Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\nPublication Date: 2022-04-10\narXiv paper ID: 2204.04704v1\n\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n, Your response: ```SUMMARY\nedge image classification adaptive inference techniques\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification adaptive inference techniques, here was the response\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection to Suppress Adversarial Perturbation\nSummary: This paper presents a novel context-aware image denoising algorithm that\ncombines an adaptive image smoothing technique and color reduction techniques\nto remove perturbation from adversarial images. Adaptive image smoothing is\nachieved using auto-threshold canny edge detection to produce an accurate edge\nmap used to produce a blurred image that preserves more edge features. The\nproposed algorithm then uses color reduction techniques to reconstruct the\nimage using only a few representative colors. Through this technique, the\nalgorithm can reduce the effects of adversarial perturbations on images. We\nalso discuss experimental data on classification accuracy. Our results showed\nthat the proposed approach reduces adversarial perturbation in adversarial\nattacks and increases the robustness of the deep convolutional neural network\nmodels.\nPublication Date: 2021-01-14\narXiv paper ID: 2101.05833v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n, Your response: ```SUMMARY\nedge image classification early exit networks\n```\n~~~~~~~~~~\nCurrent Step #28, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification early exit networks, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification early exit networks\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 30:
\n~~~~~~~~~~\nHistory: Step #12, Phase: literature review, Feedback: Successfully added paper 2211.09897v1, Your response: ```SUMMARY\nedge image classification spiking neural networks\n```\nStep #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #25, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge image classification dynamic computation optimization\n```\nStep #26, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification dynamic computation optimization, here was the response\nTitle: Immiscible Color Flows in Optimal Transport Networks for Image Classification\nSummary: In classification tasks, it is crucial to meaningfully exploit the\ninformation contained in data. While much of the work in addressing these tasks\nis devoted to building complex algorithmic infrastructures to process inputs in\na black-box fashion, less is known about how to exploit the various facets of\nthe data, before inputting this into an algorithm. Here, we focus on this\nlatter perspective, by proposing a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nThe resulting optimal flows can then be fed into standard classifiers to\ndistinguish images in different classes. We show how our method can outperform\ncompeting approaches on image classification tasks in datasets where color\ninformation matters.\nPublication Date: 2022-05-04\narXiv paper ID: 2205.02938v2\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)\nSummary: Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\nPublication Date: 2022-04-10\narXiv paper ID: 2204.04704v1\n\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n, Your response: ```SUMMARY\nedge image classification adaptive inference techniques\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification adaptive inference techniques, here was the response\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection to Suppress Adversarial Perturbation\nSummary: This paper presents a novel context-aware image denoising algorithm that\ncombines an adaptive image smoothing technique and color reduction techniques\nto remove perturbation from adversarial images. Adaptive image smoothing is\nachieved using auto-threshold canny edge detection to produce an accurate edge\nmap used to produce a blurred image that preserves more edge features. The\nproposed algorithm then uses color reduction techniques to reconstruct the\nimage using only a few representative colors. Through this technique, the\nalgorithm can reduce the effects of adversarial perturbations on images. We\nalso discuss experimental data on classification accuracy. Our results showed\nthat the proposed approach reduces adversarial perturbation in adversarial\nattacks and increases the robustness of the deep convolutional neural network\nmodels.\nPublication Date: 2021-01-14\narXiv paper ID: 2101.05833v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n, Your response: ```SUMMARY\nedge image classification early exit networks\n```\nStep #28, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit networks, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\n~~~~~~~~~~\nCurrent Step #29, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Single-Layer Vision Transformers for More Accurate Early Exits with Less OverheadArian Bakhtiarniaa,\u2217, Qi Zhanga, Alexandros Iosi\ufb01disaaDIGIT, Department of Electrical and Computer Engineering, Aarhus University, Finlandsgade 22, Aarhus, 8200, Midtjylland, DenmarkAbstractDeploying deep learning models in time-critical applications with limited computational resources, for instance in edgecomputing systems and IoT networks, is a challenging task that often relies on dynamic inference methods such as earlyexiting. In this paper, we introduce a novel architecture for early exiting based on the vision transformer architecture,as well as a \ufb01ne-tuning strategy that signi\ufb01cantly increase the accuracy of early exit branches compared to conventionalapproaches while introducing less overhead. Through extensive experiments on image and audio classi\ufb01cation as well asaudiovisual crowd counting, we show that our method works for both classi\ufb01cation and regression problems, and in bothsingle- and multi-modal settings. Additionally, we introduce a novel method for integrating audio and visual modalitieswithin early exits in audiovisual data analysis, that can lead to a more \ufb01ne-grained dynamic inference.Keywords: dynamic inference, early exiting, multi-exit architecture, vision transformer, multi-modal, deep learning1. IntroductionOver the past decade, deep learning has shown tremen-dous success across various \ufb01elds, such as computer vi-sion and natural language processing [1]. However, deeplearning models are by de\ufb01nition composed of many layersof interconnected neurons, even reaching billions of pa-rameters, which makes them computationally expensive.This has sparked a great deal of research in order to makedeep learning models more lightweight, for which manyapproaches have been proposed, for instance, model com-pression methods [2] such as quantization [3], pruning [4],low-rank approximation [5] and knowledge distillation [6].More and more emerging internet of things (IoT) appli-cations are integrating deep learning models, such as videosurveillance, voice assistants, augmented reality and coop-erative autonomous driving, which are often time-sensitiveand require inputs to be processed within speci\ufb01c deadlines[7, 8]. The heavy computational burden of deep learningbecomes problematic for these time-critical IoT applica-tions, due to resource-constrained IoT devices. Edge com-puting is a promising computing paradigm for addressingthis issue, in which the deep learning task is o\ufb04oaded toedge servers in the proximity of IoT devices.Since edge computing systems introduce computationo\ufb04oading over a communication network and involve mul-tiple nodes working collaboratively in order to completethe task in a timely manner, transmission time has tobe taken into account in addition to the deep learningcomputation time. However, transmission time may vary\u2217Corresponding authorEmail addresses: arianbakh@ece.au.dk (Arian Bakhtiarnia),qz@ece.au.dk (Qi Zhang), ai@ece.au.dk (Alexandros Iosi\ufb01dis)greatly over time and across di\ufb00erent channels. Conse-quently, deep learning models running on edge computingsystems and IoT networks should be capable of anytimeprediction, meaning they should be able to provide a validresponse even if they are interrupted before traversing theentire neural network, although the model is expected toprovide a better answer if it is allowed to run for longertime.Dynamic inference approaches [9] modify the computa-tion graph based on each input during the inference phasein order to \ufb01t the time constraints. A dynamic inferenceapproach that particularly suits anytime prediction isearlyexiting [10], also referred to as multi-exit architectures orauxiliary classi\ufb01ers in the literature. In multi-exit archi-tectures, one or more early exit branches are placed aftersome of the intermediate hidden layers of thebackbone net-work. The goal of each of these branches is to provide anearly result similar to the \ufb01nal result of the neural net-work using only the features extracted up to that particu-lar branch location. These early results are inevitably lessaccurate than the \ufb01nal result of the network. In order toachieve anytime prediction using early exiting, the latestearly result can be used whenever the execution is inter-rupted, for instance, whenever a hard deadline is reached.Computation time can be further decreased by applyingmodel compression techniques on the backbone of multi-exit architectures. Besides anytime prediction, early exit-ing can also be used in budgeted batch classi\ufb01cation wherea \ufb01xed amount of time is available in order to classify aset of input samples. In such a setting, the result of ear-lier branches can be used for \u201ceasier\u201d samples whereas theresult of later branches or the \ufb01nal result can be used for\u201charder\u201d ones. The di\ufb03culty of each sample can be de-termined based on the con\ufb01dence of the network about itsPreprint submitted to Neural Networks June 30, 2022arXiv:2105.09121v3  [cs.LG]  29 Jun 2022--- Page 2 ---output [11], although other approaches exist in the litera-ture [10].Early exit branches are expected to have a low over-head in terms of the extra computation they introduce,since a high overhead would defeat the purpose. There-fore, they often contain only a handful of layers. Ideally,we want the accuracy of the early results to be close to thatof the \ufb01nal result, since a higher accuracy for early exitbranches means that the overall reliability of the systemincreases. However, the low-overhead constraint makes itquite challenging to achieve a high accuracy since the earlyexit branches have signi\ufb01cantly less trainable parameterscompared to the rest of the network. Several approachesfor increasing the accuracy of early exits such as knowledgedistillation [12], curriculum learning [13] and architecturesdesigned speci\ufb01cally for early exit branches [14] have beensuggested. In this paper, we propose a novel architecturein order to obtain more accurate early exits for convolu-tional neural network (CNN) backbones.A neural architecture called vision transformer (ViT )[15] has been recently introduced for image classi\ufb01cationwhich is radically di\ufb00erent from convolutional neural net-works. The building blocks of Vision Transformer havebeen used for early exits placed on Vision Transformerbackbones [14], however, using Transformer-based earlyexit branches on CNN backbones is not intuitive and re-quires additional steps and architectural modi\ufb01cations. Weuse a modi\ufb01ed version of this architecture instead of theusual convolution and pooling layers in early exit branchesand show that our method can signi\ufb01cantly increase theaccuracy of early exits compared to conventional archi-tectures by fusing local and global receptive \ufb01elds 1. Thecontributions of this paper can be summarized as follows:\u2022 We propose a novel architecture for early exit branchesin multi-exit architectures based on vision transform-ers, called single-layer vision transformer (SL-ViT ).We compare our method with conventional CNN-based early exit architectures across 27 scenarios in-volving di\ufb00erent datasets, branch locations and back-bone networks and show that our method is signi\ufb01-cantly more accurate in 26 of these scenarios, whilehaving less overhead in terms of number of param-eters and \ufb02oating point operators (FLOPS). To thebest of our knowledge the fusion of global and localscope in early exits has never been used in multi-exitarchitectures before.\u2022 We show that our method is a general purpose ap-proach that works across di\ufb00erent modalities as wellas multi-modal settings by investigating image clas-si\ufb01cation, audio classi\ufb01cation and audiovisual crowdcounting scenarios. We also show that our methodworks for both classi\ufb01cation and regression prob-lems.1Our code will be available athttps://gitlab.au.dk/maleci/sl_vit.\u2022 We introduce a novel way of integrating audio andvisual features in early exits using vision transform-ers. To the best of our knowledge, this is the \ufb01rsttime early exits have been studied in multi-modalsettings.\u2022 We provide insight into why our method achievesbetter results compared to conventional CNN-basedarchitectures by investigating the role of attentionand receptive \ufb01eld.\u2022 We introduce a \ufb01ne-tuning strategy for SL-ViT calledcopycat single-layer vision transformer (CC-SL-ViT )which is based on the copycat strategy developed forCNNs [16] and show that this method can furtherincrease the accuracy of SL-ViT early exits. To thebest of our knowledge this is the \ufb01rst time the copy-cat strategy is used for vision transformers or earlyexits.The rest of this paper is organized as follows: Section2 provides an overview of the relevant literature; Section 3describes our proposed method in detail; Section 4 explainsthe details of our experiments; Section 5 showcases theexperiment results; and, \ufb01nally, Section 6 brie\ufb02y discussesthe results and concludes the paper.2. Related WorkThis section provides the necessary prerequisites forunderstanding our method and experiments. We startby describing the particulars of multi-exit architectures.Subsequently, we provide the details of the vision trans-former architecture, which is the foundation of the pro-posed method. Then, we brie\ufb02y touch on how audio clas-si\ufb01cation is normally carried out, which is included in sev-eral scenarios in our experiments. Finally, we explain an-other scenario investigated in our experiments, i.e. crowdcounting, and how it can be approached in a multi-modalmanner.2.1. Multi-Exit ArchitecturesIn order to describe multi-exit architectures, we use thesame notation as Scardapane et al. [10] where a neural net-work is formulated as a functionf(X) = fL(fL\u22121(...f1(X))).In this formulation Lsigni\ufb01es the total number of layers inthe network andfi is the operator corresponding to layeri,which can be a convolutional layer, a fully-connected layer,a normalization layer, or any other di\ufb00erentiable operator.hi = fi(hi\u22121) denotes the output of layeri, where h0 is theinput X. Finally, \u03b8i symbolizes the trainable parametersof layer i.Equation (1) formulates the training process for theneural network which is achieved by tuning its parametersusing an optimization algorithm on the landscape de\ufb01nedby a loss function. In this equation, the parameters of theneural network are denoted by \u03b8 = \u22c3Li=1 \u03b8i, the training2--- Page 3 ---samples are signi\ufb01ed by {(Xn,yn)}Nn=1, and l(\xb7,\xb7) is theloss function.f\u2217= arg min\u03b8N\u2211n=1l(yn,f(Xn)) (1)Extending this notation to multi-exit architectures,B \u2286{1,..,L }signi\ufb01es the set of selected branch locations afterwhich early exit branches will be placed. cb(hb) = yb is theclassi\ufb01er or regressor representing the early exit branch ateach branch location b, where yb denotes the early resultat that location. The schematic illustration of a multi-exitarchitecture is presented in Figure 1. However, since thereare multiple outputs, and thus multiple loss signals in amulti-exit architecture, its training is not as straightfor-ward.Figure 1: Schematic illustration of a multi-exit architecture with twoearly exits.Three di\ufb00erent approaches for training multi-exit ar-chitectures exist in the literature [10, 17, 13]. In the \ufb01rstapproach, called end-to-end training, the loss signals ofall exits are combined and backpropagated through thenetwork at the same time. With end-to-end training, thecontribution of each loss signal to the total loss is expressedwith weight values, which are therefore hyper-parametersof the model.The second approach, called layer-wise training, \ufb01rsttrains the network up to and including the \ufb01rst exit branch.Subsequently, the part of the network that has been trainedso far is frozen, meaning its parameters are not modi\ufb01edany further, and the remainder of the network up to andincluding the second exit branch is trained. This processcontinues until the entire network is trained. Note thatwith this approach, there is no guarantee that the accu-racy of the \ufb01nal exit remains unchanged.In the \ufb01nal approach, called classi\ufb01er-wise training,the backbone network is completely frozen and each branchis trained independent of the rest of the network and otherbranches, meaning the parameters \u03b8 are not modi\ufb01ed andonly the parameters of the classifers/regressors {cb},b \u2208Bare trained. With this approach, no new hyper-parametersare introduced and the backbone remains unchanged. How-ever, the early exit branches a\ufb00ect a lower number of train-able parameters compared to the other approaches.In this paper, we choose to follow the classi\ufb01er-wisetraining approach due to its practical importance. Thisis because with classi\ufb01er-wise training, early exit branchescan be easily added on top of existing backbone networkswithout the need for re-training and hyper-parameter op-timization, which can be computationally expensive andtime consuming. Furthermore, with end-to-end and layer-wise training strategies, the number of branches and theirplacement can lead to further trade-o\ufb00s and a\ufb00ect theoverall performance of the model. Since branches are inde-pendently trained in the classi\ufb01er-wise strategy, any num-ber of branches can exist and a branch can be placed atany location without a\ufb00ecting the performance of otherbranches or the backbone.It is important to mention that branches placed laterin the backbone network do not necessarily result in ahigher accuracy compared to branches placed earlier. Theusage of such branches would therefore not be sensiblesince earlier branches exist that require less computationand provide more accurate results. We hereby use the termimpractical to refer to such branches.As previously mentioned, there are several methodsthat try to improve the accuracy of early exits. The methodin [12] uses the combination of the distillation loss fromthe \ufb01nal exit and the loss signal from ground truth labelsto train more accurate early exits using in the end-to-endtraining setting. The method in [18] expands on this ideaby adding a third loss signal based on the di\ufb00erence be-tween features of the latest early exit with earlier exits.The method in [19] proposes a technique called gradientequilibrium to combat the problem of gradient imbalancethat surfaces when using the end-to-end strategy, whichis when the variance of the gradients becomes very largewhen loss signals from multiple exits are combined, lead-ing to unstable training. Moreover, this paper introducesforward and backward knowledge transfer that aims to en-courage collaboration among di\ufb00erent exits. The methodin [20] improves the accuracy of later exits by reusing pre-dictions from earlier exits. The method in [21] circumventsthe problem of impractical branches by adaptively select-ing the exit location based on time budget and the speci\ufb01cinput. The method in [22] simpli\ufb01es the design of multi-exit architectures by removing the hyper-parameters of theend-to-end training strategy that specify the contributionof each loss signal.Besides e\ufb03cient inference, early exits can prove use-ful in several other applications, for instance, the methodin [23] allows for parallel training of the segments of theDNN that exist between early exits, by training each seg-ment based on the loss signal of the next segment obtainedin the previous training stage. Moreover, early exits canbe added to the network during the training in order toincrease the accuracy of the backbone network and dis-carded after the training phase, for instance, the widelyused Inception model [24] was trained in this way.Besides early exiting, several other approaches existfor dynamic inference, for instance, layer skipping [25, 26,27, 28] where the execution of some of the layers of theDNN are skipped, and channel skipping [29] where lessimpactful channels of convolutional neural networks areignored and their computation is skipped during the infer-3--- Page 4 ---ence phase. However, unlike early exits, these approachescannot provide an output if the execution is interrupteddue to a strict deadline, as these methods need to performthe computations until the very last layer.2.2. Vision TransformerThe transformer architecture was \ufb01rst introduced byVaswani et al. [30] for natural language processing, andit has recently been adapted for solving computer visionproblems by Dosovitskiy et al. [15]. Vision transformerwas originally developed for the problem of image clas-si\ufb01cation, however, variations of vision transformer havesince been applied to many computer vision problems, suchas object detection, depth estimation, semantic segmenta-tion, image generation and action recognition, as well asmulti-modal data analysis tasks such as text-to-image syn-thesis and visual question answering [31, 32, 33].In order to describe the vision transformer architecture,we \ufb01rst explain the self-attention layer. The input of thislayer is in the form of a sequence X = (x1,...,x n) whereX \u2208Rn\xd7d and dis the embedding dimension to representeach entity. Its output is in the form of Z = (z1,...,z n)where Z \u2208Rn\xd7dv . The goal of self-attention is to capturethe interaction between the entities in the sequence. Forthis purpose, each vector xi in the sequence is transformedinto three separate vectors: the query vector qi \u2208Rdq ,the key vector ki \u2208Rdk and the value vector vi \u2208Rdv ,where dq = dk. To construct the output vector zi thatcorresponds to the input xi, for each vector xj in X (in-cluding xi itself), the scalar aij is calculated by the innerproduct of qi and kj. Output vector zi is then calculatedby summing the value vectors v1,...,v n weighted by theircorresponding scalars, that is, zi = \u2211nj=1 aijvj. The scalaraij basically speci\ufb01es how much attention the i-th entityshould pay to the j-th entity, since aij determines the con-tribution of vj to the combined output zi. In practice, thescalars are normalized by \u221adk and converted into proba-bilities using the softmax function.If the key, query and value vectors are packed into ma-trices Q= XWQ, K = XWK and V = XWV, where WQ,WK and WV are learnable weight matrices, the above op-eration can be rephrased as follows:Z = softmax(QKT\u221adk)V (2)In order to enable the model to capture more than onetype of relationship between the entities in the sequence,self-attention is extended to multi-head attention by con-catenating the output of h di\ufb00erent self-attention blocksZ1,...,Z h each with its own set of learnable weight ma-trices, into a single matrix Z\u2032 = [Z0,...,Z h] \u2208Rn\xd7h.dv ,which is then projected using a weight matrixW\u2032\u2208Rh.dv\xd7d.A transformer encoder is constructed by passing theinput sequence into a normalization layer, a multi-headattention layer, a second normalization layer and a multi-layer perceptron (MLP), respectively. Two residual con-nections are added, one by adding the input sequence tothe output of the multi-head attention, and the other byadding the output of the multi-head attention to the out-put of the MLP.Putting it all together, a vision transformer is createdby \ufb01rst splitting the input image into patches. Subse-quently, the sequence of patches is projected into a se-quence of vectors and a positional embedding is addedto the corresponding vector of each patch. An additionallearnable embedding called classi\ufb01cation token is added tothe beginning of the sequence. The sequence then passesthrough L transformer encoders. Finally, the \ufb01rst vectorin the output of the last transformer encoder, which cor-responds to the classi\ufb01cation token, is passed to a MLPwhich outputs the \ufb01nal classi\ufb01cation result. The architec-ture of vision transformer is depicted in Figure 2.Figure 2: The vision transformer (ViT) architecture for image clas-si\ufb01cation.ViT-EE is a method which uses transformer encodersfor early exits placed on ViT backbones [14]. ViT-EE usesthe exact same layer as the ViT backbone. Using the build-ing blocks of the backbone network for early exit branchesis simple and intuitive, and it is the reason why so far,mostly convolutional layers have been used for early exit-ing CNN backbones. However, as we show in this work,carefully designing the architecture of early exit branchescan lead to signi\ufb01cant improvements. Using Transformer-based early exit branches on CNN backbones is not intu-itive, and requires additional steps such as converting ten-sors to patches, dealing with the classi\ufb01cation token and\ufb01ne-tuning the architecture parameters including patchsize, attention heads, embedding representation, the sizeand number of layers for MLP, and dropout. Moreover,we show that removing the last residual connection in thetransformer encoder can improve the performance in somecases.Furthermore, ViT backbones have a global receptive\ufb01eld in every layer, this means that ViT-EE is not neces-sarily ideal for early exits at all layers, as it adds too muchoverhead without providing improvements in terms of re-4--- Page 5 ---ceptive \ufb01eld. On the other hand, CNN backbones have alimited receptive \ufb01eld particularly in earlier layers, there-fore fusing this receptive \ufb01eld with a global one leads toimprovements.2.3. Audio Classi\ufb01cationSimilar to image classi\ufb01cation, audio classi\ufb01cation isthe problem of categorizing a given audio waveform intoone of several predetermined classes. For instance, thegiven audio waveform could be a musical recording, andthe goal could be to specify which genre of music it belongsto. To represent the input features, spectrogramsobtainedby applying short-time Fourier transform (STFT) andMelspectrograms are commonly used [34], although raw audiowaveforms can been used as well [35]. Mel spectrogramsare spectrograms that are constructed using the Mel scalewhich is a nonlinear transformation of the frequency scaledesigned based on domain knowledge about the humanauditory system. Various deep learning models for au-dio classi\ufb01cation exist in the literature, including modelsthat are commonly used for image classi\ufb01cation, namelyResNet [36], DenseNet [37] and Inception [38], which havebeen shown to be quite e\ufb00ective for audio classi\ufb01cationas well [39]. Conveniently, the same three networks havepreviously been used as backbone networks when investi-gating early exiting for image classi\ufb01cation [13]. Thereforewe use these backbone networks for both image and audioclassi\ufb01cation in our experiments.2.4. Audiovisual Crowd CountingCrowd counting refers to the problem of identifying thetotal number of people present in a given image. Crowdcounting has many applications such as safety monitoring,disaster management, design of public spaces, intelligencegathering and analysis, creation of virtual environmentsand forensic search [40]. With many of these applications,it is vital for the model to perform in near real-time. How-ever, the input images in these scenarios often have highresolutions, such as HD or Full HD. Moreover, many ofthe available methods contain an immense number of pa-rameters [41]. This means that crowd counting modelsare often very computationally expensive, therefore, dy-namic inference methods such as early exiting and otherlightweight deep learning methods become essential in realworld applications.Although the main objective of this task is to obtain asingle count from an image, many methods treat this prob-lem as dense prediction where the output is a density mapdepicting the density of the crowd across the input image,and the total count is calculated by the sum of all valuesin the density map. Therefore, in most crowd countingdatasets, such as Shanghai Tech [42] and World Expo \u201910[43], the locations of the heads of individuals in the imageare annotated and provided as targets. A ground truthdensity map can then be obtained from these head anno-tations using Gaussian kernels or more complicated andFigure 3: An example image from the Shanghai Tech dataset and itscorresponding ground truth density map.specialized methods [41]. Figure 3 shows an image fromthe Shanghai Tech dataset and the ground truth densitymap that was generated from the provided head annota-tions using the method presented in Zhang et al [42]. Incrowd counting, Mean Absolute Error (MAE) is usuallyused as a measure of accuracy whereas Mean Squared Er-ror (MSE) is used as a measure of robustness [44].Many crowd counting methods exist in the literature[41], however, most of these methods are applied in asingle-modal fashion where the input is an image or a videoframe. In contrast, AudioCSRNet [45], a multi-modal ex-tension of the widely-used CSRNet model for crowd count-ing [46], takes as input the ambient audio of a scene inaddition to its image. The authors show that the ambi-ent audio improves the result in situations where the im-age quality is not ideal, for instance, low image resolution,presence of noise, occlusion and low illumination.In CSRNet, the features extracted from the input im-age by the \ufb01rst 10 layers of a VGG-16 [47] network pre-trained on the ImageNet dataset [48] are passed through6 dilated convolution layers and a 1 \xd71 convolution layerin order to obtain the density map. AudioCSRNet ex-tends this architecture by converting each of the dilatedconvolution layers into a fusion block. The architecture ofAudioCSRNet is depicted in Figure 4. First, a Mel spec-trogram is obtained from the raw audio waveform. Subse-quently, in each fusion block, the features extracted fromthe input Mel spectrogram by the \ufb01rst 6 layers of a VG-Gish [49] network pre-trained on the AudioSet dataset [49]are projected to two vectors called \u03b3 and \u03b2 which repre-sent the multiplicative and additive aspects of the audiofeatures. The \u03b3 and \u03b2 vectors are then tiled in order tomatch the size of the visual features. Finally, the outputof the dilated convolution is element-wise multiplied by \u03b3and added to \u03b2.The fusion operation can be summarized asvl+1 = Fl(\u03b3l \u2299Dl(vl) + \u03b2l), (3)where vl \u2208 RCl\xd7Wl\xd7Hl is the output of the l-th fusionblock, Fl denotes an activation function, \u03b3l and \u03b2l are thetiled vectors and Dl represents the l-th dilated convolu-tion.In practice, a batch normalization layer [50] is addedimmediately after each dilated convolution. Furthermore,the height and width of the intermediate features remain5--- Page 6 ---Figure 4: Architecture of AudioCSRNet.unchanged by using padding in the convolution operations,meaning Hl = Hl+1 and Wl = Wl+1. Additionally, sincethe \ufb01rst 10 layers of VGG-16 decrease both height andwidth by a factor of 8 via several pooling operations, the\ufb01nal result of the network needs to be upsampled by afactor of 8 in order to match the resolution of the inputimage. It is important to preserve the total sum of thedensity map during this upsampling operation, since itrepresents the total count.3. Single-Layer Vision Transformers for Early Ex-itsWe assume a pre-trained and high performing back-bone network is already available. Due to time constraintsarising from the particular application, it is desirable thatthe network provides a result within the speci\ufb01c deadlinerather than not providing a result at all, even though thisresult may be less accurate than it would be if time con-straints did not exist. Therefore, the backbone needs tobe augmented with early exit branches to allow for dy-namic inference and anytime prediction. As previouslymentioned, we use the classi\ufb01er-wise approach for trainingthe early exit branches since it results in \u201cplug-and-play\u201dbranches that can easily be added to the backbone networkwithout any re-training or hyper-parameter tuning.3.1. SL-ViTTypically, the architecture of early exit branches startswith one or more convolution layers, although some mayhave no convolutions at all. Afterwards, they may have apooling layer, which may be global pooling, and one MLP[51, 11]. Here, as a baseline, we choose to utilize the ar-chitecture depicted in Figure 5 with one 3 \xd73 convolution,followed by a 2 \xd72 max pooling layer and \ufb01nally a MLP.The size of the max pooling layer is increased to 4 \xd74 forcrowd counting since the input images have a very highresolution. Additionally, we use dropout [52] inside theMLP to avoid over\ufb01tting. We use a single convolutionsince early exits with two or more convolution layers havea high overhead and may even lead to lower accuracy [11].Early exits without convolutions are sometimes used verylate in the network, however, since they are straightfor-ward and leave no room for modi\ufb01cations, we do not applyour method for such cases. The resulting architecture is acommon setup within the literature, and is e\ufb00ectively thesame architecture used for earlier exits by Hu et al. [51].Figure 5: Architecture of CNN early exit branches. Size of the \ufb02at-tened feature vector depends on the dimensions of the features at thespeci\ufb01c branch location. For branches placed on the AudioCSRNetbackbone, max pooling size is increased to 4x4 since the input im-ages have a high resolution. Figure created using the NN-SVG tool[53].Our method called single-layer vision transformer orSL-ViT for short, is an alternative architecture for earlyexit branches that can achieve a higher accuracy comparedto the aforementioned baseline, while having less overheadin terms of the number of parameters as well as \ufb02oatingpoint operations per second (FLOPS). Our proposed ar-chitecture is based on the vision transformer architectureintroduced in section 2.2, where instead of the input im-age, we split the intermediate features at the branch loca-tion into patches (sub-tensors) and pass them to a visiontransformer.The choice of vision transformer architecture is pri-marily due to its global receptive \ufb01eld. Receptive \ufb01eld6--- Page 7 ---is crucial in many deep learning problems, including onesstudied in this work. The receptive \ufb01eld of state-of-the-art CNNs developed for image classi\ufb01cation has steadilyincreased over time and is correlated with increased classi-\ufb01cation accuracy [54]. Additionally, in audio classi\ufb01cationusing spectrograms, each location relates to a di\ufb00erent fre-quency band in a di\ufb00erent window of time. It is reasonableto assume that processing combinations of frequencies andwindows that are not necessarily adjacent could be of im-portance. Moreover, many crowd counting methods havemade use of global information through visual attentionmechanisms and dilated convolutions [41]. Since the re-ceptive \ufb01eld is particularly limited in early layers of CNNbackbones, choosing an architecture for early exit brancheswith a global receptive \ufb01eld could be bene\ufb01cial.Many other designs strive to increase the receptive \ufb01eldin their building blocks, for instance, the pyramid pool-ing module (PPM) in PSPNet [55] or atrous spatial pyra-mid pooling (ASPP) in DeepLab [56]. However, they allfall short in comparison with the global receptive \ufb01eld oftransformers. PPM increases the receptive \ufb01eld throughaggregating di\ufb00erent levels of pooling, which means far lo-cations have only access to coarse representations of eachother, and ASPP has holes in its receptive \ufb01eld.It is important to mention that the local receptive \ufb01eldof convolutional layers is not fundamentally bad. On thecontrary, it plays a key role in representation learning andextracting local information, especially in the early layersof the network where the receptive \ufb01eld of the convolu-tional \ufb01lters is small. Filters in successive convolutionallayers have increasingly larger receptive \ufb01elds, therefore,\ufb01nal layers in a CNN architecture have \ufb01lters of largeenough receptive \ufb01elds that can e\ufb00ectively aggregate in-formation from the entire input image to provide a properresponse. However, this process of cascading local recep-tive \ufb01elds of increasing size requires the number of layersin the CNN to be large, or at least all the layers in thenetwork to be traversed in order to provide the network\u2019sresponse. When an early exit is added at an early layer,this chain of increasingly larger receptive \ufb01elds is broken,and an early exit that has a local receptive \ufb01eld may notbe able to e\ufb00ectively aggregate all required informationin the image to provide a suitable response. This situa-tion is the motivation behind the proposed branch archi-tecture, which fuses the local receptive \ufb01eld of the layerin the network where the early exit branch is attached,with the global receptive \ufb01eld of the early exit, in orderto e\ufb00ectively aggregate information from the entire inputand provide a more accurate response. Indeed, the orig-inal Vision Transformer paper [15] attributes the successof their model to the combination of local and global re-ceptive \ufb01elds and shows that even in very early layers, thisability to integrate information globally is indeed used bythe model.There are some crucial di\ufb00erences between the origi-nal vision transformer and the architecture in our method.First, in order to introduce a low overhead for early exitbranches, we only use a single transformer encoder layerinstead of the original 12 to 36 layers, meaning that L= 1in our case. Secondly, we do not utilize a separate clas-si\ufb01cation token and instead pass the entire output of thetransformer encoder layer to the MLP head. This is pos-sible because the width and height of tensors are gener-ally reduced throughout CNN backbones by pooling oper-ations, and thus the number of patches in our architectureis lower than that of the original vision transformer. Inaddition to the number of patches, the size of the embed-ding dimension ( d) is also reduced in our proposed archi-tecture, introducing far less parameters when passing theentire output of the last transformer encoder layer to theMLP head, even with high-resolution inputs such as inour crowd counting experiments. Variations of our archi-tecture have 5 \xd75, 7 \xd77 or 16 \xd79 patches and embeddingdimensions of 32 or 36, whereas di\ufb00erent versions of theoriginal vision transformer have 14 \xd714 or 16 \xd716 patchesand embedding dimensions of 768, 1024 or 1280. We em-pirically found that using the entire transformer encoderoutput instead of just one classi\ufb01cation token can increasethe accuracy, perhaps because in a single-layer version,there are not enough layers for the classi\ufb01cation token tolearn to properly summarize other patches. Our proposedarchitecture is shown in Figure 6. It is also important tonote that the MLP head used in our architecture is exactlythe same as the MLP in the CNN early exit architecture.Figure 6: Architecture of SL-ViT early exit branches. Unlike typicalvision transformers, only a single transformer encoder layer is used,extra learnable classi\ufb01cation token is not added to the sequence andthe entire output of the transformer encoder is passed on to the MLPhead. The MLP head is the same as CNN early exit branches.Our model has several hyper-parameters, namely thesize of each patch, the embedding dimension d and thenumber of attention heads hin multi-head attention. Thepatch size creates a trade-o\ufb00 where smaller patches resultin a more \ufb01ne-grained attention mechanism while increas-ing the total number of parameters in a bi-quadratic fash-ion. Therefore, similar to the original vision transformer,we choose the size of the patch to be close to the square7--- Page 8 ---root of the height and width of the input features. We alsomake sure that the size of the patch can divide the size ofthe input features to avoid padding, for instance, a patchsize of 4 \xd74 for input features of size 28 \xd728. We performa grid search to \ufb01nd the values of d and h that result inthe highest accuracy, while keeping the total number ofparameters less than or equal to that of the CNN earlyexit counterpart.At a \ufb01rst glance, it might seem like the SL-ViT archi-tecture introduces more hyper-parameters than the con-ventional CNN architecture, however, the CNN architec-ture includes many design choices as well, such as the num-ber of \ufb01lters, \ufb01lter size, padding, dilation, stride, poolingtype and pooling size. The design choices for CNN archi-tectures might seem simpler since they have been studiedmore extensively compared to vision transformers whichwere introduced more recently.3.2. Audiovisual SL-ViTWith audiovisual backbones such as the AudioCSR-Net model for audiovisual crowd counting, described insection 2.4, it is desirable to have audiovisual early exitsthat use both visual and audio features in order to achievea higher accuracy. The simplest way to have such branchesis to add the branches after the blocks where the fusionof visual and audio features take place. However, withour proposed SL-ViT architecture, it is also possible toinclude audio features as one or more patches alongsideother patches, and directly fuse the features in the earlyexit. The advantage of this approach is that since in vi-sion transformers, any of the patches can pay attention toany other patch, the visual features can be fused with theaudio features without being directly impacted and mod-i\ufb01ed. In contrast, since convolutional \ufb01lters only take theimmediate vicinity into account, the audio features mustbe present in every location. One option is to concatenatethe visual features and the tiled audio features along thedepth. However, that would greatly increase the amount ofcomputation for each fusion operation, therefore intrusiveoperations such as element-wise multiplication and addi-tion are used instead.3.3. Copycat SL-ViTFinally, we introduce a \ufb01ne-tuning strategy for SL-ViTbranches that can further increase their accuracy. Correia-Silva et al. [16] developed a method called copycat CNNwhere they create a \u201cfake\u201d dataset by taking images fromanother domain, giving them as input to a network trainedon the target domain, and recording the output of thenetwork as labels for these images. For instance, imagesfrom the ImageNet dataset [48] can be given to a networktrained on the CIFAR-10 dataset [57], where the imageof a camel may be labelled as a \u201cdog\u201d since there are nolabels for \u201ccamel\u201d in CIFAR-10. This fake dataset is thencombined with a dataset for the target domain and usedto train a new network. We use this strategy to \ufb01ne-tunean already trained SL-ViT branch and obtain a copycatsingle-layer vision transformer (CC-SL-ViT ). Note thatthe ratio of the fake data mixed with the available datasetis a hyper-parameter of this \ufb01ne-tuning strategy.4. Experimental SetupIn this section, we provide the details of our experi-ments. We begin by giving a short summary of the datasetsas well as the training details for the backbone networks.We then lay out the details of the branch architectures,their training procedure and their placement on the back-bone networks, and \ufb01nally explain how the copycat strat-egy was used to \ufb01ne-tune the branches.A total of 27 di\ufb00erent scenarios were tested in our ex-periments. For both image and audio classi\ufb01cation, twodatasets, three backbone networks and two di\ufb00erent branchlocations on each backbone were tested. In addition, threedi\ufb00erent branch locations for the audiovisual crowd count-ing backbone network were covered. All experiments wererepeated 5 times and the average accuracy as well as thestandard deviation were recorded. 4 \xd7 Nvidia 2080TiGPUs were used for the training of our models.4.1. Datasets4.1.1. CIFAR-10 and CIFAR-100These are widely-used datasets for image classi\ufb01cation[57]. Both datasets consist of 60,000 color images of size32 \xd732 pixels and their corresponding class labels. Theimages in CIFAR-10 and CIFAR-100 are categorized into10 and 100 di\ufb00erent classes, respectively. We use 40,000examples for training, 10,000 for validation and another10,000 for testing. Since our backbone networks are pre-trained on ImageNet which consists of 224 \xd7224 pixelimages, we resize each image to these dimensions beforepassing them into the network.4.1.2. Speech Commands (SC)A well-known audio dataset of spoken words [58]. Itconsists of 100,503 1-second audio clips with a samplingrate of 16kHz, each labelled as one of 12 classes: 10 dif-ferent spoken words such as \u201cYes\u201d, \u201cNo\u201d, \u201cDown\u201d and\u201cStop\u201d as well as one class for background noise and an-other for unknown words. We use 85,511 examples fortraining, 10,102 for validation and 4,890 for testing. Weconvert the raw audio waveforms into spectrograms usingshort-time Fourier transform (STFT) with a window sizeof 255 samples and step size of 128 samples, and resize theresulting spectrograms to 224 \xd7224 before passing theminto the network.4.1.3. GTZANIt is the most widely-used dataset for music genre recog-nition [59]. The original dataset consists of 10 genres suchas \u201cPop\u201d and \u201cRock\u201d and 100 30-second audio clips pergenre with a sampling rate of 22,050Hz. We follow the8--- Page 9 ---common approach to split each audio clip into 10 separate3-second clips in order to increase the size of the datasetto 10,000. We use 8,000 examples for training, 1,000 forvalidation and another 1,000 for testing. Following the ap-proach of Palanisamy et al. [39] where di\ufb00erent spectro-grams with di\ufb00erent parameters are placed in each channelof the input image, we use one spectrogram obtained fromSTFT with window size of 512 samples and step size of 256samples as well as two Mel spectrograms with 128 bins andwindow sizes of 100 and 50 milliseconds, and step sizes of50 and 25 milliseconds, respectively.4.1.4. DISCOAn audiovisual dataset for crowd counting which con-tains 1,935 images of Full HD resolution (1920 \xd71080)[45]. For each image, a corresponding 1-second audio clipof ambient sounds with a sampling rate of 48kHz, starting0.5 seconds before the image was taken and ending 0.5 sec-onds afterwards, exists as well. The labels are provided inthe form of head annotations in the image. At the time ofthis writing, DISCO is the only publicly available datasetfor audiovisual crowd counting. We use 1435 examples fortraining, 200 for validation and 300 for testing. The inputimage is resized to 1024 \xd7576 pixels to reduce memoryand computation requirements. Similar to Hershey et al.[49], the input audio waveform is transformed into a Melspectrogram with 64 bins, window size of 25 millisecondsand step size of 10 milliseconds. Following Hu et al. [45]the ground truth density maps are obtained by convolv-ing the head annotations with a 15 \xd715 Gaussian kernelK\u223cN (0,4.0).4.2. Backbone networksTransfer learning is used to train the ResNet152,DenseNet201 and InceptionV3 backbone networks for bothimage and audio classi\ufb01cation. The backbone networks areall pre-trained on the ImageNet dataset and the top layeris replaced. We found that instead of adding just one denselayer at the top, as is common in transfer learning, usingtwo dense layers and a dropout layer in between leads to ahigher accuracy in our case. The resulting network is thentrained using the Adam optimizer [60] with a learning rateof 10\u22124 and categorical cross-entropy loss function. Thelearning rate is reduced by a factor of 0.6 on plateau witha tolerance of 2 epochs, and an early stopping mechanismwith a tolerance of 5 epochs is used.The audiovisual crowd counting backbone is trained intwo stages. We \ufb01rst train a network with the AudioCSR-Net architecture described in Section 2.4 for 100 epochs.L2 norm is used as loss function and AdamW [61] with alearning rate of 10 \u22125 and weight decay of 10 \u22124 is used asoptimizer, where the learning rate is multiplied by a factorof 0.99 each epoch. This is the same training procedureused in the original paper [45]. Subsequently, in order toconvert the problem from dense prediction to regression,a dense layer with an output size of one is added after thelast layer of the trained AudioCSRNet. This layer is ini-tialized as a sum, meaning the initial weights are all equalto one and no bias is used. Then the entire network isre-trained for another 100 epochs using MAE as loss func-tion instead of the previous L2 loss, a learning rate of 10\u22126and weight decay of 10 \u22125. The learning rate is similarlymultiplied by a factor of 0.99 every epoch. The resultingmodel achieves a MAE of 13.63 which is even lower thanthe MAE of 14.27 reported in the original paper. However,the output of the network is just a single number repre-senting the total count instead of a density map. The \ufb01nalaccuracy of all trained backbones can be seen in Table 1.When training the backbone networks, in order to \ufb01tthe limitations of our available computational resources,the batch sizes are adjusted and some layers of the back-bone networks are frozen. All backbone networks weretrained with a batch size of 32 except AudioCSRNet whichhas a batch size of 4 as well as InceptionV3 when trainedon CIFAR-10 and CIFAR-100 which has a batch size of 64.All layers of the backbone networks were trained, exceptin the case of ResNet152 and DenseNet201 when trainedon CIFAR-10 and CIFAR-100 where only the batch nor-malization layers were trained. We found that trainingonly the batch normalization layers is su\ufb03cient to achievea high-performing backbone network in these cases [62].Table 1: Performance of backbone networks on each datasetBackboneCIFAR-10 Acc. CIFAR-100 Acc. SC Acc. GTZAN Acc. DISCO MAEResNet152 95.36% 82.25% 95.85% 91.29% -DenseNet20196.48% 82.53% 96.36% 92.09% -InceptionV396.56% 83.80% 94.93% 87.79% -AudioCSRNet- - - - 13.634.3. BranchesAll branches were trained from scratch using the Heinitialization method [63] and the Adam optimizer with alearning rate of 10 \u22124 where the learning rate is reducedby a factor of 0.6 on plateau with a tolerance of 2 epochs,and an early stopping mechanism with a tolerance of 5epochs is utilized. The branches on classi\ufb01cation back-bones use a categorical cross-entropy loss function whereasthe branches on the audiovisual crowd counting backboneuse mean absolute error loss. The training batch size forbranches were 64 in scenarios involving CIFAR-10, CIFAR-100 and Speech Commands, 32 in scenarios involvingGTZAN and 4 in scenarios involving DISCO.Table 2 shows the location of the branches placed oneach backbone network. For the AudioCSRNet backbonenetwork, branch V1 uses only the output of the VGG-16layers, therefore, it only has access to the visual features.Branch AV1 uses the outputs of both VGG-16 and VG-Gish, therefore it has access to both audio and visual fea-tures. In this branch location, the fusion of audio andvisual features is performed as described in Section 3 forthe SL-ViT architecture, and similar to the fusion blocks9--- Page 10 ---in AudioCSRNet for the CNN architecture, however, with-out dilation. Finally, branch AV2 is placed after the \ufb01rstfusion block in AudioCSRNet, therefore audio and visualfeatures have already been fused and thus fusion operationis not required within the branches. Adding branches afterthe second fusion block or later would not be reasonablesince more than 85% of the computation of the backboneis carried out before that point, and thus the accelerationresulting from early exits would be negligible.Table 2: Placement of branches for each backbone betworkBackbone BN \u2217 Branch Placed AfterDenseNet201 1 Transition Layer 12 Transition Layer 2ResNet152 1 12th Convolution2 36th ConvolutionInceptionV3 1 First Filter Concat2 Second Filter ConcatAudioCSRNet V1 Last Layer of VGGAV1 Last Layers of VGG and VGGishAV2 First Fusion Block\u2217Branch Number4.4. SL-ViT and CC-SL-ViT ParametersTable 3 summarizes the hyper-parameters used for theSL-ViT branches in each scenario. \u201cPatch Size\u201d shows thewidth and height of each image patch, \u201cPatches\u201d denotesthe resulting number of patches across width and heightof the input image, d is the size of embedding dimensionand h is the number of heads in multi-head attention.For copycat SL-ViT, images from the Tiny ImageNetdataset, which are the images from ImageNet down-sampledto 32\xd732, were given to the InceptionV3 backbone trainedon CIFAR-10, and the outputs were used to create the fakedataset. Then the fake dataset was mixed with CIFAR-10with a 2-to-1 ratio and used for re-training.Table 3: Hyper-parameters of SL-ViT for di\ufb00erent backbone net-works and branchesBackbone Dataset BN \u2217 Patch Size Patchesd hDenseNet201 all all 4x4 7x7 32 12ResNet152 SC 2 4x4 7x7 32 24GTZAN 2 4x4 7x7 32 24Other 4x4 7x7 32 12InceptionV3 CIFAR-100 all 5x5 5x5 36 8Other 5x5 5x5 32 12AudioCSRNet DISCO all 8x8 16x9 32 12\u2217Branch Number5. ResultsThe results of our experiments are presented in Tables4 to 8. In these Tables, the \ufb01nal accuracy, the total FLOPSof the model up to and including the branch and the num-ber of parameters of just the early exit branch are com-pared between the CNN architecture and the SL-ViT ar-chitecture. Higher accuracies, lower errors, lower numberof parameters and lower total FLOPS are highlighted inthese tables. Furthermore, the acceleration caused by SL-ViT early exits, de\ufb01ned as the total FLOPS of the back-bone network divided by the total FLOPS of the modelup to and including the SL-ViT branch, is also provided.Several observations can be made about these results.First, in all scenarios except one, SL-ViT early exits achievea signi\ufb01cantly higher accuracy. Even in the one excep-tional scenario, namely branch 2 of ResNet152 in Table6, the accuracy of SL-ViT is very close to its CNN coun-terpart. Secondly, while in some cases SL-ViT brancheshave an equal number of parameters compared to CNNbranches, in all scenarios, the total FLOPS of SL-ViTbranches is \n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2105.09121v3\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 31:
\n~~~~~~~~~~\nHistory: Step #13, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification spiking neural networks, here was the response\nTitle: Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes\nSummary: Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\nPublication Date: 2020-01-31\narXiv paper ID: 2002.00860v4\n\nTitle: An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks\nSummary: Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\nPublication Date: 2021-10-19\narXiv paper ID: 2110.10217v1\n\nTitle: Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network\nSummary: Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI)\nclassification tasks, but its high energy consumption and complex network\nstructure make it difficult to directly apply it to edge computing devices. At\npresent, spiking neural networks (SNN) have developed rapidly in HSI\nclassification tasks due to their low energy consumption and event driven\ncharacteristics. However, it usually requires a longer time step to achieve\noptimal accuracy. In response to the above problems, this paper builds a\nspiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF)\nneuron model for HSI classification tasks. The network uses the spiking width\nmixed residual (SWMR) module as the basic unit to perform feature extraction\noperations. The spiking width mixed residual module is composed of spiking\nmixed convolution (SMC), which can effectively extract spatial-spectral\nfeatures. Secondly, this paper designs a simple and efficient arcsine\napproximate derivative (AAD), which solves the non-differentiable problem of\nspike firing by fitting the Dirac function. Through AAD, we can directly train\nsupervised spike neural networks. Finally, this paper conducts comparative\nexperiments with multiple advanced HSI classification algorithms based on\nspiking neural networks on six public hyperspectral data sets. Experimental\nresults show that the AAD function has strong robustness and a good fitting\neffect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time\nstep reduction of about 84%, training time, and testing time reduction of about\n63% and 70% at the same accuracy. This study solves the key problem of SNN\nbased HSI classification algorithms, which has important practical significance\nfor promoting the practical application of HSI classification algorithms in\nedge devices such as spaceborne and airborne devices.\nPublication Date: 2024-09-18\narXiv paper ID: 2409.11619v1\n\nTitle: LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding\nSummary: The biological neurons use precise spike times, in addition to the spike\nfiring rate, to communicate with each other. The time-to-first-spike (TTFS)\ncoding is inspired by such biological observation. However, there is a lack of\neffective solutions for training TTFS-based spiking neural network (SNN). In\nthis paper, we put forward a simple yet effective network conversion algorithm,\nwhich is referred to as LC-TTFS, by addressing two main problems that hinder an\neffective conversion from a high-performance artificial neural network (ANN) to\na TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping\nbetween the activation values of an ANN and the spike times of an SNN on a\nnumber of challenging AI tasks, including image classification, image\nreconstruction, and speech enhancement. With TTFS coding, we can achieve up to\norders of magnitude saving in computation over ANN and other rate-based SNNs.\nThe study, therefore, paves the way for deploying ultra-low-power TTFS-based\nSNNs on power-constrained edge computing platforms.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14978v1\n\nTitle: A Deep Unsupervised Feature Learning Spiking Neural Network with Binarized Classification Layers for EMNIST Classification using SpykeFlow\nSummary: End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\nPublication Date: 2020-02-26\narXiv paper ID: 2002.11843v4\n, Your response: ```FULL_TEXT\n2002.00860v4\n```\nStep #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #25, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge image classification dynamic computation optimization\n```\nStep #26, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification dynamic computation optimization, here was the response\nTitle: Immiscible Color Flows in Optimal Transport Networks for Image Classification\nSummary: In classification tasks, it is crucial to meaningfully exploit the\ninformation contained in data. While much of the work in addressing these tasks\nis devoted to building complex algorithmic infrastructures to process inputs in\na black-box fashion, less is known about how to exploit the various facets of\nthe data, before inputting this into an algorithm. Here, we focus on this\nlatter perspective, by proposing a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nThe resulting optimal flows can then be fed into standard classifiers to\ndistinguish images in different classes. We show how our method can outperform\ncompeting approaches on image classification tasks in datasets where color\ninformation matters.\nPublication Date: 2022-05-04\narXiv paper ID: 2205.02938v2\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)\nSummary: Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\nPublication Date: 2022-04-10\narXiv paper ID: 2204.04704v1\n\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n, Your response: ```SUMMARY\nedge image classification adaptive inference techniques\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification adaptive inference techniques, here was the response\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection to Suppress Adversarial Perturbation\nSummary: This paper presents a novel context-aware image denoising algorithm that\ncombines an adaptive image smoothing technique and color reduction techniques\nto remove perturbation from adversarial images. Adaptive image smoothing is\nachieved using auto-threshold canny edge detection to produce an accurate edge\nmap used to produce a blurred image that preserves more edge features. The\nproposed algorithm then uses color reduction techniques to reconstruct the\nimage using only a few representative colors. Through this technique, the\nalgorithm can reduce the effects of adversarial perturbations on images. We\nalso discuss experimental data on classification accuracy. Our results showed\nthat the proposed approach reduces adversarial perturbation in adversarial\nattacks and increases the robustness of the deep convolutional neural network\nmodels.\nPublication Date: 2021-01-14\narXiv paper ID: 2101.05833v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n, Your response: ```SUMMARY\nedge image classification early exit networks\n```\nStep #28, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit networks, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\nStep #29, Phase: literature review, Feedback: 3\n--- Page 1 ---Single-Layer Vision Transformers for More Accurate Early Exits with Less OverheadArian Bakhtiarniaa,\u2217, Qi Zhanga, Alexandros Iosi\ufb01disaaDIGIT, Department of Electrical and Computer Engineering, Aarhus University, Finlandsgade 22, Aarhus, 8200, Midtjylland, DenmarkAbstractDeploying deep learning models in time-critical applications with limited computational resources, for instance in edgecomputing systems and IoT networks, is a challenging task that often relies on dynamic inference methods such as earlyexiting. In this paper, we introduce a novel architecture for early exiting based on the vision transformer architecture,as well as a \ufb01ne-tuning strategy that signi\ufb01cantly increase the accuracy of early exit branches compared to conventionalapproaches while introducing less overhead. Through extensive experiments on image and audio classi\ufb01cation as well asaudiovisual crowd counting, we show that our method works for both classi\ufb01cation and regression problems, and in bothsingle- and multi-modal settings. Additionally, we introduce a novel method for integrating audio and visual modalitieswithin early exits in audiovisual data analysis, that can lead to a more \ufb01ne-grained dynamic inference.Keywords: dynamic inference, early exiting, multi-exit architecture, vision transformer, multi-modal, deep learning1. IntroductionOver the past decade, deep learning has shown tremen-dous success across various \ufb01elds, such as computer vi-sion and natural language processing [1]. However, deeplearning models are by de\ufb01nition composed of many layersof interconnected neurons, even reaching billions of pa-rameters, which makes them computationally expensive.This has sparked a great deal of research in order to makedeep learning models more lightweight, for which manyapproaches have been proposed, for instance, model com-pression methods [2] such as quantization [3], pruning [4],low-rank approximation [5] and knowledge distillation [6].More and more emerging internet of things (IoT) appli-cations are integrating deep learning models, such as videosurveillance, voice assistants, augmented reality and coop-erative autonomous driving, which are often time-sensitiveand require inputs to be processed within speci\ufb01c deadlines[7, 8]. The heavy computational burden of deep learningbecomes problematic for these time-critical IoT applica-tions, due to resource-constrained IoT devices. Edge com-puting is a promising computing paradigm for addressingthis issue, in which the deep learning task is o\ufb04oaded toedge servers in the proximity of IoT devices.Since edge computing systems introduce computationo\ufb04oading over a communication network and involve mul-tiple nodes working collaboratively in order to completethe task in a timely manner, transmission time has tobe taken into account in addition to the deep learningcomputation time. However, transmission time may vary\u2217Corresponding authorEmail addresses: arianbakh@ece.au.dk (Arian Bakhtiarnia),qz@ece.au.dk (Qi Zhang), ai@ece.au.dk (Alexandros Iosi\ufb01dis)greatly over time and across di\ufb00erent channels. Conse-quently, deep learning models running on edge computingsystems and IoT networks should be capable of anytimeprediction, meaning they should be able to provide a validresponse even if they are interrupted before traversing theentire neural network, although the model is expected toprovide a better answer if it is allowed to run for longertime.Dynamic inference approaches [9] modify the computa-tion graph based on each input during the inference phasein order to \ufb01t the time constraints. A dynamic inferenceapproach that particularly suits anytime prediction isearlyexiting [10], also referred to as multi-exit architectures orauxiliary classi\ufb01ers in the literature. In multi-exit archi-tectures, one or more early exit branches are placed aftersome of the intermediate hidden layers of thebackbone net-work. The goal of each of these branches is to provide anearly result similar to the \ufb01nal result of the neural net-work using only the features extracted up to that particu-lar branch location. These early results are inevitably lessaccurate than the \ufb01nal result of the network. In order toachieve anytime prediction using early exiting, the latestearly result can be used whenever the execution is inter-rupted, for instance, whenever a hard deadline is reached.Computation time can be further decreased by applyingmodel compression techniques on the backbone of multi-exit architectures. Besides anytime prediction, early exit-ing can also be used in budgeted batch classi\ufb01cation wherea \ufb01xed amount of time is available in order to classify aset of input samples. In such a setting, the result of ear-lier branches can be used for \u201ceasier\u201d samples whereas theresult of later branches or the \ufb01nal result can be used for\u201charder\u201d ones. The di\ufb03culty of each sample can be de-termined based on the con\ufb01dence of the network about itsPreprint submitted to Neural Networks June 30, 2022arXiv:2105.09121v3  [cs.LG]  29 Jun 2022--- Page 2 ---output [11], although other approaches exist in the litera-ture [10].Early exit branches are expected to have a low over-head in terms of the extra computation they introduce,since a high overhead would defeat the purpose. There-fore, they often contain only a handful of layers. Ideally,we want the accuracy of the early results to be close to thatof the \ufb01nal result, since a higher accuracy for early exitbranches means that the overall reliability of the systemincreases. However, the low-overhead constraint makes itquite challenging to achieve a high accuracy since the earlyexit branches have signi\ufb01cantly less trainable parameterscompared to the rest of the network. Several approachesfor increasing the accuracy of early exits such as knowledgedistillation [12], curriculum learning [13] and architecturesdesigned speci\ufb01cally for early exit branches [14] have beensuggested. In this paper, we propose a novel architecturein order to obtain more accurate early exits for convolu-tional neural network (CNN) backbones.A neural architecture called vision transformer (ViT )[15] has been recently introduced for image classi\ufb01cationwhich is radically di\ufb00erent from convolutional neural net-works. The building blocks of Vision Transformer havebeen used for early exits placed on Vision Transformerbackbones [14], however, using Transformer-based earlyexit branches on CNN backbones is not intuitive and re-quires additional steps and architectural modi\ufb01cations. Weuse a modi\ufb01ed version of this architecture instead of theusual convolution and pooling layers in early exit branchesand show that our method can signi\ufb01cantly increase theaccuracy of early exits compared to conventional archi-tectures by fusing local and global receptive \ufb01elds 1. Thecontributions of this paper can be summarized as follows:\u2022 We propose a novel architecture for early exit branchesin multi-exit architectures based on vision transform-ers, called single-layer vision transformer (SL-ViT ).We compare our method with conventional CNN-based early exit architectures across 27 scenarios in-volving di\ufb00erent datasets, branch locations and back-bone networks and show that our method is signi\ufb01-cantly more accurate in 26 of these scenarios, whilehaving less overhead in terms of number of param-eters and \ufb02oating point operators (FLOPS). To thebest of our knowledge the fusion of global and localscope in early exits has never been used in multi-exitarchitectures before.\u2022 We show that our method is a general purpose ap-proach that works across di\ufb00erent modalities as wellas multi-modal settings by investigating image clas-si\ufb01cation, audio classi\ufb01cation and audiovisual crowdcounting scenarios. We also show that our methodworks for both classi\ufb01cation and regression prob-lems.1Our code will be available athttps://gitlab.au.dk/maleci/sl_vit.\u2022 We introduce a novel way of integrating audio andvisual features in early exits using vision transform-ers. To the best of our knowledge, this is the \ufb01rsttime early exits have been studied in multi-modalsettings.\u2022 We provide insight into why our method achievesbetter results compared to conventional CNN-basedarchitectures by investigating the role of attentionand receptive \ufb01eld.\u2022 We introduce a \ufb01ne-tuning strategy for SL-ViT calledcopycat single-layer vision transformer (CC-SL-ViT )which is based on the copycat strategy developed forCNNs [16] and show that this method can furtherincrease the accuracy of SL-ViT early exits. To thebest of our knowledge this is the \ufb01rst time the copy-cat strategy is used for vision transformers or earlyexits.The rest of this paper is organized as follows: Section2 provides an overview of the relevant literature; Section 3describes our proposed method in detail; Section 4 explainsthe details of our experiments; Section 5 showcases theexperiment results; and, \ufb01nally, Section 6 brie\ufb02y discussesthe results and concludes the paper.2. Related WorkThis section provides the necessary prerequisites forunderstanding our method and experiments. We startby describing the particulars of multi-exit architectures.Subsequently, we provide the details of the vision trans-former architecture, which is the foundation of the pro-posed method. Then, we brie\ufb02y touch on how audio clas-si\ufb01cation is normally carried out, which is included in sev-eral scenarios in our experiments. Finally, we explain an-other scenario investigated in our experiments, i.e. crowdcounting, and how it can be approached in a multi-modalmanner.2.1. Multi-Exit ArchitecturesIn order to describe multi-exit architectures, we use thesame notation as Scardapane et al. [10] where a neural net-work is formulated as a functionf(X) = fL(fL\u22121(...f1(X))).In this formulation Lsigni\ufb01es the total number of layers inthe network andfi is the operator corresponding to layeri,which can be a convolutional layer, a fully-connected layer,a normalization layer, or any other di\ufb00erentiable operator.hi = fi(hi\u22121) denotes the output of layeri, where h0 is theinput X. Finally, \u03b8i symbolizes the trainable parametersof layer i.Equation (1) formulates the training process for theneural network which is achieved by tuning its parametersusing an optimization algorithm on the landscape de\ufb01nedby a loss function. In this equation, the parameters of theneural network are denoted by \u03b8 = \u22c3Li=1 \u03b8i, the training2--- Page 3 ---samples are signi\ufb01ed by {(Xn,yn)}Nn=1, and l(\xb7,\xb7) is theloss function.f\u2217= arg min\u03b8N\u2211n=1l(yn,f(Xn)) (1)Extending this notation to multi-exit architectures,B \u2286{1,..,L }signi\ufb01es the set of selected branch locations afterwhich early exit branches will be placed. cb(hb) = yb is theclassi\ufb01er or regressor representing the early exit branch ateach branch location b, where yb denotes the early resultat that location. The schematic illustration of a multi-exitarchitecture is presented in Figure 1. However, since thereare multiple outputs, and thus multiple loss signals in amulti-exit architecture, its training is not as straightfor-ward.Figure 1: Schematic illustration of a multi-exit architecture with twoearly exits.Three di\ufb00erent approaches for training multi-exit ar-chitectures exist in the literature [10, 17, 13]. In the \ufb01rstapproach, called end-to-end training, the loss signals ofall exits are combined and backpropagated through thenetwork at the same time. With end-to-end training, thecontribution of each loss signal to the total loss is expressedwith weight values, which are therefore hyper-parametersof the model.The second approach, called layer-wise training, \ufb01rsttrains the network up to and including the \ufb01rst exit branch.Subsequently, the part of the network that has been trainedso far is frozen, meaning its parameters are not modi\ufb01edany further, and the remainder of the network up to andincluding the second exit branch is trained. This processcontinues until the entire network is trained. Note thatwith this approach, there is no guarantee that the accu-racy of the \ufb01nal exit remains unchanged.In the \ufb01nal approach, called classi\ufb01er-wise training,the backbone network is completely frozen and each branchis trained independent of the rest of the network and otherbranches, meaning the parameters \u03b8 are not modi\ufb01ed andonly the parameters of the classifers/regressors {cb},b \u2208Bare trained. With this approach, no new hyper-parametersare introduced and the backbone remains unchanged. How-ever, the early exit branches a\ufb00ect a lower number of train-able parameters compared to the other approaches.In this paper, we choose to follow the classi\ufb01er-wisetraining approach due to its practical importance. Thisis because with classi\ufb01er-wise training, early exit branchescan be easily added on top of existing backbone networkswithout the need for re-training and hyper-parameter op-timization, which can be computationally expensive andtime consuming. Furthermore, with end-to-end and layer-wise training strategies, the number of branches and theirplacement can lead to further trade-o\ufb00s and a\ufb00ect theoverall performance of the model. Since branches are inde-pendently trained in the classi\ufb01er-wise strategy, any num-ber of branches can exist and a branch can be placed atany location without a\ufb00ecting the performance of otherbranches or the backbone.It is important to mention that branches placed laterin the backbone network do not necessarily result in ahigher accuracy compared to branches placed earlier. Theusage of such branches would therefore not be sensiblesince earlier branches exist that require less computationand provide more accurate results. We hereby use the termimpractical to refer to such branches.As previously mentioned, there are several methodsthat try to improve the accuracy of early exits. The methodin [12] uses the combination of the distillation loss fromthe \ufb01nal exit and the loss signal from ground truth labelsto train more accurate early exits using in the end-to-endtraining setting. The method in [18] expands on this ideaby adding a third loss signal based on the di\ufb00erence be-tween features of the latest early exit with earlier exits.The method in [19] proposes a technique called gradientequilibrium to combat the problem of gradient imbalancethat surfaces when using the end-to-end strategy, whichis when the variance of the gradients becomes very largewhen loss signals from multiple exits are combined, lead-ing to unstable training. Moreover, this paper introducesforward and backward knowledge transfer that aims to en-courage collaboration among di\ufb00erent exits. The methodin [20] improves the accuracy of later exits by reusing pre-dictions from earlier exits. The method in [21] circumventsthe problem of impractical branches by adaptively select-ing the exit location based on time budget and the speci\ufb01cinput. The method in [22] simpli\ufb01es the design of multi-exit architectures by removing the hyper-parameters of theend-to-end training strategy that specify the contributionof each loss signal.Besides e\ufb03cient inference, early exits can prove use-ful in several other applications, for instance, the methodin [23] allows for parallel training of the segments of theDNN that exist between early exits, by training each seg-ment based on the loss signal of the next segment obtainedin the previous training stage. Moreover, early exits canbe added to the network during the training in order toincrease the accuracy of the backbone network and dis-carded after the training phase, for instance, the widelyused Inception model [24] was trained in this way.Besides early exiting, several other approaches existfor dynamic inference, for instance, layer skipping [25, 26,27, 28] where the execution of some of the layers of theDNN are skipped, and channel skipping [29] where lessimpactful channels of convolutional neural networks areignored and their computation is skipped during the infer-3--- Page 4 ---ence phase. However, unlike early exits, these approachescannot provide an output if the execution is interrupteddue to a strict deadline, as these methods need to performthe computations until the very last layer.2.2. Vision TransformerThe transformer architecture was \ufb01rst introduced byVaswani et al. [30] for natural language processing, andit has recently been adapted for solving computer visionproblems by Dosovitskiy et al. [15]. Vision transformerwas originally developed for the problem of image clas-si\ufb01cation, however, variations of vision transformer havesince been applied to many computer vision problems, suchas object detection, depth estimation, semantic segmenta-tion, image generation and action recognition, as well asmulti-modal data analysis tasks such as text-to-image syn-thesis and visual question answering [31, 32, 33].In order to describe the vision transformer architecture,we \ufb01rst explain the self-attention layer. The input of thislayer is in the form of a sequence X = (x1,...,x n) whereX \u2208Rn\xd7d and dis the embedding dimension to representeach entity. Its output is in the form of Z = (z1,...,z n)where Z \u2208Rn\xd7dv . The goal of self-attention is to capturethe interaction between the entities in the sequence. Forthis purpose, each vector xi in the sequence is transformedinto three separate vectors: the query vector qi \u2208Rdq ,the key vector ki \u2208Rdk and the value vector vi \u2208Rdv ,where dq = dk. To construct the output vector zi thatcorresponds to the input xi, for each vector xj in X (in-cluding xi itself), the scalar aij is calculated by the innerproduct of qi and kj. Output vector zi is then calculatedby summing the value vectors v1,...,v n weighted by theircorresponding scalars, that is, zi = \u2211nj=1 aijvj. The scalaraij basically speci\ufb01es how much attention the i-th entityshould pay to the j-th entity, since aij determines the con-tribution of vj to the combined output zi. In practice, thescalars are normalized by \u221adk and converted into proba-bilities using the softmax function.If the key, query and value vectors are packed into ma-trices Q= XWQ, K = XWK and V = XWV, where WQ,WK and WV are learnable weight matrices, the above op-eration can be rephrased as follows:Z = softmax(QKT\u221adk)V (2)In order to enable the model to capture more than onetype of relationship between the entities in the sequence,self-attention is extended to multi-head attention by con-catenating the output of h di\ufb00erent self-attention blocksZ1,...,Z h each with its own set of learnable weight ma-trices, into a single matrix Z\u2032 = [Z0,...,Z h] \u2208Rn\xd7h.dv ,which is then projected using a weight matrixW\u2032\u2208Rh.dv\xd7d.A transformer encoder is constructed by passing theinput sequence into a normalization layer, a multi-headattention layer, a second normalization layer and a multi-layer perceptron (MLP), respectively. Two residual con-nections are added, one by adding the input sequence tothe output of the multi-head attention, and the other byadding the output of the multi-head attention to the out-put of the MLP.Putting it all together, a vision transformer is createdby \ufb01rst splitting the input image into patches. Subse-quently, the sequence of patches is projected into a se-quence of vectors and a positional embedding is addedto the corresponding vector of each patch. An additionallearnable embedding called classi\ufb01cation token is added tothe beginning of the sequence. The sequence then passesthrough L transformer encoders. Finally, the \ufb01rst vectorin the output of the last transformer encoder, which cor-responds to the classi\ufb01cation token, is passed to a MLPwhich outputs the \ufb01nal classi\ufb01cation result. The architec-ture of vision transformer is depicted in Figure 2.Figure 2: The vision transformer (ViT) architecture for image clas-si\ufb01cation.ViT-EE is a method which uses transformer encodersfor early exits placed on ViT backbones [14]. ViT-EE usesthe exact same layer as the ViT backbone. Using the build-ing blocks of the backbone network for early exit branchesis simple and intuitive, and it is the reason why so far,mostly convolutional layers have been used for early exit-ing CNN backbones. However, as we show in this work,carefully designing the architecture of early exit branchescan lead to signi\ufb01cant improvements. Using Transformer-based early exit branches on CNN backbones is not intu-itive, and requires additional steps such as converting ten-sors to patches, dealing with the classi\ufb01cation token and\ufb01ne-tuning the architecture parameters including patchsize, attention heads, embedding representation, the sizeand number of layers for MLP, and dropout. Moreover,we show that removing the last residual connection in thetransformer encoder can improve the performance in somecases.Furthermore, ViT backbones have a global receptive\ufb01eld in every layer, this means that ViT-EE is not neces-sarily ideal for early exits at all layers, as it adds too muchoverhead without providing improvements in terms of re-4--- Page 5 ---ceptive \ufb01eld. On the other hand, CNN backbones have alimited receptive \ufb01eld particularly in earlier layers, there-fore fusing this receptive \ufb01eld with a global one leads toimprovements.2.3. Audio Classi\ufb01cationSimilar to image classi\ufb01cation, audio classi\ufb01cation isthe problem of categorizing a given audio waveform intoone of several predetermined classes. For instance, thegiven audio waveform could be a musical recording, andthe goal could be to specify which genre of music it belongsto. To represent the input features, spectrogramsobtainedby applying short-time Fourier transform (STFT) andMelspectrograms are commonly used [34], although raw audiowaveforms can been used as well [35]. Mel spectrogramsare spectrograms that are constructed using the Mel scalewhich is a nonlinear transformation of the frequency scaledesigned based on domain knowledge about the humanauditory system. Various deep learning models for au-dio classi\ufb01cation exist in the literature, including modelsthat are commonly used for image classi\ufb01cation, namelyResNet [36], DenseNet [37] and Inception [38], which havebeen shown to be quite e\ufb00ective for audio classi\ufb01cationas well [39]. Conveniently, the same three networks havepreviously been used as backbone networks when investi-gating early exiting for image classi\ufb01cation [13]. Thereforewe use these backbone networks for both image and audioclassi\ufb01cation in our experiments.2.4. Audiovisual Crowd CountingCrowd counting refers to the problem of identifying thetotal number of people present in a given image. Crowdcounting has many applications such as safety monitoring,disaster management, design of public spaces, intelligencegathering and analysis, creation of virtual environmentsand forensic search [40]. With many of these applications,it is vital for the model to perform in near real-time. How-ever, the input images in these scenarios often have highresolutions, such as HD or Full HD. Moreover, many ofthe available methods contain an immense number of pa-rameters [41]. This means that crowd counting modelsare often very computationally expensive, therefore, dy-namic inference methods such as early exiting and otherlightweight deep learning methods become essential in realworld applications.Although the main objective of this task is to obtain asingle count from an image, many methods treat this prob-lem as dense prediction where the output is a density mapdepicting the density of the crowd across the input image,and the total count is calculated by the sum of all valuesin the density map. Therefore, in most crowd countingdatasets, such as Shanghai Tech [42] and World Expo \u201910[43], the locations of the heads of individuals in the imageare annotated and provided as targets. A ground truthdensity map can then be obtained from these head anno-tations using Gaussian kernels or more complicated andFigure 3: An example image from the Shanghai Tech dataset and itscorresponding ground truth density map.specialized methods [41]. Figure 3 shows an image fromthe Shanghai Tech dataset and the ground truth densitymap that was generated from the provided head annota-tions using the method presented in Zhang et al [42]. Incrowd counting, Mean Absolute Error (MAE) is usuallyused as a measure of accuracy whereas Mean Squared Er-ror (MSE) is used as a measure of robustness [44].Many crowd counting methods exist in the literature[41], however, most of these methods are applied in asingle-modal fashion where the input is an image or a videoframe. In contrast, AudioCSRNet [45], a multi-modal ex-tension of the widely-used CSRNet model for crowd count-ing [46], takes as input the ambient audio of a scene inaddition to its image. The authors show that the ambi-ent audio improves the result in situations where the im-age quality is not ideal, for instance, low image resolution,presence of noise, occlusion and low illumination.In CSRNet, the features extracted from the input im-age by the \ufb01rst 10 layers of a VGG-16 [47] network pre-trained on the ImageNet dataset [48] are passed through6 dilated convolution layers and a 1 \xd71 convolution layerin order to obtain the density map. AudioCSRNet ex-tends this architecture by converting each of the dilatedconvolution layers into a fusion block. The architecture ofAudioCSRNet is depicted in Figure 4. First, a Mel spec-trogram is obtained from the raw audio waveform. Subse-quently, in each fusion block, the features extracted fromthe input Mel spectrogram by the \ufb01rst 6 layers of a VG-Gish [49] network pre-trained on the AudioSet dataset [49]are projected to two vectors called \u03b3 and \u03b2 which repre-sent the multiplicative and additive aspects of the audiofeatures. The \u03b3 and \u03b2 vectors are then tiled in order tomatch the size of the visual features. Finally, the outputof the dilated convolution is element-wise multiplied by \u03b3and added to \u03b2.The fusion operation can be summarized asvl+1 = Fl(\u03b3l \u2299Dl(vl) + \u03b2l), (3)where vl \u2208 RCl\xd7Wl\xd7Hl is the output of the l-th fusionblock, Fl denotes an activation function, \u03b3l and \u03b2l are thetiled vectors and Dl represents the l-th dilated convolu-tion.In practice, a batch normalization layer [50] is addedimmediately after each dilated convolution. Furthermore,the height and width of the intermediate features remain5--- Page 6 ---Figure 4: Architecture of AudioCSRNet.unchanged by using padding in the convolution operations,meaning Hl = Hl+1 and Wl = Wl+1. Additionally, sincethe \ufb01rst 10 layers of VGG-16 decrease both height andwidth by a factor of 8 via several pooling operations, the\ufb01nal result of the network needs to be upsampled by afactor of 8 in order to match the resolution of the inputimage. It is important to preserve the total sum of thedensity map during this upsampling operation, since itrepresents the total count.3. Single-Layer Vision Transformers for Early Ex-itsWe assume a pre-trained and high performing back-bone network is already available. Due to time constraintsarising from the particular application, it is desirable thatthe network provides a result within the speci\ufb01c deadlinerather than not providing a result at all, even though thisresult may be less accurate than it would be if time con-straints did not exist. Therefore, the backbone needs tobe augmented with early exit branches to allow for dy-namic inference and anytime prediction. As previouslymentioned, we use the classi\ufb01er-wise approach for trainingthe early exit branches since it results in \u201cplug-and-play\u201dbranches that can easily be added to the backbone networkwithout any re-training or hyper-parameter tuning.3.1. SL-ViTTypically, the architecture of early exit branches startswith one or more convolution layers, although some mayhave no convolutions at all. Afterwards, they may have apooling layer, which may be global pooling, and one MLP[51, 11]. Here, as a baseline, we choose to utilize the ar-chitecture depicted in Figure 5 with one 3 \xd73 convolution,followed by a 2 \xd72 max pooling layer and \ufb01nally a MLP.The size of the max pooling layer is increased to 4 \xd74 forcrowd counting since the input images have a very highresolution. Additionally, we use dropout [52] inside theMLP to avoid over\ufb01tting. We use a single convolutionsince early exits with two or more convolution layers havea high overhead and may even lead to lower accuracy [11].Early exits without convolutions are sometimes used verylate in the network, however, since they are straightfor-ward and leave no room for modi\ufb01cations, we do not applyour method for such cases. The resulting architecture is acommon setup within the literature, and is e\ufb00ectively thesame architecture used for earlier exits by Hu et al. [51].Figure 5: Architecture of CNN early exit branches. Size of the \ufb02at-tened feature vector depends on the dimensions of the features at thespeci\ufb01c branch location. For branches placed on the AudioCSRNetbackbone, max pooling size is increased to 4x4 since the input im-ages have a high resolution. Figure created using the NN-SVG tool[53].Our method called single-layer vision transformer orSL-ViT for short, is an alternative architecture for earlyexit branches that can achieve a higher accuracy comparedto the aforementioned baseline, while having less overheadin terms of the number of parameters as well as \ufb02oatingpoint operations per second (FLOPS). Our proposed ar-chitecture is based on the vision transformer architectureintroduced in section 2.2, where instead of the input im-age, we split the intermediate features at the branch loca-tion into patches (sub-tensors) and pass them to a visiontransformer.The choice of vision transformer architecture is pri-marily due to its global receptive \ufb01eld. Receptive \ufb01eld6--- Page 7 ---is crucial in many deep learning problems, including onesstudied in this work. The receptive \ufb01eld of state-of-the-art CNNs developed for image classi\ufb01cation has steadilyincreased over time and is correlated with increased classi-\ufb01cation accuracy [54]. Additionally, in audio classi\ufb01cationusing spectrograms, each location relates to a di\ufb00erent fre-quency band in a di\ufb00erent window of time. It is reasonableto assume that processing combinations of frequencies andwindows that are not necessarily adjacent could be of im-portance. Moreover, many crowd counting methods havemade use of global information through visual attentionmechanisms and dilated convolutions [41]. Since the re-ceptive \ufb01eld is particularly limited in early layers of CNNbackbones, choosing an architecture for early exit brancheswith a global receptive \ufb01eld could be bene\ufb01cial.Many other designs strive to increase the receptive \ufb01eldin their building blocks, for instance, the pyramid pool-ing module (PPM) in PSPNet [55] or atrous spatial pyra-mid pooling (ASPP) in DeepLab [56]. However, they allfall short in comparison with the global receptive \ufb01eld oftransformers. PPM increases the receptive \ufb01eld throughaggregating di\ufb00erent levels of pooling, which means far lo-cations have only access to coarse representations of eachother, and ASPP has holes in its receptive \ufb01eld.It is important to mention that the local receptive \ufb01eldof convolutional layers is not fundamentally bad. On thecontrary, it plays a key role in representation learning andextracting local information, especially in the early layersof the network where the receptive \ufb01eld of the convolu-tional \ufb01lters is small. Filters in successive convolutionallayers have increasingly larger receptive \ufb01elds, therefore,\ufb01nal layers in a CNN architecture have \ufb01lters of largeenough receptive \ufb01elds that can e\ufb00ectively aggregate in-formation from the entire input image to provide a properresponse. However, this process of cascading local recep-tive \ufb01elds of increasing size requires the number of layersin the CNN to be large, or at least all the layers in thenetwork to be traversed in order to provide the network\u2019sresponse. When an early exit is added at an early layer,this chain of increasingly larger receptive \ufb01elds is broken,and an early exit that has a local receptive \ufb01eld may notbe able to e\ufb00ectively aggregate all required informationin the image to provide a suitable response. This situa-tion is the motivation behind the proposed branch archi-tecture, which fuses the local receptive \ufb01eld of the layerin the network where the early exit branch is attached,with the global receptive \ufb01eld of the early exit, in orderto e\ufb00ectively aggregate information from the entire inputand provide a more accurate response. Indeed, the orig-inal Vision Transformer paper [15] attributes the successof their model to the combination of local and global re-ceptive \ufb01elds and shows that even in very early layers, thisability to integrate information globally is indeed used bythe model.There are some crucial di\ufb00erences between the origi-nal vision transformer and the architecture in our method.First, in order to introduce a low overhead for early exitbranches, we only use a single transformer encoder layerinstead of the original 12 to 36 layers, meaning that L= 1in our case. Secondly, we do not utilize a separate clas-si\ufb01cation token and instead pass the entire output of thetransformer encoder layer to the MLP head. This is pos-sible because the width and height of tensors are gener-ally reduced throughout CNN backbones by pooling oper-ations, and thus the number of patches in our architectureis lower than that of the original vision transformer. Inaddition to the number of patches, the size of the embed-ding dimension ( d) is also reduced in our proposed archi-tecture, introducing far less parameters when passing theentire output of the last transformer encoder layer to theMLP head, even with high-resolution inputs such as inour crowd counting experiments. Variations of our archi-tecture have 5 \xd75, 7 \xd77 or 16 \xd79 patches and embeddingdimensions of 32 or 36, whereas di\ufb00erent versions of theoriginal vision transformer have 14 \xd714 or 16 \xd716 patchesand embedding dimensions of 768, 1024 or 1280. We em-pirically found that using the entire transformer encoderoutput instead of just one classi\ufb01cation token can increasethe accuracy, perhaps because in a single-layer version,there are not enough layers for the classi\ufb01cation token tolearn to properly summarize other patches. Our proposedarchitecture is shown in Figure 6. It is also important tonote that the MLP head used in our architecture is exactlythe same as the MLP in the CNN early exit architecture.Figure 6: Architecture of SL-ViT early exit branches. Unlike typicalvision transformers, only a single transformer encoder layer is used,extra learnable classi\ufb01cation token is not added to the sequence andthe entire output of the transformer encoder is passed on to the MLPhead. The MLP head is the same as CNN early exit branches.Our model has several hyper-parameters, namely thesize of each patch, the embedding dimension d and thenumber of attention heads hin multi-head attention. Thepatch size creates a trade-o\ufb00 where smaller patches resultin a more \ufb01ne-grained attention mechanism while increas-ing the total number of parameters in a bi-quadratic fash-ion. Therefore, similar to the original vision transformer,we choose the size of the patch to be close to the square7--- Page 8 ---root of the height and width of the input features. We alsomake sure that the size of the patch can divide the size ofthe input features to avoid padding, for instance, a patchsize of 4 \xd74 for input features of size 28 \xd728. We performa grid search to \ufb01nd the values of d and h that result inthe highest accuracy, while keeping the total number ofparameters less than or equal to that of the CNN earlyexit counterpart.At a \ufb01rst glance, it might seem like the SL-ViT archi-tecture introduces more hyper-parameters than the con-ventional CNN architecture, however, the CNN architec-ture includes many design choices as well, such as the num-ber of \ufb01lters, \ufb01lter size, padding, dilation, stride, poolingtype and pooling size. The design choices for CNN archi-tectures might seem simpler since they have been studiedmore extensively compared to vision transformers whichwere introduced more recently.3.2. Audiovisual SL-ViTWith audiovisual backbones such as the AudioCSR-Net model for audiovisual crowd counting, described insection 2.4, it is desirable to have audiovisual early exitsthat use both visual and audio features in order to achievea higher accuracy. The simplest way to have such branchesis to add the branches after the blocks where the fusionof visual and audio features take place. However, withour proposed SL-ViT architecture, it is also possible toinclude audio features as one or more patches alongsideother patches, and directly fuse the features in the earlyexit. The advantage of this approach is that since in vi-sion transformers, any of the patches can pay attention toany other patch, the visual features can be fused with theaudio features without being directly impacted and mod-i\ufb01ed. In contrast, since convolutional \ufb01lters only take theimmediate vicinity into account, the audio features mustbe present in every location. One option is to concatenatethe visual features and the tiled audio features along thedepth. However, that would greatly increase the amount ofcomputation for each fusion operation, therefore intrusiveoperations such as element-wise multiplication and addi-tion are used instead.3.3. Copycat SL-ViTFinally, we introduce a \ufb01ne-tuning strategy for SL-ViTbranches that can further increase their accuracy. Correia-Silva et al. [16] developed a method called copycat CNNwhere they create a \u201cfake\u201d dataset by taking images fromanother domain, giving them as input to a network trainedon the target domain, and recording the output of thenetwork as labels for these images. For instance, imagesfrom the ImageNet dataset [48] can be given to a networktrained on the CIFAR-10 dataset [57], where the imageof a camel may be labelled as a \u201cdog\u201d since there are nolabels for \u201ccamel\u201d in CIFAR-10. This fake dataset is thencombined with a dataset for the target domain and usedto train a new network. We use this strategy to \ufb01ne-tunean already trained SL-ViT branch and obtain a copycatsingle-layer vision transformer (CC-SL-ViT ). Note thatthe ratio of the fake data mixed with the available datasetis a hyper-parameter of this \ufb01ne-tuning strategy.4. Experimental SetupIn this section, we provide the details of our experi-ments. We begin by giving a short summary of the datasetsas well as the training details for the backbone networks.We then lay out the details of the branch architectures,their training procedure and their placement on the back-bone networks, and \ufb01nally explain how the copycat strat-egy was used to \ufb01ne-tune the branches.A total of 27 di\ufb00erent scenarios were tested in our ex-periments. For both image and audio classi\ufb01cation, twodatasets, three backbone networks and two di\ufb00erent branchlocations on each backbone were tested. In addition, threedi\ufb00erent branch locations for the audiovisual crowd count-ing backbone network were covered. All experiments wererepeated 5 times and the average accuracy as well as thestandard deviation were recorded. 4 \xd7 Nvidia 2080TiGPUs were used for the training of our models.4.1. Datasets4.1.1. CIFAR-10 and CIFAR-100These are widely-used datasets for image classi\ufb01cation[57]. Both datasets consist of 60,000 color images of size32 \xd732 pixels and their corresponding class labels. Theimages in CIFAR-10 and CIFAR-100 are categorized into10 and 100 di\ufb00erent classes, respectively. We use 40,000examples for training, 10,000 for validation and another10,000 for testing. Since our backbone networks are pre-trained on ImageNet which consists of 224 \xd7224 pixelimages, we resize each image to these dimensions beforepassing them into the network.4.1.2. Speech Commands (SC)A well-known audio dataset of spoken words [58]. Itconsists of 100,503 1-second audio clips with a samplingrate of 16kHz, each labelled as one of 12 classes: 10 dif-ferent spoken words such as \u201cYes\u201d, \u201cNo\u201d, \u201cDown\u201d and\u201cStop\u201d as well as one class for background noise and an-other for unknown words. We use 85,511 examples fortraining, 10,102 for validation and 4,890 for testing. Weconvert the raw audio waveforms into spectrograms usingshort-time Fourier transform (STFT) with a window sizeof 255 samples and step size of 128 samples, and resize theresulting spectrograms to 224 \xd7224 before passing theminto the network.4.1.3. GTZANIt is the most widely-used dataset for music genre recog-nition [59]. The original dataset consists of 10 genres suchas \u201cPop\u201d and \u201cRock\u201d and 100 30-second audio clips pergenre with a sampling rate of 22,050Hz. We follow the8--- Page 9 ---common approach to split each audio clip into 10 separate3-second clips in order to increase the size of the datasetto 10,000. We use 8,000 examples for training, 1,000 forvalidation and another 1,000 for testing. Following the ap-proach of Palanisamy et al. [39] where di\ufb00erent spectro-grams with di\ufb00erent parameters are placed in each channelof the input image, we use one spectrogram obtained fromSTFT with window size of 512 samples and step size of 256samples as well as two Mel spectrograms with 128 bins andwindow sizes of 100 and 50 milliseconds, and step sizes of50 and 25 milliseconds, respectively.4.1.4. DISCOAn audiovisual dataset for crowd counting which con-tains 1,935 images of Full HD resolution (1920 \xd71080)[45]. For each image, a corresponding 1-second audio clipof ambient sounds with a sampling rate of 48kHz, starting0.5 seconds before the image was taken and ending 0.5 sec-onds afterwards, exists as well. The labels are provided inthe form of head annotations in the image. At the time ofthis writing, DISCO is the only publicly available datasetfor audiovisual crowd counting. We use 1435 examples fortraining, 200 for validation and 300 for testing. The inputimage is resized to 1024 \xd7576 pixels to reduce memoryand computation requirements. Similar to Hershey et al.[49], the input audio waveform is transformed into a Melspectrogram with 64 bins, window size of 25 millisecondsand step size of 10 milliseconds. Following Hu et al. [45]the ground truth density maps are obtained by convolv-ing the head annotations with a 15 \xd715 Gaussian kernelK\u223cN (0,4.0).4.2. Backbone networksTransfer learning is used to train the ResNet152,DenseNet201 and InceptionV3 backbone networks for bothimage and audio classi\ufb01cation. The backbone networks areall pre-trained on the ImageNet dataset and the top layeris replaced. We found that instead of adding just one denselayer at the top, as is common in transfer learning, usingtwo dense layers and a dropout layer in between leads to ahigher accuracy in our case. The resulting network is thentrained using the Adam optimizer [60] with a learning rateof 10\u22124 and categorical cross-entropy loss function. Thelearning rate is reduced by a factor of 0.6 on plateau witha tolerance of 2 epochs, and an early stopping mechanismwith a tolerance of 5 epochs is used.The audiovisual crowd counting backbone is trained intwo stages. We \ufb01rst train a network with the AudioCSR-Net architecture described in Section 2.4 for 100 epochs.L2 norm is used as loss function and AdamW [61] with alearning rate of 10 \u22125 and weight decay of 10 \u22124 is used asoptimizer, where the learning rate is multiplied by a factorof 0.99 each epoch. This is the same training procedureused in the original paper [45]. Subsequently, in order toconvert the problem from dense prediction to regression,a dense layer with an output size of one is added after thelast layer of the trained AudioCSRNet. This layer is ini-tialized as a sum, meaning the initial weights are all equalto one and no bias is used. Then the entire network isre-trained for another 100 epochs using MAE as loss func-tion instead of the previous L2 loss, a learning rate of 10\u22126and weight decay of 10 \u22125. The learning rate is similarlymultiplied by a factor of 0.99 every epoch. The resultingmodel achieves a MAE of 13.63 which is even lower thanthe MAE of 14.27 reported in the original paper. However,the output of the network is just a single number repre-senting the total count instead of a density map. The \ufb01nalaccuracy of all trained backbones can be seen in Table 1.When training the backbone networks, in order to \ufb01tthe limitations of our available computational resources,the batch sizes are adjusted and some layers of the back-bone networks are frozen. All backbone networks weretrained with a batch size of 32 except AudioCSRNet whichhas a batch size of 4 as well as InceptionV3 when trainedon CIFAR-10 and CIFAR-100 which has a batch size of 64.All layers of the backbone networks were trained, exceptin the case of ResNet152 and DenseNet201 when trainedon CIFAR-10 and CIFAR-100 where only the batch nor-malization layers were trained. We found that trainingonly the batch normalization layers is su\ufb03cient to achievea high-performing backbone network in these cases [62].Table 1: Performance of backbone networks on each datasetBackboneCIFAR-10 Acc. CIFAR-100 Acc. SC Acc. GTZAN Acc. DISCO MAEResNet152 95.36% 82.25% 95.85% 91.29% -DenseNet20196.48% 82.53% 96.36% 92.09% -InceptionV396.56% 83.80% 94.93% 87.79% -AudioCSRNet- - - - 13.634.3. BranchesAll branches were trained from scratch using the Heinitialization method [63] and the Adam optimizer with alearning rate of 10 \u22124 where the learning rate is reducedby a factor of 0.6 on plateau with a tolerance of 2 epochs,and an early stopping mechanism with a tolerance of 5epochs is utilized. The branches on classi\ufb01cation back-bones use a categorical cross-entropy loss function whereasthe branches on the audiovisual crowd counting backboneuse mean absolute error loss. The training batch size forbranches were 64 in scenarios involving CIFAR-10, CIFAR-100 and Speech Commands, 32 in scenarios involvingGTZAN and 4 in scenarios involving DISCO.Table 2 shows the location of the branches placed oneach backbone network. For the AudioCSRNet backbonenetwork, branch V1 uses only the output of the VGG-16layers, therefore, it only has access to the visual features.Branch AV1 uses the outputs of both VGG-16 and VG-Gish, therefore it has access to both audio and visual fea-tures. In this branch location, the fusion of audio andvisual features is performed as described in Section 3 forthe SL-ViT architecture, and similar to the fusion blocks9--- Page 10 ---in AudioCSRNet for the CNN architecture, however, with-out dilation. Finally, branch AV2 is placed after the \ufb01rstfusion block in AudioCSRNet, therefore audio and visualfeatures have already been fused and thus fusion operationis not required within the branches. Adding branches afterthe second fusion block or later would not be reasonablesince more than 85% of the computation of the backboneis carried out before that point, and thus the accelerationresulting from early exits would be negligible.Table 2: Placement of branches for each backbone betworkBackbone BN \u2217 Branch Placed AfterDenseNet201 1 Transition Layer 12 Transition Layer 2ResNet152 1 12th Convolution2 36th ConvolutionInceptionV3 1 First Filter Concat2 Second Filter ConcatAudioCSRNet V1 Last Layer of VGGAV1 Last Layers of VGG and VGGishAV2 First Fusion Block\u2217Branch Number4.4. SL-ViT and CC-SL-ViT ParametersTable 3 summarizes the hyper-parameters used for theSL-ViT branches in each scenario. \u201cPatch Size\u201d shows thewidth and height of each image patch, \u201cPatches\u201d denotesthe resulting number of patches across width and heightof the input image, d is the size of embedding dimensionand h is the number of heads in multi-head attention.For copycat SL-ViT, images from the Tiny ImageNetdataset, which are the images from ImageNet down-sampledto 32\xd732, were given to the InceptionV3 backbone trainedon CIFAR-10, and the outputs were used to create the fakedataset. Then the fake dataset was mixed with CIFAR-10with a 2-to-1 ratio and used for re-training.Table 3: Hyper-parameters of SL-ViT for di\ufb00erent backbone net-works and branchesBackbone Dataset BN \u2217 Patch Size Patchesd hDenseNet201 all all 4x4 7x7 32 12ResNet152 SC 2 4x4 7x7 32 24GTZAN 2 4x4 7x7 32 24Other 4x4 7x7 32 12InceptionV3 CIFAR-100 all 5x5 5x5 36 8Other 5x5 5x5 32 12AudioCSRNet DISCO all 8x8 16x9 32 12\u2217Branch Number5. ResultsThe results of our experiments are presented in Tables4 to 8. In these Tables, the \ufb01nal accuracy, the total FLOPSof the model up to and including the branch and the num-ber of parameters of just the early exit branch are com-pared between the CNN architecture and the SL-ViT ar-chitecture. Higher accuracies, lower errors, lower numberof parameters and lower total FLOPS are highlighted inthese tables. Furthermore, the acceleration caused by SL-ViT early exits, de\ufb01ned as the total FLOPS of the back-bone network divided by the total FLOPS of the modelup to and including the SL-ViT branch, is also provided.Several observations can be made about these results.First, in all scenarios except one, SL-ViT early exits achievea signi\ufb01cantly higher accuracy. Even in the one excep-tional scenario, namely branch 2 of ResNet152 in Table6, the accuracy of SL-ViT is very close to its CNN coun-terpart. Secondly, while in some cases SL-ViT brancheshave an equal number of parameters compared to CNNbranches, in all scenarios, the total FLOPS of SL-ViTbranches is, Your response: ```ADD_PAPER\n2105.09121v3\nThis paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with just one transformer encoder layer. Experiments across image classification, audio classification, and audiovisual crowd counting demonstrate SL-ViT's effectiveness, achieving higher accuracy with fewer parameters and FLOPS compared to CNN-based approaches. The paper also presents a novel fine-tuning strategy (CC-SL-ViT) that further enhances performance. Key results show SL-ViT branches outperform CNN counterparts in 26 out of 27 tested scenarios while reducing computational costs, making it particularly suitable for resource-constrained edge devices requiring dynamic inference capabilities.\n```\n~~~~~~~~~~\nCurrent Step #30, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2105.09121v3\nNotes: \nYour previous command was: ```ADD_PAPER\n2105.09121v3\nThis paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with just one transformer encoder layer. Experiments across image classification, audio classification, and audiovisual crowd counting demonstrate SL-ViT's effectiveness, achieving higher accuracy with fewer parameters and FLOPS compared to CNN-based approaches. The paper also presents a novel fine-tuning strategy (CC-SL-ViT) that further enhances performance. Key results show SL-ViT branches outperform CNN counterparts in 26 out of 27 tested scenarios while reducing computational costs, making it particularly suitable for resource-constrained edge devices requiring dynamic inference capabilities.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 32:
\n~~~~~~~~~~\nHistory: Step #15, Phase: literature review, Feedback: Successfully added paper 2002.00860v4, Your response: ```SUMMARY\nedge image classification model compression techniques\n```\nStep #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #25, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge image classification dynamic computation optimization\n```\nStep #26, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification dynamic computation optimization, here was the response\nTitle: Immiscible Color Flows in Optimal Transport Networks for Image Classification\nSummary: In classification tasks, it is crucial to meaningfully exploit the\ninformation contained in data. While much of the work in addressing these tasks\nis devoted to building complex algorithmic infrastructures to process inputs in\na black-box fashion, less is known about how to exploit the various facets of\nthe data, before inputting this into an algorithm. Here, we focus on this\nlatter perspective, by proposing a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nThe resulting optimal flows can then be fed into standard classifiers to\ndistinguish images in different classes. We show how our method can outperform\ncompeting approaches on image classification tasks in datasets where color\ninformation matters.\nPublication Date: 2022-05-04\narXiv paper ID: 2205.02938v2\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)\nSummary: Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\nPublication Date: 2022-04-10\narXiv paper ID: 2204.04704v1\n\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n, Your response: ```SUMMARY\nedge image classification adaptive inference techniques\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification adaptive inference techniques, here was the response\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection to Suppress Adversarial Perturbation\nSummary: This paper presents a novel context-aware image denoising algorithm that\ncombines an adaptive image smoothing technique and color reduction techniques\nto remove perturbation from adversarial images. Adaptive image smoothing is\nachieved using auto-threshold canny edge detection to produce an accurate edge\nmap used to produce a blurred image that preserves more edge features. The\nproposed algorithm then uses color reduction techniques to reconstruct the\nimage using only a few representative colors. Through this technique, the\nalgorithm can reduce the effects of adversarial perturbations on images. We\nalso discuss experimental data on classification accuracy. Our results showed\nthat the proposed approach reduces adversarial perturbation in adversarial\nattacks and increases the robustness of the deep convolutional neural network\nmodels.\nPublication Date: 2021-01-14\narXiv paper ID: 2101.05833v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n, Your response: ```SUMMARY\nedge image classification early exit networks\n```\nStep #28, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit networks, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\nStep #29, Phase: literature review, Feedback: 3\n--- Page 1 ---Single-Layer Vision Transformers for More Accurate Early Exits with Less OverheadArian Bakhtiarniaa,\u2217, Qi Zhanga, Alexandros Iosi\ufb01disaaDIGIT, Department of Electrical and Computer Engineering, Aarhus University, Finlandsgade 22, Aarhus, 8200, Midtjylland, DenmarkAbstractDeploying deep learning models in time-critical applications with limited computational resources, for instance in edgecomputing systems and IoT networks, is a challenging task that often relies on dynamic inference methods such as earlyexiting. In this paper, we introduce a novel architecture for early exiting based on the vision transformer architecture,as well as a \ufb01ne-tuning strategy that signi\ufb01cantly increase the accuracy of early exit branches compared to conventionalapproaches while introducing less overhead. Through extensive experiments on image and audio classi\ufb01cation as well asaudiovisual crowd counting, we show that our method works for both classi\ufb01cation and regression problems, and in bothsingle- and multi-modal settings. Additionally, we introduce a novel method for integrating audio and visual modalitieswithin early exits in audiovisual data analysis, that can lead to a more \ufb01ne-grained dynamic inference.Keywords: dynamic inference, early exiting, multi-exit architecture, vision transformer, multi-modal, deep learning1. IntroductionOver the past decade, deep learning has shown tremen-dous success across various \ufb01elds, such as computer vi-sion and natural language processing [1]. However, deeplearning models are by de\ufb01nition composed of many layersof interconnected neurons, even reaching billions of pa-rameters, which makes them computationally expensive.This has sparked a great deal of research in order to makedeep learning models more lightweight, for which manyapproaches have been proposed, for instance, model com-pression methods [2] such as quantization [3], pruning [4],low-rank approximation [5] and knowledge distillation [6].More and more emerging internet of things (IoT) appli-cations are integrating deep learning models, such as videosurveillance, voice assistants, augmented reality and coop-erative autonomous driving, which are often time-sensitiveand require inputs to be processed within speci\ufb01c deadlines[7, 8]. The heavy computational burden of deep learningbecomes problematic for these time-critical IoT applica-tions, due to resource-constrained IoT devices. Edge com-puting is a promising computing paradigm for addressingthis issue, in which the deep learning task is o\ufb04oaded toedge servers in the proximity of IoT devices.Since edge computing systems introduce computationo\ufb04oading over a communication network and involve mul-tiple nodes working collaboratively in order to completethe task in a timely manner, transmission time has tobe taken into account in addition to the deep learningcomputation time. However, transmission time may vary\u2217Corresponding authorEmail addresses: arianbakh@ece.au.dk (Arian Bakhtiarnia),qz@ece.au.dk (Qi Zhang), ai@ece.au.dk (Alexandros Iosi\ufb01dis)greatly over time and across di\ufb00erent channels. Conse-quently, deep learning models running on edge computingsystems and IoT networks should be capable of anytimeprediction, meaning they should be able to provide a validresponse even if they are interrupted before traversing theentire neural network, although the model is expected toprovide a better answer if it is allowed to run for longertime.Dynamic inference approaches [9] modify the computa-tion graph based on each input during the inference phasein order to \ufb01t the time constraints. A dynamic inferenceapproach that particularly suits anytime prediction isearlyexiting [10], also referred to as multi-exit architectures orauxiliary classi\ufb01ers in the literature. In multi-exit archi-tectures, one or more early exit branches are placed aftersome of the intermediate hidden layers of thebackbone net-work. The goal of each of these branches is to provide anearly result similar to the \ufb01nal result of the neural net-work using only the features extracted up to that particu-lar branch location. These early results are inevitably lessaccurate than the \ufb01nal result of the network. In order toachieve anytime prediction using early exiting, the latestearly result can be used whenever the execution is inter-rupted, for instance, whenever a hard deadline is reached.Computation time can be further decreased by applyingmodel compression techniques on the backbone of multi-exit architectures. Besides anytime prediction, early exit-ing can also be used in budgeted batch classi\ufb01cation wherea \ufb01xed amount of time is available in order to classify aset of input samples. In such a setting, the result of ear-lier branches can be used for \u201ceasier\u201d samples whereas theresult of later branches or the \ufb01nal result can be used for\u201charder\u201d ones. The di\ufb03culty of each sample can be de-termined based on the con\ufb01dence of the network about itsPreprint submitted to Neural Networks June 30, 2022arXiv:2105.09121v3  [cs.LG]  29 Jun 2022--- Page 2 ---output [11], although other approaches exist in the litera-ture [10].Early exit branches are expected to have a low over-head in terms of the extra computation they introduce,since a high overhead would defeat the purpose. There-fore, they often contain only a handful of layers. Ideally,we want the accuracy of the early results to be close to thatof the \ufb01nal result, since a higher accuracy for early exitbranches means that the overall reliability of the systemincreases. However, the low-overhead constraint makes itquite challenging to achieve a high accuracy since the earlyexit branches have signi\ufb01cantly less trainable parameterscompared to the rest of the network. Several approachesfor increasing the accuracy of early exits such as knowledgedistillation [12], curriculum learning [13] and architecturesdesigned speci\ufb01cally for early exit branches [14] have beensuggested. In this paper, we propose a novel architecturein order to obtain more accurate early exits for convolu-tional neural network (CNN) backbones.A neural architecture called vision transformer (ViT )[15] has been recently introduced for image classi\ufb01cationwhich is radically di\ufb00erent from convolutional neural net-works. The building blocks of Vision Transformer havebeen used for early exits placed on Vision Transformerbackbones [14], however, using Transformer-based earlyexit branches on CNN backbones is not intuitive and re-quires additional steps and architectural modi\ufb01cations. Weuse a modi\ufb01ed version of this architecture instead of theusual convolution and pooling layers in early exit branchesand show that our method can signi\ufb01cantly increase theaccuracy of early exits compared to conventional archi-tectures by fusing local and global receptive \ufb01elds 1. Thecontributions of this paper can be summarized as follows:\u2022 We propose a novel architecture for early exit branchesin multi-exit architectures based on vision transform-ers, called single-layer vision transformer (SL-ViT ).We compare our method with conventional CNN-based early exit architectures across 27 scenarios in-volving di\ufb00erent datasets, branch locations and back-bone networks and show that our method is signi\ufb01-cantly more accurate in 26 of these scenarios, whilehaving less overhead in terms of number of param-eters and \ufb02oating point operators (FLOPS). To thebest of our knowledge the fusion of global and localscope in early exits has never been used in multi-exitarchitectures before.\u2022 We show that our method is a general purpose ap-proach that works across di\ufb00erent modalities as wellas multi-modal settings by investigating image clas-si\ufb01cation, audio classi\ufb01cation and audiovisual crowdcounting scenarios. We also show that our methodworks for both classi\ufb01cation and regression prob-lems.1Our code will be available athttps://gitlab.au.dk/maleci/sl_vit.\u2022 We introduce a novel way of integrating audio andvisual features in early exits using vision transform-ers. To the best of our knowledge, this is the \ufb01rsttime early exits have been studied in multi-modalsettings.\u2022 We provide insight into why our method achievesbetter results compared to conventional CNN-basedarchitectures by investigating the role of attentionand receptive \ufb01eld.\u2022 We introduce a \ufb01ne-tuning strategy for SL-ViT calledcopycat single-layer vision transformer (CC-SL-ViT )which is based on the copycat strategy developed forCNNs [16] and show that this method can furtherincrease the accuracy of SL-ViT early exits. To thebest of our knowledge this is the \ufb01rst time the copy-cat strategy is used for vision transformers or earlyexits.The rest of this paper is organized as follows: Section2 provides an overview of the relevant literature; Section 3describes our proposed method in detail; Section 4 explainsthe details of our experiments; Section 5 showcases theexperiment results; and, \ufb01nally, Section 6 brie\ufb02y discussesthe results and concludes the paper.2. Related WorkThis section provides the necessary prerequisites forunderstanding our method and experiments. We startby describing the particulars of multi-exit architectures.Subsequently, we provide the details of the vision trans-former architecture, which is the foundation of the pro-posed method. Then, we brie\ufb02y touch on how audio clas-si\ufb01cation is normally carried out, which is included in sev-eral scenarios in our experiments. Finally, we explain an-other scenario investigated in our experiments, i.e. crowdcounting, and how it can be approached in a multi-modalmanner.2.1. Multi-Exit ArchitecturesIn order to describe multi-exit architectures, we use thesame notation as Scardapane et al. [10] where a neural net-work is formulated as a functionf(X) = fL(fL\u22121(...f1(X))).In this formulation Lsigni\ufb01es the total number of layers inthe network andfi is the operator corresponding to layeri,which can be a convolutional layer, a fully-connected layer,a normalization layer, or any other di\ufb00erentiable operator.hi = fi(hi\u22121) denotes the output of layeri, where h0 is theinput X. Finally, \u03b8i symbolizes the trainable parametersof layer i.Equation (1) formulates the training process for theneural network which is achieved by tuning its parametersusing an optimization algorithm on the landscape de\ufb01nedby a loss function. In this equation, the parameters of theneural network are denoted by \u03b8 = \u22c3Li=1 \u03b8i, the training2--- Page 3 ---samples are signi\ufb01ed by {(Xn,yn)}Nn=1, and l(\xb7,\xb7) is theloss function.f\u2217= arg min\u03b8N\u2211n=1l(yn,f(Xn)) (1)Extending this notation to multi-exit architectures,B \u2286{1,..,L }signi\ufb01es the set of selected branch locations afterwhich early exit branches will be placed. cb(hb) = yb is theclassi\ufb01er or regressor representing the early exit branch ateach branch location b, where yb denotes the early resultat that location. The schematic illustration of a multi-exitarchitecture is presented in Figure 1. However, since thereare multiple outputs, and thus multiple loss signals in amulti-exit architecture, its training is not as straightfor-ward.Figure 1: Schematic illustration of a multi-exit architecture with twoearly exits.Three di\ufb00erent approaches for training multi-exit ar-chitectures exist in the literature [10, 17, 13]. In the \ufb01rstapproach, called end-to-end training, the loss signals ofall exits are combined and backpropagated through thenetwork at the same time. With end-to-end training, thecontribution of each loss signal to the total loss is expressedwith weight values, which are therefore hyper-parametersof the model.The second approach, called layer-wise training, \ufb01rsttrains the network up to and including the \ufb01rst exit branch.Subsequently, the part of the network that has been trainedso far is frozen, meaning its parameters are not modi\ufb01edany further, and the remainder of the network up to andincluding the second exit branch is trained. This processcontinues until the entire network is trained. Note thatwith this approach, there is no guarantee that the accu-racy of the \ufb01nal exit remains unchanged.In the \ufb01nal approach, called classi\ufb01er-wise training,the backbone network is completely frozen and each branchis trained independent of the rest of the network and otherbranches, meaning the parameters \u03b8 are not modi\ufb01ed andonly the parameters of the classifers/regressors {cb},b \u2208Bare trained. With this approach, no new hyper-parametersare introduced and the backbone remains unchanged. How-ever, the early exit branches a\ufb00ect a lower number of train-able parameters compared to the other approaches.In this paper, we choose to follow the classi\ufb01er-wisetraining approach due to its practical importance. Thisis because with classi\ufb01er-wise training, early exit branchescan be easily added on top of existing backbone networkswithout the need for re-training and hyper-parameter op-timization, which can be computationally expensive andtime consuming. Furthermore, with end-to-end and layer-wise training strategies, the number of branches and theirplacement can lead to further trade-o\ufb00s and a\ufb00ect theoverall performance of the model. Since branches are inde-pendently trained in the classi\ufb01er-wise strategy, any num-ber of branches can exist and a branch can be placed atany location without a\ufb00ecting the performance of otherbranches or the backbone.It is important to mention that branches placed laterin the backbone network do not necessarily result in ahigher accuracy compared to branches placed earlier. Theusage of such branches would therefore not be sensiblesince earlier branches exist that require less computationand provide more accurate results. We hereby use the termimpractical to refer to such branches.As previously mentioned, there are several methodsthat try to improve the accuracy of early exits. The methodin [12] uses the combination of the distillation loss fromthe \ufb01nal exit and the loss signal from ground truth labelsto train more accurate early exits using in the end-to-endtraining setting. The method in [18] expands on this ideaby adding a third loss signal based on the di\ufb00erence be-tween features of the latest early exit with earlier exits.The method in [19] proposes a technique called gradientequilibrium to combat the problem of gradient imbalancethat surfaces when using the end-to-end strategy, whichis when the variance of the gradients becomes very largewhen loss signals from multiple exits are combined, lead-ing to unstable training. Moreover, this paper introducesforward and backward knowledge transfer that aims to en-courage collaboration among di\ufb00erent exits. The methodin [20] improves the accuracy of later exits by reusing pre-dictions from earlier exits. The method in [21] circumventsthe problem of impractical branches by adaptively select-ing the exit location based on time budget and the speci\ufb01cinput. The method in [22] simpli\ufb01es the design of multi-exit architectures by removing the hyper-parameters of theend-to-end training strategy that specify the contributionof each loss signal.Besides e\ufb03cient inference, early exits can prove use-ful in several other applications, for instance, the methodin [23] allows for parallel training of the segments of theDNN that exist between early exits, by training each seg-ment based on the loss signal of the next segment obtainedin the previous training stage. Moreover, early exits canbe added to the network during the training in order toincrease the accuracy of the backbone network and dis-carded after the training phase, for instance, the widelyused Inception model [24] was trained in this way.Besides early exiting, several other approaches existfor dynamic inference, for instance, layer skipping [25, 26,27, 28] where the execution of some of the layers of theDNN are skipped, and channel skipping [29] where lessimpactful channels of convolutional neural networks areignored and their computation is skipped during the infer-3--- Page 4 ---ence phase. However, unlike early exits, these approachescannot provide an output if the execution is interrupteddue to a strict deadline, as these methods need to performthe computations until the very last layer.2.2. Vision TransformerThe transformer architecture was \ufb01rst introduced byVaswani et al. [30] for natural language processing, andit has recently been adapted for solving computer visionproblems by Dosovitskiy et al. [15]. Vision transformerwas originally developed for the problem of image clas-si\ufb01cation, however, variations of vision transformer havesince been applied to many computer vision problems, suchas object detection, depth estimation, semantic segmenta-tion, image generation and action recognition, as well asmulti-modal data analysis tasks such as text-to-image syn-thesis and visual question answering [31, 32, 33].In order to describe the vision transformer architecture,we \ufb01rst explain the self-attention layer. The input of thislayer is in the form of a sequence X = (x1,...,x n) whereX \u2208Rn\xd7d and dis the embedding dimension to representeach entity. Its output is in the form of Z = (z1,...,z n)where Z \u2208Rn\xd7dv . The goal of self-attention is to capturethe interaction between the entities in the sequence. Forthis purpose, each vector xi in the sequence is transformedinto three separate vectors: the query vector qi \u2208Rdq ,the key vector ki \u2208Rdk and the value vector vi \u2208Rdv ,where dq = dk. To construct the output vector zi thatcorresponds to the input xi, for each vector xj in X (in-cluding xi itself), the scalar aij is calculated by the innerproduct of qi and kj. Output vector zi is then calculatedby summing the value vectors v1,...,v n weighted by theircorresponding scalars, that is, zi = \u2211nj=1 aijvj. The scalaraij basically speci\ufb01es how much attention the i-th entityshould pay to the j-th entity, since aij determines the con-tribution of vj to the combined output zi. In practice, thescalars are normalized by \u221adk and converted into proba-bilities using the softmax function.If the key, query and value vectors are packed into ma-trices Q= XWQ, K = XWK and V = XWV, where WQ,WK and WV are learnable weight matrices, the above op-eration can be rephrased as follows:Z = softmax(QKT\u221adk)V (2)In order to enable the model to capture more than onetype of relationship between the entities in the sequence,self-attention is extended to multi-head attention by con-catenating the output of h di\ufb00erent self-attention blocksZ1,...,Z h each with its own set of learnable weight ma-trices, into a single matrix Z\u2032 = [Z0,...,Z h] \u2208Rn\xd7h.dv ,which is then projected using a weight matrixW\u2032\u2208Rh.dv\xd7d.A transformer encoder is constructed by passing theinput sequence into a normalization layer, a multi-headattention layer, a second normalization layer and a multi-layer perceptron (MLP), respectively. Two residual con-nections are added, one by adding the input sequence tothe output of the multi-head attention, and the other byadding the output of the multi-head attention to the out-put of the MLP.Putting it all together, a vision transformer is createdby \ufb01rst splitting the input image into patches. Subse-quently, the sequence of patches is projected into a se-quence of vectors and a positional embedding is addedto the corresponding vector of each patch. An additionallearnable embedding called classi\ufb01cation token is added tothe beginning of the sequence. The sequence then passesthrough L transformer encoders. Finally, the \ufb01rst vectorin the output of the last transformer encoder, which cor-responds to the classi\ufb01cation token, is passed to a MLPwhich outputs the \ufb01nal classi\ufb01cation result. The architec-ture of vision transformer is depicted in Figure 2.Figure 2: The vision transformer (ViT) architecture for image clas-si\ufb01cation.ViT-EE is a method which uses transformer encodersfor early exits placed on ViT backbones [14]. ViT-EE usesthe exact same layer as the ViT backbone. Using the build-ing blocks of the backbone network for early exit branchesis simple and intuitive, and it is the reason why so far,mostly convolutional layers have been used for early exit-ing CNN backbones. However, as we show in this work,carefully designing the architecture of early exit branchescan lead to signi\ufb01cant improvements. Using Transformer-based early exit branches on CNN backbones is not intu-itive, and requires additional steps such as converting ten-sors to patches, dealing with the classi\ufb01cation token and\ufb01ne-tuning the architecture parameters including patchsize, attention heads, embedding representation, the sizeand number of layers for MLP, and dropout. Moreover,we show that removing the last residual connection in thetransformer encoder can improve the performance in somecases.Furthermore, ViT backbones have a global receptive\ufb01eld in every layer, this means that ViT-EE is not neces-sarily ideal for early exits at all layers, as it adds too muchoverhead without providing improvements in terms of re-4--- Page 5 ---ceptive \ufb01eld. On the other hand, CNN backbones have alimited receptive \ufb01eld particularly in earlier layers, there-fore fusing this receptive \ufb01eld with a global one leads toimprovements.2.3. Audio Classi\ufb01cationSimilar to image classi\ufb01cation, audio classi\ufb01cation isthe problem of categorizing a given audio waveform intoone of several predetermined classes. For instance, thegiven audio waveform could be a musical recording, andthe goal could be to specify which genre of music it belongsto. To represent the input features, spectrogramsobtainedby applying short-time Fourier transform (STFT) andMelspectrograms are commonly used [34], although raw audiowaveforms can been used as well [35]. Mel spectrogramsare spectrograms that are constructed using the Mel scalewhich is a nonlinear transformation of the frequency scaledesigned based on domain knowledge about the humanauditory system. Various deep learning models for au-dio classi\ufb01cation exist in the literature, including modelsthat are commonly used for image classi\ufb01cation, namelyResNet [36], DenseNet [37] and Inception [38], which havebeen shown to be quite e\ufb00ective for audio classi\ufb01cationas well [39]. Conveniently, the same three networks havepreviously been used as backbone networks when investi-gating early exiting for image classi\ufb01cation [13]. Thereforewe use these backbone networks for both image and audioclassi\ufb01cation in our experiments.2.4. Audiovisual Crowd CountingCrowd counting refers to the problem of identifying thetotal number of people present in a given image. Crowdcounting has many applications such as safety monitoring,disaster management, design of public spaces, intelligencegathering and analysis, creation of virtual environmentsand forensic search [40]. With many of these applications,it is vital for the model to perform in near real-time. How-ever, the input images in these scenarios often have highresolutions, such as HD or Full HD. Moreover, many ofthe available methods contain an immense number of pa-rameters [41]. This means that crowd counting modelsare often very computationally expensive, therefore, dy-namic inference methods such as early exiting and otherlightweight deep learning methods become essential in realworld applications.Although the main objective of this task is to obtain asingle count from an image, many methods treat this prob-lem as dense prediction where the output is a density mapdepicting the density of the crowd across the input image,and the total count is calculated by the sum of all valuesin the density map. Therefore, in most crowd countingdatasets, such as Shanghai Tech [42] and World Expo \u201910[43], the locations of the heads of individuals in the imageare annotated and provided as targets. A ground truthdensity map can then be obtained from these head anno-tations using Gaussian kernels or more complicated andFigure 3: An example image from the Shanghai Tech dataset and itscorresponding ground truth density map.specialized methods [41]. Figure 3 shows an image fromthe Shanghai Tech dataset and the ground truth densitymap that was generated from the provided head annota-tions using the method presented in Zhang et al [42]. Incrowd counting, Mean Absolute Error (MAE) is usuallyused as a measure of accuracy whereas Mean Squared Er-ror (MSE) is used as a measure of robustness [44].Many crowd counting methods exist in the literature[41], however, most of these methods are applied in asingle-modal fashion where the input is an image or a videoframe. In contrast, AudioCSRNet [45], a multi-modal ex-tension of the widely-used CSRNet model for crowd count-ing [46], takes as input the ambient audio of a scene inaddition to its image. The authors show that the ambi-ent audio improves the result in situations where the im-age quality is not ideal, for instance, low image resolution,presence of noise, occlusion and low illumination.In CSRNet, the features extracted from the input im-age by the \ufb01rst 10 layers of a VGG-16 [47] network pre-trained on the ImageNet dataset [48] are passed through6 dilated convolution layers and a 1 \xd71 convolution layerin order to obtain the density map. AudioCSRNet ex-tends this architecture by converting each of the dilatedconvolution layers into a fusion block. The architecture ofAudioCSRNet is depicted in Figure 4. First, a Mel spec-trogram is obtained from the raw audio waveform. Subse-quently, in each fusion block, the features extracted fromthe input Mel spectrogram by the \ufb01rst 6 layers of a VG-Gish [49] network pre-trained on the AudioSet dataset [49]are projected to two vectors called \u03b3 and \u03b2 which repre-sent the multiplicative and additive aspects of the audiofeatures. The \u03b3 and \u03b2 vectors are then tiled in order tomatch the size of the visual features. Finally, the outputof the dilated convolution is element-wise multiplied by \u03b3and added to \u03b2.The fusion operation can be summarized asvl+1 = Fl(\u03b3l \u2299Dl(vl) + \u03b2l), (3)where vl \u2208 RCl\xd7Wl\xd7Hl is the output of the l-th fusionblock, Fl denotes an activation function, \u03b3l and \u03b2l are thetiled vectors and Dl represents the l-th dilated convolu-tion.In practice, a batch normalization layer [50] is addedimmediately after each dilated convolution. Furthermore,the height and width of the intermediate features remain5--- Page 6 ---Figure 4: Architecture of AudioCSRNet.unchanged by using padding in the convolution operations,meaning Hl = Hl+1 and Wl = Wl+1. Additionally, sincethe \ufb01rst 10 layers of VGG-16 decrease both height andwidth by a factor of 8 via several pooling operations, the\ufb01nal result of the network needs to be upsampled by afactor of 8 in order to match the resolution of the inputimage. It is important to preserve the total sum of thedensity map during this upsampling operation, since itrepresents the total count.3. Single-Layer Vision Transformers for Early Ex-itsWe assume a pre-trained and high performing back-bone network is already available. Due to time constraintsarising from the particular application, it is desirable thatthe network provides a result within the speci\ufb01c deadlinerather than not providing a result at all, even though thisresult may be less accurate than it would be if time con-straints did not exist. Therefore, the backbone needs tobe augmented with early exit branches to allow for dy-namic inference and anytime prediction. As previouslymentioned, we use the classi\ufb01er-wise approach for trainingthe early exit branches since it results in \u201cplug-and-play\u201dbranches that can easily be added to the backbone networkwithout any re-training or hyper-parameter tuning.3.1. SL-ViTTypically, the architecture of early exit branches startswith one or more convolution layers, although some mayhave no convolutions at all. Afterwards, they may have apooling layer, which may be global pooling, and one MLP[51, 11]. Here, as a baseline, we choose to utilize the ar-chitecture depicted in Figure 5 with one 3 \xd73 convolution,followed by a 2 \xd72 max pooling layer and \ufb01nally a MLP.The size of the max pooling layer is increased to 4 \xd74 forcrowd counting since the input images have a very highresolution. Additionally, we use dropout [52] inside theMLP to avoid over\ufb01tting. We use a single convolutionsince early exits with two or more convolution layers havea high overhead and may even lead to lower accuracy [11].Early exits without convolutions are sometimes used verylate in the network, however, since they are straightfor-ward and leave no room for modi\ufb01cations, we do not applyour method for such cases. The resulting architecture is acommon setup within the literature, and is e\ufb00ectively thesame architecture used for earlier exits by Hu et al. [51].Figure 5: Architecture of CNN early exit branches. Size of the \ufb02at-tened feature vector depends on the dimensions of the features at thespeci\ufb01c branch location. For branches placed on the AudioCSRNetbackbone, max pooling size is increased to 4x4 since the input im-ages have a high resolution. Figure created using the NN-SVG tool[53].Our method called single-layer vision transformer orSL-ViT for short, is an alternative architecture for earlyexit branches that can achieve a higher accuracy comparedto the aforementioned baseline, while having less overheadin terms of the number of parameters as well as \ufb02oatingpoint operations per second (FLOPS). Our proposed ar-chitecture is based on the vision transformer architectureintroduced in section 2.2, where instead of the input im-age, we split the intermediate features at the branch loca-tion into patches (sub-tensors) and pass them to a visiontransformer.The choice of vision transformer architecture is pri-marily due to its global receptive \ufb01eld. Receptive \ufb01eld6--- Page 7 ---is crucial in many deep learning problems, including onesstudied in this work. The receptive \ufb01eld of state-of-the-art CNNs developed for image classi\ufb01cation has steadilyincreased over time and is correlated with increased classi-\ufb01cation accuracy [54]. Additionally, in audio classi\ufb01cationusing spectrograms, each location relates to a di\ufb00erent fre-quency band in a di\ufb00erent window of time. It is reasonableto assume that processing combinations of frequencies andwindows that are not necessarily adjacent could be of im-portance. Moreover, many crowd counting methods havemade use of global information through visual attentionmechanisms and dilated convolutions [41]. Since the re-ceptive \ufb01eld is particularly limited in early layers of CNNbackbones, choosing an architecture for early exit brancheswith a global receptive \ufb01eld could be bene\ufb01cial.Many other designs strive to increase the receptive \ufb01eldin their building blocks, for instance, the pyramid pool-ing module (PPM) in PSPNet [55] or atrous spatial pyra-mid pooling (ASPP) in DeepLab [56]. However, they allfall short in comparison with the global receptive \ufb01eld oftransformers. PPM increases the receptive \ufb01eld throughaggregating di\ufb00erent levels of pooling, which means far lo-cations have only access to coarse representations of eachother, and ASPP has holes in its receptive \ufb01eld.It is important to mention that the local receptive \ufb01eldof convolutional layers is not fundamentally bad. On thecontrary, it plays a key role in representation learning andextracting local information, especially in the early layersof the network where the receptive \ufb01eld of the convolu-tional \ufb01lters is small. Filters in successive convolutionallayers have increasingly larger receptive \ufb01elds, therefore,\ufb01nal layers in a CNN architecture have \ufb01lters of largeenough receptive \ufb01elds that can e\ufb00ectively aggregate in-formation from the entire input image to provide a properresponse. However, this process of cascading local recep-tive \ufb01elds of increasing size requires the number of layersin the CNN to be large, or at least all the layers in thenetwork to be traversed in order to provide the network\u2019sresponse. When an early exit is added at an early layer,this chain of increasingly larger receptive \ufb01elds is broken,and an early exit that has a local receptive \ufb01eld may notbe able to e\ufb00ectively aggregate all required informationin the image to provide a suitable response. This situa-tion is the motivation behind the proposed branch archi-tecture, which fuses the local receptive \ufb01eld of the layerin the network where the early exit branch is attached,with the global receptive \ufb01eld of the early exit, in orderto e\ufb00ectively aggregate information from the entire inputand provide a more accurate response. Indeed, the orig-inal Vision Transformer paper [15] attributes the successof their model to the combination of local and global re-ceptive \ufb01elds and shows that even in very early layers, thisability to integrate information globally is indeed used bythe model.There are some crucial di\ufb00erences between the origi-nal vision transformer and the architecture in our method.First, in order to introduce a low overhead for early exitbranches, we only use a single transformer encoder layerinstead of the original 12 to 36 layers, meaning that L= 1in our case. Secondly, we do not utilize a separate clas-si\ufb01cation token and instead pass the entire output of thetransformer encoder layer to the MLP head. This is pos-sible because the width and height of tensors are gener-ally reduced throughout CNN backbones by pooling oper-ations, and thus the number of patches in our architectureis lower than that of the original vision transformer. Inaddition to the number of patches, the size of the embed-ding dimension ( d) is also reduced in our proposed archi-tecture, introducing far less parameters when passing theentire output of the last transformer encoder layer to theMLP head, even with high-resolution inputs such as inour crowd counting experiments. Variations of our archi-tecture have 5 \xd75, 7 \xd77 or 16 \xd79 patches and embeddingdimensions of 32 or 36, whereas di\ufb00erent versions of theoriginal vision transformer have 14 \xd714 or 16 \xd716 patchesand embedding dimensions of 768, 1024 or 1280. We em-pirically found that using the entire transformer encoderoutput instead of just one classi\ufb01cation token can increasethe accuracy, perhaps because in a single-layer version,there are not enough layers for the classi\ufb01cation token tolearn to properly summarize other patches. Our proposedarchitecture is shown in Figure 6. It is also important tonote that the MLP head used in our architecture is exactlythe same as the MLP in the CNN early exit architecture.Figure 6: Architecture of SL-ViT early exit branches. Unlike typicalvision transformers, only a single transformer encoder layer is used,extra learnable classi\ufb01cation token is not added to the sequence andthe entire output of the transformer encoder is passed on to the MLPhead. The MLP head is the same as CNN early exit branches.Our model has several hyper-parameters, namely thesize of each patch, the embedding dimension d and thenumber of attention heads hin multi-head attention. Thepatch size creates a trade-o\ufb00 where smaller patches resultin a more \ufb01ne-grained attention mechanism while increas-ing the total number of parameters in a bi-quadratic fash-ion. Therefore, similar to the original vision transformer,we choose the size of the patch to be close to the square7--- Page 8 ---root of the height and width of the input features. We alsomake sure that the size of the patch can divide the size ofthe input features to avoid padding, for instance, a patchsize of 4 \xd74 for input features of size 28 \xd728. We performa grid search to \ufb01nd the values of d and h that result inthe highest accuracy, while keeping the total number ofparameters less than or equal to that of the CNN earlyexit counterpart.At a \ufb01rst glance, it might seem like the SL-ViT archi-tecture introduces more hyper-parameters than the con-ventional CNN architecture, however, the CNN architec-ture includes many design choices as well, such as the num-ber of \ufb01lters, \ufb01lter size, padding, dilation, stride, poolingtype and pooling size. The design choices for CNN archi-tectures might seem simpler since they have been studiedmore extensively compared to vision transformers whichwere introduced more recently.3.2. Audiovisual SL-ViTWith audiovisual backbones such as the AudioCSR-Net model for audiovisual crowd counting, described insection 2.4, it is desirable to have audiovisual early exitsthat use both visual and audio features in order to achievea higher accuracy. The simplest way to have such branchesis to add the branches after the blocks where the fusionof visual and audio features take place. However, withour proposed SL-ViT architecture, it is also possible toinclude audio features as one or more patches alongsideother patches, and directly fuse the features in the earlyexit. The advantage of this approach is that since in vi-sion transformers, any of the patches can pay attention toany other patch, the visual features can be fused with theaudio features without being directly impacted and mod-i\ufb01ed. In contrast, since convolutional \ufb01lters only take theimmediate vicinity into account, the audio features mustbe present in every location. One option is to concatenatethe visual features and the tiled audio features along thedepth. However, that would greatly increase the amount ofcomputation for each fusion operation, therefore intrusiveoperations such as element-wise multiplication and addi-tion are used instead.3.3. Copycat SL-ViTFinally, we introduce a \ufb01ne-tuning strategy for SL-ViTbranches that can further increase their accuracy. Correia-Silva et al. [16] developed a method called copycat CNNwhere they create a \u201cfake\u201d dataset by taking images fromanother domain, giving them as input to a network trainedon the target domain, and recording the output of thenetwork as labels for these images. For instance, imagesfrom the ImageNet dataset [48] can be given to a networktrained on the CIFAR-10 dataset [57], where the imageof a camel may be labelled as a \u201cdog\u201d since there are nolabels for \u201ccamel\u201d in CIFAR-10. This fake dataset is thencombined with a dataset for the target domain and usedto train a new network. We use this strategy to \ufb01ne-tunean already trained SL-ViT branch and obtain a copycatsingle-layer vision transformer (CC-SL-ViT ). Note thatthe ratio of the fake data mixed with the available datasetis a hyper-parameter of this \ufb01ne-tuning strategy.4. Experimental SetupIn this section, we provide the details of our experi-ments. We begin by giving a short summary of the datasetsas well as the training details for the backbone networks.We then lay out the details of the branch architectures,their training procedure and their placement on the back-bone networks, and \ufb01nally explain how the copycat strat-egy was used to \ufb01ne-tune the branches.A total of 27 di\ufb00erent scenarios were tested in our ex-periments. For both image and audio classi\ufb01cation, twodatasets, three backbone networks and two di\ufb00erent branchlocations on each backbone were tested. In addition, threedi\ufb00erent branch locations for the audiovisual crowd count-ing backbone network were covered. All experiments wererepeated 5 times and the average accuracy as well as thestandard deviation were recorded. 4 \xd7 Nvidia 2080TiGPUs were used for the training of our models.4.1. Datasets4.1.1. CIFAR-10 and CIFAR-100These are widely-used datasets for image classi\ufb01cation[57]. Both datasets consist of 60,000 color images of size32 \xd732 pixels and their corresponding class labels. Theimages in CIFAR-10 and CIFAR-100 are categorized into10 and 100 di\ufb00erent classes, respectively. We use 40,000examples for training, 10,000 for validation and another10,000 for testing. Since our backbone networks are pre-trained on ImageNet which consists of 224 \xd7224 pixelimages, we resize each image to these dimensions beforepassing them into the network.4.1.2. Speech Commands (SC)A well-known audio dataset of spoken words [58]. Itconsists of 100,503 1-second audio clips with a samplingrate of 16kHz, each labelled as one of 12 classes: 10 dif-ferent spoken words such as \u201cYes\u201d, \u201cNo\u201d, \u201cDown\u201d and\u201cStop\u201d as well as one class for background noise and an-other for unknown words. We use 85,511 examples fortraining, 10,102 for validation and 4,890 for testing. Weconvert the raw audio waveforms into spectrograms usingshort-time Fourier transform (STFT) with a window sizeof 255 samples and step size of 128 samples, and resize theresulting spectrograms to 224 \xd7224 before passing theminto the network.4.1.3. GTZANIt is the most widely-used dataset for music genre recog-nition [59]. The original dataset consists of 10 genres suchas \u201cPop\u201d and \u201cRock\u201d and 100 30-second audio clips pergenre with a sampling rate of 22,050Hz. We follow the8--- Page 9 ---common approach to split each audio clip into 10 separate3-second clips in order to increase the size of the datasetto 10,000. We use 8,000 examples for training, 1,000 forvalidation and another 1,000 for testing. Following the ap-proach of Palanisamy et al. [39] where di\ufb00erent spectro-grams with di\ufb00erent parameters are placed in each channelof the input image, we use one spectrogram obtained fromSTFT with window size of 512 samples and step size of 256samples as well as two Mel spectrograms with 128 bins andwindow sizes of 100 and 50 milliseconds, and step sizes of50 and 25 milliseconds, respectively.4.1.4. DISCOAn audiovisual dataset for crowd counting which con-tains 1,935 images of Full HD resolution (1920 \xd71080)[45]. For each image, a corresponding 1-second audio clipof ambient sounds with a sampling rate of 48kHz, starting0.5 seconds before the image was taken and ending 0.5 sec-onds afterwards, exists as well. The labels are provided inthe form of head annotations in the image. At the time ofthis writing, DISCO is the only publicly available datasetfor audiovisual crowd counting. We use 1435 examples fortraining, 200 for validation and 300 for testing. The inputimage is resized to 1024 \xd7576 pixels to reduce memoryand computation requirements. Similar to Hershey et al.[49], the input audio waveform is transformed into a Melspectrogram with 64 bins, window size of 25 millisecondsand step size of 10 milliseconds. Following Hu et al. [45]the ground truth density maps are obtained by convolv-ing the head annotations with a 15 \xd715 Gaussian kernelK\u223cN (0,4.0).4.2. Backbone networksTransfer learning is used to train the ResNet152,DenseNet201 and InceptionV3 backbone networks for bothimage and audio classi\ufb01cation. The backbone networks areall pre-trained on the ImageNet dataset and the top layeris replaced. We found that instead of adding just one denselayer at the top, as is common in transfer learning, usingtwo dense layers and a dropout layer in between leads to ahigher accuracy in our case. The resulting network is thentrained using the Adam optimizer [60] with a learning rateof 10\u22124 and categorical cross-entropy loss function. Thelearning rate is reduced by a factor of 0.6 on plateau witha tolerance of 2 epochs, and an early stopping mechanismwith a tolerance of 5 epochs is used.The audiovisual crowd counting backbone is trained intwo stages. We \ufb01rst train a network with the AudioCSR-Net architecture described in Section 2.4 for 100 epochs.L2 norm is used as loss function and AdamW [61] with alearning rate of 10 \u22125 and weight decay of 10 \u22124 is used asoptimizer, where the learning rate is multiplied by a factorof 0.99 each epoch. This is the same training procedureused in the original paper [45]. Subsequently, in order toconvert the problem from dense prediction to regression,a dense layer with an output size of one is added after thelast layer of the trained AudioCSRNet. This layer is ini-tialized as a sum, meaning the initial weights are all equalto one and no bias is used. Then the entire network isre-trained for another 100 epochs using MAE as loss func-tion instead of the previous L2 loss, a learning rate of 10\u22126and weight decay of 10 \u22125. The learning rate is similarlymultiplied by a factor of 0.99 every epoch. The resultingmodel achieves a MAE of 13.63 which is even lower thanthe MAE of 14.27 reported in the original paper. However,the output of the network is just a single number repre-senting the total count instead of a density map. The \ufb01nalaccuracy of all trained backbones can be seen in Table 1.When training the backbone networks, in order to \ufb01tthe limitations of our available computational resources,the batch sizes are adjusted and some layers of the back-bone networks are frozen. All backbone networks weretrained with a batch size of 32 except AudioCSRNet whichhas a batch size of 4 as well as InceptionV3 when trainedon CIFAR-10 and CIFAR-100 which has a batch size of 64.All layers of the backbone networks were trained, exceptin the case of ResNet152 and DenseNet201 when trainedon CIFAR-10 and CIFAR-100 where only the batch nor-malization layers were trained. We found that trainingonly the batch normalization layers is su\ufb03cient to achievea high-performing backbone network in these cases [62].Table 1: Performance of backbone networks on each datasetBackboneCIFAR-10 Acc. CIFAR-100 Acc. SC Acc. GTZAN Acc. DISCO MAEResNet152 95.36% 82.25% 95.85% 91.29% -DenseNet20196.48% 82.53% 96.36% 92.09% -InceptionV396.56% 83.80% 94.93% 87.79% -AudioCSRNet- - - - 13.634.3. BranchesAll branches were trained from scratch using the Heinitialization method [63] and the Adam optimizer with alearning rate of 10 \u22124 where the learning rate is reducedby a factor of 0.6 on plateau with a tolerance of 2 epochs,and an early stopping mechanism with a tolerance of 5epochs is utilized. The branches on classi\ufb01cation back-bones use a categorical cross-entropy loss function whereasthe branches on the audiovisual crowd counting backboneuse mean absolute error loss. The training batch size forbranches were 64 in scenarios involving CIFAR-10, CIFAR-100 and Speech Commands, 32 in scenarios involvingGTZAN and 4 in scenarios involving DISCO.Table 2 shows the location of the branches placed oneach backbone network. For the AudioCSRNet backbonenetwork, branch V1 uses only the output of the VGG-16layers, therefore, it only has access to the visual features.Branch AV1 uses the outputs of both VGG-16 and VG-Gish, therefore it has access to both audio and visual fea-tures. In this branch location, the fusion of audio andvisual features is performed as described in Section 3 forthe SL-ViT architecture, and similar to the fusion blocks9--- Page 10 ---in AudioCSRNet for the CNN architecture, however, with-out dilation. Finally, branch AV2 is placed after the \ufb01rstfusion block in AudioCSRNet, therefore audio and visualfeatures have already been fused and thus fusion operationis not required within the branches. Adding branches afterthe second fusion block or later would not be reasonablesince more than 85% of the computation of the backboneis carried out before that point, and thus the accelerationresulting from early exits would be negligible.Table 2: Placement of branches for each backbone betworkBackbone BN \u2217 Branch Placed AfterDenseNet201 1 Transition Layer 12 Transition Layer 2ResNet152 1 12th Convolution2 36th ConvolutionInceptionV3 1 First Filter Concat2 Second Filter ConcatAudioCSRNet V1 Last Layer of VGGAV1 Last Layers of VGG and VGGishAV2 First Fusion Block\u2217Branch Number4.4. SL-ViT and CC-SL-ViT ParametersTable 3 summarizes the hyper-parameters used for theSL-ViT branches in each scenario. \u201cPatch Size\u201d shows thewidth and height of each image patch, \u201cPatches\u201d denotesthe resulting number of patches across width and heightof the input image, d is the size of embedding dimensionand h is the number of heads in multi-head attention.For copycat SL-ViT, images from the Tiny ImageNetdataset, which are the images from ImageNet down-sampledto 32\xd732, were given to the InceptionV3 backbone trainedon CIFAR-10, and the outputs were used to create the fakedataset. Then the fake dataset was mixed with CIFAR-10with a 2-to-1 ratio and used for re-training.Table 3: Hyper-parameters of SL-ViT for di\ufb00erent backbone net-works and branchesBackbone Dataset BN \u2217 Patch Size Patchesd hDenseNet201 all all 4x4 7x7 32 12ResNet152 SC 2 4x4 7x7 32 24GTZAN 2 4x4 7x7 32 24Other 4x4 7x7 32 12InceptionV3 CIFAR-100 all 5x5 5x5 36 8Other 5x5 5x5 32 12AudioCSRNet DISCO all 8x8 16x9 32 12\u2217Branch Number5. ResultsThe results of our experiments are presented in Tables4 to 8. In these Tables, the \ufb01nal accuracy, the total FLOPSof the model up to and including the branch and the num-ber of parameters of just the early exit branch are com-pared between the CNN architecture and the SL-ViT ar-chitecture. Higher accuracies, lower errors, lower numberof parameters and lower total FLOPS are highlighted inthese tables. Furthermore, the acceleration caused by SL-ViT early exits, de\ufb01ned as the total FLOPS of the back-bone network divided by the total FLOPS of the modelup to and including the SL-ViT branch, is also provided.Several observations can be made about these results.First, in all scenarios except one, SL-ViT early exits achievea signi\ufb01cantly higher accuracy. Even in the one excep-tional scenario, namely branch 2 of ResNet152 in Table6, the accuracy of SL-ViT is very close to its CNN coun-terpart. Secondly, while in some cases SL-ViT brancheshave an equal number of parameters compared to CNNbranches, in all scenarios, the total FLOPS of SL-ViTbranches is, Your response: ```ADD_PAPER\n2105.09121v3\nThis paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with just one transformer encoder layer. Experiments across image classification, audio classification, and audiovisual crowd counting demonstrate SL-ViT's effectiveness, achieving higher accuracy with fewer parameters and FLOPS compared to CNN-based approaches. The paper also presents a novel fine-tuning strategy (CC-SL-ViT) that further enhances performance. Key results show SL-ViT branches outperform CNN counterparts in 26 out of 27 tested scenarios while reducing computational costs, making it particularly suitable for resource-constrained edge devices requiring dynamic inference capabilities.\n```\nStep #30, Phase: literature review, Feedback: Successfully added paper 2105.09121v3, Your response: ```SUMMARY\nedge image classification sparsity techniques\n```\n~~~~~~~~~~\nCurrent Step #31, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification sparsity techniques, here was the response\nTitle: Exploiting Activation based Gradient Output Sparsity to Accelerate Backpropagation in CNNs\nSummary: Machine/deep-learning (ML/DL) based techniques are emerging as a driving\nforce behind many cutting-edge technologies, achieving high accuracy on\ncomputer vision workloads such as image classification and object detection.\nHowever, training these models involving large parameters is both\ntime-consuming and energy-hogging. In this regard, several prior works have\nadvocated for sparsity to speed up the of DL training and more so, the\ninference phase. This work begins with the observation that during training,\nsparsity in the forward and backward passes are correlated. In that context, we\ninvestigate two types of sparsity (input and output type) inherent in gradient\ndescent-based optimization algorithms and propose a hardware micro-architecture\nto leverage the same. Our experimental results use five state-of-the-art CNN\nmodels on the Imagenet dataset, and show back propagation speedups in the range\nof 1.69$\\times$ to 5.43$\\times$, compared to the dense baseline execution. By\nexploiting sparsity in both the forward and backward passes, speedup\nimprovements range from 1.68$\\times$ to 3.30$\\times$ over the sparsity-agnostic\nbaseline execution. Our work also achieves significant reduction in training\niteration time over several previously proposed dense as well as sparse\naccelerator based platforms, in addition to achieving order of magnitude energy\nefficiency improvements over GPU based execution.\nPublication Date: 2021-09-16\narXiv paper ID: 2109.07710v1\n\nTitle: Super-Resolution of Brain MRI Images using Overcomplete Dictionaries and Nonlocal Similarity\nSummary: Recently, the Magnetic Resonance Imaging (MRI) images have limited and\nunsatisfactory resolutions due to various constraints such as physical,\ntechnological and economic considerations. Super-resolution techniques can\nobtain high-resolution MRI images. The traditional methods obtained the\nresolution enhancement of brain MRI by interpolations, affecting the accuracy\nof the following diagnose process. The requirement for brain image quality is\nfast increasing. In this paper, we propose an image super-resolution (SR)\nmethod based on overcomplete dictionaries and inherent similarity of an image\nto recover the high-resolution (HR) image from a single low-resolution (LR)\nimage. We explore the nonlocal similarity of the image to tentatively search\nfor similar blocks in the whole image and present a joint reconstruction method\nbased on compressive sensing (CS) and similarity constraints. The sparsity and\nself-similarity of the image blocks are taken as the constraints. The proposed\nmethod is summarized in the following steps. First, a dictionary classification\nmethod based on the measurement domain is presented. The image blocks are\nclassified into smooth, texture and edge parts by analyzing their features in\nthe measurement domain. Then, the corresponding dictionaries are trained using\nthe classified image blocks. Equally important, in the reconstruction part, we\nuse the CS reconstruction method to recover the HR brain MRI image, considering\nboth nonlocal similarity and the sparsity of an image as the constraints. This\nmethod performs better both visually and quantitatively than some existing\nmethods.\nPublication Date: 2019-02-13\narXiv paper ID: 1902.04902v1\n\nTitle: Sparsity Turns Adversarial: Energy and Latency Attacks on Deep Neural Networks\nSummary: Adversarial attacks have exposed serious vulnerabilities in Deep Neural\nNetworks (DNNs) through their ability to force misclassifications through\nhuman-imperceptible perturbations to DNN inputs. We explore a new direction in\nthe field of adversarial attacks by suggesting attacks that aim to degrade the\ncomputational efficiency of DNNs rather than their classification accuracy.\nSpecifically, we propose and demonstrate sparsity attacks, which adversarial\nmodify a DNN's inputs so as to reduce sparsity (or the presence of zero values)\nin its internal activation values. In resource-constrained systems, a wide\nrange of hardware and software techniques have been proposed that exploit\nsparsity to improve DNN efficiency. The proposed attack increases the execution\ntime and energy consumption of sparsity-optimized DNN implementations, raising\nconcern over their deployment in latency and energy-critical applications.\n  We propose a systematic methodology to generate adversarial inputs for\nsparsity attacks by formulating an objective function that quantifies the\nnetwork's activation sparsity, and minimizing this function using iterative\ngradient-descent techniques. We launch both white-box and black-box versions of\nadversarial sparsity attacks on image recognition DNNs and demonstrate that\nthey decrease activation sparsity by up to 1.82x. We also evaluate the impact\nof the attack on a sparsity-optimized DNN accelerator and demonstrate\ndegradations up to 1.59x in latency, and also study the performance of the\nattack on a sparsity-optimized general-purpose processor. Finally, we evaluate\ndefense techniques such as activation thresholding and input quantization and\ndemonstrate that the proposed attack is able to withstand them, highlighting\nthe need for further efforts in this new direction within the field of\nadversarial machine learning.\nPublication Date: 2020-06-14\narXiv paper ID: 2006.08020v3\n\nTitle: Leaf segmentation through the classification of edges\nSummary: We present an approach to leaf level segmentation of images of Arabidopsis\nthaliana plants based upon detected edges. We introduce a novel approach to\nedge classification, which forms an important part of a method to both count\nthe leaves and establish the leaf area of a growing plant from images obtained\nin a high-throughput phenotyping system. Our technique uses a relatively\nshallow convolutional neural network to classify image edges as background,\nplant edge, leaf-on-leaf edge or internal leaf noise. The edges themselves were\nfound using the Canny edge detector and the classified edges can be used with\nsimple image processing techniques to generate a region-based segmentation in\nwhich the leaves are distinct. This approach is strong at distinguishing\noccluding pairs of leaves where one leaf is largely hidden, a situation which\nhas proved troublesome for plant image analysis systems in the past. In\naddition, we introduce the publicly available plant image dataset that was used\nfor this work.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.03124v1\n\nTitle: Network classification with applications to brain connectomics\nSummary: While statistical analysis of a single network has received a lot of\nattention in recent years, with a focus on social networks, analysis of a\nsample of networks presents its own challenges which require a different set of\nanalytic tools. Here we study the problem of classification of networks with\nlabeled nodes, motivated by applications in neuroimaging. Brain networks are\nconstructed from imaging data to represent functional connectivity between\nregions of the brain, and previous work has shown the potential of such\nnetworks to distinguish between various brain disorders, giving rise to a\nnetwork classification problem. Existing approaches tend to either treat all\nedge weights as a long vector, ignoring the network structure, or focus on\ngraph topology as represented by summary measures while ignoring the edge\nweights. Our goal is to design a classification method that uses both the\nindividual edge information and the network structure of the data in a\ncomputationally efficient way, and that can produce a parsimonious and\ninterpretable representation of differences in brain connectivity patterns\nbetween classes. We propose a graph classification method that uses edge\nweights as predictors but incorporates the network nature of the data via\npenalties that promote sparsity in the number of nodes, in addition to the\nusual sparsity penalties that encourage selection of edges. We implement the\nmethod via efficient convex optimization and provide a detailed analysis of\ndata from two fMRI studies of schizophrenia.\nPublication Date: 2017-01-27\narXiv paper ID: 1701.08140v3\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification sparsity techniques\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 33:
\n~~~~~~~~~~\nHistory: Step #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #25, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge image classification dynamic computation optimization\n```\nStep #26, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification dynamic computation optimization, here was the response\nTitle: Immiscible Color Flows in Optimal Transport Networks for Image Classification\nSummary: In classification tasks, it is crucial to meaningfully exploit the\ninformation contained in data. While much of the work in addressing these tasks\nis devoted to building complex algorithmic infrastructures to process inputs in\na black-box fashion, less is known about how to exploit the various facets of\nthe data, before inputting this into an algorithm. Here, we focus on this\nlatter perspective, by proposing a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nThe resulting optimal flows can then be fed into standard classifiers to\ndistinguish images in different classes. We show how our method can outperform\ncompeting approaches on image classification tasks in datasets where color\ninformation matters.\nPublication Date: 2022-05-04\narXiv paper ID: 2205.02938v2\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)\nSummary: Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\nPublication Date: 2022-04-10\narXiv paper ID: 2204.04704v1\n\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n, Your response: ```SUMMARY\nedge image classification adaptive inference techniques\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification adaptive inference techniques, here was the response\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection to Suppress Adversarial Perturbation\nSummary: This paper presents a novel context-aware image denoising algorithm that\ncombines an adaptive image smoothing technique and color reduction techniques\nto remove perturbation from adversarial images. Adaptive image smoothing is\nachieved using auto-threshold canny edge detection to produce an accurate edge\nmap used to produce a blurred image that preserves more edge features. The\nproposed algorithm then uses color reduction techniques to reconstruct the\nimage using only a few representative colors. Through this technique, the\nalgorithm can reduce the effects of adversarial perturbations on images. We\nalso discuss experimental data on classification accuracy. Our results showed\nthat the proposed approach reduces adversarial perturbation in adversarial\nattacks and increases the robustness of the deep convolutional neural network\nmodels.\nPublication Date: 2021-01-14\narXiv paper ID: 2101.05833v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n, Your response: ```SUMMARY\nedge image classification early exit networks\n```\nStep #28, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit networks, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\nStep #29, Phase: literature review, Feedback: 3\n--- Page 1 ---Single-Layer Vision Transformers for More Accurate Early Exits with Less OverheadArian Bakhtiarniaa,\u2217, Qi Zhanga, Alexandros Iosi\ufb01disaaDIGIT, Department of Electrical and Computer Engineering, Aarhus University, Finlandsgade 22, Aarhus, 8200, Midtjylland, DenmarkAbstractDeploying deep learning models in time-critical applications with limited computational resources, for instance in edgecomputing systems and IoT networks, is a challenging task that often relies on dynamic inference methods such as earlyexiting. In this paper, we introduce a novel architecture for early exiting based on the vision transformer architecture,as well as a \ufb01ne-tuning strategy that signi\ufb01cantly increase the accuracy of early exit branches compared to conventionalapproaches while introducing less overhead. Through extensive experiments on image and audio classi\ufb01cation as well asaudiovisual crowd counting, we show that our method works for both classi\ufb01cation and regression problems, and in bothsingle- and multi-modal settings. Additionally, we introduce a novel method for integrating audio and visual modalitieswithin early exits in audiovisual data analysis, that can lead to a more \ufb01ne-grained dynamic inference.Keywords: dynamic inference, early exiting, multi-exit architecture, vision transformer, multi-modal, deep learning1. IntroductionOver the past decade, deep learning has shown tremen-dous success across various \ufb01elds, such as computer vi-sion and natural language processing [1]. However, deeplearning models are by de\ufb01nition composed of many layersof interconnected neurons, even reaching billions of pa-rameters, which makes them computationally expensive.This has sparked a great deal of research in order to makedeep learning models more lightweight, for which manyapproaches have been proposed, for instance, model com-pression methods [2] such as quantization [3], pruning [4],low-rank approximation [5] and knowledge distillation [6].More and more emerging internet of things (IoT) appli-cations are integrating deep learning models, such as videosurveillance, voice assistants, augmented reality and coop-erative autonomous driving, which are often time-sensitiveand require inputs to be processed within speci\ufb01c deadlines[7, 8]. The heavy computational burden of deep learningbecomes problematic for these time-critical IoT applica-tions, due to resource-constrained IoT devices. Edge com-puting is a promising computing paradigm for addressingthis issue, in which the deep learning task is o\ufb04oaded toedge servers in the proximity of IoT devices.Since edge computing systems introduce computationo\ufb04oading over a communication network and involve mul-tiple nodes working collaboratively in order to completethe task in a timely manner, transmission time has tobe taken into account in addition to the deep learningcomputation time. However, transmission time may vary\u2217Corresponding authorEmail addresses: arianbakh@ece.au.dk (Arian Bakhtiarnia),qz@ece.au.dk (Qi Zhang), ai@ece.au.dk (Alexandros Iosi\ufb01dis)greatly over time and across di\ufb00erent channels. Conse-quently, deep learning models running on edge computingsystems and IoT networks should be capable of anytimeprediction, meaning they should be able to provide a validresponse even if they are interrupted before traversing theentire neural network, although the model is expected toprovide a better answer if it is allowed to run for longertime.Dynamic inference approaches [9] modify the computa-tion graph based on each input during the inference phasein order to \ufb01t the time constraints. A dynamic inferenceapproach that particularly suits anytime prediction isearlyexiting [10], also referred to as multi-exit architectures orauxiliary classi\ufb01ers in the literature. In multi-exit archi-tectures, one or more early exit branches are placed aftersome of the intermediate hidden layers of thebackbone net-work. The goal of each of these branches is to provide anearly result similar to the \ufb01nal result of the neural net-work using only the features extracted up to that particu-lar branch location. These early results are inevitably lessaccurate than the \ufb01nal result of the network. In order toachieve anytime prediction using early exiting, the latestearly result can be used whenever the execution is inter-rupted, for instance, whenever a hard deadline is reached.Computation time can be further decreased by applyingmodel compression techniques on the backbone of multi-exit architectures. Besides anytime prediction, early exit-ing can also be used in budgeted batch classi\ufb01cation wherea \ufb01xed amount of time is available in order to classify aset of input samples. In such a setting, the result of ear-lier branches can be used for \u201ceasier\u201d samples whereas theresult of later branches or the \ufb01nal result can be used for\u201charder\u201d ones. The di\ufb03culty of each sample can be de-termined based on the con\ufb01dence of the network about itsPreprint submitted to Neural Networks June 30, 2022arXiv:2105.09121v3  [cs.LG]  29 Jun 2022--- Page 2 ---output [11], although other approaches exist in the litera-ture [10].Early exit branches are expected to have a low over-head in terms of the extra computation they introduce,since a high overhead would defeat the purpose. There-fore, they often contain only a handful of layers. Ideally,we want the accuracy of the early results to be close to thatof the \ufb01nal result, since a higher accuracy for early exitbranches means that the overall reliability of the systemincreases. However, the low-overhead constraint makes itquite challenging to achieve a high accuracy since the earlyexit branches have signi\ufb01cantly less trainable parameterscompared to the rest of the network. Several approachesfor increasing the accuracy of early exits such as knowledgedistillation [12], curriculum learning [13] and architecturesdesigned speci\ufb01cally for early exit branches [14] have beensuggested. In this paper, we propose a novel architecturein order to obtain more accurate early exits for convolu-tional neural network (CNN) backbones.A neural architecture called vision transformer (ViT )[15] has been recently introduced for image classi\ufb01cationwhich is radically di\ufb00erent from convolutional neural net-works. The building blocks of Vision Transformer havebeen used for early exits placed on Vision Transformerbackbones [14], however, using Transformer-based earlyexit branches on CNN backbones is not intuitive and re-quires additional steps and architectural modi\ufb01cations. Weuse a modi\ufb01ed version of this architecture instead of theusual convolution and pooling layers in early exit branchesand show that our method can signi\ufb01cantly increase theaccuracy of early exits compared to conventional archi-tectures by fusing local and global receptive \ufb01elds 1. Thecontributions of this paper can be summarized as follows:\u2022 We propose a novel architecture for early exit branchesin multi-exit architectures based on vision transform-ers, called single-layer vision transformer (SL-ViT ).We compare our method with conventional CNN-based early exit architectures across 27 scenarios in-volving di\ufb00erent datasets, branch locations and back-bone networks and show that our method is signi\ufb01-cantly more accurate in 26 of these scenarios, whilehaving less overhead in terms of number of param-eters and \ufb02oating point operators (FLOPS). To thebest of our knowledge the fusion of global and localscope in early exits has never been used in multi-exitarchitectures before.\u2022 We show that our method is a general purpose ap-proach that works across di\ufb00erent modalities as wellas multi-modal settings by investigating image clas-si\ufb01cation, audio classi\ufb01cation and audiovisual crowdcounting scenarios. We also show that our methodworks for both classi\ufb01cation and regression prob-lems.1Our code will be available athttps://gitlab.au.dk/maleci/sl_vit.\u2022 We introduce a novel way of integrating audio andvisual features in early exits using vision transform-ers. To the best of our knowledge, this is the \ufb01rsttime early exits have been studied in multi-modalsettings.\u2022 We provide insight into why our method achievesbetter results compared to conventional CNN-basedarchitectures by investigating the role of attentionand receptive \ufb01eld.\u2022 We introduce a \ufb01ne-tuning strategy for SL-ViT calledcopycat single-layer vision transformer (CC-SL-ViT )which is based on the copycat strategy developed forCNNs [16] and show that this method can furtherincrease the accuracy of SL-ViT early exits. To thebest of our knowledge this is the \ufb01rst time the copy-cat strategy is used for vision transformers or earlyexits.The rest of this paper is organized as follows: Section2 provides an overview of the relevant literature; Section 3describes our proposed method in detail; Section 4 explainsthe details of our experiments; Section 5 showcases theexperiment results; and, \ufb01nally, Section 6 brie\ufb02y discussesthe results and concludes the paper.2. Related WorkThis section provides the necessary prerequisites forunderstanding our method and experiments. We startby describing the particulars of multi-exit architectures.Subsequently, we provide the details of the vision trans-former architecture, which is the foundation of the pro-posed method. Then, we brie\ufb02y touch on how audio clas-si\ufb01cation is normally carried out, which is included in sev-eral scenarios in our experiments. Finally, we explain an-other scenario investigated in our experiments, i.e. crowdcounting, and how it can be approached in a multi-modalmanner.2.1. Multi-Exit ArchitecturesIn order to describe multi-exit architectures, we use thesame notation as Scardapane et al. [10] where a neural net-work is formulated as a functionf(X) = fL(fL\u22121(...f1(X))).In this formulation Lsigni\ufb01es the total number of layers inthe network andfi is the operator corresponding to layeri,which can be a convolutional layer, a fully-connected layer,a normalization layer, or any other di\ufb00erentiable operator.hi = fi(hi\u22121) denotes the output of layeri, where h0 is theinput X. Finally, \u03b8i symbolizes the trainable parametersof layer i.Equation (1) formulates the training process for theneural network which is achieved by tuning its parametersusing an optimization algorithm on the landscape de\ufb01nedby a loss function. In this equation, the parameters of theneural network are denoted by \u03b8 = \u22c3Li=1 \u03b8i, the training2--- Page 3 ---samples are signi\ufb01ed by {(Xn,yn)}Nn=1, and l(\xb7,\xb7) is theloss function.f\u2217= arg min\u03b8N\u2211n=1l(yn,f(Xn)) (1)Extending this notation to multi-exit architectures,B \u2286{1,..,L }signi\ufb01es the set of selected branch locations afterwhich early exit branches will be placed. cb(hb) = yb is theclassi\ufb01er or regressor representing the early exit branch ateach branch location b, where yb denotes the early resultat that location. The schematic illustration of a multi-exitarchitecture is presented in Figure 1. However, since thereare multiple outputs, and thus multiple loss signals in amulti-exit architecture, its training is not as straightfor-ward.Figure 1: Schematic illustration of a multi-exit architecture with twoearly exits.Three di\ufb00erent approaches for training multi-exit ar-chitectures exist in the literature [10, 17, 13]. In the \ufb01rstapproach, called end-to-end training, the loss signals ofall exits are combined and backpropagated through thenetwork at the same time. With end-to-end training, thecontribution of each loss signal to the total loss is expressedwith weight values, which are therefore hyper-parametersof the model.The second approach, called layer-wise training, \ufb01rsttrains the network up to and including the \ufb01rst exit branch.Subsequently, the part of the network that has been trainedso far is frozen, meaning its parameters are not modi\ufb01edany further, and the remainder of the network up to andincluding the second exit branch is trained. This processcontinues until the entire network is trained. Note thatwith this approach, there is no guarantee that the accu-racy of the \ufb01nal exit remains unchanged.In the \ufb01nal approach, called classi\ufb01er-wise training,the backbone network is completely frozen and each branchis trained independent of the rest of the network and otherbranches, meaning the parameters \u03b8 are not modi\ufb01ed andonly the parameters of the classifers/regressors {cb},b \u2208Bare trained. With this approach, no new hyper-parametersare introduced and the backbone remains unchanged. How-ever, the early exit branches a\ufb00ect a lower number of train-able parameters compared to the other approaches.In this paper, we choose to follow the classi\ufb01er-wisetraining approach due to its practical importance. Thisis because with classi\ufb01er-wise training, early exit branchescan be easily added on top of existing backbone networkswithout the need for re-training and hyper-parameter op-timization, which can be computationally expensive andtime consuming. Furthermore, with end-to-end and layer-wise training strategies, the number of branches and theirplacement can lead to further trade-o\ufb00s and a\ufb00ect theoverall performance of the model. Since branches are inde-pendently trained in the classi\ufb01er-wise strategy, any num-ber of branches can exist and a branch can be placed atany location without a\ufb00ecting the performance of otherbranches or the backbone.It is important to mention that branches placed laterin the backbone network do not necessarily result in ahigher accuracy compared to branches placed earlier. Theusage of such branches would therefore not be sensiblesince earlier branches exist that require less computationand provide more accurate results. We hereby use the termimpractical to refer to such branches.As previously mentioned, there are several methodsthat try to improve the accuracy of early exits. The methodin [12] uses the combination of the distillation loss fromthe \ufb01nal exit and the loss signal from ground truth labelsto train more accurate early exits using in the end-to-endtraining setting. The method in [18] expands on this ideaby adding a third loss signal based on the di\ufb00erence be-tween features of the latest early exit with earlier exits.The method in [19] proposes a technique called gradientequilibrium to combat the problem of gradient imbalancethat surfaces when using the end-to-end strategy, whichis when the variance of the gradients becomes very largewhen loss signals from multiple exits are combined, lead-ing to unstable training. Moreover, this paper introducesforward and backward knowledge transfer that aims to en-courage collaboration among di\ufb00erent exits. The methodin [20] improves the accuracy of later exits by reusing pre-dictions from earlier exits. The method in [21] circumventsthe problem of impractical branches by adaptively select-ing the exit location based on time budget and the speci\ufb01cinput. The method in [22] simpli\ufb01es the design of multi-exit architectures by removing the hyper-parameters of theend-to-end training strategy that specify the contributionof each loss signal.Besides e\ufb03cient inference, early exits can prove use-ful in several other applications, for instance, the methodin [23] allows for parallel training of the segments of theDNN that exist between early exits, by training each seg-ment based on the loss signal of the next segment obtainedin the previous training stage. Moreover, early exits canbe added to the network during the training in order toincrease the accuracy of the backbone network and dis-carded after the training phase, for instance, the widelyused Inception model [24] was trained in this way.Besides early exiting, several other approaches existfor dynamic inference, for instance, layer skipping [25, 26,27, 28] where the execution of some of the layers of theDNN are skipped, and channel skipping [29] where lessimpactful channels of convolutional neural networks areignored and their computation is skipped during the infer-3--- Page 4 ---ence phase. However, unlike early exits, these approachescannot provide an output if the execution is interrupteddue to a strict deadline, as these methods need to performthe computations until the very last layer.2.2. Vision TransformerThe transformer architecture was \ufb01rst introduced byVaswani et al. [30] for natural language processing, andit has recently been adapted for solving computer visionproblems by Dosovitskiy et al. [15]. Vision transformerwas originally developed for the problem of image clas-si\ufb01cation, however, variations of vision transformer havesince been applied to many computer vision problems, suchas object detection, depth estimation, semantic segmenta-tion, image generation and action recognition, as well asmulti-modal data analysis tasks such as text-to-image syn-thesis and visual question answering [31, 32, 33].In order to describe the vision transformer architecture,we \ufb01rst explain the self-attention layer. The input of thislayer is in the form of a sequence X = (x1,...,x n) whereX \u2208Rn\xd7d and dis the embedding dimension to representeach entity. Its output is in the form of Z = (z1,...,z n)where Z \u2208Rn\xd7dv . The goal of self-attention is to capturethe interaction between the entities in the sequence. Forthis purpose, each vector xi in the sequence is transformedinto three separate vectors: the query vector qi \u2208Rdq ,the key vector ki \u2208Rdk and the value vector vi \u2208Rdv ,where dq = dk. To construct the output vector zi thatcorresponds to the input xi, for each vector xj in X (in-cluding xi itself), the scalar aij is calculated by the innerproduct of qi and kj. Output vector zi is then calculatedby summing the value vectors v1,...,v n weighted by theircorresponding scalars, that is, zi = \u2211nj=1 aijvj. The scalaraij basically speci\ufb01es how much attention the i-th entityshould pay to the j-th entity, since aij determines the con-tribution of vj to the combined output zi. In practice, thescalars are normalized by \u221adk and converted into proba-bilities using the softmax function.If the key, query and value vectors are packed into ma-trices Q= XWQ, K = XWK and V = XWV, where WQ,WK and WV are learnable weight matrices, the above op-eration can be rephrased as follows:Z = softmax(QKT\u221adk)V (2)In order to enable the model to capture more than onetype of relationship between the entities in the sequence,self-attention is extended to multi-head attention by con-catenating the output of h di\ufb00erent self-attention blocksZ1,...,Z h each with its own set of learnable weight ma-trices, into a single matrix Z\u2032 = [Z0,...,Z h] \u2208Rn\xd7h.dv ,which is then projected using a weight matrixW\u2032\u2208Rh.dv\xd7d.A transformer encoder is constructed by passing theinput sequence into a normalization layer, a multi-headattention layer, a second normalization layer and a multi-layer perceptron (MLP), respectively. Two residual con-nections are added, one by adding the input sequence tothe output of the multi-head attention, and the other byadding the output of the multi-head attention to the out-put of the MLP.Putting it all together, a vision transformer is createdby \ufb01rst splitting the input image into patches. Subse-quently, the sequence of patches is projected into a se-quence of vectors and a positional embedding is addedto the corresponding vector of each patch. An additionallearnable embedding called classi\ufb01cation token is added tothe beginning of the sequence. The sequence then passesthrough L transformer encoders. Finally, the \ufb01rst vectorin the output of the last transformer encoder, which cor-responds to the classi\ufb01cation token, is passed to a MLPwhich outputs the \ufb01nal classi\ufb01cation result. The architec-ture of vision transformer is depicted in Figure 2.Figure 2: The vision transformer (ViT) architecture for image clas-si\ufb01cation.ViT-EE is a method which uses transformer encodersfor early exits placed on ViT backbones [14]. ViT-EE usesthe exact same layer as the ViT backbone. Using the build-ing blocks of the backbone network for early exit branchesis simple and intuitive, and it is the reason why so far,mostly convolutional layers have been used for early exit-ing CNN backbones. However, as we show in this work,carefully designing the architecture of early exit branchescan lead to signi\ufb01cant improvements. Using Transformer-based early exit branches on CNN backbones is not intu-itive, and requires additional steps such as converting ten-sors to patches, dealing with the classi\ufb01cation token and\ufb01ne-tuning the architecture parameters including patchsize, attention heads, embedding representation, the sizeand number of layers for MLP, and dropout. Moreover,we show that removing the last residual connection in thetransformer encoder can improve the performance in somecases.Furthermore, ViT backbones have a global receptive\ufb01eld in every layer, this means that ViT-EE is not neces-sarily ideal for early exits at all layers, as it adds too muchoverhead without providing improvements in terms of re-4--- Page 5 ---ceptive \ufb01eld. On the other hand, CNN backbones have alimited receptive \ufb01eld particularly in earlier layers, there-fore fusing this receptive \ufb01eld with a global one leads toimprovements.2.3. Audio Classi\ufb01cationSimilar to image classi\ufb01cation, audio classi\ufb01cation isthe problem of categorizing a given audio waveform intoone of several predetermined classes. For instance, thegiven audio waveform could be a musical recording, andthe goal could be to specify which genre of music it belongsto. To represent the input features, spectrogramsobtainedby applying short-time Fourier transform (STFT) andMelspectrograms are commonly used [34], although raw audiowaveforms can been used as well [35]. Mel spectrogramsare spectrograms that are constructed using the Mel scalewhich is a nonlinear transformation of the frequency scaledesigned based on domain knowledge about the humanauditory system. Various deep learning models for au-dio classi\ufb01cation exist in the literature, including modelsthat are commonly used for image classi\ufb01cation, namelyResNet [36], DenseNet [37] and Inception [38], which havebeen shown to be quite e\ufb00ective for audio classi\ufb01cationas well [39]. Conveniently, the same three networks havepreviously been used as backbone networks when investi-gating early exiting for image classi\ufb01cation [13]. Thereforewe use these backbone networks for both image and audioclassi\ufb01cation in our experiments.2.4. Audiovisual Crowd CountingCrowd counting refers to the problem of identifying thetotal number of people present in a given image. Crowdcounting has many applications such as safety monitoring,disaster management, design of public spaces, intelligencegathering and analysis, creation of virtual environmentsand forensic search [40]. With many of these applications,it is vital for the model to perform in near real-time. How-ever, the input images in these scenarios often have highresolutions, such as HD or Full HD. Moreover, many ofthe available methods contain an immense number of pa-rameters [41]. This means that crowd counting modelsare often very computationally expensive, therefore, dy-namic inference methods such as early exiting and otherlightweight deep learning methods become essential in realworld applications.Although the main objective of this task is to obtain asingle count from an image, many methods treat this prob-lem as dense prediction where the output is a density mapdepicting the density of the crowd across the input image,and the total count is calculated by the sum of all valuesin the density map. Therefore, in most crowd countingdatasets, such as Shanghai Tech [42] and World Expo \u201910[43], the locations of the heads of individuals in the imageare annotated and provided as targets. A ground truthdensity map can then be obtained from these head anno-tations using Gaussian kernels or more complicated andFigure 3: An example image from the Shanghai Tech dataset and itscorresponding ground truth density map.specialized methods [41]. Figure 3 shows an image fromthe Shanghai Tech dataset and the ground truth densitymap that was generated from the provided head annota-tions using the method presented in Zhang et al [42]. Incrowd counting, Mean Absolute Error (MAE) is usuallyused as a measure of accuracy whereas Mean Squared Er-ror (MSE) is used as a measure of robustness [44].Many crowd counting methods exist in the literature[41], however, most of these methods are applied in asingle-modal fashion where the input is an image or a videoframe. In contrast, AudioCSRNet [45], a multi-modal ex-tension of the widely-used CSRNet model for crowd count-ing [46], takes as input the ambient audio of a scene inaddition to its image. The authors show that the ambi-ent audio improves the result in situations where the im-age quality is not ideal, for instance, low image resolution,presence of noise, occlusion and low illumination.In CSRNet, the features extracted from the input im-age by the \ufb01rst 10 layers of a VGG-16 [47] network pre-trained on the ImageNet dataset [48] are passed through6 dilated convolution layers and a 1 \xd71 convolution layerin order to obtain the density map. AudioCSRNet ex-tends this architecture by converting each of the dilatedconvolution layers into a fusion block. The architecture ofAudioCSRNet is depicted in Figure 4. First, a Mel spec-trogram is obtained from the raw audio waveform. Subse-quently, in each fusion block, the features extracted fromthe input Mel spectrogram by the \ufb01rst 6 layers of a VG-Gish [49] network pre-trained on the AudioSet dataset [49]are projected to two vectors called \u03b3 and \u03b2 which repre-sent the multiplicative and additive aspects of the audiofeatures. The \u03b3 and \u03b2 vectors are then tiled in order tomatch the size of the visual features. Finally, the outputof the dilated convolution is element-wise multiplied by \u03b3and added to \u03b2.The fusion operation can be summarized asvl+1 = Fl(\u03b3l \u2299Dl(vl) + \u03b2l), (3)where vl \u2208 RCl\xd7Wl\xd7Hl is the output of the l-th fusionblock, Fl denotes an activation function, \u03b3l and \u03b2l are thetiled vectors and Dl represents the l-th dilated convolu-tion.In practice, a batch normalization layer [50] is addedimmediately after each dilated convolution. Furthermore,the height and width of the intermediate features remain5--- Page 6 ---Figure 4: Architecture of AudioCSRNet.unchanged by using padding in the convolution operations,meaning Hl = Hl+1 and Wl = Wl+1. Additionally, sincethe \ufb01rst 10 layers of VGG-16 decrease both height andwidth by a factor of 8 via several pooling operations, the\ufb01nal result of the network needs to be upsampled by afactor of 8 in order to match the resolution of the inputimage. It is important to preserve the total sum of thedensity map during this upsampling operation, since itrepresents the total count.3. Single-Layer Vision Transformers for Early Ex-itsWe assume a pre-trained and high performing back-bone network is already available. Due to time constraintsarising from the particular application, it is desirable thatthe network provides a result within the speci\ufb01c deadlinerather than not providing a result at all, even though thisresult may be less accurate than it would be if time con-straints did not exist. Therefore, the backbone needs tobe augmented with early exit branches to allow for dy-namic inference and anytime prediction. As previouslymentioned, we use the classi\ufb01er-wise approach for trainingthe early exit branches since it results in \u201cplug-and-play\u201dbranches that can easily be added to the backbone networkwithout any re-training or hyper-parameter tuning.3.1. SL-ViTTypically, the architecture of early exit branches startswith one or more convolution layers, although some mayhave no convolutions at all. Afterwards, they may have apooling layer, which may be global pooling, and one MLP[51, 11]. Here, as a baseline, we choose to utilize the ar-chitecture depicted in Figure 5 with one 3 \xd73 convolution,followed by a 2 \xd72 max pooling layer and \ufb01nally a MLP.The size of the max pooling layer is increased to 4 \xd74 forcrowd counting since the input images have a very highresolution. Additionally, we use dropout [52] inside theMLP to avoid over\ufb01tting. We use a single convolutionsince early exits with two or more convolution layers havea high overhead and may even lead to lower accuracy [11].Early exits without convolutions are sometimes used verylate in the network, however, since they are straightfor-ward and leave no room for modi\ufb01cations, we do not applyour method for such cases. The resulting architecture is acommon setup within the literature, and is e\ufb00ectively thesame architecture used for earlier exits by Hu et al. [51].Figure 5: Architecture of CNN early exit branches. Size of the \ufb02at-tened feature vector depends on the dimensions of the features at thespeci\ufb01c branch location. For branches placed on the AudioCSRNetbackbone, max pooling size is increased to 4x4 since the input im-ages have a high resolution. Figure created using the NN-SVG tool[53].Our method called single-layer vision transformer orSL-ViT for short, is an alternative architecture for earlyexit branches that can achieve a higher accuracy comparedto the aforementioned baseline, while having less overheadin terms of the number of parameters as well as \ufb02oatingpoint operations per second (FLOPS). Our proposed ar-chitecture is based on the vision transformer architectureintroduced in section 2.2, where instead of the input im-age, we split the intermediate features at the branch loca-tion into patches (sub-tensors) and pass them to a visiontransformer.The choice of vision transformer architecture is pri-marily due to its global receptive \ufb01eld. Receptive \ufb01eld6--- Page 7 ---is crucial in many deep learning problems, including onesstudied in this work. The receptive \ufb01eld of state-of-the-art CNNs developed for image classi\ufb01cation has steadilyincreased over time and is correlated with increased classi-\ufb01cation accuracy [54]. Additionally, in audio classi\ufb01cationusing spectrograms, each location relates to a di\ufb00erent fre-quency band in a di\ufb00erent window of time. It is reasonableto assume that processing combinations of frequencies andwindows that are not necessarily adjacent could be of im-portance. Moreover, many crowd counting methods havemade use of global information through visual attentionmechanisms and dilated convolutions [41]. Since the re-ceptive \ufb01eld is particularly limited in early layers of CNNbackbones, choosing an architecture for early exit brancheswith a global receptive \ufb01eld could be bene\ufb01cial.Many other designs strive to increase the receptive \ufb01eldin their building blocks, for instance, the pyramid pool-ing module (PPM) in PSPNet [55] or atrous spatial pyra-mid pooling (ASPP) in DeepLab [56]. However, they allfall short in comparison with the global receptive \ufb01eld oftransformers. PPM increases the receptive \ufb01eld throughaggregating di\ufb00erent levels of pooling, which means far lo-cations have only access to coarse representations of eachother, and ASPP has holes in its receptive \ufb01eld.It is important to mention that the local receptive \ufb01eldof convolutional layers is not fundamentally bad. On thecontrary, it plays a key role in representation learning andextracting local information, especially in the early layersof the network where the receptive \ufb01eld of the convolu-tional \ufb01lters is small. Filters in successive convolutionallayers have increasingly larger receptive \ufb01elds, therefore,\ufb01nal layers in a CNN architecture have \ufb01lters of largeenough receptive \ufb01elds that can e\ufb00ectively aggregate in-formation from the entire input image to provide a properresponse. However, this process of cascading local recep-tive \ufb01elds of increasing size requires the number of layersin the CNN to be large, or at least all the layers in thenetwork to be traversed in order to provide the network\u2019sresponse. When an early exit is added at an early layer,this chain of increasingly larger receptive \ufb01elds is broken,and an early exit that has a local receptive \ufb01eld may notbe able to e\ufb00ectively aggregate all required informationin the image to provide a suitable response. This situa-tion is the motivation behind the proposed branch archi-tecture, which fuses the local receptive \ufb01eld of the layerin the network where the early exit branch is attached,with the global receptive \ufb01eld of the early exit, in orderto e\ufb00ectively aggregate information from the entire inputand provide a more accurate response. Indeed, the orig-inal Vision Transformer paper [15] attributes the successof their model to the combination of local and global re-ceptive \ufb01elds and shows that even in very early layers, thisability to integrate information globally is indeed used bythe model.There are some crucial di\ufb00erences between the origi-nal vision transformer and the architecture in our method.First, in order to introduce a low overhead for early exitbranches, we only use a single transformer encoder layerinstead of the original 12 to 36 layers, meaning that L= 1in our case. Secondly, we do not utilize a separate clas-si\ufb01cation token and instead pass the entire output of thetransformer encoder layer to the MLP head. This is pos-sible because the width and height of tensors are gener-ally reduced throughout CNN backbones by pooling oper-ations, and thus the number of patches in our architectureis lower than that of the original vision transformer. Inaddition to the number of patches, the size of the embed-ding dimension ( d) is also reduced in our proposed archi-tecture, introducing far less parameters when passing theentire output of the last transformer encoder layer to theMLP head, even with high-resolution inputs such as inour crowd counting experiments. Variations of our archi-tecture have 5 \xd75, 7 \xd77 or 16 \xd79 patches and embeddingdimensions of 32 or 36, whereas di\ufb00erent versions of theoriginal vision transformer have 14 \xd714 or 16 \xd716 patchesand embedding dimensions of 768, 1024 or 1280. We em-pirically found that using the entire transformer encoderoutput instead of just one classi\ufb01cation token can increasethe accuracy, perhaps because in a single-layer version,there are not enough layers for the classi\ufb01cation token tolearn to properly summarize other patches. Our proposedarchitecture is shown in Figure 6. It is also important tonote that the MLP head used in our architecture is exactlythe same as the MLP in the CNN early exit architecture.Figure 6: Architecture of SL-ViT early exit branches. Unlike typicalvision transformers, only a single transformer encoder layer is used,extra learnable classi\ufb01cation token is not added to the sequence andthe entire output of the transformer encoder is passed on to the MLPhead. The MLP head is the same as CNN early exit branches.Our model has several hyper-parameters, namely thesize of each patch, the embedding dimension d and thenumber of attention heads hin multi-head attention. Thepatch size creates a trade-o\ufb00 where smaller patches resultin a more \ufb01ne-grained attention mechanism while increas-ing the total number of parameters in a bi-quadratic fash-ion. Therefore, similar to the original vision transformer,we choose the size of the patch to be close to the square7--- Page 8 ---root of the height and width of the input features. We alsomake sure that the size of the patch can divide the size ofthe input features to avoid padding, for instance, a patchsize of 4 \xd74 for input features of size 28 \xd728. We performa grid search to \ufb01nd the values of d and h that result inthe highest accuracy, while keeping the total number ofparameters less than or equal to that of the CNN earlyexit counterpart.At a \ufb01rst glance, it might seem like the SL-ViT archi-tecture introduces more hyper-parameters than the con-ventional CNN architecture, however, the CNN architec-ture includes many design choices as well, such as the num-ber of \ufb01lters, \ufb01lter size, padding, dilation, stride, poolingtype and pooling size. The design choices for CNN archi-tectures might seem simpler since they have been studiedmore extensively compared to vision transformers whichwere introduced more recently.3.2. Audiovisual SL-ViTWith audiovisual backbones such as the AudioCSR-Net model for audiovisual crowd counting, described insection 2.4, it is desirable to have audiovisual early exitsthat use both visual and audio features in order to achievea higher accuracy. The simplest way to have such branchesis to add the branches after the blocks where the fusionof visual and audio features take place. However, withour proposed SL-ViT architecture, it is also possible toinclude audio features as one or more patches alongsideother patches, and directly fuse the features in the earlyexit. The advantage of this approach is that since in vi-sion transformers, any of the patches can pay attention toany other patch, the visual features can be fused with theaudio features without being directly impacted and mod-i\ufb01ed. In contrast, since convolutional \ufb01lters only take theimmediate vicinity into account, the audio features mustbe present in every location. One option is to concatenatethe visual features and the tiled audio features along thedepth. However, that would greatly increase the amount ofcomputation for each fusion operation, therefore intrusiveoperations such as element-wise multiplication and addi-tion are used instead.3.3. Copycat SL-ViTFinally, we introduce a \ufb01ne-tuning strategy for SL-ViTbranches that can further increase their accuracy. Correia-Silva et al. [16] developed a method called copycat CNNwhere they create a \u201cfake\u201d dataset by taking images fromanother domain, giving them as input to a network trainedon the target domain, and recording the output of thenetwork as labels for these images. For instance, imagesfrom the ImageNet dataset [48] can be given to a networktrained on the CIFAR-10 dataset [57], where the imageof a camel may be labelled as a \u201cdog\u201d since there are nolabels for \u201ccamel\u201d in CIFAR-10. This fake dataset is thencombined with a dataset for the target domain and usedto train a new network. We use this strategy to \ufb01ne-tunean already trained SL-ViT branch and obtain a copycatsingle-layer vision transformer (CC-SL-ViT ). Note thatthe ratio of the fake data mixed with the available datasetis a hyper-parameter of this \ufb01ne-tuning strategy.4. Experimental SetupIn this section, we provide the details of our experi-ments. We begin by giving a short summary of the datasetsas well as the training details for the backbone networks.We then lay out the details of the branch architectures,their training procedure and their placement on the back-bone networks, and \ufb01nally explain how the copycat strat-egy was used to \ufb01ne-tune the branches.A total of 27 di\ufb00erent scenarios were tested in our ex-periments. For both image and audio classi\ufb01cation, twodatasets, three backbone networks and two di\ufb00erent branchlocations on each backbone were tested. In addition, threedi\ufb00erent branch locations for the audiovisual crowd count-ing backbone network were covered. All experiments wererepeated 5 times and the average accuracy as well as thestandard deviation were recorded. 4 \xd7 Nvidia 2080TiGPUs were used for the training of our models.4.1. Datasets4.1.1. CIFAR-10 and CIFAR-100These are widely-used datasets for image classi\ufb01cation[57]. Both datasets consist of 60,000 color images of size32 \xd732 pixels and their corresponding class labels. Theimages in CIFAR-10 and CIFAR-100 are categorized into10 and 100 di\ufb00erent classes, respectively. We use 40,000examples for training, 10,000 for validation and another10,000 for testing. Since our backbone networks are pre-trained on ImageNet which consists of 224 \xd7224 pixelimages, we resize each image to these dimensions beforepassing them into the network.4.1.2. Speech Commands (SC)A well-known audio dataset of spoken words [58]. Itconsists of 100,503 1-second audio clips with a samplingrate of 16kHz, each labelled as one of 12 classes: 10 dif-ferent spoken words such as \u201cYes\u201d, \u201cNo\u201d, \u201cDown\u201d and\u201cStop\u201d as well as one class for background noise and an-other for unknown words. We use 85,511 examples fortraining, 10,102 for validation and 4,890 for testing. Weconvert the raw audio waveforms into spectrograms usingshort-time Fourier transform (STFT) with a window sizeof 255 samples and step size of 128 samples, and resize theresulting spectrograms to 224 \xd7224 before passing theminto the network.4.1.3. GTZANIt is the most widely-used dataset for music genre recog-nition [59]. The original dataset consists of 10 genres suchas \u201cPop\u201d and \u201cRock\u201d and 100 30-second audio clips pergenre with a sampling rate of 22,050Hz. We follow the8--- Page 9 ---common approach to split each audio clip into 10 separate3-second clips in order to increase the size of the datasetto 10,000. We use 8,000 examples for training, 1,000 forvalidation and another 1,000 for testing. Following the ap-proach of Palanisamy et al. [39] where di\ufb00erent spectro-grams with di\ufb00erent parameters are placed in each channelof the input image, we use one spectrogram obtained fromSTFT with window size of 512 samples and step size of 256samples as well as two Mel spectrograms with 128 bins andwindow sizes of 100 and 50 milliseconds, and step sizes of50 and 25 milliseconds, respectively.4.1.4. DISCOAn audiovisual dataset for crowd counting which con-tains 1,935 images of Full HD resolution (1920 \xd71080)[45]. For each image, a corresponding 1-second audio clipof ambient sounds with a sampling rate of 48kHz, starting0.5 seconds before the image was taken and ending 0.5 sec-onds afterwards, exists as well. The labels are provided inthe form of head annotations in the image. At the time ofthis writing, DISCO is the only publicly available datasetfor audiovisual crowd counting. We use 1435 examples fortraining, 200 for validation and 300 for testing. The inputimage is resized to 1024 \xd7576 pixels to reduce memoryand computation requirements. Similar to Hershey et al.[49], the input audio waveform is transformed into a Melspectrogram with 64 bins, window size of 25 millisecondsand step size of 10 milliseconds. Following Hu et al. [45]the ground truth density maps are obtained by convolv-ing the head annotations with a 15 \xd715 Gaussian kernelK\u223cN (0,4.0).4.2. Backbone networksTransfer learning is used to train the ResNet152,DenseNet201 and InceptionV3 backbone networks for bothimage and audio classi\ufb01cation. The backbone networks areall pre-trained on the ImageNet dataset and the top layeris replaced. We found that instead of adding just one denselayer at the top, as is common in transfer learning, usingtwo dense layers and a dropout layer in between leads to ahigher accuracy in our case. The resulting network is thentrained using the Adam optimizer [60] with a learning rateof 10\u22124 and categorical cross-entropy loss function. Thelearning rate is reduced by a factor of 0.6 on plateau witha tolerance of 2 epochs, and an early stopping mechanismwith a tolerance of 5 epochs is used.The audiovisual crowd counting backbone is trained intwo stages. We \ufb01rst train a network with the AudioCSR-Net architecture described in Section 2.4 for 100 epochs.L2 norm is used as loss function and AdamW [61] with alearning rate of 10 \u22125 and weight decay of 10 \u22124 is used asoptimizer, where the learning rate is multiplied by a factorof 0.99 each epoch. This is the same training procedureused in the original paper [45]. Subsequently, in order toconvert the problem from dense prediction to regression,a dense layer with an output size of one is added after thelast layer of the trained AudioCSRNet. This layer is ini-tialized as a sum, meaning the initial weights are all equalto one and no bias is used. Then the entire network isre-trained for another 100 epochs using MAE as loss func-tion instead of the previous L2 loss, a learning rate of 10\u22126and weight decay of 10 \u22125. The learning rate is similarlymultiplied by a factor of 0.99 every epoch. The resultingmodel achieves a MAE of 13.63 which is even lower thanthe MAE of 14.27 reported in the original paper. However,the output of the network is just a single number repre-senting the total count instead of a density map. The \ufb01nalaccuracy of all trained backbones can be seen in Table 1.When training the backbone networks, in order to \ufb01tthe limitations of our available computational resources,the batch sizes are adjusted and some layers of the back-bone networks are frozen. All backbone networks weretrained with a batch size of 32 except AudioCSRNet whichhas a batch size of 4 as well as InceptionV3 when trainedon CIFAR-10 and CIFAR-100 which has a batch size of 64.All layers of the backbone networks were trained, exceptin the case of ResNet152 and DenseNet201 when trainedon CIFAR-10 and CIFAR-100 where only the batch nor-malization layers were trained. We found that trainingonly the batch normalization layers is su\ufb03cient to achievea high-performing backbone network in these cases [62].Table 1: Performance of backbone networks on each datasetBackboneCIFAR-10 Acc. CIFAR-100 Acc. SC Acc. GTZAN Acc. DISCO MAEResNet152 95.36% 82.25% 95.85% 91.29% -DenseNet20196.48% 82.53% 96.36% 92.09% -InceptionV396.56% 83.80% 94.93% 87.79% -AudioCSRNet- - - - 13.634.3. BranchesAll branches were trained from scratch using the Heinitialization method [63] and the Adam optimizer with alearning rate of 10 \u22124 where the learning rate is reducedby a factor of 0.6 on plateau with a tolerance of 2 epochs,and an early stopping mechanism with a tolerance of 5epochs is utilized. The branches on classi\ufb01cation back-bones use a categorical cross-entropy loss function whereasthe branches on the audiovisual crowd counting backboneuse mean absolute error loss. The training batch size forbranches were 64 in scenarios involving CIFAR-10, CIFAR-100 and Speech Commands, 32 in scenarios involvingGTZAN and 4 in scenarios involving DISCO.Table 2 shows the location of the branches placed oneach backbone network. For the AudioCSRNet backbonenetwork, branch V1 uses only the output of the VGG-16layers, therefore, it only has access to the visual features.Branch AV1 uses the outputs of both VGG-16 and VG-Gish, therefore it has access to both audio and visual fea-tures. In this branch location, the fusion of audio andvisual features is performed as described in Section 3 forthe SL-ViT architecture, and similar to the fusion blocks9--- Page 10 ---in AudioCSRNet for the CNN architecture, however, with-out dilation. Finally, branch AV2 is placed after the \ufb01rstfusion block in AudioCSRNet, therefore audio and visualfeatures have already been fused and thus fusion operationis not required within the branches. Adding branches afterthe second fusion block or later would not be reasonablesince more than 85% of the computation of the backboneis carried out before that point, and thus the accelerationresulting from early exits would be negligible.Table 2: Placement of branches for each backbone betworkBackbone BN \u2217 Branch Placed AfterDenseNet201 1 Transition Layer 12 Transition Layer 2ResNet152 1 12th Convolution2 36th ConvolutionInceptionV3 1 First Filter Concat2 Second Filter ConcatAudioCSRNet V1 Last Layer of VGGAV1 Last Layers of VGG and VGGishAV2 First Fusion Block\u2217Branch Number4.4. SL-ViT and CC-SL-ViT ParametersTable 3 summarizes the hyper-parameters used for theSL-ViT branches in each scenario. \u201cPatch Size\u201d shows thewidth and height of each image patch, \u201cPatches\u201d denotesthe resulting number of patches across width and heightof the input image, d is the size of embedding dimensionand h is the number of heads in multi-head attention.For copycat SL-ViT, images from the Tiny ImageNetdataset, which are the images from ImageNet down-sampledto 32\xd732, were given to the InceptionV3 backbone trainedon CIFAR-10, and the outputs were used to create the fakedataset. Then the fake dataset was mixed with CIFAR-10with a 2-to-1 ratio and used for re-training.Table 3: Hyper-parameters of SL-ViT for di\ufb00erent backbone net-works and branchesBackbone Dataset BN \u2217 Patch Size Patchesd hDenseNet201 all all 4x4 7x7 32 12ResNet152 SC 2 4x4 7x7 32 24GTZAN 2 4x4 7x7 32 24Other 4x4 7x7 32 12InceptionV3 CIFAR-100 all 5x5 5x5 36 8Other 5x5 5x5 32 12AudioCSRNet DISCO all 8x8 16x9 32 12\u2217Branch Number5. ResultsThe results of our experiments are presented in Tables4 to 8. In these Tables, the \ufb01nal accuracy, the total FLOPSof the model up to and including the branch and the num-ber of parameters of just the early exit branch are com-pared between the CNN architecture and the SL-ViT ar-chitecture. Higher accuracies, lower errors, lower numberof parameters and lower total FLOPS are highlighted inthese tables. Furthermore, the acceleration caused by SL-ViT early exits, de\ufb01ned as the total FLOPS of the back-bone network divided by the total FLOPS of the modelup to and including the SL-ViT branch, is also provided.Several observations can be made about these results.First, in all scenarios except one, SL-ViT early exits achievea signi\ufb01cantly higher accuracy. Even in the one excep-tional scenario, namely branch 2 of ResNet152 in Table6, the accuracy of SL-ViT is very close to its CNN coun-terpart. Secondly, while in some cases SL-ViT brancheshave an equal number of parameters compared to CNNbranches, in all scenarios, the total FLOPS of SL-ViTbranches is, Your response: ```ADD_PAPER\n2105.09121v3\nThis paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with just one transformer encoder layer. Experiments across image classification, audio classification, and audiovisual crowd counting demonstrate SL-ViT's effectiveness, achieving higher accuracy with fewer parameters and FLOPS compared to CNN-based approaches. The paper also presents a novel fine-tuning strategy (CC-SL-ViT) that further enhances performance. Key results show SL-ViT branches outperform CNN counterparts in 26 out of 27 tested scenarios while reducing computational costs, making it particularly suitable for resource-constrained edge devices requiring dynamic inference capabilities.\n```\nStep #30, Phase: literature review, Feedback: Successfully added paper 2105.09121v3, Your response: ```SUMMARY\nedge image classification sparsity techniques\n```\nStep #31, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification sparsity techniques, here was the response\nTitle: Exploiting Activation based Gradient Output Sparsity to Accelerate Backpropagation in CNNs\nSummary: Machine/deep-learning (ML/DL) based techniques are emerging as a driving\nforce behind many cutting-edge technologies, achieving high accuracy on\ncomputer vision workloads such as image classification and object detection.\nHowever, training these models involving large parameters is both\ntime-consuming and energy-hogging. In this regard, several prior works have\nadvocated for sparsity to speed up the of DL training and more so, the\ninference phase. This work begins with the observation that during training,\nsparsity in the forward and backward passes are correlated. In that context, we\ninvestigate two types of sparsity (input and output type) inherent in gradient\ndescent-based optimization algorithms and propose a hardware micro-architecture\nto leverage the same. Our experimental results use five state-of-the-art CNN\nmodels on the Imagenet dataset, and show back propagation speedups in the range\nof 1.69$\\times$ to 5.43$\\times$, compared to the dense baseline execution. By\nexploiting sparsity in both the forward and backward passes, speedup\nimprovements range from 1.68$\\times$ to 3.30$\\times$ over the sparsity-agnostic\nbaseline execution. Our work also achieves significant reduction in training\niteration time over several previously proposed dense as well as sparse\naccelerator based platforms, in addition to achieving order of magnitude energy\nefficiency improvements over GPU based execution.\nPublication Date: 2021-09-16\narXiv paper ID: 2109.07710v1\n\nTitle: Super-Resolution of Brain MRI Images using Overcomplete Dictionaries and Nonlocal Similarity\nSummary: Recently, the Magnetic Resonance Imaging (MRI) images have limited and\nunsatisfactory resolutions due to various constraints such as physical,\ntechnological and economic considerations. Super-resolution techniques can\nobtain high-resolution MRI images. The traditional methods obtained the\nresolution enhancement of brain MRI by interpolations, affecting the accuracy\nof the following diagnose process. The requirement for brain image quality is\nfast increasing. In this paper, we propose an image super-resolution (SR)\nmethod based on overcomplete dictionaries and inherent similarity of an image\nto recover the high-resolution (HR) image from a single low-resolution (LR)\nimage. We explore the nonlocal similarity of the image to tentatively search\nfor similar blocks in the whole image and present a joint reconstruction method\nbased on compressive sensing (CS) and similarity constraints. The sparsity and\nself-similarity of the image blocks are taken as the constraints. The proposed\nmethod is summarized in the following steps. First, a dictionary classification\nmethod based on the measurement domain is presented. The image blocks are\nclassified into smooth, texture and edge parts by analyzing their features in\nthe measurement domain. Then, the corresponding dictionaries are trained using\nthe classified image blocks. Equally important, in the reconstruction part, we\nuse the CS reconstruction method to recover the HR brain MRI image, considering\nboth nonlocal similarity and the sparsity of an image as the constraints. This\nmethod performs better both visually and quantitatively than some existing\nmethods.\nPublication Date: 2019-02-13\narXiv paper ID: 1902.04902v1\n\nTitle: Sparsity Turns Adversarial: Energy and Latency Attacks on Deep Neural Networks\nSummary: Adversarial attacks have exposed serious vulnerabilities in Deep Neural\nNetworks (DNNs) through their ability to force misclassifications through\nhuman-imperceptible perturbations to DNN inputs. We explore a new direction in\nthe field of adversarial attacks by suggesting attacks that aim to degrade the\ncomputational efficiency of DNNs rather than their classification accuracy.\nSpecifically, we propose and demonstrate sparsity attacks, which adversarial\nmodify a DNN's inputs so as to reduce sparsity (or the presence of zero values)\nin its internal activation values. In resource-constrained systems, a wide\nrange of hardware and software techniques have been proposed that exploit\nsparsity to improve DNN efficiency. The proposed attack increases the execution\ntime and energy consumption of sparsity-optimized DNN implementations, raising\nconcern over their deployment in latency and energy-critical applications.\n  We propose a systematic methodology to generate adversarial inputs for\nsparsity attacks by formulating an objective function that quantifies the\nnetwork's activation sparsity, and minimizing this function using iterative\ngradient-descent techniques. We launch both white-box and black-box versions of\nadversarial sparsity attacks on image recognition DNNs and demonstrate that\nthey decrease activation sparsity by up to 1.82x. We also evaluate the impact\nof the attack on a sparsity-optimized DNN accelerator and demonstrate\ndegradations up to 1.59x in latency, and also study the performance of the\nattack on a sparsity-optimized general-purpose processor. Finally, we evaluate\ndefense techniques such as activation thresholding and input quantization and\ndemonstrate that the proposed attack is able to withstand them, highlighting\nthe need for further efforts in this new direction within the field of\nadversarial machine learning.\nPublication Date: 2020-06-14\narXiv paper ID: 2006.08020v3\n\nTitle: Leaf segmentation through the classification of edges\nSummary: We present an approach to leaf level segmentation of images of Arabidopsis\nthaliana plants based upon detected edges. We introduce a novel approach to\nedge classification, which forms an important part of a method to both count\nthe leaves and establish the leaf area of a growing plant from images obtained\nin a high-throughput phenotyping system. Our technique uses a relatively\nshallow convolutional neural network to classify image edges as background,\nplant edge, leaf-on-leaf edge or internal leaf noise. The edges themselves were\nfound using the Canny edge detector and the classified edges can be used with\nsimple image processing techniques to generate a region-based segmentation in\nwhich the leaves are distinct. This approach is strong at distinguishing\noccluding pairs of leaves where one leaf is largely hidden, a situation which\nhas proved troublesome for plant image analysis systems in the past. In\naddition, we introduce the publicly available plant image dataset that was used\nfor this work.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.03124v1\n\nTitle: Network classification with applications to brain connectomics\nSummary: While statistical analysis of a single network has received a lot of\nattention in recent years, with a focus on social networks, analysis of a\nsample of networks presents its own challenges which require a different set of\nanalytic tools. Here we study the problem of classification of networks with\nlabeled nodes, motivated by applications in neuroimaging. Brain networks are\nconstructed from imaging data to represent functional connectivity between\nregions of the brain, and previous work has shown the potential of such\nnetworks to distinguish between various brain disorders, giving rise to a\nnetwork classification problem. Existing approaches tend to either treat all\nedge weights as a long vector, ignoring the network structure, or focus on\ngraph topology as represented by summary measures while ignoring the edge\nweights. Our goal is to design a classification method that uses both the\nindividual edge information and the network structure of the data in a\ncomputationally efficient way, and that can produce a parsimonious and\ninterpretable representation of differences in brain connectivity patterns\nbetween classes. We propose a graph classification method that uses edge\nweights as predictors but incorporates the network nature of the data via\npenalties that promote sparsity in the number of nodes, in addition to the\nusual sparsity penalties that encourage selection of edges. We implement the\nmethod via efficient convex optimization and provide a detailed analysis of\ndata from two fMRI studies of schizophrenia.\nPublication Date: 2017-01-27\narXiv paper ID: 1701.08140v3\n, Your response: ```SUMMARY\nedge image classification pruning quantization techniques\n```\n~~~~~~~~~~\nCurrent Step #32, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification pruning quantization techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n\nTitle: Digital-SC: Digital Semantic Communication with Adaptive Network Split and Learned Non-Linear Quantization\nSummary: Semantic communication, an intelligent communication paradigm that aims to\ntransmit useful information in the semantic domain, is facilitated by deep\nlearning techniques. Robust semantic features can be learned and transmitted in\nan analog fashion, but it poses new challenges to hardware, protocol, and\nencryption. In this paper, we propose a digital semantic communication system,\nwhich consists of an encoding network deployed on a resource-limited device and\na decoding network deployed at the edge. To acquire better semantic\nrepresentation for digital transmission, a novel non-linear quantization module\nis proposed to efficiently quantize semantic features with trainable\nquantization levels. Additionally, structured pruning is incorporated to reduce\nthe dimension of the transmitted features. We also introduce a semantic\nlearning loss (SLL) function to reduce semantic error. To adapt to various\nchannel conditions and inputs under constraints of communication and computing\nresources, a policy network is designed to adaptively choose the split point\nand the dimension of the transmitted semantic features. Experiments using the\nCIFAR-10 and ImageNet dataset for image classification are employed to evaluate\nthe proposed digital semantic communication network, and ablation studies are\nconducted to assess the proposed quantization module, structured pruning and\nSLL.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.13553v2\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification pruning quantization techniques\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 34:
\n~~~~~~~~~~\nHistory: Step #16, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: A Rate-Distortion-Classification Approach for Lossy Image Compression\nSummary: In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\nPublication Date: 2024-05-06\narXiv paper ID: 2405.03500v1\n\nTitle: Federated learning compression designed for lightweight communications\nSummary: Federated Learning (FL) is a promising distributed method for edge-level\nmachine learning, particularly for privacysensitive applications such as those\nin military and medical domains, where client data cannot be shared or\ntransferred to a cloud computing server. In many use-cases, communication cost\nis a major challenge in FL due to its natural intensive network usage. Client\ndevices, such as smartphones or Internet of Things (IoT) nodes, have limited\nresources in terms of energy, computation, and memory. To address these\nhardware constraints, lightweight models and compression techniques such as\npruning and quantization are commonly adopted in centralised paradigms. In this\npaper, we investigate the impact of compression techniques on FL for a typical\nimage classification task. Going further, we demonstrate that a straightforward\nmethod can compresses messages up to 50% while having less than 1% of accuracy\nloss, competing with state-of-the-art techniques.\nPublication Date: 2023-10-23\narXiv paper ID: 2310.14693v1\n, Your response: ```SUMMARY\nedge image classification quantization techniques\n```\nStep #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #25, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge image classification dynamic computation optimization\n```\nStep #26, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification dynamic computation optimization, here was the response\nTitle: Immiscible Color Flows in Optimal Transport Networks for Image Classification\nSummary: In classification tasks, it is crucial to meaningfully exploit the\ninformation contained in data. While much of the work in addressing these tasks\nis devoted to building complex algorithmic infrastructures to process inputs in\na black-box fashion, less is known about how to exploit the various facets of\nthe data, before inputting this into an algorithm. Here, we focus on this\nlatter perspective, by proposing a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nThe resulting optimal flows can then be fed into standard classifiers to\ndistinguish images in different classes. We show how our method can outperform\ncompeting approaches on image classification tasks in datasets where color\ninformation matters.\nPublication Date: 2022-05-04\narXiv paper ID: 2205.02938v2\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)\nSummary: Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\nPublication Date: 2022-04-10\narXiv paper ID: 2204.04704v1\n\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n, Your response: ```SUMMARY\nedge image classification adaptive inference techniques\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification adaptive inference techniques, here was the response\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection to Suppress Adversarial Perturbation\nSummary: This paper presents a novel context-aware image denoising algorithm that\ncombines an adaptive image smoothing technique and color reduction techniques\nto remove perturbation from adversarial images. Adaptive image smoothing is\nachieved using auto-threshold canny edge detection to produce an accurate edge\nmap used to produce a blurred image that preserves more edge features. The\nproposed algorithm then uses color reduction techniques to reconstruct the\nimage using only a few representative colors. Through this technique, the\nalgorithm can reduce the effects of adversarial perturbations on images. We\nalso discuss experimental data on classification accuracy. Our results showed\nthat the proposed approach reduces adversarial perturbation in adversarial\nattacks and increases the robustness of the deep convolutional neural network\nmodels.\nPublication Date: 2021-01-14\narXiv paper ID: 2101.05833v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n, Your response: ```SUMMARY\nedge image classification early exit networks\n```\nStep #28, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit networks, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\nStep #30, Phase: literature review, Feedback: Successfully added paper 2105.09121v3, Your response: ```SUMMARY\nedge image classification sparsity techniques\n```\nStep #31, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification sparsity techniques, here was the response\nTitle: Exploiting Activation based Gradient Output Sparsity to Accelerate Backpropagation in CNNs\nSummary: Machine/deep-learning (ML/DL) based techniques are emerging as a driving\nforce behind many cutting-edge technologies, achieving high accuracy on\ncomputer vision workloads such as image classification and object detection.\nHowever, training these models involving large parameters is both\ntime-consuming and energy-hogging. In this regard, several prior works have\nadvocated for sparsity to speed up the of DL training and more so, the\ninference phase. This work begins with the observation that during training,\nsparsity in the forward and backward passes are correlated. In that context, we\ninvestigate two types of sparsity (input and output type) inherent in gradient\ndescent-based optimization algorithms and propose a hardware micro-architecture\nto leverage the same. Our experimental results use five state-of-the-art CNN\nmodels on the Imagenet dataset, and show back propagation speedups in the range\nof 1.69$\\times$ to 5.43$\\times$, compared to the dense baseline execution. By\nexploiting sparsity in both the forward and backward passes, speedup\nimprovements range from 1.68$\\times$ to 3.30$\\times$ over the sparsity-agnostic\nbaseline execution. Our work also achieves significant reduction in training\niteration time over several previously proposed dense as well as sparse\naccelerator based platforms, in addition to achieving order of magnitude energy\nefficiency improvements over GPU based execution.\nPublication Date: 2021-09-16\narXiv paper ID: 2109.07710v1\n\nTitle: Super-Resolution of Brain MRI Images using Overcomplete Dictionaries and Nonlocal Similarity\nSummary: Recently, the Magnetic Resonance Imaging (MRI) images have limited and\nunsatisfactory resolutions due to various constraints such as physical,\ntechnological and economic considerations. Super-resolution techniques can\nobtain high-resolution MRI images. The traditional methods obtained the\nresolution enhancement of brain MRI by interpolations, affecting the accuracy\nof the following diagnose process. The requirement for brain image quality is\nfast increasing. In this paper, we propose an image super-resolution (SR)\nmethod based on overcomplete dictionaries and inherent similarity of an image\nto recover the high-resolution (HR) image from a single low-resolution (LR)\nimage. We explore the nonlocal similarity of the image to tentatively search\nfor similar blocks in the whole image and present a joint reconstruction method\nbased on compressive sensing (CS) and similarity constraints. The sparsity and\nself-similarity of the image blocks are taken as the constraints. The proposed\nmethod is summarized in the following steps. First, a dictionary classification\nmethod based on the measurement domain is presented. The image blocks are\nclassified into smooth, texture and edge parts by analyzing their features in\nthe measurement domain. Then, the corresponding dictionaries are trained using\nthe classified image blocks. Equally important, in the reconstruction part, we\nuse the CS reconstruction method to recover the HR brain MRI image, considering\nboth nonlocal similarity and the sparsity of an image as the constraints. This\nmethod performs better both visually and quantitatively than some existing\nmethods.\nPublication Date: 2019-02-13\narXiv paper ID: 1902.04902v1\n\nTitle: Sparsity Turns Adversarial: Energy and Latency Attacks on Deep Neural Networks\nSummary: Adversarial attacks have exposed serious vulnerabilities in Deep Neural\nNetworks (DNNs) through their ability to force misclassifications through\nhuman-imperceptible perturbations to DNN inputs. We explore a new direction in\nthe field of adversarial attacks by suggesting attacks that aim to degrade the\ncomputational efficiency of DNNs rather than their classification accuracy.\nSpecifically, we propose and demonstrate sparsity attacks, which adversarial\nmodify a DNN's inputs so as to reduce sparsity (or the presence of zero values)\nin its internal activation values. In resource-constrained systems, a wide\nrange of hardware and software techniques have been proposed that exploit\nsparsity to improve DNN efficiency. The proposed attack increases the execution\ntime and energy consumption of sparsity-optimized DNN implementations, raising\nconcern over their deployment in latency and energy-critical applications.\n  We propose a systematic methodology to generate adversarial inputs for\nsparsity attacks by formulating an objective function that quantifies the\nnetwork's activation sparsity, and minimizing this function using iterative\ngradient-descent techniques. We launch both white-box and black-box versions of\nadversarial sparsity attacks on image recognition DNNs and demonstrate that\nthey decrease activation sparsity by up to 1.82x. We also evaluate the impact\nof the attack on a sparsity-optimized DNN accelerator and demonstrate\ndegradations up to 1.59x in latency, and also study the performance of the\nattack on a sparsity-optimized general-purpose processor. Finally, we evaluate\ndefense techniques such as activation thresholding and input quantization and\ndemonstrate that the proposed attack is able to withstand them, highlighting\nthe need for further efforts in this new direction within the field of\nadversarial machine learning.\nPublication Date: 2020-06-14\narXiv paper ID: 2006.08020v3\n\nTitle: Leaf segmentation through the classification of edges\nSummary: We present an approach to leaf level segmentation of images of Arabidopsis\nthaliana plants based upon detected edges. We introduce a novel approach to\nedge classification, which forms an important part of a method to both count\nthe leaves and establish the leaf area of a growing plant from images obtained\nin a high-throughput phenotyping system. Our technique uses a relatively\nshallow convolutional neural network to classify image edges as background,\nplant edge, leaf-on-leaf edge or internal leaf noise. The edges themselves were\nfound using the Canny edge detector and the classified edges can be used with\nsimple image processing techniques to generate a region-based segmentation in\nwhich the leaves are distinct. This approach is strong at distinguishing\noccluding pairs of leaves where one leaf is largely hidden, a situation which\nhas proved troublesome for plant image analysis systems in the past. In\naddition, we introduce the publicly available plant image dataset that was used\nfor this work.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.03124v1\n\nTitle: Network classification with applications to brain connectomics\nSummary: While statistical analysis of a single network has received a lot of\nattention in recent years, with a focus on social networks, analysis of a\nsample of networks presents its own challenges which require a different set of\nanalytic tools. Here we study the problem of classification of networks with\nlabeled nodes, motivated by applications in neuroimaging. Brain networks are\nconstructed from imaging data to represent functional connectivity between\nregions of the brain, and previous work has shown the potential of such\nnetworks to distinguish between various brain disorders, giving rise to a\nnetwork classification problem. Existing approaches tend to either treat all\nedge weights as a long vector, ignoring the network structure, or focus on\ngraph topology as represented by summary measures while ignoring the edge\nweights. Our goal is to design a classification method that uses both the\nindividual edge information and the network structure of the data in a\ncomputationally efficient way, and that can produce a parsimonious and\ninterpretable representation of differences in brain connectivity patterns\nbetween classes. We propose a graph classification method that uses edge\nweights as predictors but incorporates the network nature of the data via\npenalties that promote sparsity in the number of nodes, in addition to the\nusual sparsity penalties that encourage selection of edges. We implement the\nmethod via efficient convex optimization and provide a detailed analysis of\ndata from two fMRI studies of schizophrenia.\nPublication Date: 2017-01-27\narXiv paper ID: 1701.08140v3\n, Your response: ```SUMMARY\nedge image classification pruning quantization techniques\n```\nStep #32, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification pruning quantization techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n\nTitle: Digital-SC: Digital Semantic Communication with Adaptive Network Split and Learned Non-Linear Quantization\nSummary: Semantic communication, an intelligent communication paradigm that aims to\ntransmit useful information in the semantic domain, is facilitated by deep\nlearning techniques. Robust semantic features can be learned and transmitted in\nan analog fashion, but it poses new challenges to hardware, protocol, and\nencryption. In this paper, we propose a digital semantic communication system,\nwhich consists of an encoding network deployed on a resource-limited device and\na decoding network deployed at the edge. To acquire better semantic\nrepresentation for digital transmission, a novel non-linear quantization module\nis proposed to efficiently quantize semantic features with trainable\nquantization levels. Additionally, structured pruning is incorporated to reduce\nthe dimension of the transmitted features. We also introduce a semantic\nlearning loss (SLL) function to reduce semantic error. To adapt to various\nchannel conditions and inputs under constraints of communication and computing\nresources, a policy network is designed to adaptively choose the split point\nand the dimension of the transmitted semantic features. Experiments using the\nCIFAR-10 and ImageNet dataset for image classification are employed to evaluate\nthe proposed digital semantic communication network, and ablation studies are\nconducted to assess the proposed quantization module, structured pruning and\nSLL.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.13553v2\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\n~~~~~~~~~~\nCurrent Step #33, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---1Edge AI: Evaluation of Model CompressionTechniques for Convolutional Neural NetworksSamer Francy, Member, IEEE,Raghubir Singh, Member, IEEEAbstract\u2014This work evaluates the compression techniqueson ConvNeXt models in image classification tasks using theCIFAR-10 dataset. Structured pruning, unstructured pruning,and dynamic quantization methods are evaluated to reduce modelsize and computational complexity while maintaining accuracy.The experiments, conducted on cloud-based platforms and edgedevice, assess the performance of these techniques. Results showsignificant reductions in model size, with up to 75% reduc-tion achieved using structured pruning techniques. Additionally,dynamic quantization achieves a reduction of up to 95% inthe number of parameters. Fine-tuned models exhibit improvedcompression performance, indicating the benefits of pre-trainingin conjunction with compression techniques. Unstructured prun-ing methods reveal trends in accuracy and compression, withlimited reductions in computational complexity. The combinationof OTOV3 pruning and dynamic quantization further enhancescompression performance, resulting 89.7% reduction in size, 95%reduction with number of parameters and MACs, and 3.8%increase with accuracy. The deployment of the final compressedmodel on edge device demonstrates high accuracy 92.5% and lowinference time 20 ms, validating the effectiveness of compressiontechniques for real-world edge computing applications.Index Terms\u2014edge AI, ConvNeXt, CNN, pruning, quantiza-tion, compression, OTO.I. I NTRODUCTIONEDGE devices such as Internet of Things (IoT) are becom-ing increasingly important and widely used in our dailylives and industrial facilities. IoT is a network of things thatempowered by sensors, identifiers, software intelligence, andinternet connectivity, it can be considered as the intersectionof the internet, things/objects (anything/everything), and data[1]. The number of these devices is expected to increase evenmore [2]. These devices have the potential to perform complexArtificial Intelligence (AI) tasks locally, without relying heav-ily on cloud infrastructure [3]. The rapid advancement of AIhas led to the development of complex deep learning modelsthat show high performance in different domains. DeployingAI models on edge devices has many advantages such as lowlatency, privacy and data security, bandwidth optimization, andreduced network dependence. Low latency is achieved due toreal-time processing by instant data analysis on edge withoutwaiting for remote server processing, this data analysis on theedge reduces transmitting data to the cloud which enhancessecurity against breaches, reduces the bandwidth consumption,and reduces network dependence.Samer Francy is with the Department of Computer Science, University ofBath, Bath, England (e-mail: samer.francy@bath.edu).Raghubir Singh is with the Department of Computer Science, Universityof Bath, Bath, England (e-mail: rs3022@bath.ac.uk).A. Overview of Edge AIEdge AI represents a paradigm shift in the way AI isimplemented in the context of the IoT. It capitalizes on thecapabilities of IoT devices, enhancing real-time processing,analytics, and decision-making directly at the edge of thenetwork. The IoT architecture, which is the foundation forEdge AI, typically involves three core layers [1]. The layersare perceptual layer, where data is collected from varioussensors and devices, network layer, where data is transmittedand routed through this layer, which is responsible for commu-nication between devices and cloud services, and applicationlayer, which processes and utilizes the data, providing insightsand enabling actions.B. Convolutional Neural Networks (CNNs)CNN models are subsets of Deep Neural Networks (DNN)models. CNN models are effective for image and video-relatedtasks due to their ability to learn relevant features from thedata by recognizing patterns, shapes, and structures in images,which is challenging for traditional machine learning models,that\u2019s why they are used for computer vision tasks such asimage classification, object detection, and image segmentation[4].1) CNN Architecture: In general CNN models consist ofbelow parts:\u2022 Input Image : Pixels form the binary basis of computerimages, while the human visual system operates throughneurons with receptive fields. Similarly, CNNs functionwithin their receptive areas, starting with simple patternsand advancing to more complex ones, making CNNs apromising tool for computer vision [4].\u2022 Convolutional Layer : A convolutional layer in a CNNuses a small filter (e.g., 3x3 or 5x5) that slides over theinput image. At each position, it multiplies its valueswith the overlapping image pixels and sums the results toproduce an output. This sliding operation helps identifylocal features like edges and colors, building a hierar-chical representation. The depth of the filter matches theinput image\u2019s channels (e.g., 3 for RGB images). Stackingmultiple filters allows the network to learn features atdifferent abstraction levels [4].\u2022 Pooling Layer : Pooling reduces the spatial size of fea-ture maps. This not only lowers computational demandsbut also helps in extracting position and orientation-independent features essential for training. Two com-mon pooling methods are maximum pooling and averagepooling. In maximum pooling, a small kernel (e.g., 2x2)arXiv:2409.02134v1  [cs.LG]  2 Sep 2024--- Page 2 ---2Fig. 1. Unbalanced Demand For Computation (Left) and Memory (Right) in AlexNet [5].selects the maximum value within its range and places itin the output. In average pooling, a similar-sized kernelcomputes the average value within its area for eachchannel, maintaining the same depth. Pooling simplifiescomputation and weight requirements, with maximumpooling being the most commonly used method [4].\u2022 Activation Function : The activation function, applies amathematical operation to the filter\u2019s output to concludethe output of the network. The common choice is the Rec-tified Linear Unit (ReLU). They fall into two categories,linear and non-linear activation functions [4].\u2022 Fully Connected Layer : It functions as a feed-forwardneural network (NN) typically situated at the network\u2019slower layers. It receives input from the output of the lastpooling or convolutional layer, which is flattened into aone-dimensional vector, enabling it to learn complex datarelationships [4].2) Computation and Memory Demands: In CNNs, unbal-ance exists in resource demands between the layers. Convolu-tional layers primarily serve as feature extractors and heavilydominate the computational workload. In the case of AlexNet,for instance, the convolutional layers account for just 2 millionweights but demand a substantial 1.33 Giga Operations PerSecond (GOPS) of computation. In contrast, fully connectedlayers function as classifiers, accumulating information forhigh-level decisions, and bear the weight of the network witharound 59 million parameters, yet they contribute significantlyless to computation, requiring only 0.12 GOPS. This obviouscontrast in resource allocation (Figure 1) highlights the unbal-anced demands between these two layers in CNNs [5].3) Key CNN Architectures:In 1989, the use of a NN archi-tecture with convolutional layers for recognizing handwrittendigits in the context of ZIP code recognition was introduced[6], That architecture consisted of input layer, 3 hidden layers,and output layer. Since then, CNN models have developed(Figure 2) and became much deeper.4) CNN on Edge: Deploying CNN models on edge hasa wide range of practical and industrial applications acrossvarious sectors. Here are some specific examples:\u2022 Surveillance and Security : It can perform real-time ob-ject detection and facial recognition for security monitor-ing, identifying intruders, and managing access control.Face identification was deployed using VGGFace [7].Video analysis was deployed using YOLOX [8]. Infraredand Visible Image Fusion for security systems was de-ployed using DenseNet [9]. Human action recognitionapplications were deployed using ResNet [10].\u2022 Manufacturing and Quality Control : It can inspectproducts on assembly lines for defects, ensuring qualitycontrol and minimizing errors. Real-time detection ofsteel strip surface defects was deployed using Faster R-CNN model [11].\u2022 Agriculture: Drones can monitor crops, detect pests,diseases, and nutrient deficiencies, enabling precisionagriculture. Identifying rice leaf diseases in natural en-vironments was deployed using GoogLeNet [12]. Pepperleaf disease identification was deployed using GoogLeNet[13]. Detection for insect pests was deployed on YOLOX[14].\u2022 Healthcare and Wearables : Wearable devices can con-tinuously monitor vital signs, detect anomalies, and evendiagnose certain health conditions. Medical diagnosis(Covid and Lung Disease Detection) was deployed usingVGG, MobileNet, and AlexNet [15]. Automatically diag-nose pneumonia and COVID-19 from chest X-ray imageswas deployed on DenseNet [16]. Medical applications(e.g., COVID-19 detection, cardiomegaly diagnosis, braintumor classification) were deployed using ResNet [10].\u2022 Energy Management: It can monitor energy usage, opti-mize consumption patterns, and identify areas for energyefficiency improvements. Wind Turbine Maintenance andfault diagnosis was deployed using AlexNet [17].\u2022 Environmental Monitoring : It can monitor air quality,pollution levels, and weather conditions, providing valu-able insights for urban planning. A smartphone app toperform fine-grained classification of animals in the wildwas deployed using AlexNet, GoogLeNet, and ResNet[18]. Identification of mosquito species was deployedusing AlexNet, DenseNet, Inception, ResNet, and VGG[19].--- Page 3 ---3Fig. 2. Evolution of Key CNN Architectures Over Time.\u2022 Logistics and Inventory Management : It can automatepackage sorting, inventory tracking, and warehouse man-agement. Mobile robot to map its surroundings while de-tecting objects and people was deployed using AlexNet,GoogLeNet, and ResNet [18].\u2022 Autonomous Vehicles : Vehicles can process real-timedata from cameras and sensors using CNNs, aiding inautonomous navigation and collision avoidance. Instanceobjects detection system for intelligent service robots wasdeployed using Alexnet [20]. Advanced driving assistancesystems (ADASs) and automated vehicles (A Vs) weredeployed using Faster R-CNN [21].Deploying CNNs on edge devices presents significant chal-lenges mainly due to the limited computational resources,constrained memory, and power consumption constraints in-herent to these devices. CNN models, known for their depthand complexity, often demand substantial computational powerand memory, which may exceed the capabilities of edgehardware. Hence, compressing the model before deploymentbecomes imperative. Model compression techniques aim toreduce the size of the CNN model while preserving its perfor-mance, thereby enabling efficient utilization of computationalresources and memory on edge devices. By compressingthe model, we can mitigate the challenges associated withdeploying CNNs on edge devices, ensuring that they caneffectively perform tasks such as real-time image processing,object detection, and classification within resource-constrainedenvironments.With the enormous number of compression techniques pro-posed for CNNs, the rapid evolution of CNN architectures hascreated a gap in the field. This dynamic shift in architecturedesign requires an evaluation of existing compression methods,particularly in light of the demand to make these advancedCNN models suitable for deployment on edge devices. AsCNN designs continue to advance, the challenge lies inadapting compression techniques to smoothly integrate withthese modern architectures. This evaluation (either for eachindividual techniques or combined with each other) becomesimportant, as it not only ensures the continued relevance ofcompression techniques but also addresses the urgent need tomake resource-intensive CNN models accessible and deploy-able on edge devices.This work aims to evaluate CNN compression techniquesthat assure appropriate performance on edge devices. In thesubsequent sections, this work reveals in a structured mannerto evaluate the compression techniques for CNN models.section 2 provides a detailed review of related work, offeringinsights into prior research and establishing a foundationalunderstanding of the topic. section 3 explains the methodologyemployed in conducting the experiments, describing the designand execution of the study. Following this, section 4 presentsthe experimental results and analyzes the findings to recognizetrends and implications. Section 5 critically evaluates the re-sults. Section 6 draws conclusions regarding the effectivenessand significance of the compression techniques. This organizedframework aims to comprehensively explore and contribute tothe field of model compression for efficient deployment inresource-constrained environments.II. R ELATED WORKWithin the context of edge AI, it is important to address thecritical need for model compression. The resource constrainednature of these devices requires more efficient AI modelsby minimizing memory and computational demands, ensur-ing faster inference speeds, and enhancing energy efficiency.Below will explore various model compression techniques andtheir implications for edge AI applications.A. PruningPruning is a key technique in DNN, aimed at enhancingefficiency and model generalization. It involves the removalof redundant components, such as parameters, neurons, filters,or entire layers, leading to several advantages. By reducingunnecessary parameters, it cuts down on storage require-ments, and important for models deployed on devices withlimited memory. Furthermore, it streamlines computational--- Page 4 ---4Fig. 3. Weight Pruning (a) and Neuron Pruning (b). x: input, w: weight. [22].complexity during inference, resulting in faster predictions andlower power consumption. Pruning also mitigates overfittingby simplifying the network. Various pruning techniques, likeweight pruning, neuron pruning, filter pruning, and layerpruning, offer different levels of granularity in componentremoval. Whether applied during or after training, pruningenables the creation of more compact and efficient CNNmodels tailored to specific needs, effectively balancing modelsize, computational efficiency, and accuracy. Weight pruningsets weight connections in the network to zero if they fallbelow a predefined threshold or are considered redundant.Neuron pruning focuses on removing entire neurons if they arefound to be redundant. Layer pruning allows for the removalof entire layers that are considered less important [22].1) Pruning For Fully Connected Layer: Fully connectedlayers are dense that makes the layer with high memorydemand. Pruning them effectively reduces the memory burdenand reduce size of the model.It involves selectively removing weight connections andneurons to reduce the model\u2019s complexity while preservingperformance. In a typical feed-forward NN, inputs are multi-plied by corresponding weights, and a linear sum is computedat each neuron, which is then transformed by an activationfunction. As shown in Figure 3, a network with 3 inputneurons, 2 hidden neurons, and 1 output neuron may havemultiple weight connections. Pruning can be applied to elim-inate specific weight connections or entire neurons. By doingso, the total number of weight connections can be significantlyreduced, leading to a more compact network. The concept ofpruning was first introduced by [6], who proposed removingweights based on their saliency, with small-magnitude weightshaving less impact on training error. The process involvesiterative retraining to regain accuracy, and the technique isknown as \u2019Optimal Brain Damage (OBD)\u2019 where the secondderivative of the objective function with respect to parametersis used to calculate the small saliency, facilitating informedpruning decisions. Since then, other pruning approaches havebeen introduced for fully connected layers [22].2) Pruning For Convolutional Layer: Each convolutionallayer typically consists of numerous filters that makes the layerwith high computational demand. Pruning these less significantfilters directly from the convolutional layer effectively reducesthe computational burden and speeds up the model. Inspired byearly pruning methods, new approaches have been introducedto be used to prune convolutional layers [22].Bayesian was used to decide what to prune and the level ofpruning, in this context involves employing scale mixtures ofnormals as priors for parameter weights in LeNet and VGG[23]. Differential evolution based layer-wise weight methodalongside three other pruning techniques (Naive Cut, Itera-tive Pruning, and Multi-Objective NN Pruning) was used toprune LeNet, AlexNet, and VGG16 [24]. Two fully connectedlayers are removed from the AlexNet architecture, and BatchNormalization (BN) is introduced to mitigate overfitting [20].Filters Similarity in Consecutive Layers (FSCL) for CNNs wasused to reduce the number of filters while preserving impor-tant filters, ultimately improving model efficiency for VGG,GoogLeNet, and ResNet [25]. Structured pruning throughsparsity-induced pruning was used to enhance the real-timeimplementation of the DEtection TRansformer (DETR) [26].Structured pruning was used to compress YOLOX, this processincluded sparse training to prune unimportant channels, withfine-tuning to recover accuracy [14]. Evolutionary approachto filter pruning involved sequential application of multiplepruners in a specific order to sparsify LeNet and VGG-19while maintaining model accuracy [27]. Multilayer networkswere used to represent and compress ResNets, it involvedcreating class networks, calculating arc weights, and forminga multilayer network. The overall degree of nodes in themultilayer network is used to select a subset of nodes forcompression, and convolutional layers are pruned [10]. Tooptimize the Fused-DenseNet-Tiny model for efficient detec-tion of COVID-19 and pneumonia in chest X-ray images,three steps were implemented including removing insignif-icant weights, discarding pruning casings, and applying acompression algorithm [16]. Deep Scalable Zerotree-based(DeepSZ) framework was used to address resource constraints--- Page 5 ---5by achieving significant compression for LeNet, AlexNet,and VGG while maintaining acceptable inference accuracy[28]. Compressing without retraining that was used withResNet, AlexNet, VGGNet and SqueezeNet. It focused onconvolutional and fully connected layers, while maintainingor improving classification accuracy [29].B. QuantizationQuantization plays an important role in addressing theresource-intensive nature of CNNs. By reducing the bit pre-cision of model parameters, quantization not only conservesmemory and energy but also enhances inference speed, makingit an essential technique for deploying CNNs in resource-constrained environments such as edge devices. Weight clus-tering takes quantization to a more advanced level by orga-nizing weights into clusters, where each cluster shares thesame weight value. This approach minimizes the need forfine-tuning individual weights and can lead to substantialreductions in memory and computational overhead [22].Single Level Quantization (SLQ) and Multiple Level Quan-tization (MLQ) technique were used to quantize AlexNet,VGG, GoogleNet, and ResNet to the deployment of thesemodels on resource-constrained mobile devices like mobilephones and drones [30].C. Low-Rank Decomposition/FactorizationIt is a compression technique used with feed-forward NNsand CNNs, to reduce the size of weight matrices whilepreserving model performance. Singular Value Decomposition(SVD) is a popular factorization scheme that decomposes aweight matrix A into three smaller matrices: U, S, and V T . Urepresents the left singular vectors, S is a diagonal matrix ofsingular values, and V T is the transpose of the right singularvectors. This factorization offers several advantages, such asreduced storage requirements, which is crucial for memory-constrained environments, and accelerated inference, espe-cially in CNNs, as smaller matrices can be convolved faster.Low-rank factorization can be applied to fully connected andconvolutional layers, making models more storage-efficientand faster without sacrificing performance. Careful selectionof the rank is essential for achieving a balance between sizereduction and model accuracy. Later, more approaches havebeen introduced [22].Tucker decomposition for weight tensors was used tooptimizes weight tensor dimensions of LeNet and ResNetmodels [31]. Low-rank decomposition was used as an efficientmethod for compressing AlexNet, VGG, and ResNet withoutthe need for fine-tuning to significantly reduce model sizeand computational complexity to make them more suitablefor resource-constrained mobile and embedded devices [32].Hardware-Aware Automatic Low-Rank Compression frame-work HALOC was used to compress ResNet, VGG andMobileNet, with the goal of efficiently exploring the structure-level redundancy in NNs by integrating principles from neuralarchitecture search (NAS) [33]. Automatically DifferentiableTensor Network (ADTN) method was used to significantlyreduce the number of parameters of fully connected NN,LeNet, and VGG while maintaining or enhancing the per-formance [34]. Joint Matrix Decomposition, specifically JointSVD (JSVD) was used to address the challenge of deployingResNet with numerous parameters on resource-constrainedplatforms. It included Right JSVD, Left JSVD, and BinaryJSVD algorithms [35]. Tensor Ring Networks (TR-Nets) wasused as a method to effectively factorize LeNet and ResNet,thereby reducing computational and memory requirements[36]. Tucker decomposition with rank selection and fine tuningwas used as a one-shot whole network compression schemefor deploying AlexNet, VGG, and GoogLeNet on mobiledevices while maintaining reasonable accuracy [37]. TensorDynamic Low-Rank Training (TDLRT) was used to createa training algorithm with VGG and AlexNet that maintainshigh model performance while significantly reducing memoryrequirements for convolutional layers [38].D. Knowledge Distillation (KD)It is a technique used to transfer the knowledge learnedby a larger, more complex model (the teacher model) to asmaller and lighter model (the student model). The primarygoal of KD is to enable the student model to benefit from thegeneralization capabilities of the teacher model while beingmore lightweight in terms of parameters and computations.This technique helps to recover the accuracy drop occurs dueto implementing other compression techniques.Knowledge transfer and distillation, initially introduced by[39], aimed to compress large ensemble models into smaller,faster counterparts with minimal performance loss. [40] ex-tended this concept by empirically demonstrating that theintricate knowledge within larger DNNs could be effectivelytransferred to smaller, shallower models, yielding comparableaccuracy. This involved training a large DNN and transfer-ring its knowledge to a shallower network while minimizingthe squared difference between the logits produced by thetwo models. These foundational ideas produced knowledgedistillation, a widely used technique for training efficientmodels by transferring knowledge from larger ones. Later,more approaches have been introduced [22].KD was used to improve the compression of LeNet andResNet models when fresh training data is scarce, primarilythrough the use of synthetic data generated by GenerativeAdversarial Networks (GANs) [41]. To fuse information frominfrared and visible images while reducing DenseNet complex-ity and improving inference speed. Insights from pre-trainedteacher models are transferred to the smaller student model[9]. KD was used to develop a lightweight mosquito speciesidentification model (EfficientNet) that balances efficiency andaccuracy through the compression [42].E. Mixed TechniquesDifferent compression techniques are often combined andused together to achieve more effective and comprehensivemodel compression. Each compression technique targets spe-cific aspects of the model, such as reducing model size,computation complexity, or memory footprint.--- Page 6 ---6In-Parallel Pruning-Quantization CLIP-Q method combinesnetwork pruning and weight quantization was used to com-press AlexNet, GoogLeNet, and ResNet [18]. Pruning andquantization were used to optimize the compression ofAlexNet and reduce the number of parameters significantlywhile maintaining accuracy to be implemented on Field-Programmable Gate Array (FPGA) [5]. Pruning, quantization,and Huffman encoding combined with adversarial trainingwere used to enhance the robustness and compression ofAlexNet while also addressing the model vulnerability toadversarial attacks [43]. Pruning and quantization were usedto compress VGG and ResNet for remote sensing imageclassification, balancing computational complexity constraintswhile preserving model accuracy [44]. Low-rank decompo-sition and quantization were used to compress ResNet andMobileNet, and reduce the computational complexity whilepreserving high performance [45]. Pruning, quantization, andchanging the model architecture were used to design a com-pact SqueezeNet with competitive accuracy while significantlyreducing the number of parameters [46]. Quantization andpruning were used to develop an effective model compressionframework for ResNet and MobileNet. The objective was tooptimize the allocation of compression ratios to minimizeperformance degradation while reducing model size [47]. Jointquantization and pruning were used to develop a post-trainingmodel size compression method that efficiently combines lossyand lossless compression techniques to reduce the size ofResNet, MobileNet, RegNet, MNasNet, and YOLOv5 withoutsacrificing accuracy [48].F . Other TechniquesDepthwise separable convolutions was used to improvesteel strip defect detection by creating a real-time and ef-ficient model while maintaining high accuracy using FasterR-CNN [11]. Deferential Evolution was used to develop anefficient and optimized AlexNet, VGG, and MobileNet forCovid and liver disease detection [15]. Genetic Algorithmwas used to reduce the storage space and inference timeof VGG, ResNet, AlexNet, and SqueezeNet models [49].Factorization (changing kernel size) was used to improvethe accuracy and computing efficiency of pepper leaf diseasedetection using GoogLeNet, specifically for the agriculturalindustry [13]. Flexible and Separable Convolution (FSConv)was used to reduce computational costs without compromisingthe accuracy of VGG, ResNet, Faster R-CNN and RetinaNet[50]. Efficient Layer Compression (ELC) was used to enhancethe computational efficiency of VGG, ResNet, and ConvNeXtwhile preserving their representation capabilities [51].III. D ESIGN OF THE EXPERIMENTSThe experiments aimed to evaluate various compressiontechniques, namely pruning and quantization, on differenttypes of ConvNext [52] model. The experiments includedtraining, fine-tuning, and evaluating of models using CIFAR-10 dataset. The setup involved conducting experiments bothon cloud-based platforms and on edge devices to evaluate theperformance of the compressed models.Fig. 4. Block modifications and resulted specifications. (a) is a ResNeXtblock; in (b) we create an inverted bottleneck block and in (c) the positionof the spatial depthwise conv layer is moved up [52].A. ConvNeXt [52]Is a modern CNN family produced as a journey of graduallymodernize a standard ResNet toward the design of a visionTransformer. The journey starts from a ResNet-50 model, intoa CNN architecture that mirrors some aspects of Transformers,particularly Swin Transformers. The roadmap:1) Training Techniques: Vision Transformer training pro-cedures were used to train ResNet-50 model, this includedextending the training to 300 epochs (90 epochs originally),using AdamW optimizer, and data augmentation techniques(Mixup, Cutmix, RandAugment, Random Erasing, and regu-larization schemes including Stochastic Depth).2) Macro Design: Number of blocks in each stage wasadjusted from (3, 4, 6, 3) to (3, 3, 9, 3) and the stem wasreplaced with a patchify layer implemented using a 4x4, stride4 convolutional layer (non-overlapping convolution).3) ResNeXt-ify: ResNeXt approach was adopted which isutilize grouped convolutions, where convolutional filters aredivided into groups, each handling a subset of input channels,a variation of grouped convolution known as depthwise convo-lution was adopted, and the network\u2019s width was expanded byincreasing the number of channels in the convolutional layers.4) Inverted Bottleneck: The hidden dimension of the MLPblock was changed to be four times wider than the inputdimension as shown in Figure 4 (a and b)5) Large Kernel Sizes: The position of the convolutionallayer is moved up and the kernel size was changed from (3x3)to (7x7) as shown in Figure 4 (a and c).B. Micro DesignReplacing ReLU with Gaussian Error Linear Unit (GELU),fewer normalization layers, Substituting Batch Normalization(BN) with Layer Normalization (LN), and introducing separatedownsampling layers as shown in Figure 5.C. Compression Techniques1) Pruning: Different pruning techniques have been usedincluding structured and unstructured techniques.\u2022 Only Train Once (OTO) [53]: OTO version 3 (OTOV3)is automated framework for structured pruning which in-volves removing entire structures or groups of parametersfrom a DNN. OTOv3 begins by analyzing the depen-dencies between the vertices of the target DNN. Thisanalysis involves identifying accessory, Shape-Dependent--- Page 7 ---7Fig. 5. Block designs for a ResNet, a Swin Transformer, and a ConvNeXt.Swin Transformer\u2019s block is more sophisticated due to the presence of multiplespecialized modules and two residual connections [52].(SD) joint, and unknown vertices that are adjacent andestablishing their interdependencies. The goal is to formnode groups based on these dependencies, laying thefoundation for identifying interdependent vertices duringstructured pruning.Using the information gathered from the dependencyanalysis, OTOv3 constructs a pruning dependency graph.This graph represents the interdependencies between ver-tices, with vertices in the same node group indicatingtheir interdependency during structured pruning. Thepruning dependency graph ensures the validity of theproduced subnetwork by preserving essential connectionsbetween vertices.OTOv3 partitions the trainable variables of the DNNinto Pruning Zero-Invariant Groups (PZIGs) based on thepruning dependency graph. PZIGs consist of pairwisetrainable variables grouped together, with each grouprepresenting a potential pruning structure. Node groupsadjacent to the DNN output and containing unknownvertices are excluded from forming PZIGs to preserveoutput shapes and ensure model robustness as shown inFigure 6.To jointly search for redundant pruning structures andtrain the remaining groups for optimal performance,OTOv3 employs the Dual Half-Space Projected Gradient(DHSPG) algorithm. DHSPG minimizes the objectivefunction while introducing a sparsity constraint to identifyredundant groups for removal. It employs saliency-drivenredundant identification and a hybrid training paradigmto control sparsity and achieve better generalization per-formance as shown in Figure 7.\u2022 L1 Unstructured [54] L1 unstructured pruning is atechnique used in machine learning, to reduce the sizeof neural networks by eliminating less important con-nections. Each weight in the network is assigned a scorebased on its magnitude. This score reflects the importanceof the weight in the network\u2019s performance. In l1 pruning,this score is often the absolute value of the weight.A threshold is set, typically by selecting the top x% ofweights based on their magnitude scores. The thresholddetermines which weights will be pruned and which willbe retained.Weights that fall below the threshold are pruned, meaningthey are set to zero and effectively removed from thenetwork. This results in a sparser network architecturewith fewer connections.\u2022 Random Unstructured [55] Similar to l1 unstructuredpruning, random unstructured pruning is also a techniqueused in machine learning, to reduce the size of neuralnetworks by eliminating less important connections. Thedifference is the pruned weight are selected randomlyinstead of using l1 to decide the importance of theweights.2) Dynamic Quantization [30]:Dynamic quantization is anapproach aimed at optimizing the deployment of neural net-works by reducing the precision of the weights. Unlike tradi-tional quantization methods that apply a fixed quantization bit-width across all layers of the network, dynamic quantizationadapts the quantization bit-width for each layer individuallybased on its representation abilities and capacities. This isachieved through the use of a bit-width controller module,which employs a policy gradient-based training approach tolearn the optimal bit-width for each layer. By dynamicallyadjusting the quantization bit-width, dynamic quantization canstrike a balance between maintaining accuracy and reducingmemory size and computational costs.D. CIFAR-10 [56]CIFAR-10 is a dataset used for computer vision and ma-chine learning research, offering a rich resource for trainingand evaluating image classification algorithms. Comprising60,000 32x32 RGB color images across 10 distinct classes(Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse,Ship, and Truck), CIFAR-10 facilitates comprehensive explo-ration of diverse visual concepts. With each class containing6,000 images and a balanced distribution across the dataset,CIFAR-10 presents a well-structured foundation for model de-velopment. Its division into 50,000 training images and 10,000test images, further segmented into multiple batches, enablesstrict evaluation and benchmarking of algorithms. In terms ofcomputational requirements, CIFAR-10 generally requires lesscomputation compared to CIFAR-100 and ImageNet due to itssmaller image size and fewer classes which makes it suitablefor experiments with limited computational resources.E. Experiment SetupTwo types of experiments have been conducted, cloud-basedexperiments that focused on compressing the models and eval-uating the techniques and edge-based experiment experimentto evaluate the performance of one of the compressed models.--- Page 8 ---8Fig. 6. Automated PZIG Partition [53].Fig. 7. Search Direction in DHSPG [53].1) Cloud-based Experiment Setup: Google Colab Pro+was used to utilize GPU resources (NVIDIA A100 and V100Tensor Core GPUs), facilitating accelerated model trainingand evaluation and background execution. The integrationwith Google Drive reduced overheads associated with upload-ing and downloading model data to and from cloud. Theevaluation framework was designed to profile the originalmodel, compress it, profile the compressed model, and conductcomparison between the measurements before and after thecompression as shown in Figure 8.This profiling process involved measuring several key met-rics:\u2022 Accuracy: The classification accuracy achieved by themodel on the validation dataset.\u2022 Model Size: The size of the model in megabytes (MB).\u2022 Number of Parameters : The total count of trainableparameters in the model, measured in millions (M).\u2022 Number of MACs : The number of multiply-accumulateoperations performed during inference, measured in mil-lions (M).\u2022 Number of Non-Zero Parameters : The count of non-zero parameters in the model, essential for pruning-basedtechniques.2) Edge-based Experiment Setup: A compressed modelwas deployed on edge with CPU (11th Gen Intel(R) Core(TM)i7-1165G7 @ 2.80GHz 2.80 GHz), RAM (16GB), and laptopintegrated camera.2 samples from each of CIFAR-10 classes have been se-lected randomly from the internet, printed on A4 papers, andplaced in front of the camera to measure the accuracy and theinference time.--- Page 9 ---9Fig. 8. Compression Evaluation Framework.TABLE IOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT TINY, SMALL , BASE AND LARGE .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)Tiny Full 63.81 106.26 26.53 86.88 18.41Compressed 63.81 41.31 10.29 47.80 10.30Small Full 63.48 188.89 47.16 169.28 29.55Compressed 63.48 48.04 11.94 68.24 11.96Base Full 61.22 334.28 83.50 299.20 52.22Compressed 61.22 84.12 20.96 119.40 20.98Large Full 63.40 748.82 187.12 669.30 116.93Compressed 63.40 187.32 46.75 264.69 46.78IV. R UNNING THE EXPERIMENTS AND EXPERIMENTALRESULTSA. Cloud-Base Experimentsdifferent experiments have been conducted on cloud to eval-uate different compressing techniques and different versions ofConvNeXt model.1) Evaluate OTOV3 on Untrained Torch ConvNext Tiny,Small, Base, and Large: Untrained ConvNeXt tiny, small,base, and large have been imported from Torch and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and 200epochs were used for training and pruning. OTOV3 achievedhigh performance (Table I) with reducing the model size(61% for tiny and 75% for small, base, and large), numberof parameters (61% for tiny and 75% for small, base, andlarge), and MACs (45% for tiny and 60% for small, base,and large) as shown in Figure 9. Meanwhile OTOV3 wasable to increase both the full and compressed model accuracythrough the training and pruning without any accuracy dropafter pruning comparing to the full model.To investigate the effect of OTOV3 on the model archi-tecture, a comparison has been conducted between ConvNeXtsmall before and after compression. The Torch implementationof the model consist of many CNBlocks, each CNBlock con-sist of Conv2d, Permute, LayerNorm, Linear, GELU, Linear,and Permute layers. As shown in Figure 10, OTOV3 reducednumber of output features of the Linear layer (sequence 3)and the input features of the next Linear layer (sequence 5)and considering the big number of CNBlock in the modelarchitecture, the reduction in model size and number ofparameters after compression is justified as shown in TableII.2) Evaluate OTOV3 on Untrained ConvNext Small (Torchvs. TIMM): Two untrained ConvNeXt small have been im-ported, one from Torch and one from TIMM [57] and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and200 epochs were used for training and pruning. Although thecompression performance was same with size reduction (75%)but the accuracy after 200 epochs was less for Torch model(63%) comparing to TIMM model (73%) as shown in Figure11.To investigate the accuracy performance of OTOV3 withTorch and Timm ConvNeXt Small, a comparison has beenconducted between the two model architectures. The Torchmodel uses the CNBlock structure, which includes additionaloperations such as Permute and varying StochasticDepth prob-abilities. The TIMM model follows a simpler structure withConv2d and LayerNorm, lacking the additional complexitiesintroduced by CNBlock and associated operations in theTorch model as shown in Figure 12 which effects OTOV3performance regarding the accuracy.3) Evaluate OTOV3 on Fine-Tuned Torch ConvNext Small:A pre-trained ConvNeXt small have been imported from Torchand fine-tuned on CIFAR-10 with 100 epochs, the accuracyreached 89.5%. This fine-tuned ConvNeXt small will be usedfor the rest of cloud-base experiments. This model was usedto evaluate OTOV3, CIFAR-10 was used for training andevaluation, and 200 epochs were used for training and pruning.OTOV3 achieved high performance (Table III) 74% reductionin model size and number of parameters, 60% reduction inMACs, and 3.8% increase with accuracy as shown Figure13. The accuracy of the full model in (Table III) (92.86%)is different that the accuracy of the original model used inthe experiment (89.5%), that because OTOV3 trained the full--- Page 10 ---10Fig. 9. OTOV3 Compression Performance with ConvNeXt Tiny, Small, Base and Large.Fig. 10. Comparison Between ConvNeXt Small Full (Left) and Compressed (Right).Fig. 11. OTOV3 Comparison Between Torch and Timm ConvNeXt Small.model during the process which increased the model accuracy.4) Evaluate Unstructured Pruning: The Fine-tuned Con-vNext Small was used to evaluate Pytorch L1 UnstructuredPruning and Random Unstructured Pruning by using differentcombinations of weights pruning percentages for linear (.1 to.9) and convolutional (.1 to .9) layers. In both experiments,the accuracy and the number of non-zero parameters weredropping as the values of weights pruning percentages for bothlinear and convolutional amounts were increasing as shown inFigure 14 a and b. Although the accuracy dropped but themodel size, number of parameters, and MACs didn\u2019t changeas these techniques zero the weights instead of removing them.5) Evaluate Dynamic Quantization: The Fine-tuned Con-vNext Small was used to evaluate Pytorch dynamic quantiza-tion, 8-bit integer was used during the experiment and CIFAR-10 was used for evaluation. Dynamic quantization achievedhigh performance (Table IV) 71% reduction in model size,95% reduction with number of parameters and MACs, and0.1% drop with accuracy as shown in Figure 156) Evaluate Combination of OTOV3 and Dynamic Quan-tization: Two compression stages were evaluated together asone compression pipeline, OTOV3 and Dynamic Quantization.The compressed ConvNeXt small model that was produced inexperiment 4.1.3 (OTOV3) was furtherly compressed using--- Page 11 ---11Fig. 12. Comparison Between ConvNeXt Small TIMM (Left) and Torch (Right).Fig. 13. OTOV3 Compression Performance with ConvNeXt Small Tuned.Pytorch dynamic quantization using 8-bits integer (similar toexperiment 4.1.5). The pruned models using OTOV2 [58]had dependencies on Open Nural Network Exchange (ONNX)[59] which made it not applicable to be combined with othercompression technique like quantization. In OTOv3, there wasengineering changes produced the pruned model directly inPytorch format, which enhanced the flexibility to be combinedwith quantization as this experiment did [53].Pruning using OTOV3 and Quantization using Pytorchdynamic quantization achieved high performance (Table V)89.7% reduction in model size, 95% reduction with numberof parameters and MACs, and 3.8% increase with accuracy asshown in 16.B. Edge-Base ExperimentThe final compressed ConvNeXt Small model in experimentIV .A.6 (OTV3 and Dynamic Quantization) was deployed onedge and the printed samples used to measure the accuracyand inference time by placing them in front of the camera.The compressed model achieved 92.5% accuracy and 20msinference time. Figure 17 shows samples of the output.V. A NALYSIS OF EXPERIMENTAL RESULTSHere, the experimental results obtained from running aseries of experiments will be analyzed, the experiments aimedat evaluating various compression techniques applied to Con-vNeXt models. The experiments were designed to evaluate theperformance of techniques such as pruning and quantizationon different sizes of ConvNeXt models, with a focus on bothcloud-based compression experiments and edge-based deploy-ment experiment. The analysis will be conducted with respectto the work\u2019s aim of evaluating CNN compression techniquesthat assure appropriate performance (size and inference time)on edge devices and resource-constrained environments.A. OTOV3 Evaluation on Untrained ConvNeXt ModelsThe evaluation of OTOV3 on untrained ConvNeXt modelsdemonstrated its effectiveness in achieving substantial com-pression while increasing model accuracy for both full andcompressed models. Across varying sizes of ConvNeXt mod-els, OTOV3 consistently produced impressive reductions inmodel size, number of parameters, and MACs, highlighting itsability to prune redundant structures effectively. This suggests--- Page 12 ---12Fig. 14. Unstructured Pruning Performance with ConvMeXt Small.TABLE IICOMPARISON FOR NUMBER OF OUTPUT FEATURES AND INPUTFEATURES OF THE LINEAR LAYERS IN THE CNB LOCKS BEFORE ANDAFTER OTOV3 C OMPRESSION .Layer Index Input & Output FeaturesSequential CNBlock Full Model CompressedModel1 0 384 3841 1 384 3841 2 384 3843 0 768 7673 1 768 7043 2 768 7265 0 1536 2515 1 1536 3495 2 1536 2425 3 1536 3785 4 1536 2935 5 1536 3775 6 1536 3405 7 1536 4005 8 1536 3945 9 1536 4785 10 1536 4145 11 1536 4245 12 1536 4105 13 1536 3185 14 1536 4885 15 1536 4885 16 1536 4025 17 1536 2465 18 1536 4025 19 1536 4585 20 1536 3235 21 1536 4195 22 1536 4465 23 1536 4445 24 1536 4415 25 1536 4685 26 1536 10707 0 3072 2087 1 3072 2547 2 3072 262Fig. 15. Dynamic Quantization Compression Performance with ConvNeXtSmall Tuned.Fig. 16. OTOV3 and Dynamic Quantization Compression Performance withConvNeXt Small Tuned.--- Page 13 ---13TABLE IIIOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 92.86 188.89 47.16 169.28 29.80Compressed 92.86 50.03 12.44 67.41 12.46TABLE IVDYNAMIC QUANTIZATION COMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 89.53 188.89 47.16 169.28 47.17Compressed 89.40 54.21 2.15 7.27 2.17TABLE VOTOV3 AND DYNAMIC QUANTIZATION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy % Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number of NonZeroParameters (M)Full 89.53 188.89 47.16 169.28 47.17Phase 1 (OTOV3) 92.86 50.03 12.44 67.41 12.46Phase 2 (Dynamic Quantization) 92.93 19.39 2.15 7.27 2.17Fig. 17. Samples From the Compressed Model Output on Edge.that OTOV3 efficiently identifies and removes unnecessaryparameters from the models, leading to more streamlinedarchitectures without compromising predictive performance.However, an unexpected observation arose when comparingthe accuracy of the Torch implementation with that of theTIMM implementation of ConvNeXt Small. The Torch im-plementation exhibited lower accuracy compared to its TIMMcounterpart, indicating that OTOV3\u2019s performance regardingaccuracy may be influenced by the details of the modelarchitecture. This unexpected result suggests that differenttraining strategies or adjustments may be necessary to optimizeOTOV3\u2019s performance across various model implementations,emphasizing the importance of considering architectural dif-ferences when applying compression techniques like OTOV3.B. OTOV3 Evaluation on Fine-Tuned ConvNeXt ModelsWhen evaluating OTOV3\u2019s performance on a fine-tunedConvNeXt model, notable improvements in compression per-formance were observed, confirming its effectiveness in re-ducing model size, parameters, and MACs while marginallyenhancing accuracy. This outcome highlights the potential ofintegrating fine-tuning with structured pruning techniques toachieve even greater optimization of model performance. Thefine-tuned model displayed enhanced compression capabilitiescompared to untrained models, suggesting that pre-trainingcan significantly increase the effectiveness of compressiontechniques. This finding highlights the importance of lever-aging pre-existing knowledge within models to maximize thebenefits of compression, ultimately resulting in CNN modelswith higher performance.C. Unstructured Pruning TechniquesDuring the evaluation of l1 unstructured and random un-structured pruning techniques, expected trends were observedregarding accuracy and compression. As the pruning per-centages increased for both linear and convolutional layers,a corresponding decrease in accuracy was noted, while themodel size, parameters, and MACs remained unaltered. Thisoutcome aligns with the inherent nature of unstructured prun-ing, wherein weights are zeroed out but not entirely eliminated,resulting in sparse models without substantial reductions incomputational complexity. However, the lack of significantreductions in computational complexity may constrain their ef-fectiveness, particularly in resource-constrained environmentswhere efficient utilization of computational resources is essen-tial. This highlights the importance of considering the trade-offs between model compression and computational efficiencywhen selecting pruning techniques for deployment in real---- Page 14 ---14TABLE VIOTOV3 AND DYNAMIC QUANTIZATION REDUCTION NUMBERS WITH CONV NEXT SMALL TUNED .Technique AccuracyChange Size Reduction ParametersReductionMACsReductionOTOV3 3.75% 73.51% 73.62% 60.18%Dynamic Quantization -0.15% 71.30% 95.43% 95.71%OTOV3 and DynamicQuantization 3.80% 89.74% 95.43% 95.71%world applications, especially in edge computing scenarioswhere computational resources are limited.D. Dynamic QuantizationDynamic quantization emerged as a highly effective tech-nique for model compression, demonstrating remarkable re-ductions in model size, parameters, and \n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2409.02134v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 35:
\n~~~~~~~~~~\nHistory: Step #17, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification quantization techniques, here was the response\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods\nSummary: Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is "lost in quantization" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\nPublication Date: 2014-08-20\narXiv paper ID: 1408.4692v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n, Your response: ```SUMMARY\nedge image classification hardware-aware optimization\n```\nStep #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #25, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge image classification dynamic computation optimization\n```\nStep #26, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification dynamic computation optimization, here was the response\nTitle: Immiscible Color Flows in Optimal Transport Networks for Image Classification\nSummary: In classification tasks, it is crucial to meaningfully exploit the\ninformation contained in data. While much of the work in addressing these tasks\nis devoted to building complex algorithmic infrastructures to process inputs in\na black-box fashion, less is known about how to exploit the various facets of\nthe data, before inputting this into an algorithm. Here, we focus on this\nlatter perspective, by proposing a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nThe resulting optimal flows can then be fed into standard classifiers to\ndistinguish images in different classes. We show how our method can outperform\ncompeting approaches on image classification tasks in datasets where color\ninformation matters.\nPublication Date: 2022-05-04\narXiv paper ID: 2205.02938v2\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)\nSummary: Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\nPublication Date: 2022-04-10\narXiv paper ID: 2204.04704v1\n\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n, Your response: ```SUMMARY\nedge image classification adaptive inference techniques\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification adaptive inference techniques, here was the response\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection to Suppress Adversarial Perturbation\nSummary: This paper presents a novel context-aware image denoising algorithm that\ncombines an adaptive image smoothing technique and color reduction techniques\nto remove perturbation from adversarial images. Adaptive image smoothing is\nachieved using auto-threshold canny edge detection to produce an accurate edge\nmap used to produce a blurred image that preserves more edge features. The\nproposed algorithm then uses color reduction techniques to reconstruct the\nimage using only a few representative colors. Through this technique, the\nalgorithm can reduce the effects of adversarial perturbations on images. We\nalso discuss experimental data on classification accuracy. Our results showed\nthat the proposed approach reduces adversarial perturbation in adversarial\nattacks and increases the robustness of the deep convolutional neural network\nmodels.\nPublication Date: 2021-01-14\narXiv paper ID: 2101.05833v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n, Your response: ```SUMMARY\nedge image classification early exit networks\n```\nStep #28, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit networks, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\nStep #30, Phase: literature review, Feedback: Successfully added paper 2105.09121v3, Your response: ```SUMMARY\nedge image classification sparsity techniques\n```\nStep #31, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification sparsity techniques, here was the response\nTitle: Exploiting Activation based Gradient Output Sparsity to Accelerate Backpropagation in CNNs\nSummary: Machine/deep-learning (ML/DL) based techniques are emerging as a driving\nforce behind many cutting-edge technologies, achieving high accuracy on\ncomputer vision workloads such as image classification and object detection.\nHowever, training these models involving large parameters is both\ntime-consuming and energy-hogging. In this regard, several prior works have\nadvocated for sparsity to speed up the of DL training and more so, the\ninference phase. This work begins with the observation that during training,\nsparsity in the forward and backward passes are correlated. In that context, we\ninvestigate two types of sparsity (input and output type) inherent in gradient\ndescent-based optimization algorithms and propose a hardware micro-architecture\nto leverage the same. Our experimental results use five state-of-the-art CNN\nmodels on the Imagenet dataset, and show back propagation speedups in the range\nof 1.69$\\times$ to 5.43$\\times$, compared to the dense baseline execution. By\nexploiting sparsity in both the forward and backward passes, speedup\nimprovements range from 1.68$\\times$ to 3.30$\\times$ over the sparsity-agnostic\nbaseline execution. Our work also achieves significant reduction in training\niteration time over several previously proposed dense as well as sparse\naccelerator based platforms, in addition to achieving order of magnitude energy\nefficiency improvements over GPU based execution.\nPublication Date: 2021-09-16\narXiv paper ID: 2109.07710v1\n\nTitle: Super-Resolution of Brain MRI Images using Overcomplete Dictionaries and Nonlocal Similarity\nSummary: Recently, the Magnetic Resonance Imaging (MRI) images have limited and\nunsatisfactory resolutions due to various constraints such as physical,\ntechnological and economic considerations. Super-resolution techniques can\nobtain high-resolution MRI images. The traditional methods obtained the\nresolution enhancement of brain MRI by interpolations, affecting the accuracy\nof the following diagnose process. The requirement for brain image quality is\nfast increasing. In this paper, we propose an image super-resolution (SR)\nmethod based on overcomplete dictionaries and inherent similarity of an image\nto recover the high-resolution (HR) image from a single low-resolution (LR)\nimage. We explore the nonlocal similarity of the image to tentatively search\nfor similar blocks in the whole image and present a joint reconstruction method\nbased on compressive sensing (CS) and similarity constraints. The sparsity and\nself-similarity of the image blocks are taken as the constraints. The proposed\nmethod is summarized in the following steps. First, a dictionary classification\nmethod based on the measurement domain is presented. The image blocks are\nclassified into smooth, texture and edge parts by analyzing their features in\nthe measurement domain. Then, the corresponding dictionaries are trained using\nthe classified image blocks. Equally important, in the reconstruction part, we\nuse the CS reconstruction method to recover the HR brain MRI image, considering\nboth nonlocal similarity and the sparsity of an image as the constraints. This\nmethod performs better both visually and quantitatively than some existing\nmethods.\nPublication Date: 2019-02-13\narXiv paper ID: 1902.04902v1\n\nTitle: Sparsity Turns Adversarial: Energy and Latency Attacks on Deep Neural Networks\nSummary: Adversarial attacks have exposed serious vulnerabilities in Deep Neural\nNetworks (DNNs) through their ability to force misclassifications through\nhuman-imperceptible perturbations to DNN inputs. We explore a new direction in\nthe field of adversarial attacks by suggesting attacks that aim to degrade the\ncomputational efficiency of DNNs rather than their classification accuracy.\nSpecifically, we propose and demonstrate sparsity attacks, which adversarial\nmodify a DNN's inputs so as to reduce sparsity (or the presence of zero values)\nin its internal activation values. In resource-constrained systems, a wide\nrange of hardware and software techniques have been proposed that exploit\nsparsity to improve DNN efficiency. The proposed attack increases the execution\ntime and energy consumption of sparsity-optimized DNN implementations, raising\nconcern over their deployment in latency and energy-critical applications.\n  We propose a systematic methodology to generate adversarial inputs for\nsparsity attacks by formulating an objective function that quantifies the\nnetwork's activation sparsity, and minimizing this function using iterative\ngradient-descent techniques. We launch both white-box and black-box versions of\nadversarial sparsity attacks on image recognition DNNs and demonstrate that\nthey decrease activation sparsity by up to 1.82x. We also evaluate the impact\nof the attack on a sparsity-optimized DNN accelerator and demonstrate\ndegradations up to 1.59x in latency, and also study the performance of the\nattack on a sparsity-optimized general-purpose processor. Finally, we evaluate\ndefense techniques such as activation thresholding and input quantization and\ndemonstrate that the proposed attack is able to withstand them, highlighting\nthe need for further efforts in this new direction within the field of\nadversarial machine learning.\nPublication Date: 2020-06-14\narXiv paper ID: 2006.08020v3\n\nTitle: Leaf segmentation through the classification of edges\nSummary: We present an approach to leaf level segmentation of images of Arabidopsis\nthaliana plants based upon detected edges. We introduce a novel approach to\nedge classification, which forms an important part of a method to both count\nthe leaves and establish the leaf area of a growing plant from images obtained\nin a high-throughput phenotyping system. Our technique uses a relatively\nshallow convolutional neural network to classify image edges as background,\nplant edge, leaf-on-leaf edge or internal leaf noise. The edges themselves were\nfound using the Canny edge detector and the classified edges can be used with\nsimple image processing techniques to generate a region-based segmentation in\nwhich the leaves are distinct. This approach is strong at distinguishing\noccluding pairs of leaves where one leaf is largely hidden, a situation which\nhas proved troublesome for plant image analysis systems in the past. In\naddition, we introduce the publicly available plant image dataset that was used\nfor this work.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.03124v1\n\nTitle: Network classification with applications to brain connectomics\nSummary: While statistical analysis of a single network has received a lot of\nattention in recent years, with a focus on social networks, analysis of a\nsample of networks presents its own challenges which require a different set of\nanalytic tools. Here we study the problem of classification of networks with\nlabeled nodes, motivated by applications in neuroimaging. Brain networks are\nconstructed from imaging data to represent functional connectivity between\nregions of the brain, and previous work has shown the potential of such\nnetworks to distinguish between various brain disorders, giving rise to a\nnetwork classification problem. Existing approaches tend to either treat all\nedge weights as a long vector, ignoring the network structure, or focus on\ngraph topology as represented by summary measures while ignoring the edge\nweights. Our goal is to design a classification method that uses both the\nindividual edge information and the network structure of the data in a\ncomputationally efficient way, and that can produce a parsimonious and\ninterpretable representation of differences in brain connectivity patterns\nbetween classes. We propose a graph classification method that uses edge\nweights as predictors but incorporates the network nature of the data via\npenalties that promote sparsity in the number of nodes, in addition to the\nusual sparsity penalties that encourage selection of edges. We implement the\nmethod via efficient convex optimization and provide a detailed analysis of\ndata from two fMRI studies of schizophrenia.\nPublication Date: 2017-01-27\narXiv paper ID: 1701.08140v3\n, Your response: ```SUMMARY\nedge image classification pruning quantization techniques\n```\nStep #32, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification pruning quantization techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n\nTitle: Digital-SC: Digital Semantic Communication with Adaptive Network Split and Learned Non-Linear Quantization\nSummary: Semantic communication, an intelligent communication paradigm that aims to\ntransmit useful information in the semantic domain, is facilitated by deep\nlearning techniques. Robust semantic features can be learned and transmitted in\nan analog fashion, but it poses new challenges to hardware, protocol, and\nencryption. In this paper, we propose a digital semantic communication system,\nwhich consists of an encoding network deployed on a resource-limited device and\na decoding network deployed at the edge. To acquire better semantic\nrepresentation for digital transmission, a novel non-linear quantization module\nis proposed to efficiently quantize semantic features with trainable\nquantization levels. Additionally, structured pruning is incorporated to reduce\nthe dimension of the transmitted features. We also introduce a semantic\nlearning loss (SLL) function to reduce semantic error. To adapt to various\nchannel conditions and inputs under constraints of communication and computing\nresources, a policy network is designed to adaptively choose the split point\nand the dimension of the transmitted semantic features. Experiments using the\nCIFAR-10 and ImageNet dataset for image classification are employed to evaluate\nthe proposed digital semantic communication network, and ablation studies are\nconducted to assess the proposed quantization module, structured pruning and\nSLL.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.13553v2\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #33, Phase: literature review, Feedback: 3\n--- Page 1 ---1Edge AI: Evaluation of Model CompressionTechniques for Convolutional Neural NetworksSamer Francy, Member, IEEE,Raghubir Singh, Member, IEEEAbstract\u2014This work evaluates the compression techniqueson ConvNeXt models in image classification tasks using theCIFAR-10 dataset. Structured pruning, unstructured pruning,and dynamic quantization methods are evaluated to reduce modelsize and computational complexity while maintaining accuracy.The experiments, conducted on cloud-based platforms and edgedevice, assess the performance of these techniques. Results showsignificant reductions in model size, with up to 75% reduc-tion achieved using structured pruning techniques. Additionally,dynamic quantization achieves a reduction of up to 95% inthe number of parameters. Fine-tuned models exhibit improvedcompression performance, indicating the benefits of pre-trainingin conjunction with compression techniques. Unstructured prun-ing methods reveal trends in accuracy and compression, withlimited reductions in computational complexity. The combinationof OTOV3 pruning and dynamic quantization further enhancescompression performance, resulting 89.7% reduction in size, 95%reduction with number of parameters and MACs, and 3.8%increase with accuracy. The deployment of the final compressedmodel on edge device demonstrates high accuracy 92.5% and lowinference time 20 ms, validating the effectiveness of compressiontechniques for real-world edge computing applications.Index Terms\u2014edge AI, ConvNeXt, CNN, pruning, quantiza-tion, compression, OTO.I. I NTRODUCTIONEDGE devices such as Internet of Things (IoT) are becom-ing increasingly important and widely used in our dailylives and industrial facilities. IoT is a network of things thatempowered by sensors, identifiers, software intelligence, andinternet connectivity, it can be considered as the intersectionof the internet, things/objects (anything/everything), and data[1]. The number of these devices is expected to increase evenmore [2]. These devices have the potential to perform complexArtificial Intelligence (AI) tasks locally, without relying heav-ily on cloud infrastructure [3]. The rapid advancement of AIhas led to the development of complex deep learning modelsthat show high performance in different domains. DeployingAI models on edge devices has many advantages such as lowlatency, privacy and data security, bandwidth optimization, andreduced network dependence. Low latency is achieved due toreal-time processing by instant data analysis on edge withoutwaiting for remote server processing, this data analysis on theedge reduces transmitting data to the cloud which enhancessecurity against breaches, reduces the bandwidth consumption,and reduces network dependence.Samer Francy is with the Department of Computer Science, University ofBath, Bath, England (e-mail: samer.francy@bath.edu).Raghubir Singh is with the Department of Computer Science, Universityof Bath, Bath, England (e-mail: rs3022@bath.ac.uk).A. Overview of Edge AIEdge AI represents a paradigm shift in the way AI isimplemented in the context of the IoT. It capitalizes on thecapabilities of IoT devices, enhancing real-time processing,analytics, and decision-making directly at the edge of thenetwork. The IoT architecture, which is the foundation forEdge AI, typically involves three core layers [1]. The layersare perceptual layer, where data is collected from varioussensors and devices, network layer, where data is transmittedand routed through this layer, which is responsible for commu-nication between devices and cloud services, and applicationlayer, which processes and utilizes the data, providing insightsand enabling actions.B. Convolutional Neural Networks (CNNs)CNN models are subsets of Deep Neural Networks (DNN)models. CNN models are effective for image and video-relatedtasks due to their ability to learn relevant features from thedata by recognizing patterns, shapes, and structures in images,which is challenging for traditional machine learning models,that\u2019s why they are used for computer vision tasks such asimage classification, object detection, and image segmentation[4].1) CNN Architecture: In general CNN models consist ofbelow parts:\u2022 Input Image : Pixels form the binary basis of computerimages, while the human visual system operates throughneurons with receptive fields. Similarly, CNNs functionwithin their receptive areas, starting with simple patternsand advancing to more complex ones, making CNNs apromising tool for computer vision [4].\u2022 Convolutional Layer : A convolutional layer in a CNNuses a small filter (e.g., 3x3 or 5x5) that slides over theinput image. At each position, it multiplies its valueswith the overlapping image pixels and sums the results toproduce an output. This sliding operation helps identifylocal features like edges and colors, building a hierar-chical representation. The depth of the filter matches theinput image\u2019s channels (e.g., 3 for RGB images). Stackingmultiple filters allows the network to learn features atdifferent abstraction levels [4].\u2022 Pooling Layer : Pooling reduces the spatial size of fea-ture maps. This not only lowers computational demandsbut also helps in extracting position and orientation-independent features essential for training. Two com-mon pooling methods are maximum pooling and averagepooling. In maximum pooling, a small kernel (e.g., 2x2)arXiv:2409.02134v1  [cs.LG]  2 Sep 2024--- Page 2 ---2Fig. 1. Unbalanced Demand For Computation (Left) and Memory (Right) in AlexNet [5].selects the maximum value within its range and places itin the output. In average pooling, a similar-sized kernelcomputes the average value within its area for eachchannel, maintaining the same depth. Pooling simplifiescomputation and weight requirements, with maximumpooling being the most commonly used method [4].\u2022 Activation Function : The activation function, applies amathematical operation to the filter\u2019s output to concludethe output of the network. The common choice is the Rec-tified Linear Unit (ReLU). They fall into two categories,linear and non-linear activation functions [4].\u2022 Fully Connected Layer : It functions as a feed-forwardneural network (NN) typically situated at the network\u2019slower layers. It receives input from the output of the lastpooling or convolutional layer, which is flattened into aone-dimensional vector, enabling it to learn complex datarelationships [4].2) Computation and Memory Demands: In CNNs, unbal-ance exists in resource demands between the layers. Convolu-tional layers primarily serve as feature extractors and heavilydominate the computational workload. In the case of AlexNet,for instance, the convolutional layers account for just 2 millionweights but demand a substantial 1.33 Giga Operations PerSecond (GOPS) of computation. In contrast, fully connectedlayers function as classifiers, accumulating information forhigh-level decisions, and bear the weight of the network witharound 59 million parameters, yet they contribute significantlyless to computation, requiring only 0.12 GOPS. This obviouscontrast in resource allocation (Figure 1) highlights the unbal-anced demands between these two layers in CNNs [5].3) Key CNN Architectures:In 1989, the use of a NN archi-tecture with convolutional layers for recognizing handwrittendigits in the context of ZIP code recognition was introduced[6], That architecture consisted of input layer, 3 hidden layers,and output layer. Since then, CNN models have developed(Figure 2) and became much deeper.4) CNN on Edge: Deploying CNN models on edge hasa wide range of practical and industrial applications acrossvarious sectors. Here are some specific examples:\u2022 Surveillance and Security : It can perform real-time ob-ject detection and facial recognition for security monitor-ing, identifying intruders, and managing access control.Face identification was deployed using VGGFace [7].Video analysis was deployed using YOLOX [8]. Infraredand Visible Image Fusion for security systems was de-ployed using DenseNet [9]. Human action recognitionapplications were deployed using ResNet [10].\u2022 Manufacturing and Quality Control : It can inspectproducts on assembly lines for defects, ensuring qualitycontrol and minimizing errors. Real-time detection ofsteel strip surface defects was deployed using Faster R-CNN model [11].\u2022 Agriculture: Drones can monitor crops, detect pests,diseases, and nutrient deficiencies, enabling precisionagriculture. Identifying rice leaf diseases in natural en-vironments was deployed using GoogLeNet [12]. Pepperleaf disease identification was deployed using GoogLeNet[13]. Detection for insect pests was deployed on YOLOX[14].\u2022 Healthcare and Wearables : Wearable devices can con-tinuously monitor vital signs, detect anomalies, and evendiagnose certain health conditions. Medical diagnosis(Covid and Lung Disease Detection) was deployed usingVGG, MobileNet, and AlexNet [15]. Automatically diag-nose pneumonia and COVID-19 from chest X-ray imageswas deployed on DenseNet [16]. Medical applications(e.g., COVID-19 detection, cardiomegaly diagnosis, braintumor classification) were deployed using ResNet [10].\u2022 Energy Management: It can monitor energy usage, opti-mize consumption patterns, and identify areas for energyefficiency improvements. Wind Turbine Maintenance andfault diagnosis was deployed using AlexNet [17].\u2022 Environmental Monitoring : It can monitor air quality,pollution levels, and weather conditions, providing valu-able insights for urban planning. A smartphone app toperform fine-grained classification of animals in the wildwas deployed using AlexNet, GoogLeNet, and ResNet[18]. Identification of mosquito species was deployedusing AlexNet, DenseNet, Inception, ResNet, and VGG[19].--- Page 3 ---3Fig. 2. Evolution of Key CNN Architectures Over Time.\u2022 Logistics and Inventory Management : It can automatepackage sorting, inventory tracking, and warehouse man-agement. Mobile robot to map its surroundings while de-tecting objects and people was deployed using AlexNet,GoogLeNet, and ResNet [18].\u2022 Autonomous Vehicles : Vehicles can process real-timedata from cameras and sensors using CNNs, aiding inautonomous navigation and collision avoidance. Instanceobjects detection system for intelligent service robots wasdeployed using Alexnet [20]. Advanced driving assistancesystems (ADASs) and automated vehicles (A Vs) weredeployed using Faster R-CNN [21].Deploying CNNs on edge devices presents significant chal-lenges mainly due to the limited computational resources,constrained memory, and power consumption constraints in-herent to these devices. CNN models, known for their depthand complexity, often demand substantial computational powerand memory, which may exceed the capabilities of edgehardware. Hence, compressing the model before deploymentbecomes imperative. Model compression techniques aim toreduce the size of the CNN model while preserving its perfor-mance, thereby enabling efficient utilization of computationalresources and memory on edge devices. By compressingthe model, we can mitigate the challenges associated withdeploying CNNs on edge devices, ensuring that they caneffectively perform tasks such as real-time image processing,object detection, and classification within resource-constrainedenvironments.With the enormous number of compression techniques pro-posed for CNNs, the rapid evolution of CNN architectures hascreated a gap in the field. This dynamic shift in architecturedesign requires an evaluation of existing compression methods,particularly in light of the demand to make these advancedCNN models suitable for deployment on edge devices. AsCNN designs continue to advance, the challenge lies inadapting compression techniques to smoothly integrate withthese modern architectures. This evaluation (either for eachindividual techniques or combined with each other) becomesimportant, as it not only ensures the continued relevance ofcompression techniques but also addresses the urgent need tomake resource-intensive CNN models accessible and deploy-able on edge devices.This work aims to evaluate CNN compression techniquesthat assure appropriate performance on edge devices. In thesubsequent sections, this work reveals in a structured mannerto evaluate the compression techniques for CNN models.section 2 provides a detailed review of related work, offeringinsights into prior research and establishing a foundationalunderstanding of the topic. section 3 explains the methodologyemployed in conducting the experiments, describing the designand execution of the study. Following this, section 4 presentsthe experimental results and analyzes the findings to recognizetrends and implications. Section 5 critically evaluates the re-sults. Section 6 draws conclusions regarding the effectivenessand significance of the compression techniques. This organizedframework aims to comprehensively explore and contribute tothe field of model compression for efficient deployment inresource-constrained environments.II. R ELATED WORKWithin the context of edge AI, it is important to address thecritical need for model compression. The resource constrainednature of these devices requires more efficient AI modelsby minimizing memory and computational demands, ensur-ing faster inference speeds, and enhancing energy efficiency.Below will explore various model compression techniques andtheir implications for edge AI applications.A. PruningPruning is a key technique in DNN, aimed at enhancingefficiency and model generalization. It involves the removalof redundant components, such as parameters, neurons, filters,or entire layers, leading to several advantages. By reducingunnecessary parameters, it cuts down on storage require-ments, and important for models deployed on devices withlimited memory. Furthermore, it streamlines computational--- Page 4 ---4Fig. 3. Weight Pruning (a) and Neuron Pruning (b). x: input, w: weight. [22].complexity during inference, resulting in faster predictions andlower power consumption. Pruning also mitigates overfittingby simplifying the network. Various pruning techniques, likeweight pruning, neuron pruning, filter pruning, and layerpruning, offer different levels of granularity in componentremoval. Whether applied during or after training, pruningenables the creation of more compact and efficient CNNmodels tailored to specific needs, effectively balancing modelsize, computational efficiency, and accuracy. Weight pruningsets weight connections in the network to zero if they fallbelow a predefined threshold or are considered redundant.Neuron pruning focuses on removing entire neurons if they arefound to be redundant. Layer pruning allows for the removalof entire layers that are considered less important [22].1) Pruning For Fully Connected Layer: Fully connectedlayers are dense that makes the layer with high memorydemand. Pruning them effectively reduces the memory burdenand reduce size of the model.It involves selectively removing weight connections andneurons to reduce the model\u2019s complexity while preservingperformance. In a typical feed-forward NN, inputs are multi-plied by corresponding weights, and a linear sum is computedat each neuron, which is then transformed by an activationfunction. As shown in Figure 3, a network with 3 inputneurons, 2 hidden neurons, and 1 output neuron may havemultiple weight connections. Pruning can be applied to elim-inate specific weight connections or entire neurons. By doingso, the total number of weight connections can be significantlyreduced, leading to a more compact network. The concept ofpruning was first introduced by [6], who proposed removingweights based on their saliency, with small-magnitude weightshaving less impact on training error. The process involvesiterative retraining to regain accuracy, and the technique isknown as \u2019Optimal Brain Damage (OBD)\u2019 where the secondderivative of the objective function with respect to parametersis used to calculate the small saliency, facilitating informedpruning decisions. Since then, other pruning approaches havebeen introduced for fully connected layers [22].2) Pruning For Convolutional Layer: Each convolutionallayer typically consists of numerous filters that makes the layerwith high computational demand. Pruning these less significantfilters directly from the convolutional layer effectively reducesthe computational burden and speeds up the model. Inspired byearly pruning methods, new approaches have been introducedto be used to prune convolutional layers [22].Bayesian was used to decide what to prune and the level ofpruning, in this context involves employing scale mixtures ofnormals as priors for parameter weights in LeNet and VGG[23]. Differential evolution based layer-wise weight methodalongside three other pruning techniques (Naive Cut, Itera-tive Pruning, and Multi-Objective NN Pruning) was used toprune LeNet, AlexNet, and VGG16 [24]. Two fully connectedlayers are removed from the AlexNet architecture, and BatchNormalization (BN) is introduced to mitigate overfitting [20].Filters Similarity in Consecutive Layers (FSCL) for CNNs wasused to reduce the number of filters while preserving impor-tant filters, ultimately improving model efficiency for VGG,GoogLeNet, and ResNet [25]. Structured pruning throughsparsity-induced pruning was used to enhance the real-timeimplementation of the DEtection TRansformer (DETR) [26].Structured pruning was used to compress YOLOX, this processincluded sparse training to prune unimportant channels, withfine-tuning to recover accuracy [14]. Evolutionary approachto filter pruning involved sequential application of multiplepruners in a specific order to sparsify LeNet and VGG-19while maintaining model accuracy [27]. Multilayer networkswere used to represent and compress ResNets, it involvedcreating class networks, calculating arc weights, and forminga multilayer network. The overall degree of nodes in themultilayer network is used to select a subset of nodes forcompression, and convolutional layers are pruned [10]. Tooptimize the Fused-DenseNet-Tiny model for efficient detec-tion of COVID-19 and pneumonia in chest X-ray images,three steps were implemented including removing insignif-icant weights, discarding pruning casings, and applying acompression algorithm [16]. Deep Scalable Zerotree-based(DeepSZ) framework was used to address resource constraints--- Page 5 ---5by achieving significant compression for LeNet, AlexNet,and VGG while maintaining acceptable inference accuracy[28]. Compressing without retraining that was used withResNet, AlexNet, VGGNet and SqueezeNet. It focused onconvolutional and fully connected layers, while maintainingor improving classification accuracy [29].B. QuantizationQuantization plays an important role in addressing theresource-intensive nature of CNNs. By reducing the bit pre-cision of model parameters, quantization not only conservesmemory and energy but also enhances inference speed, makingit an essential technique for deploying CNNs in resource-constrained environments such as edge devices. Weight clus-tering takes quantization to a more advanced level by orga-nizing weights into clusters, where each cluster shares thesame weight value. This approach minimizes the need forfine-tuning individual weights and can lead to substantialreductions in memory and computational overhead [22].Single Level Quantization (SLQ) and Multiple Level Quan-tization (MLQ) technique were used to quantize AlexNet,VGG, GoogleNet, and ResNet to the deployment of thesemodels on resource-constrained mobile devices like mobilephones and drones [30].C. Low-Rank Decomposition/FactorizationIt is a compression technique used with feed-forward NNsand CNNs, to reduce the size of weight matrices whilepreserving model performance. Singular Value Decomposition(SVD) is a popular factorization scheme that decomposes aweight matrix A into three smaller matrices: U, S, and V T . Urepresents the left singular vectors, S is a diagonal matrix ofsingular values, and V T is the transpose of the right singularvectors. This factorization offers several advantages, such asreduced storage requirements, which is crucial for memory-constrained environments, and accelerated inference, espe-cially in CNNs, as smaller matrices can be convolved faster.Low-rank factorization can be applied to fully connected andconvolutional layers, making models more storage-efficientand faster without sacrificing performance. Careful selectionof the rank is essential for achieving a balance between sizereduction and model accuracy. Later, more approaches havebeen introduced [22].Tucker decomposition for weight tensors was used tooptimizes weight tensor dimensions of LeNet and ResNetmodels [31]. Low-rank decomposition was used as an efficientmethod for compressing AlexNet, VGG, and ResNet withoutthe need for fine-tuning to significantly reduce model sizeand computational complexity to make them more suitablefor resource-constrained mobile and embedded devices [32].Hardware-Aware Automatic Low-Rank Compression frame-work HALOC was used to compress ResNet, VGG andMobileNet, with the goal of efficiently exploring the structure-level redundancy in NNs by integrating principles from neuralarchitecture search (NAS) [33]. Automatically DifferentiableTensor Network (ADTN) method was used to significantlyreduce the number of parameters of fully connected NN,LeNet, and VGG while maintaining or enhancing the per-formance [34]. Joint Matrix Decomposition, specifically JointSVD (JSVD) was used to address the challenge of deployingResNet with numerous parameters on resource-constrainedplatforms. It included Right JSVD, Left JSVD, and BinaryJSVD algorithms [35]. Tensor Ring Networks (TR-Nets) wasused as a method to effectively factorize LeNet and ResNet,thereby reducing computational and memory requirements[36]. Tucker decomposition with rank selection and fine tuningwas used as a one-shot whole network compression schemefor deploying AlexNet, VGG, and GoogLeNet on mobiledevices while maintaining reasonable accuracy [37]. TensorDynamic Low-Rank Training (TDLRT) was used to createa training algorithm with VGG and AlexNet that maintainshigh model performance while significantly reducing memoryrequirements for convolutional layers [38].D. Knowledge Distillation (KD)It is a technique used to transfer the knowledge learnedby a larger, more complex model (the teacher model) to asmaller and lighter model (the student model). The primarygoal of KD is to enable the student model to benefit from thegeneralization capabilities of the teacher model while beingmore lightweight in terms of parameters and computations.This technique helps to recover the accuracy drop occurs dueto implementing other compression techniques.Knowledge transfer and distillation, initially introduced by[39], aimed to compress large ensemble models into smaller,faster counterparts with minimal performance loss. [40] ex-tended this concept by empirically demonstrating that theintricate knowledge within larger DNNs could be effectivelytransferred to smaller, shallower models, yielding comparableaccuracy. This involved training a large DNN and transfer-ring its knowledge to a shallower network while minimizingthe squared difference between the logits produced by thetwo models. These foundational ideas produced knowledgedistillation, a widely used technique for training efficientmodels by transferring knowledge from larger ones. Later,more approaches have been introduced [22].KD was used to improve the compression of LeNet andResNet models when fresh training data is scarce, primarilythrough the use of synthetic data generated by GenerativeAdversarial Networks (GANs) [41]. To fuse information frominfrared and visible images while reducing DenseNet complex-ity and improving inference speed. Insights from pre-trainedteacher models are transferred to the smaller student model[9]. KD was used to develop a lightweight mosquito speciesidentification model (EfficientNet) that balances efficiency andaccuracy through the compression [42].E. Mixed TechniquesDifferent compression techniques are often combined andused together to achieve more effective and comprehensivemodel compression. Each compression technique targets spe-cific aspects of the model, such as reducing model size,computation complexity, or memory footprint.--- Page 6 ---6In-Parallel Pruning-Quantization CLIP-Q method combinesnetwork pruning and weight quantization was used to com-press AlexNet, GoogLeNet, and ResNet [18]. Pruning andquantization were used to optimize the compression ofAlexNet and reduce the number of parameters significantlywhile maintaining accuracy to be implemented on Field-Programmable Gate Array (FPGA) [5]. Pruning, quantization,and Huffman encoding combined with adversarial trainingwere used to enhance the robustness and compression ofAlexNet while also addressing the model vulnerability toadversarial attacks [43]. Pruning and quantization were usedto compress VGG and ResNet for remote sensing imageclassification, balancing computational complexity constraintswhile preserving model accuracy [44]. Low-rank decompo-sition and quantization were used to compress ResNet andMobileNet, and reduce the computational complexity whilepreserving high performance [45]. Pruning, quantization, andchanging the model architecture were used to design a com-pact SqueezeNet with competitive accuracy while significantlyreducing the number of parameters [46]. Quantization andpruning were used to develop an effective model compressionframework for ResNet and MobileNet. The objective was tooptimize the allocation of compression ratios to minimizeperformance degradation while reducing model size [47]. Jointquantization and pruning were used to develop a post-trainingmodel size compression method that efficiently combines lossyand lossless compression techniques to reduce the size ofResNet, MobileNet, RegNet, MNasNet, and YOLOv5 withoutsacrificing accuracy [48].F . Other TechniquesDepthwise separable convolutions was used to improvesteel strip defect detection by creating a real-time and ef-ficient model while maintaining high accuracy using FasterR-CNN [11]. Deferential Evolution was used to develop anefficient and optimized AlexNet, VGG, and MobileNet forCovid and liver disease detection [15]. Genetic Algorithmwas used to reduce the storage space and inference timeof VGG, ResNet, AlexNet, and SqueezeNet models [49].Factorization (changing kernel size) was used to improvethe accuracy and computing efficiency of pepper leaf diseasedetection using GoogLeNet, specifically for the agriculturalindustry [13]. Flexible and Separable Convolution (FSConv)was used to reduce computational costs without compromisingthe accuracy of VGG, ResNet, Faster R-CNN and RetinaNet[50]. Efficient Layer Compression (ELC) was used to enhancethe computational efficiency of VGG, ResNet, and ConvNeXtwhile preserving their representation capabilities [51].III. D ESIGN OF THE EXPERIMENTSThe experiments aimed to evaluate various compressiontechniques, namely pruning and quantization, on differenttypes of ConvNext [52] model. The experiments includedtraining, fine-tuning, and evaluating of models using CIFAR-10 dataset. The setup involved conducting experiments bothon cloud-based platforms and on edge devices to evaluate theperformance of the compressed models.Fig. 4. Block modifications and resulted specifications. (a) is a ResNeXtblock; in (b) we create an inverted bottleneck block and in (c) the positionof the spatial depthwise conv layer is moved up [52].A. ConvNeXt [52]Is a modern CNN family produced as a journey of graduallymodernize a standard ResNet toward the design of a visionTransformer. The journey starts from a ResNet-50 model, intoa CNN architecture that mirrors some aspects of Transformers,particularly Swin Transformers. The roadmap:1) Training Techniques: Vision Transformer training pro-cedures were used to train ResNet-50 model, this includedextending the training to 300 epochs (90 epochs originally),using AdamW optimizer, and data augmentation techniques(Mixup, Cutmix, RandAugment, Random Erasing, and regu-larization schemes including Stochastic Depth).2) Macro Design: Number of blocks in each stage wasadjusted from (3, 4, 6, 3) to (3, 3, 9, 3) and the stem wasreplaced with a patchify layer implemented using a 4x4, stride4 convolutional layer (non-overlapping convolution).3) ResNeXt-ify: ResNeXt approach was adopted which isutilize grouped convolutions, where convolutional filters aredivided into groups, each handling a subset of input channels,a variation of grouped convolution known as depthwise convo-lution was adopted, and the network\u2019s width was expanded byincreasing the number of channels in the convolutional layers.4) Inverted Bottleneck: The hidden dimension of the MLPblock was changed to be four times wider than the inputdimension as shown in Figure 4 (a and b)5) Large Kernel Sizes: The position of the convolutionallayer is moved up and the kernel size was changed from (3x3)to (7x7) as shown in Figure 4 (a and c).B. Micro DesignReplacing ReLU with Gaussian Error Linear Unit (GELU),fewer normalization layers, Substituting Batch Normalization(BN) with Layer Normalization (LN), and introducing separatedownsampling layers as shown in Figure 5.C. Compression Techniques1) Pruning: Different pruning techniques have been usedincluding structured and unstructured techniques.\u2022 Only Train Once (OTO) [53]: OTO version 3 (OTOV3)is automated framework for structured pruning which in-volves removing entire structures or groups of parametersfrom a DNN. OTOv3 begins by analyzing the depen-dencies between the vertices of the target DNN. Thisanalysis involves identifying accessory, Shape-Dependent--- Page 7 ---7Fig. 5. Block designs for a ResNet, a Swin Transformer, and a ConvNeXt.Swin Transformer\u2019s block is more sophisticated due to the presence of multiplespecialized modules and two residual connections [52].(SD) joint, and unknown vertices that are adjacent andestablishing their interdependencies. The goal is to formnode groups based on these dependencies, laying thefoundation for identifying interdependent vertices duringstructured pruning.Using the information gathered from the dependencyanalysis, OTOv3 constructs a pruning dependency graph.This graph represents the interdependencies between ver-tices, with vertices in the same node group indicatingtheir interdependency during structured pruning. Thepruning dependency graph ensures the validity of theproduced subnetwork by preserving essential connectionsbetween vertices.OTOv3 partitions the trainable variables of the DNNinto Pruning Zero-Invariant Groups (PZIGs) based on thepruning dependency graph. PZIGs consist of pairwisetrainable variables grouped together, with each grouprepresenting a potential pruning structure. Node groupsadjacent to the DNN output and containing unknownvertices are excluded from forming PZIGs to preserveoutput shapes and ensure model robustness as shown inFigure 6.To jointly search for redundant pruning structures andtrain the remaining groups for optimal performance,OTOv3 employs the Dual Half-Space Projected Gradient(DHSPG) algorithm. DHSPG minimizes the objectivefunction while introducing a sparsity constraint to identifyredundant groups for removal. It employs saliency-drivenredundant identification and a hybrid training paradigmto control sparsity and achieve better generalization per-formance as shown in Figure 7.\u2022 L1 Unstructured [54] L1 unstructured pruning is atechnique used in machine learning, to reduce the sizeof neural networks by eliminating less important con-nections. Each weight in the network is assigned a scorebased on its magnitude. This score reflects the importanceof the weight in the network\u2019s performance. In l1 pruning,this score is often the absolute value of the weight.A threshold is set, typically by selecting the top x% ofweights based on their magnitude scores. The thresholddetermines which weights will be pruned and which willbe retained.Weights that fall below the threshold are pruned, meaningthey are set to zero and effectively removed from thenetwork. This results in a sparser network architecturewith fewer connections.\u2022 Random Unstructured [55] Similar to l1 unstructuredpruning, random unstructured pruning is also a techniqueused in machine learning, to reduce the size of neuralnetworks by eliminating less important connections. Thedifference is the pruned weight are selected randomlyinstead of using l1 to decide the importance of theweights.2) Dynamic Quantization [30]:Dynamic quantization is anapproach aimed at optimizing the deployment of neural net-works by reducing the precision of the weights. Unlike tradi-tional quantization methods that apply a fixed quantization bit-width across all layers of the network, dynamic quantizationadapts the quantization bit-width for each layer individuallybased on its representation abilities and capacities. This isachieved through the use of a bit-width controller module,which employs a policy gradient-based training approach tolearn the optimal bit-width for each layer. By dynamicallyadjusting the quantization bit-width, dynamic quantization canstrike a balance between maintaining accuracy and reducingmemory size and computational costs.D. CIFAR-10 [56]CIFAR-10 is a dataset used for computer vision and ma-chine learning research, offering a rich resource for trainingand evaluating image classification algorithms. Comprising60,000 32x32 RGB color images across 10 distinct classes(Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse,Ship, and Truck), CIFAR-10 facilitates comprehensive explo-ration of diverse visual concepts. With each class containing6,000 images and a balanced distribution across the dataset,CIFAR-10 presents a well-structured foundation for model de-velopment. Its division into 50,000 training images and 10,000test images, further segmented into multiple batches, enablesstrict evaluation and benchmarking of algorithms. In terms ofcomputational requirements, CIFAR-10 generally requires lesscomputation compared to CIFAR-100 and ImageNet due to itssmaller image size and fewer classes which makes it suitablefor experiments with limited computational resources.E. Experiment SetupTwo types of experiments have been conducted, cloud-basedexperiments that focused on compressing the models and eval-uating the techniques and edge-based experiment experimentto evaluate the performance of one of the compressed models.--- Page 8 ---8Fig. 6. Automated PZIG Partition [53].Fig. 7. Search Direction in DHSPG [53].1) Cloud-based Experiment Setup: Google Colab Pro+was used to utilize GPU resources (NVIDIA A100 and V100Tensor Core GPUs), facilitating accelerated model trainingand evaluation and background execution. The integrationwith Google Drive reduced overheads associated with upload-ing and downloading model data to and from cloud. Theevaluation framework was designed to profile the originalmodel, compress it, profile the compressed model, and conductcomparison between the measurements before and after thecompression as shown in Figure 8.This profiling process involved measuring several key met-rics:\u2022 Accuracy: The classification accuracy achieved by themodel on the validation dataset.\u2022 Model Size: The size of the model in megabytes (MB).\u2022 Number of Parameters : The total count of trainableparameters in the model, measured in millions (M).\u2022 Number of MACs : The number of multiply-accumulateoperations performed during inference, measured in mil-lions (M).\u2022 Number of Non-Zero Parameters : The count of non-zero parameters in the model, essential for pruning-basedtechniques.2) Edge-based Experiment Setup: A compressed modelwas deployed on edge with CPU (11th Gen Intel(R) Core(TM)i7-1165G7 @ 2.80GHz 2.80 GHz), RAM (16GB), and laptopintegrated camera.2 samples from each of CIFAR-10 classes have been se-lected randomly from the internet, printed on A4 papers, andplaced in front of the camera to measure the accuracy and theinference time.--- Page 9 ---9Fig. 8. Compression Evaluation Framework.TABLE IOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT TINY, SMALL , BASE AND LARGE .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)Tiny Full 63.81 106.26 26.53 86.88 18.41Compressed 63.81 41.31 10.29 47.80 10.30Small Full 63.48 188.89 47.16 169.28 29.55Compressed 63.48 48.04 11.94 68.24 11.96Base Full 61.22 334.28 83.50 299.20 52.22Compressed 61.22 84.12 20.96 119.40 20.98Large Full 63.40 748.82 187.12 669.30 116.93Compressed 63.40 187.32 46.75 264.69 46.78IV. R UNNING THE EXPERIMENTS AND EXPERIMENTALRESULTSA. Cloud-Base Experimentsdifferent experiments have been conducted on cloud to eval-uate different compressing techniques and different versions ofConvNeXt model.1) Evaluate OTOV3 on Untrained Torch ConvNext Tiny,Small, Base, and Large: Untrained ConvNeXt tiny, small,base, and large have been imported from Torch and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and 200epochs were used for training and pruning. OTOV3 achievedhigh performance (Table I) with reducing the model size(61% for tiny and 75% for small, base, and large), numberof parameters (61% for tiny and 75% for small, base, andlarge), and MACs (45% for tiny and 60% for small, base,and large) as shown in Figure 9. Meanwhile OTOV3 wasable to increase both the full and compressed model accuracythrough the training and pruning without any accuracy dropafter pruning comparing to the full model.To investigate the effect of OTOV3 on the model archi-tecture, a comparison has been conducted between ConvNeXtsmall before and after compression. The Torch implementationof the model consist of many CNBlocks, each CNBlock con-sist of Conv2d, Permute, LayerNorm, Linear, GELU, Linear,and Permute layers. As shown in Figure 10, OTOV3 reducednumber of output features of the Linear layer (sequence 3)and the input features of the next Linear layer (sequence 5)and considering the big number of CNBlock in the modelarchitecture, the reduction in model size and number ofparameters after compression is justified as shown in TableII.2) Evaluate OTOV3 on Untrained ConvNext Small (Torchvs. TIMM): Two untrained ConvNeXt small have been im-ported, one from Torch and one from TIMM [57] and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and200 epochs were used for training and pruning. Although thecompression performance was same with size reduction (75%)but the accuracy after 200 epochs was less for Torch model(63%) comparing to TIMM model (73%) as shown in Figure11.To investigate the accuracy performance of OTOV3 withTorch and Timm ConvNeXt Small, a comparison has beenconducted between the two model architectures. The Torchmodel uses the CNBlock structure, which includes additionaloperations such as Permute and varying StochasticDepth prob-abilities. The TIMM model follows a simpler structure withConv2d and LayerNorm, lacking the additional complexitiesintroduced by CNBlock and associated operations in theTorch model as shown in Figure 12 which effects OTOV3performance regarding the accuracy.3) Evaluate OTOV3 on Fine-Tuned Torch ConvNext Small:A pre-trained ConvNeXt small have been imported from Torchand fine-tuned on CIFAR-10 with 100 epochs, the accuracyreached 89.5%. This fine-tuned ConvNeXt small will be usedfor the rest of cloud-base experiments. This model was usedto evaluate OTOV3, CIFAR-10 was used for training andevaluation, and 200 epochs were used for training and pruning.OTOV3 achieved high performance (Table III) 74% reductionin model size and number of parameters, 60% reduction inMACs, and 3.8% increase with accuracy as shown Figure13. The accuracy of the full model in (Table III) (92.86%)is different that the accuracy of the original model used inthe experiment (89.5%), that because OTOV3 trained the full--- Page 10 ---10Fig. 9. OTOV3 Compression Performance with ConvNeXt Tiny, Small, Base and Large.Fig. 10. Comparison Between ConvNeXt Small Full (Left) and Compressed (Right).Fig. 11. OTOV3 Comparison Between Torch and Timm ConvNeXt Small.model during the process which increased the model accuracy.4) Evaluate Unstructured Pruning: The Fine-tuned Con-vNext Small was used to evaluate Pytorch L1 UnstructuredPruning and Random Unstructured Pruning by using differentcombinations of weights pruning percentages for linear (.1 to.9) and convolutional (.1 to .9) layers. In both experiments,the accuracy and the number of non-zero parameters weredropping as the values of weights pruning percentages for bothlinear and convolutional amounts were increasing as shown inFigure 14 a and b. Although the accuracy dropped but themodel size, number of parameters, and MACs didn\u2019t changeas these techniques zero the weights instead of removing them.5) Evaluate Dynamic Quantization: The Fine-tuned Con-vNext Small was used to evaluate Pytorch dynamic quantiza-tion, 8-bit integer was used during the experiment and CIFAR-10 was used for evaluation. Dynamic quantization achievedhigh performance (Table IV) 71% reduction in model size,95% reduction with number of parameters and MACs, and0.1% drop with accuracy as shown in Figure 156) Evaluate Combination of OTOV3 and Dynamic Quan-tization: Two compression stages were evaluated together asone compression pipeline, OTOV3 and Dynamic Quantization.The compressed ConvNeXt small model that was produced inexperiment 4.1.3 (OTOV3) was furtherly compressed using--- Page 11 ---11Fig. 12. Comparison Between ConvNeXt Small TIMM (Left) and Torch (Right).Fig. 13. OTOV3 Compression Performance with ConvNeXt Small Tuned.Pytorch dynamic quantization using 8-bits integer (similar toexperiment 4.1.5). The pruned models using OTOV2 [58]had dependencies on Open Nural Network Exchange (ONNX)[59] which made it not applicable to be combined with othercompression technique like quantization. In OTOv3, there wasengineering changes produced the pruned model directly inPytorch format, which enhanced the flexibility to be combinedwith quantization as this experiment did [53].Pruning using OTOV3 and Quantization using Pytorchdynamic quantization achieved high performance (Table V)89.7% reduction in model size, 95% reduction with numberof parameters and MACs, and 3.8% increase with accuracy asshown in 16.B. Edge-Base ExperimentThe final compressed ConvNeXt Small model in experimentIV .A.6 (OTV3 and Dynamic Quantization) was deployed onedge and the printed samples used to measure the accuracyand inference time by placing them in front of the camera.The compressed model achieved 92.5% accuracy and 20msinference time. Figure 17 shows samples of the output.V. A NALYSIS OF EXPERIMENTAL RESULTSHere, the experimental results obtained from running aseries of experiments will be analyzed, the experiments aimedat evaluating various compression techniques applied to Con-vNeXt models. The experiments were designed to evaluate theperformance of techniques such as pruning and quantizationon different sizes of ConvNeXt models, with a focus on bothcloud-based compression experiments and edge-based deploy-ment experiment. The analysis will be conducted with respectto the work\u2019s aim of evaluating CNN compression techniquesthat assure appropriate performance (size and inference time)on edge devices and resource-constrained environments.A. OTOV3 Evaluation on Untrained ConvNeXt ModelsThe evaluation of OTOV3 on untrained ConvNeXt modelsdemonstrated its effectiveness in achieving substantial com-pression while increasing model accuracy for both full andcompressed models. Across varying sizes of ConvNeXt mod-els, OTOV3 consistently produced impressive reductions inmodel size, number of parameters, and MACs, highlighting itsability to prune redundant structures effectively. This suggests--- Page 12 ---12Fig. 14. Unstructured Pruning Performance with ConvMeXt Small.TABLE IICOMPARISON FOR NUMBER OF OUTPUT FEATURES AND INPUTFEATURES OF THE LINEAR LAYERS IN THE CNB LOCKS BEFORE ANDAFTER OTOV3 C OMPRESSION .Layer Index Input & Output FeaturesSequential CNBlock Full Model CompressedModel1 0 384 3841 1 384 3841 2 384 3843 0 768 7673 1 768 7043 2 768 7265 0 1536 2515 1 1536 3495 2 1536 2425 3 1536 3785 4 1536 2935 5 1536 3775 6 1536 3405 7 1536 4005 8 1536 3945 9 1536 4785 10 1536 4145 11 1536 4245 12 1536 4105 13 1536 3185 14 1536 4885 15 1536 4885 16 1536 4025 17 1536 2465 18 1536 4025 19 1536 4585 20 1536 3235 21 1536 4195 22 1536 4465 23 1536 4445 24 1536 4415 25 1536 4685 26 1536 10707 0 3072 2087 1 3072 2547 2 3072 262Fig. 15. Dynamic Quantization Compression Performance with ConvNeXtSmall Tuned.Fig. 16. OTOV3 and Dynamic Quantization Compression Performance withConvNeXt Small Tuned.--- Page 13 ---13TABLE IIIOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 92.86 188.89 47.16 169.28 29.80Compressed 92.86 50.03 12.44 67.41 12.46TABLE IVDYNAMIC QUANTIZATION COMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 89.53 188.89 47.16 169.28 47.17Compressed 89.40 54.21 2.15 7.27 2.17TABLE VOTOV3 AND DYNAMIC QUANTIZATION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy % Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number of NonZeroParameters (M)Full 89.53 188.89 47.16 169.28 47.17Phase 1 (OTOV3) 92.86 50.03 12.44 67.41 12.46Phase 2 (Dynamic Quantization) 92.93 19.39 2.15 7.27 2.17Fig. 17. Samples From the Compressed Model Output on Edge.that OTOV3 efficiently identifies and removes unnecessaryparameters from the models, leading to more streamlinedarchitectures without compromising predictive performance.However, an unexpected observation arose when comparingthe accuracy of the Torch implementation with that of theTIMM implementation of ConvNeXt Small. The Torch im-plementation exhibited lower accuracy compared to its TIMMcounterpart, indicating that OTOV3\u2019s performance regardingaccuracy may be influenced by the details of the modelarchitecture. This unexpected result suggests that differenttraining strategies or adjustments may be necessary to optimizeOTOV3\u2019s performance across various model implementations,emphasizing the importance of considering architectural dif-ferences when applying compression techniques like OTOV3.B. OTOV3 Evaluation on Fine-Tuned ConvNeXt ModelsWhen evaluating OTOV3\u2019s performance on a fine-tunedConvNeXt model, notable improvements in compression per-formance were observed, confirming its effectiveness in re-ducing model size, parameters, and MACs while marginallyenhancing accuracy. This outcome highlights the potential ofintegrating fine-tuning with structured pruning techniques toachieve even greater optimization of model performance. Thefine-tuned model displayed enhanced compression capabilitiescompared to untrained models, suggesting that pre-trainingcan significantly increase the effectiveness of compressiontechniques. This finding highlights the importance of lever-aging pre-existing knowledge within models to maximize thebenefits of compression, ultimately resulting in CNN modelswith higher performance.C. Unstructured Pruning TechniquesDuring the evaluation of l1 unstructured and random un-structured pruning techniques, expected trends were observedregarding accuracy and compression. As the pruning per-centages increased for both linear and convolutional layers,a corresponding decrease in accuracy was noted, while themodel size, parameters, and MACs remained unaltered. Thisoutcome aligns with the inherent nature of unstructured prun-ing, wherein weights are zeroed out but not entirely eliminated,resulting in sparse models without substantial reductions incomputational complexity. However, the lack of significantreductions in computational complexity may constrain their ef-fectiveness, particularly in resource-constrained environmentswhere efficient utilization of computational resources is essen-tial. This highlights the importance of considering the trade-offs between model compression and computational efficiencywhen selecting pruning techniques for deployment in real---- Page 14 ---14TABLE VIOTOV3 AND DYNAMIC QUANTIZATION REDUCTION NUMBERS WITH CONV NEXT SMALL TUNED .Technique AccuracyChange Size Reduction ParametersReductionMACsReductionOTOV3 3.75% 73.51% 73.62% 60.18%Dynamic Quantization -0.15% 71.30% 95.43% 95.71%OTOV3 and DynamicQuantization 3.80% 89.74% 95.43% 95.71%world applications, especially in edge computing scenarioswhere computational resources are limited.D. Dynamic QuantizationDynamic quantization emerged as a highly effective tech-nique for model compression, demonstrating remarkable re-ductions in model size, parameters, and, Your response: ```ADD_PAPER\n2409.02134v1\nThis paper comprehensively evaluates model compression techniques (structured/unstructured pruning and dynamic quantization) for ConvNeXt models on image classification tasks. Key findings include: 1) OTOV3 pruning achieves 75% reduction in model size/parameters while maintaining accuracy, 2) Dynamic quantization achieves 95% parameter reduction with minimal accuracy drop, 3) Combined OTOV3+quantization achieves 89.7% size reduction and 95% parameter/MAC reduction while improving accuracy by 3.8%. The compressed model demonstrates 92.5% accuracy with 20ms inference time on edge devices, validating effectiveness for real-world deployment.\n```\n~~~~~~~~~~\nCurrent Step #34, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: Successfully added paper 2409.02134v1\nNotes: \nYour previous command was: ```ADD_PAPER\n2409.02134v1\nThis paper comprehensively evaluates model compression techniques (structured/unstructured pruning and dynamic quantization) for ConvNeXt models on image classification tasks. Key findings include: 1) OTOV3 pruning achieves 75% reduction in model size/parameters while maintaining accuracy, 2) Dynamic quantization achieves 95% parameter reduction with minimal accuracy drop, 3) Combined OTOV3+quantization achieves 89.7% size reduction and 95% parameter/MAC reduction while improving accuracy by 3.8%. The compressed model demonstrates 92.5% accuracy with 20ms inference time on edge devices, validating effectiveness for real-world deployment.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 36:
\n~~~~~~~~~~\nHistory: Step #18, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware optimization, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Structure Tensor Based Image Interpolation Method\nSummary: Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\nPublication Date: 2014-02-22\narXiv paper ID: 1402.5564v3\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n\nTitle: Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method\nSummary: Remote sensing image classification can be performed in many different ways\nto extract meaningful features. One common approach is to perform edge\ndetection. A second approach is to try and detect whole shapes, given the fact\nthat these shapes usually tend to have distinctive properties such as object\nforeground or background. To get optimal results, these two approaches can be\ncombined. This paper adopts a combinatorial optimization method to adaptively\nselect threshold based features to improve remote sensing image. Feature\nselection is an important combinatorial optimization problem in the remote\nsensing image classification. The feature selection method has to achieve three\ncharacteristics: first the performance issues by facilitating data collection\nand reducing storage space and classification time, second to perform semantics\nanalysis helping to understand the problem, and third to improve prediction\naccuracy by avoiding the curse of dimensionality. The goal of this thresholding\nan image is to classify pixels as either dark or light and evaluation of\nclassification results. Interactive adaptive thresholding is a form of\nthresholding that takes into account spatial variations in illumination of\nremote sensing image. We present a technique for remote sensing based adaptive\nthresholding using the interactive satellite image of the input. However, our\nsolution is more robust to illumination changes in the remote sensing image.\nAdditionally, our method is simple and easy to implement but it is effective\nalgorithm to classify the image pixels. This technique is suitable for\npreprocessing the remote sensing image classification, making it a valuable\ntool for interactive remote based applications such as augmented reality of the\nclassification procedure.\nPublication Date: 2014-01-30\narXiv paper ID: 1401.7743v1\n\nTitle: Improved Multiple-Image-Based Reflection Removal Algorithm Using Deep Neural Networks\nSummary: When imaging through a semi-reflective medium such as glass, the reflection\nof another scene can often be found in the captured images. It degrades the\nquality of the images and affects their subsequent analyses. In this paper, a\nnovel deep neural network approach for solving the reflection problem in\nimaging is presented. Traditional reflection removal methods not only require\nlong computation time for solving different optimization functions, their\nperformance is also not guaranteed. As array cameras are readily available in\nnowadays imaging devices, we first suggest in this paper a multiple-image based\ndepth estimation method using a convolutional neural network (CNN). The\nproposed network avoids the depth ambiguity problem due to the reflection in\nthe image, and directly estimates the depths along the image edges. They are\nthen used to classify the edges as belonging to the background or reflection.\nSince edges having similar depth values are error prone in the classification,\nthey are removed from the reflection removal process. We suggest a generative\nadversarial network (GAN) to regenerate the removed background edges. Finally,\nthe estimated background edge map is fed to another auto-encoder network to\nassist the extraction of the background from the original image. Experimental\nresults show that the proposed reflection removal algorithm achieves superior\nperformance both quantitatively and qualitatively as compared to the\nstate-of-the-art methods. The proposed algorithm also shows much faster speed\ncompared to the existing approaches using the traditional optimization methods.\nPublication Date: 2022-08-09\narXiv paper ID: 2208.04679v2\n, Your response: ```SUMMARY\nedge image classification energy-efficient hardware acceleration\n```\nStep #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #25, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge image classification dynamic computation optimization\n```\nStep #26, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification dynamic computation optimization, here was the response\nTitle: Immiscible Color Flows in Optimal Transport Networks for Image Classification\nSummary: In classification tasks, it is crucial to meaningfully exploit the\ninformation contained in data. While much of the work in addressing these tasks\nis devoted to building complex algorithmic infrastructures to process inputs in\na black-box fashion, less is known about how to exploit the various facets of\nthe data, before inputting this into an algorithm. Here, we focus on this\nlatter perspective, by proposing a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nThe resulting optimal flows can then be fed into standard classifiers to\ndistinguish images in different classes. We show how our method can outperform\ncompeting approaches on image classification tasks in datasets where color\ninformation matters.\nPublication Date: 2022-05-04\narXiv paper ID: 2205.02938v2\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)\nSummary: Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\nPublication Date: 2022-04-10\narXiv paper ID: 2204.04704v1\n\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n, Your response: ```SUMMARY\nedge image classification adaptive inference techniques\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification adaptive inference techniques, here was the response\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection to Suppress Adversarial Perturbation\nSummary: This paper presents a novel context-aware image denoising algorithm that\ncombines an adaptive image smoothing technique and color reduction techniques\nto remove perturbation from adversarial images. Adaptive image smoothing is\nachieved using auto-threshold canny edge detection to produce an accurate edge\nmap used to produce a blurred image that preserves more edge features. The\nproposed algorithm then uses color reduction techniques to reconstruct the\nimage using only a few representative colors. Through this technique, the\nalgorithm can reduce the effects of adversarial perturbations on images. We\nalso discuss experimental data on classification accuracy. Our results showed\nthat the proposed approach reduces adversarial perturbation in adversarial\nattacks and increases the robustness of the deep convolutional neural network\nmodels.\nPublication Date: 2021-01-14\narXiv paper ID: 2101.05833v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n, Your response: ```SUMMARY\nedge image classification early exit networks\n```\nStep #28, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit networks, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\nStep #30, Phase: literature review, Feedback: Successfully added paper 2105.09121v3, Your response: ```SUMMARY\nedge image classification sparsity techniques\n```\nStep #31, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification sparsity techniques, here was the response\nTitle: Exploiting Activation based Gradient Output Sparsity to Accelerate Backpropagation in CNNs\nSummary: Machine/deep-learning (ML/DL) based techniques are emerging as a driving\nforce behind many cutting-edge technologies, achieving high accuracy on\ncomputer vision workloads such as image classification and object detection.\nHowever, training these models involving large parameters is both\ntime-consuming and energy-hogging. In this regard, several prior works have\nadvocated for sparsity to speed up the of DL training and more so, the\ninference phase. This work begins with the observation that during training,\nsparsity in the forward and backward passes are correlated. In that context, we\ninvestigate two types of sparsity (input and output type) inherent in gradient\ndescent-based optimization algorithms and propose a hardware micro-architecture\nto leverage the same. Our experimental results use five state-of-the-art CNN\nmodels on the Imagenet dataset, and show back propagation speedups in the range\nof 1.69$\\times$ to 5.43$\\times$, compared to the dense baseline execution. By\nexploiting sparsity in both the forward and backward passes, speedup\nimprovements range from 1.68$\\times$ to 3.30$\\times$ over the sparsity-agnostic\nbaseline execution. Our work also achieves significant reduction in training\niteration time over several previously proposed dense as well as sparse\naccelerator based platforms, in addition to achieving order of magnitude energy\nefficiency improvements over GPU based execution.\nPublication Date: 2021-09-16\narXiv paper ID: 2109.07710v1\n\nTitle: Super-Resolution of Brain MRI Images using Overcomplete Dictionaries and Nonlocal Similarity\nSummary: Recently, the Magnetic Resonance Imaging (MRI) images have limited and\nunsatisfactory resolutions due to various constraints such as physical,\ntechnological and economic considerations. Super-resolution techniques can\nobtain high-resolution MRI images. The traditional methods obtained the\nresolution enhancement of brain MRI by interpolations, affecting the accuracy\nof the following diagnose process. The requirement for brain image quality is\nfast increasing. In this paper, we propose an image super-resolution (SR)\nmethod based on overcomplete dictionaries and inherent similarity of an image\nto recover the high-resolution (HR) image from a single low-resolution (LR)\nimage. We explore the nonlocal similarity of the image to tentatively search\nfor similar blocks in the whole image and present a joint reconstruction method\nbased on compressive sensing (CS) and similarity constraints. The sparsity and\nself-similarity of the image blocks are taken as the constraints. The proposed\nmethod is summarized in the following steps. First, a dictionary classification\nmethod based on the measurement domain is presented. The image blocks are\nclassified into smooth, texture and edge parts by analyzing their features in\nthe measurement domain. Then, the corresponding dictionaries are trained using\nthe classified image blocks. Equally important, in the reconstruction part, we\nuse the CS reconstruction method to recover the HR brain MRI image, considering\nboth nonlocal similarity and the sparsity of an image as the constraints. This\nmethod performs better both visually and quantitatively than some existing\nmethods.\nPublication Date: 2019-02-13\narXiv paper ID: 1902.04902v1\n\nTitle: Sparsity Turns Adversarial: Energy and Latency Attacks on Deep Neural Networks\nSummary: Adversarial attacks have exposed serious vulnerabilities in Deep Neural\nNetworks (DNNs) through their ability to force misclassifications through\nhuman-imperceptible perturbations to DNN inputs. We explore a new direction in\nthe field of adversarial attacks by suggesting attacks that aim to degrade the\ncomputational efficiency of DNNs rather than their classification accuracy.\nSpecifically, we propose and demonstrate sparsity attacks, which adversarial\nmodify a DNN's inputs so as to reduce sparsity (or the presence of zero values)\nin its internal activation values. In resource-constrained systems, a wide\nrange of hardware and software techniques have been proposed that exploit\nsparsity to improve DNN efficiency. The proposed attack increases the execution\ntime and energy consumption of sparsity-optimized DNN implementations, raising\nconcern over their deployment in latency and energy-critical applications.\n  We propose a systematic methodology to generate adversarial inputs for\nsparsity attacks by formulating an objective function that quantifies the\nnetwork's activation sparsity, and minimizing this function using iterative\ngradient-descent techniques. We launch both white-box and black-box versions of\nadversarial sparsity attacks on image recognition DNNs and demonstrate that\nthey decrease activation sparsity by up to 1.82x. We also evaluate the impact\nof the attack on a sparsity-optimized DNN accelerator and demonstrate\ndegradations up to 1.59x in latency, and also study the performance of the\nattack on a sparsity-optimized general-purpose processor. Finally, we evaluate\ndefense techniques such as activation thresholding and input quantization and\ndemonstrate that the proposed attack is able to withstand them, highlighting\nthe need for further efforts in this new direction within the field of\nadversarial machine learning.\nPublication Date: 2020-06-14\narXiv paper ID: 2006.08020v3\n\nTitle: Leaf segmentation through the classification of edges\nSummary: We present an approach to leaf level segmentation of images of Arabidopsis\nthaliana plants based upon detected edges. We introduce a novel approach to\nedge classification, which forms an important part of a method to both count\nthe leaves and establish the leaf area of a growing plant from images obtained\nin a high-throughput phenotyping system. Our technique uses a relatively\nshallow convolutional neural network to classify image edges as background,\nplant edge, leaf-on-leaf edge or internal leaf noise. The edges themselves were\nfound using the Canny edge detector and the classified edges can be used with\nsimple image processing techniques to generate a region-based segmentation in\nwhich the leaves are distinct. This approach is strong at distinguishing\noccluding pairs of leaves where one leaf is largely hidden, a situation which\nhas proved troublesome for plant image analysis systems in the past. In\naddition, we introduce the publicly available plant image dataset that was used\nfor this work.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.03124v1\n\nTitle: Network classification with applications to brain connectomics\nSummary: While statistical analysis of a single network has received a lot of\nattention in recent years, with a focus on social networks, analysis of a\nsample of networks presents its own challenges which require a different set of\nanalytic tools. Here we study the problem of classification of networks with\nlabeled nodes, motivated by applications in neuroimaging. Brain networks are\nconstructed from imaging data to represent functional connectivity between\nregions of the brain, and previous work has shown the potential of such\nnetworks to distinguish between various brain disorders, giving rise to a\nnetwork classification problem. Existing approaches tend to either treat all\nedge weights as a long vector, ignoring the network structure, or focus on\ngraph topology as represented by summary measures while ignoring the edge\nweights. Our goal is to design a classification method that uses both the\nindividual edge information and the network structure of the data in a\ncomputationally efficient way, and that can produce a parsimonious and\ninterpretable representation of differences in brain connectivity patterns\nbetween classes. We propose a graph classification method that uses edge\nweights as predictors but incorporates the network nature of the data via\npenalties that promote sparsity in the number of nodes, in addition to the\nusual sparsity penalties that encourage selection of edges. We implement the\nmethod via efficient convex optimization and provide a detailed analysis of\ndata from two fMRI studies of schizophrenia.\nPublication Date: 2017-01-27\narXiv paper ID: 1701.08140v3\n, Your response: ```SUMMARY\nedge image classification pruning quantization techniques\n```\nStep #32, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification pruning quantization techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n\nTitle: Digital-SC: Digital Semantic Communication with Adaptive Network Split and Learned Non-Linear Quantization\nSummary: Semantic communication, an intelligent communication paradigm that aims to\ntransmit useful information in the semantic domain, is facilitated by deep\nlearning techniques. Robust semantic features can be learned and transmitted in\nan analog fashion, but it poses new challenges to hardware, protocol, and\nencryption. In this paper, we propose a digital semantic communication system,\nwhich consists of an encoding network deployed on a resource-limited device and\na decoding network deployed at the edge. To acquire better semantic\nrepresentation for digital transmission, a novel non-linear quantization module\nis proposed to efficiently quantize semantic features with trainable\nquantization levels. Additionally, structured pruning is incorporated to reduce\nthe dimension of the transmitted features. We also introduce a semantic\nlearning loss (SLL) function to reduce semantic error. To adapt to various\nchannel conditions and inputs under constraints of communication and computing\nresources, a policy network is designed to adaptively choose the split point\nand the dimension of the transmitted semantic features. Experiments using the\nCIFAR-10 and ImageNet dataset for image classification are employed to evaluate\nthe proposed digital semantic communication network, and ablation studies are\nconducted to assess the proposed quantization module, structured pruning and\nSLL.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.13553v2\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #33, Phase: literature review, Feedback: 3\n--- Page 1 ---1Edge AI: Evaluation of Model CompressionTechniques for Convolutional Neural NetworksSamer Francy, Member, IEEE,Raghubir Singh, Member, IEEEAbstract\u2014This work evaluates the compression techniqueson ConvNeXt models in image classification tasks using theCIFAR-10 dataset. Structured pruning, unstructured pruning,and dynamic quantization methods are evaluated to reduce modelsize and computational complexity while maintaining accuracy.The experiments, conducted on cloud-based platforms and edgedevice, assess the performance of these techniques. Results showsignificant reductions in model size, with up to 75% reduc-tion achieved using structured pruning techniques. Additionally,dynamic quantization achieves a reduction of up to 95% inthe number of parameters. Fine-tuned models exhibit improvedcompression performance, indicating the benefits of pre-trainingin conjunction with compression techniques. Unstructured prun-ing methods reveal trends in accuracy and compression, withlimited reductions in computational complexity. The combinationof OTOV3 pruning and dynamic quantization further enhancescompression performance, resulting 89.7% reduction in size, 95%reduction with number of parameters and MACs, and 3.8%increase with accuracy. The deployment of the final compressedmodel on edge device demonstrates high accuracy 92.5% and lowinference time 20 ms, validating the effectiveness of compressiontechniques for real-world edge computing applications.Index Terms\u2014edge AI, ConvNeXt, CNN, pruning, quantiza-tion, compression, OTO.I. I NTRODUCTIONEDGE devices such as Internet of Things (IoT) are becom-ing increasingly important and widely used in our dailylives and industrial facilities. IoT is a network of things thatempowered by sensors, identifiers, software intelligence, andinternet connectivity, it can be considered as the intersectionof the internet, things/objects (anything/everything), and data[1]. The number of these devices is expected to increase evenmore [2]. These devices have the potential to perform complexArtificial Intelligence (AI) tasks locally, without relying heav-ily on cloud infrastructure [3]. The rapid advancement of AIhas led to the development of complex deep learning modelsthat show high performance in different domains. DeployingAI models on edge devices has many advantages such as lowlatency, privacy and data security, bandwidth optimization, andreduced network dependence. Low latency is achieved due toreal-time processing by instant data analysis on edge withoutwaiting for remote server processing, this data analysis on theedge reduces transmitting data to the cloud which enhancessecurity against breaches, reduces the bandwidth consumption,and reduces network dependence.Samer Francy is with the Department of Computer Science, University ofBath, Bath, England (e-mail: samer.francy@bath.edu).Raghubir Singh is with the Department of Computer Science, Universityof Bath, Bath, England (e-mail: rs3022@bath.ac.uk).A. Overview of Edge AIEdge AI represents a paradigm shift in the way AI isimplemented in the context of the IoT. It capitalizes on thecapabilities of IoT devices, enhancing real-time processing,analytics, and decision-making directly at the edge of thenetwork. The IoT architecture, which is the foundation forEdge AI, typically involves three core layers [1]. The layersare perceptual layer, where data is collected from varioussensors and devices, network layer, where data is transmittedand routed through this layer, which is responsible for commu-nication between devices and cloud services, and applicationlayer, which processes and utilizes the data, providing insightsand enabling actions.B. Convolutional Neural Networks (CNNs)CNN models are subsets of Deep Neural Networks (DNN)models. CNN models are effective for image and video-relatedtasks due to their ability to learn relevant features from thedata by recognizing patterns, shapes, and structures in images,which is challenging for traditional machine learning models,that\u2019s why they are used for computer vision tasks such asimage classification, object detection, and image segmentation[4].1) CNN Architecture: In general CNN models consist ofbelow parts:\u2022 Input Image : Pixels form the binary basis of computerimages, while the human visual system operates throughneurons with receptive fields. Similarly, CNNs functionwithin their receptive areas, starting with simple patternsand advancing to more complex ones, making CNNs apromising tool for computer vision [4].\u2022 Convolutional Layer : A convolutional layer in a CNNuses a small filter (e.g., 3x3 or 5x5) that slides over theinput image. At each position, it multiplies its valueswith the overlapping image pixels and sums the results toproduce an output. This sliding operation helps identifylocal features like edges and colors, building a hierar-chical representation. The depth of the filter matches theinput image\u2019s channels (e.g., 3 for RGB images). Stackingmultiple filters allows the network to learn features atdifferent abstraction levels [4].\u2022 Pooling Layer : Pooling reduces the spatial size of fea-ture maps. This not only lowers computational demandsbut also helps in extracting position and orientation-independent features essential for training. Two com-mon pooling methods are maximum pooling and averagepooling. In maximum pooling, a small kernel (e.g., 2x2)arXiv:2409.02134v1  [cs.LG]  2 Sep 2024--- Page 2 ---2Fig. 1. Unbalanced Demand For Computation (Left) and Memory (Right) in AlexNet [5].selects the maximum value within its range and places itin the output. In average pooling, a similar-sized kernelcomputes the average value within its area for eachchannel, maintaining the same depth. Pooling simplifiescomputation and weight requirements, with maximumpooling being the most commonly used method [4].\u2022 Activation Function : The activation function, applies amathematical operation to the filter\u2019s output to concludethe output of the network. The common choice is the Rec-tified Linear Unit (ReLU). They fall into two categories,linear and non-linear activation functions [4].\u2022 Fully Connected Layer : It functions as a feed-forwardneural network (NN) typically situated at the network\u2019slower layers. It receives input from the output of the lastpooling or convolutional layer, which is flattened into aone-dimensional vector, enabling it to learn complex datarelationships [4].2) Computation and Memory Demands: In CNNs, unbal-ance exists in resource demands between the layers. Convolu-tional layers primarily serve as feature extractors and heavilydominate the computational workload. In the case of AlexNet,for instance, the convolutional layers account for just 2 millionweights but demand a substantial 1.33 Giga Operations PerSecond (GOPS) of computation. In contrast, fully connectedlayers function as classifiers, accumulating information forhigh-level decisions, and bear the weight of the network witharound 59 million parameters, yet they contribute significantlyless to computation, requiring only 0.12 GOPS. This obviouscontrast in resource allocation (Figure 1) highlights the unbal-anced demands between these two layers in CNNs [5].3) Key CNN Architectures:In 1989, the use of a NN archi-tecture with convolutional layers for recognizing handwrittendigits in the context of ZIP code recognition was introduced[6], That architecture consisted of input layer, 3 hidden layers,and output layer. Since then, CNN models have developed(Figure 2) and became much deeper.4) CNN on Edge: Deploying CNN models on edge hasa wide range of practical and industrial applications acrossvarious sectors. Here are some specific examples:\u2022 Surveillance and Security : It can perform real-time ob-ject detection and facial recognition for security monitor-ing, identifying intruders, and managing access control.Face identification was deployed using VGGFace [7].Video analysis was deployed using YOLOX [8]. Infraredand Visible Image Fusion for security systems was de-ployed using DenseNet [9]. Human action recognitionapplications were deployed using ResNet [10].\u2022 Manufacturing and Quality Control : It can inspectproducts on assembly lines for defects, ensuring qualitycontrol and minimizing errors. Real-time detection ofsteel strip surface defects was deployed using Faster R-CNN model [11].\u2022 Agriculture: Drones can monitor crops, detect pests,diseases, and nutrient deficiencies, enabling precisionagriculture. Identifying rice leaf diseases in natural en-vironments was deployed using GoogLeNet [12]. Pepperleaf disease identification was deployed using GoogLeNet[13]. Detection for insect pests was deployed on YOLOX[14].\u2022 Healthcare and Wearables : Wearable devices can con-tinuously monitor vital signs, detect anomalies, and evendiagnose certain health conditions. Medical diagnosis(Covid and Lung Disease Detection) was deployed usingVGG, MobileNet, and AlexNet [15]. Automatically diag-nose pneumonia and COVID-19 from chest X-ray imageswas deployed on DenseNet [16]. Medical applications(e.g., COVID-19 detection, cardiomegaly diagnosis, braintumor classification) were deployed using ResNet [10].\u2022 Energy Management: It can monitor energy usage, opti-mize consumption patterns, and identify areas for energyefficiency improvements. Wind Turbine Maintenance andfault diagnosis was deployed using AlexNet [17].\u2022 Environmental Monitoring : It can monitor air quality,pollution levels, and weather conditions, providing valu-able insights for urban planning. A smartphone app toperform fine-grained classification of animals in the wildwas deployed using AlexNet, GoogLeNet, and ResNet[18]. Identification of mosquito species was deployedusing AlexNet, DenseNet, Inception, ResNet, and VGG[19].--- Page 3 ---3Fig. 2. Evolution of Key CNN Architectures Over Time.\u2022 Logistics and Inventory Management : It can automatepackage sorting, inventory tracking, and warehouse man-agement. Mobile robot to map its surroundings while de-tecting objects and people was deployed using AlexNet,GoogLeNet, and ResNet [18].\u2022 Autonomous Vehicles : Vehicles can process real-timedata from cameras and sensors using CNNs, aiding inautonomous navigation and collision avoidance. Instanceobjects detection system for intelligent service robots wasdeployed using Alexnet [20]. Advanced driving assistancesystems (ADASs) and automated vehicles (A Vs) weredeployed using Faster R-CNN [21].Deploying CNNs on edge devices presents significant chal-lenges mainly due to the limited computational resources,constrained memory, and power consumption constraints in-herent to these devices. CNN models, known for their depthand complexity, often demand substantial computational powerand memory, which may exceed the capabilities of edgehardware. Hence, compressing the model before deploymentbecomes imperative. Model compression techniques aim toreduce the size of the CNN model while preserving its perfor-mance, thereby enabling efficient utilization of computationalresources and memory on edge devices. By compressingthe model, we can mitigate the challenges associated withdeploying CNNs on edge devices, ensuring that they caneffectively perform tasks such as real-time image processing,object detection, and classification within resource-constrainedenvironments.With the enormous number of compression techniques pro-posed for CNNs, the rapid evolution of CNN architectures hascreated a gap in the field. This dynamic shift in architecturedesign requires an evaluation of existing compression methods,particularly in light of the demand to make these advancedCNN models suitable for deployment on edge devices. AsCNN designs continue to advance, the challenge lies inadapting compression techniques to smoothly integrate withthese modern architectures. This evaluation (either for eachindividual techniques or combined with each other) becomesimportant, as it not only ensures the continued relevance ofcompression techniques but also addresses the urgent need tomake resource-intensive CNN models accessible and deploy-able on edge devices.This work aims to evaluate CNN compression techniquesthat assure appropriate performance on edge devices. In thesubsequent sections, this work reveals in a structured mannerto evaluate the compression techniques for CNN models.section 2 provides a detailed review of related work, offeringinsights into prior research and establishing a foundationalunderstanding of the topic. section 3 explains the methodologyemployed in conducting the experiments, describing the designand execution of the study. Following this, section 4 presentsthe experimental results and analyzes the findings to recognizetrends and implications. Section 5 critically evaluates the re-sults. Section 6 draws conclusions regarding the effectivenessand significance of the compression techniques. This organizedframework aims to comprehensively explore and contribute tothe field of model compression for efficient deployment inresource-constrained environments.II. R ELATED WORKWithin the context of edge AI, it is important to address thecritical need for model compression. The resource constrainednature of these devices requires more efficient AI modelsby minimizing memory and computational demands, ensur-ing faster inference speeds, and enhancing energy efficiency.Below will explore various model compression techniques andtheir implications for edge AI applications.A. PruningPruning is a key technique in DNN, aimed at enhancingefficiency and model generalization. It involves the removalof redundant components, such as parameters, neurons, filters,or entire layers, leading to several advantages. By reducingunnecessary parameters, it cuts down on storage require-ments, and important for models deployed on devices withlimited memory. Furthermore, it streamlines computational--- Page 4 ---4Fig. 3. Weight Pruning (a) and Neuron Pruning (b). x: input, w: weight. [22].complexity during inference, resulting in faster predictions andlower power consumption. Pruning also mitigates overfittingby simplifying the network. Various pruning techniques, likeweight pruning, neuron pruning, filter pruning, and layerpruning, offer different levels of granularity in componentremoval. Whether applied during or after training, pruningenables the creation of more compact and efficient CNNmodels tailored to specific needs, effectively balancing modelsize, computational efficiency, and accuracy. Weight pruningsets weight connections in the network to zero if they fallbelow a predefined threshold or are considered redundant.Neuron pruning focuses on removing entire neurons if they arefound to be redundant. Layer pruning allows for the removalof entire layers that are considered less important [22].1) Pruning For Fully Connected Layer: Fully connectedlayers are dense that makes the layer with high memorydemand. Pruning them effectively reduces the memory burdenand reduce size of the model.It involves selectively removing weight connections andneurons to reduce the model\u2019s complexity while preservingperformance. In a typical feed-forward NN, inputs are multi-plied by corresponding weights, and a linear sum is computedat each neuron, which is then transformed by an activationfunction. As shown in Figure 3, a network with 3 inputneurons, 2 hidden neurons, and 1 output neuron may havemultiple weight connections. Pruning can be applied to elim-inate specific weight connections or entire neurons. By doingso, the total number of weight connections can be significantlyreduced, leading to a more compact network. The concept ofpruning was first introduced by [6], who proposed removingweights based on their saliency, with small-magnitude weightshaving less impact on training error. The process involvesiterative retraining to regain accuracy, and the technique isknown as \u2019Optimal Brain Damage (OBD)\u2019 where the secondderivative of the objective function with respect to parametersis used to calculate the small saliency, facilitating informedpruning decisions. Since then, other pruning approaches havebeen introduced for fully connected layers [22].2) Pruning For Convolutional Layer: Each convolutionallayer typically consists of numerous filters that makes the layerwith high computational demand. Pruning these less significantfilters directly from the convolutional layer effectively reducesthe computational burden and speeds up the model. Inspired byearly pruning methods, new approaches have been introducedto be used to prune convolutional layers [22].Bayesian was used to decide what to prune and the level ofpruning, in this context involves employing scale mixtures ofnormals as priors for parameter weights in LeNet and VGG[23]. Differential evolution based layer-wise weight methodalongside three other pruning techniques (Naive Cut, Itera-tive Pruning, and Multi-Objective NN Pruning) was used toprune LeNet, AlexNet, and VGG16 [24]. Two fully connectedlayers are removed from the AlexNet architecture, and BatchNormalization (BN) is introduced to mitigate overfitting [20].Filters Similarity in Consecutive Layers (FSCL) for CNNs wasused to reduce the number of filters while preserving impor-tant filters, ultimately improving model efficiency for VGG,GoogLeNet, and ResNet [25]. Structured pruning throughsparsity-induced pruning was used to enhance the real-timeimplementation of the DEtection TRansformer (DETR) [26].Structured pruning was used to compress YOLOX, this processincluded sparse training to prune unimportant channels, withfine-tuning to recover accuracy [14]. Evolutionary approachto filter pruning involved sequential application of multiplepruners in a specific order to sparsify LeNet and VGG-19while maintaining model accuracy [27]. Multilayer networkswere used to represent and compress ResNets, it involvedcreating class networks, calculating arc weights, and forminga multilayer network. The overall degree of nodes in themultilayer network is used to select a subset of nodes forcompression, and convolutional layers are pruned [10]. Tooptimize the Fused-DenseNet-Tiny model for efficient detec-tion of COVID-19 and pneumonia in chest X-ray images,three steps were implemented including removing insignif-icant weights, discarding pruning casings, and applying acompression algorithm [16]. Deep Scalable Zerotree-based(DeepSZ) framework was used to address resource constraints--- Page 5 ---5by achieving significant compression for LeNet, AlexNet,and VGG while maintaining acceptable inference accuracy[28]. Compressing without retraining that was used withResNet, AlexNet, VGGNet and SqueezeNet. It focused onconvolutional and fully connected layers, while maintainingor improving classification accuracy [29].B. QuantizationQuantization plays an important role in addressing theresource-intensive nature of CNNs. By reducing the bit pre-cision of model parameters, quantization not only conservesmemory and energy but also enhances inference speed, makingit an essential technique for deploying CNNs in resource-constrained environments such as edge devices. Weight clus-tering takes quantization to a more advanced level by orga-nizing weights into clusters, where each cluster shares thesame weight value. This approach minimizes the need forfine-tuning individual weights and can lead to substantialreductions in memory and computational overhead [22].Single Level Quantization (SLQ) and Multiple Level Quan-tization (MLQ) technique were used to quantize AlexNet,VGG, GoogleNet, and ResNet to the deployment of thesemodels on resource-constrained mobile devices like mobilephones and drones [30].C. Low-Rank Decomposition/FactorizationIt is a compression technique used with feed-forward NNsand CNNs, to reduce the size of weight matrices whilepreserving model performance. Singular Value Decomposition(SVD) is a popular factorization scheme that decomposes aweight matrix A into three smaller matrices: U, S, and V T . Urepresents the left singular vectors, S is a diagonal matrix ofsingular values, and V T is the transpose of the right singularvectors. This factorization offers several advantages, such asreduced storage requirements, which is crucial for memory-constrained environments, and accelerated inference, espe-cially in CNNs, as smaller matrices can be convolved faster.Low-rank factorization can be applied to fully connected andconvolutional layers, making models more storage-efficientand faster without sacrificing performance. Careful selectionof the rank is essential for achieving a balance between sizereduction and model accuracy. Later, more approaches havebeen introduced [22].Tucker decomposition for weight tensors was used tooptimizes weight tensor dimensions of LeNet and ResNetmodels [31]. Low-rank decomposition was used as an efficientmethod for compressing AlexNet, VGG, and ResNet withoutthe need for fine-tuning to significantly reduce model sizeand computational complexity to make them more suitablefor resource-constrained mobile and embedded devices [32].Hardware-Aware Automatic Low-Rank Compression frame-work HALOC was used to compress ResNet, VGG andMobileNet, with the goal of efficiently exploring the structure-level redundancy in NNs by integrating principles from neuralarchitecture search (NAS) [33]. Automatically DifferentiableTensor Network (ADTN) method was used to significantlyreduce the number of parameters of fully connected NN,LeNet, and VGG while maintaining or enhancing the per-formance [34]. Joint Matrix Decomposition, specifically JointSVD (JSVD) was used to address the challenge of deployingResNet with numerous parameters on resource-constrainedplatforms. It included Right JSVD, Left JSVD, and BinaryJSVD algorithms [35]. Tensor Ring Networks (TR-Nets) wasused as a method to effectively factorize LeNet and ResNet,thereby reducing computational and memory requirements[36]. Tucker decomposition with rank selection and fine tuningwas used as a one-shot whole network compression schemefor deploying AlexNet, VGG, and GoogLeNet on mobiledevices while maintaining reasonable accuracy [37]. TensorDynamic Low-Rank Training (TDLRT) was used to createa training algorithm with VGG and AlexNet that maintainshigh model performance while significantly reducing memoryrequirements for convolutional layers [38].D. Knowledge Distillation (KD)It is a technique used to transfer the knowledge learnedby a larger, more complex model (the teacher model) to asmaller and lighter model (the student model). The primarygoal of KD is to enable the student model to benefit from thegeneralization capabilities of the teacher model while beingmore lightweight in terms of parameters and computations.This technique helps to recover the accuracy drop occurs dueto implementing other compression techniques.Knowledge transfer and distillation, initially introduced by[39], aimed to compress large ensemble models into smaller,faster counterparts with minimal performance loss. [40] ex-tended this concept by empirically demonstrating that theintricate knowledge within larger DNNs could be effectivelytransferred to smaller, shallower models, yielding comparableaccuracy. This involved training a large DNN and transfer-ring its knowledge to a shallower network while minimizingthe squared difference between the logits produced by thetwo models. These foundational ideas produced knowledgedistillation, a widely used technique for training efficientmodels by transferring knowledge from larger ones. Later,more approaches have been introduced [22].KD was used to improve the compression of LeNet andResNet models when fresh training data is scarce, primarilythrough the use of synthetic data generated by GenerativeAdversarial Networks (GANs) [41]. To fuse information frominfrared and visible images while reducing DenseNet complex-ity and improving inference speed. Insights from pre-trainedteacher models are transferred to the smaller student model[9]. KD was used to develop a lightweight mosquito speciesidentification model (EfficientNet) that balances efficiency andaccuracy through the compression [42].E. Mixed TechniquesDifferent compression techniques are often combined andused together to achieve more effective and comprehensivemodel compression. Each compression technique targets spe-cific aspects of the model, such as reducing model size,computation complexity, or memory footprint.--- Page 6 ---6In-Parallel Pruning-Quantization CLIP-Q method combinesnetwork pruning and weight quantization was used to com-press AlexNet, GoogLeNet, and ResNet [18]. Pruning andquantization were used to optimize the compression ofAlexNet and reduce the number of parameters significantlywhile maintaining accuracy to be implemented on Field-Programmable Gate Array (FPGA) [5]. Pruning, quantization,and Huffman encoding combined with adversarial trainingwere used to enhance the robustness and compression ofAlexNet while also addressing the model vulnerability toadversarial attacks [43]. Pruning and quantization were usedto compress VGG and ResNet for remote sensing imageclassification, balancing computational complexity constraintswhile preserving model accuracy [44]. Low-rank decompo-sition and quantization were used to compress ResNet andMobileNet, and reduce the computational complexity whilepreserving high performance [45]. Pruning, quantization, andchanging the model architecture were used to design a com-pact SqueezeNet with competitive accuracy while significantlyreducing the number of parameters [46]. Quantization andpruning were used to develop an effective model compressionframework for ResNet and MobileNet. The objective was tooptimize the allocation of compression ratios to minimizeperformance degradation while reducing model size [47]. Jointquantization and pruning were used to develop a post-trainingmodel size compression method that efficiently combines lossyand lossless compression techniques to reduce the size ofResNet, MobileNet, RegNet, MNasNet, and YOLOv5 withoutsacrificing accuracy [48].F . Other TechniquesDepthwise separable convolutions was used to improvesteel strip defect detection by creating a real-time and ef-ficient model while maintaining high accuracy using FasterR-CNN [11]. Deferential Evolution was used to develop anefficient and optimized AlexNet, VGG, and MobileNet forCovid and liver disease detection [15]. Genetic Algorithmwas used to reduce the storage space and inference timeof VGG, ResNet, AlexNet, and SqueezeNet models [49].Factorization (changing kernel size) was used to improvethe accuracy and computing efficiency of pepper leaf diseasedetection using GoogLeNet, specifically for the agriculturalindustry [13]. Flexible and Separable Convolution (FSConv)was used to reduce computational costs without compromisingthe accuracy of VGG, ResNet, Faster R-CNN and RetinaNet[50]. Efficient Layer Compression (ELC) was used to enhancethe computational efficiency of VGG, ResNet, and ConvNeXtwhile preserving their representation capabilities [51].III. D ESIGN OF THE EXPERIMENTSThe experiments aimed to evaluate various compressiontechniques, namely pruning and quantization, on differenttypes of ConvNext [52] model. The experiments includedtraining, fine-tuning, and evaluating of models using CIFAR-10 dataset. The setup involved conducting experiments bothon cloud-based platforms and on edge devices to evaluate theperformance of the compressed models.Fig. 4. Block modifications and resulted specifications. (a) is a ResNeXtblock; in (b) we create an inverted bottleneck block and in (c) the positionof the spatial depthwise conv layer is moved up [52].A. ConvNeXt [52]Is a modern CNN family produced as a journey of graduallymodernize a standard ResNet toward the design of a visionTransformer. The journey starts from a ResNet-50 model, intoa CNN architecture that mirrors some aspects of Transformers,particularly Swin Transformers. The roadmap:1) Training Techniques: Vision Transformer training pro-cedures were used to train ResNet-50 model, this includedextending the training to 300 epochs (90 epochs originally),using AdamW optimizer, and data augmentation techniques(Mixup, Cutmix, RandAugment, Random Erasing, and regu-larization schemes including Stochastic Depth).2) Macro Design: Number of blocks in each stage wasadjusted from (3, 4, 6, 3) to (3, 3, 9, 3) and the stem wasreplaced with a patchify layer implemented using a 4x4, stride4 convolutional layer (non-overlapping convolution).3) ResNeXt-ify: ResNeXt approach was adopted which isutilize grouped convolutions, where convolutional filters aredivided into groups, each handling a subset of input channels,a variation of grouped convolution known as depthwise convo-lution was adopted, and the network\u2019s width was expanded byincreasing the number of channels in the convolutional layers.4) Inverted Bottleneck: The hidden dimension of the MLPblock was changed to be four times wider than the inputdimension as shown in Figure 4 (a and b)5) Large Kernel Sizes: The position of the convolutionallayer is moved up and the kernel size was changed from (3x3)to (7x7) as shown in Figure 4 (a and c).B. Micro DesignReplacing ReLU with Gaussian Error Linear Unit (GELU),fewer normalization layers, Substituting Batch Normalization(BN) with Layer Normalization (LN), and introducing separatedownsampling layers as shown in Figure 5.C. Compression Techniques1) Pruning: Different pruning techniques have been usedincluding structured and unstructured techniques.\u2022 Only Train Once (OTO) [53]: OTO version 3 (OTOV3)is automated framework for structured pruning which in-volves removing entire structures or groups of parametersfrom a DNN. OTOv3 begins by analyzing the depen-dencies between the vertices of the target DNN. Thisanalysis involves identifying accessory, Shape-Dependent--- Page 7 ---7Fig. 5. Block designs for a ResNet, a Swin Transformer, and a ConvNeXt.Swin Transformer\u2019s block is more sophisticated due to the presence of multiplespecialized modules and two residual connections [52].(SD) joint, and unknown vertices that are adjacent andestablishing their interdependencies. The goal is to formnode groups based on these dependencies, laying thefoundation for identifying interdependent vertices duringstructured pruning.Using the information gathered from the dependencyanalysis, OTOv3 constructs a pruning dependency graph.This graph represents the interdependencies between ver-tices, with vertices in the same node group indicatingtheir interdependency during structured pruning. Thepruning dependency graph ensures the validity of theproduced subnetwork by preserving essential connectionsbetween vertices.OTOv3 partitions the trainable variables of the DNNinto Pruning Zero-Invariant Groups (PZIGs) based on thepruning dependency graph. PZIGs consist of pairwisetrainable variables grouped together, with each grouprepresenting a potential pruning structure. Node groupsadjacent to the DNN output and containing unknownvertices are excluded from forming PZIGs to preserveoutput shapes and ensure model robustness as shown inFigure 6.To jointly search for redundant pruning structures andtrain the remaining groups for optimal performance,OTOv3 employs the Dual Half-Space Projected Gradient(DHSPG) algorithm. DHSPG minimizes the objectivefunction while introducing a sparsity constraint to identifyredundant groups for removal. It employs saliency-drivenredundant identification and a hybrid training paradigmto control sparsity and achieve better generalization per-formance as shown in Figure 7.\u2022 L1 Unstructured [54] L1 unstructured pruning is atechnique used in machine learning, to reduce the sizeof neural networks by eliminating less important con-nections. Each weight in the network is assigned a scorebased on its magnitude. This score reflects the importanceof the weight in the network\u2019s performance. In l1 pruning,this score is often the absolute value of the weight.A threshold is set, typically by selecting the top x% ofweights based on their magnitude scores. The thresholddetermines which weights will be pruned and which willbe retained.Weights that fall below the threshold are pruned, meaningthey are set to zero and effectively removed from thenetwork. This results in a sparser network architecturewith fewer connections.\u2022 Random Unstructured [55] Similar to l1 unstructuredpruning, random unstructured pruning is also a techniqueused in machine learning, to reduce the size of neuralnetworks by eliminating less important connections. Thedifference is the pruned weight are selected randomlyinstead of using l1 to decide the importance of theweights.2) Dynamic Quantization [30]:Dynamic quantization is anapproach aimed at optimizing the deployment of neural net-works by reducing the precision of the weights. Unlike tradi-tional quantization methods that apply a fixed quantization bit-width across all layers of the network, dynamic quantizationadapts the quantization bit-width for each layer individuallybased on its representation abilities and capacities. This isachieved through the use of a bit-width controller module,which employs a policy gradient-based training approach tolearn the optimal bit-width for each layer. By dynamicallyadjusting the quantization bit-width, dynamic quantization canstrike a balance between maintaining accuracy and reducingmemory size and computational costs.D. CIFAR-10 [56]CIFAR-10 is a dataset used for computer vision and ma-chine learning research, offering a rich resource for trainingand evaluating image classification algorithms. Comprising60,000 32x32 RGB color images across 10 distinct classes(Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse,Ship, and Truck), CIFAR-10 facilitates comprehensive explo-ration of diverse visual concepts. With each class containing6,000 images and a balanced distribution across the dataset,CIFAR-10 presents a well-structured foundation for model de-velopment. Its division into 50,000 training images and 10,000test images, further segmented into multiple batches, enablesstrict evaluation and benchmarking of algorithms. In terms ofcomputational requirements, CIFAR-10 generally requires lesscomputation compared to CIFAR-100 and ImageNet due to itssmaller image size and fewer classes which makes it suitablefor experiments with limited computational resources.E. Experiment SetupTwo types of experiments have been conducted, cloud-basedexperiments that focused on compressing the models and eval-uating the techniques and edge-based experiment experimentto evaluate the performance of one of the compressed models.--- Page 8 ---8Fig. 6. Automated PZIG Partition [53].Fig. 7. Search Direction in DHSPG [53].1) Cloud-based Experiment Setup: Google Colab Pro+was used to utilize GPU resources (NVIDIA A100 and V100Tensor Core GPUs), facilitating accelerated model trainingand evaluation and background execution. The integrationwith Google Drive reduced overheads associated with upload-ing and downloading model data to and from cloud. Theevaluation framework was designed to profile the originalmodel, compress it, profile the compressed model, and conductcomparison between the measurements before and after thecompression as shown in Figure 8.This profiling process involved measuring several key met-rics:\u2022 Accuracy: The classification accuracy achieved by themodel on the validation dataset.\u2022 Model Size: The size of the model in megabytes (MB).\u2022 Number of Parameters : The total count of trainableparameters in the model, measured in millions (M).\u2022 Number of MACs : The number of multiply-accumulateoperations performed during inference, measured in mil-lions (M).\u2022 Number of Non-Zero Parameters : The count of non-zero parameters in the model, essential for pruning-basedtechniques.2) Edge-based Experiment Setup: A compressed modelwas deployed on edge with CPU (11th Gen Intel(R) Core(TM)i7-1165G7 @ 2.80GHz 2.80 GHz), RAM (16GB), and laptopintegrated camera.2 samples from each of CIFAR-10 classes have been se-lected randomly from the internet, printed on A4 papers, andplaced in front of the camera to measure the accuracy and theinference time.--- Page 9 ---9Fig. 8. Compression Evaluation Framework.TABLE IOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT TINY, SMALL , BASE AND LARGE .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)Tiny Full 63.81 106.26 26.53 86.88 18.41Compressed 63.81 41.31 10.29 47.80 10.30Small Full 63.48 188.89 47.16 169.28 29.55Compressed 63.48 48.04 11.94 68.24 11.96Base Full 61.22 334.28 83.50 299.20 52.22Compressed 61.22 84.12 20.96 119.40 20.98Large Full 63.40 748.82 187.12 669.30 116.93Compressed 63.40 187.32 46.75 264.69 46.78IV. R UNNING THE EXPERIMENTS AND EXPERIMENTALRESULTSA. Cloud-Base Experimentsdifferent experiments have been conducted on cloud to eval-uate different compressing techniques and different versions ofConvNeXt model.1) Evaluate OTOV3 on Untrained Torch ConvNext Tiny,Small, Base, and Large: Untrained ConvNeXt tiny, small,base, and large have been imported from Torch and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and 200epochs were used for training and pruning. OTOV3 achievedhigh performance (Table I) with reducing the model size(61% for tiny and 75% for small, base, and large), numberof parameters (61% for tiny and 75% for small, base, andlarge), and MACs (45% for tiny and 60% for small, base,and large) as shown in Figure 9. Meanwhile OTOV3 wasable to increase both the full and compressed model accuracythrough the training and pruning without any accuracy dropafter pruning comparing to the full model.To investigate the effect of OTOV3 on the model archi-tecture, a comparison has been conducted between ConvNeXtsmall before and after compression. The Torch implementationof the model consist of many CNBlocks, each CNBlock con-sist of Conv2d, Permute, LayerNorm, Linear, GELU, Linear,and Permute layers. As shown in Figure 10, OTOV3 reducednumber of output features of the Linear layer (sequence 3)and the input features of the next Linear layer (sequence 5)and considering the big number of CNBlock in the modelarchitecture, the reduction in model size and number ofparameters after compression is justified as shown in TableII.2) Evaluate OTOV3 on Untrained ConvNext Small (Torchvs. TIMM): Two untrained ConvNeXt small have been im-ported, one from Torch and one from TIMM [57] and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and200 epochs were used for training and pruning. Although thecompression performance was same with size reduction (75%)but the accuracy after 200 epochs was less for Torch model(63%) comparing to TIMM model (73%) as shown in Figure11.To investigate the accuracy performance of OTOV3 withTorch and Timm ConvNeXt Small, a comparison has beenconducted between the two model architectures. The Torchmodel uses the CNBlock structure, which includes additionaloperations such as Permute and varying StochasticDepth prob-abilities. The TIMM model follows a simpler structure withConv2d and LayerNorm, lacking the additional complexitiesintroduced by CNBlock and associated operations in theTorch model as shown in Figure 12 which effects OTOV3performance regarding the accuracy.3) Evaluate OTOV3 on Fine-Tuned Torch ConvNext Small:A pre-trained ConvNeXt small have been imported from Torchand fine-tuned on CIFAR-10 with 100 epochs, the accuracyreached 89.5%. This fine-tuned ConvNeXt small will be usedfor the rest of cloud-base experiments. This model was usedto evaluate OTOV3, CIFAR-10 was used for training andevaluation, and 200 epochs were used for training and pruning.OTOV3 achieved high performance (Table III) 74% reductionin model size and number of parameters, 60% reduction inMACs, and 3.8% increase with accuracy as shown Figure13. The accuracy of the full model in (Table III) (92.86%)is different that the accuracy of the original model used inthe experiment (89.5%), that because OTOV3 trained the full--- Page 10 ---10Fig. 9. OTOV3 Compression Performance with ConvNeXt Tiny, Small, Base and Large.Fig. 10. Comparison Between ConvNeXt Small Full (Left) and Compressed (Right).Fig. 11. OTOV3 Comparison Between Torch and Timm ConvNeXt Small.model during the process which increased the model accuracy.4) Evaluate Unstructured Pruning: The Fine-tuned Con-vNext Small was used to evaluate Pytorch L1 UnstructuredPruning and Random Unstructured Pruning by using differentcombinations of weights pruning percentages for linear (.1 to.9) and convolutional (.1 to .9) layers. In both experiments,the accuracy and the number of non-zero parameters weredropping as the values of weights pruning percentages for bothlinear and convolutional amounts were increasing as shown inFigure 14 a and b. Although the accuracy dropped but themodel size, number of parameters, and MACs didn\u2019t changeas these techniques zero the weights instead of removing them.5) Evaluate Dynamic Quantization: The Fine-tuned Con-vNext Small was used to evaluate Pytorch dynamic quantiza-tion, 8-bit integer was used during the experiment and CIFAR-10 was used for evaluation. Dynamic quantization achievedhigh performance (Table IV) 71% reduction in model size,95% reduction with number of parameters and MACs, and0.1% drop with accuracy as shown in Figure 156) Evaluate Combination of OTOV3 and Dynamic Quan-tization: Two compression stages were evaluated together asone compression pipeline, OTOV3 and Dynamic Quantization.The compressed ConvNeXt small model that was produced inexperiment 4.1.3 (OTOV3) was furtherly compressed using--- Page 11 ---11Fig. 12. Comparison Between ConvNeXt Small TIMM (Left) and Torch (Right).Fig. 13. OTOV3 Compression Performance with ConvNeXt Small Tuned.Pytorch dynamic quantization using 8-bits integer (similar toexperiment 4.1.5). The pruned models using OTOV2 [58]had dependencies on Open Nural Network Exchange (ONNX)[59] which made it not applicable to be combined with othercompression technique like quantization. In OTOv3, there wasengineering changes produced the pruned model directly inPytorch format, which enhanced the flexibility to be combinedwith quantization as this experiment did [53].Pruning using OTOV3 and Quantization using Pytorchdynamic quantization achieved high performance (Table V)89.7% reduction in model size, 95% reduction with numberof parameters and MACs, and 3.8% increase with accuracy asshown in 16.B. Edge-Base ExperimentThe final compressed ConvNeXt Small model in experimentIV .A.6 (OTV3 and Dynamic Quantization) was deployed onedge and the printed samples used to measure the accuracyand inference time by placing them in front of the camera.The compressed model achieved 92.5% accuracy and 20msinference time. Figure 17 shows samples of the output.V. A NALYSIS OF EXPERIMENTAL RESULTSHere, the experimental results obtained from running aseries of experiments will be analyzed, the experiments aimedat evaluating various compression techniques applied to Con-vNeXt models. The experiments were designed to evaluate theperformance of techniques such as pruning and quantizationon different sizes of ConvNeXt models, with a focus on bothcloud-based compression experiments and edge-based deploy-ment experiment. The analysis will be conducted with respectto the work\u2019s aim of evaluating CNN compression techniquesthat assure appropriate performance (size and inference time)on edge devices and resource-constrained environments.A. OTOV3 Evaluation on Untrained ConvNeXt ModelsThe evaluation of OTOV3 on untrained ConvNeXt modelsdemonstrated its effectiveness in achieving substantial com-pression while increasing model accuracy for both full andcompressed models. Across varying sizes of ConvNeXt mod-els, OTOV3 consistently produced impressive reductions inmodel size, number of parameters, and MACs, highlighting itsability to prune redundant structures effectively. This suggests--- Page 12 ---12Fig. 14. Unstructured Pruning Performance with ConvMeXt Small.TABLE IICOMPARISON FOR NUMBER OF OUTPUT FEATURES AND INPUTFEATURES OF THE LINEAR LAYERS IN THE CNB LOCKS BEFORE ANDAFTER OTOV3 C OMPRESSION .Layer Index Input & Output FeaturesSequential CNBlock Full Model CompressedModel1 0 384 3841 1 384 3841 2 384 3843 0 768 7673 1 768 7043 2 768 7265 0 1536 2515 1 1536 3495 2 1536 2425 3 1536 3785 4 1536 2935 5 1536 3775 6 1536 3405 7 1536 4005 8 1536 3945 9 1536 4785 10 1536 4145 11 1536 4245 12 1536 4105 13 1536 3185 14 1536 4885 15 1536 4885 16 1536 4025 17 1536 2465 18 1536 4025 19 1536 4585 20 1536 3235 21 1536 4195 22 1536 4465 23 1536 4445 24 1536 4415 25 1536 4685 26 1536 10707 0 3072 2087 1 3072 2547 2 3072 262Fig. 15. Dynamic Quantization Compression Performance with ConvNeXtSmall Tuned.Fig. 16. OTOV3 and Dynamic Quantization Compression Performance withConvNeXt Small Tuned.--- Page 13 ---13TABLE IIIOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 92.86 188.89 47.16 169.28 29.80Compressed 92.86 50.03 12.44 67.41 12.46TABLE IVDYNAMIC QUANTIZATION COMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 89.53 188.89 47.16 169.28 47.17Compressed 89.40 54.21 2.15 7.27 2.17TABLE VOTOV3 AND DYNAMIC QUANTIZATION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy % Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number of NonZeroParameters (M)Full 89.53 188.89 47.16 169.28 47.17Phase 1 (OTOV3) 92.86 50.03 12.44 67.41 12.46Phase 2 (Dynamic Quantization) 92.93 19.39 2.15 7.27 2.17Fig. 17. Samples From the Compressed Model Output on Edge.that OTOV3 efficiently identifies and removes unnecessaryparameters from the models, leading to more streamlinedarchitectures without compromising predictive performance.However, an unexpected observation arose when comparingthe accuracy of the Torch implementation with that of theTIMM implementation of ConvNeXt Small. The Torch im-plementation exhibited lower accuracy compared to its TIMMcounterpart, indicating that OTOV3\u2019s performance regardingaccuracy may be influenced by the details of the modelarchitecture. This unexpected result suggests that differenttraining strategies or adjustments may be necessary to optimizeOTOV3\u2019s performance across various model implementations,emphasizing the importance of considering architectural dif-ferences when applying compression techniques like OTOV3.B. OTOV3 Evaluation on Fine-Tuned ConvNeXt ModelsWhen evaluating OTOV3\u2019s performance on a fine-tunedConvNeXt model, notable improvements in compression per-formance were observed, confirming its effectiveness in re-ducing model size, parameters, and MACs while marginallyenhancing accuracy. This outcome highlights the potential ofintegrating fine-tuning with structured pruning techniques toachieve even greater optimization of model performance. Thefine-tuned model displayed enhanced compression capabilitiescompared to untrained models, suggesting that pre-trainingcan significantly increase the effectiveness of compressiontechniques. This finding highlights the importance of lever-aging pre-existing knowledge within models to maximize thebenefits of compression, ultimately resulting in CNN modelswith higher performance.C. Unstructured Pruning TechniquesDuring the evaluation of l1 unstructured and random un-structured pruning techniques, expected trends were observedregarding accuracy and compression. As the pruning per-centages increased for both linear and convolutional layers,a corresponding decrease in accuracy was noted, while themodel size, parameters, and MACs remained unaltered. Thisoutcome aligns with the inherent nature of unstructured prun-ing, wherein weights are zeroed out but not entirely eliminated,resulting in sparse models without substantial reductions incomputational complexity. However, the lack of significantreductions in computational complexity may constrain their ef-fectiveness, particularly in resource-constrained environmentswhere efficient utilization of computational resources is essen-tial. This highlights the importance of considering the trade-offs between model compression and computational efficiencywhen selecting pruning techniques for deployment in real---- Page 14 ---14TABLE VIOTOV3 AND DYNAMIC QUANTIZATION REDUCTION NUMBERS WITH CONV NEXT SMALL TUNED .Technique AccuracyChange Size Reduction ParametersReductionMACsReductionOTOV3 3.75% 73.51% 73.62% 60.18%Dynamic Quantization -0.15% 71.30% 95.43% 95.71%OTOV3 and DynamicQuantization 3.80% 89.74% 95.43% 95.71%world applications, especially in edge computing scenarioswhere computational resources are limited.D. Dynamic QuantizationDynamic quantization emerged as a highly effective tech-nique for model compression, demonstrating remarkable re-ductions in model size, parameters, and, Your response: ```ADD_PAPER\n2409.02134v1\nThis paper comprehensively evaluates model compression techniques (structured/unstructured pruning and dynamic quantization) for ConvNeXt models on image classification tasks. Key findings include: 1) OTOV3 pruning achieves 75% reduction in model size/parameters while maintaining accuracy, 2) Dynamic quantization achieves 95% parameter reduction with minimal accuracy drop, 3) Combined OTOV3+quantization achieves 89.7% size reduction and 95% parameter/MAC reduction while improving accuracy by 3.8%. The compressed model demonstrates 92.5% accuracy with 20ms inference time on edge devices, validating effectiveness for real-world deployment.\n```\nStep #34, Phase: literature review, Feedback: Successfully added paper 2409.02134v1, Your response: ```SUMMARY\nedge image classification hardware-aware neural architecture search\n```\n~~~~~~~~~~\nCurrent Step #35, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: You requested arXiv papers related to the query edge image classification hardware-aware neural architecture search, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Neural Architecture Search Over a Graph Search Space\nSummary: Neural Architecture Search (NAS) enabled the discovery of state-of-the-art\narchitectures in many domains. However, the success of NAS depends on the\ndefinition of the search space. Current search spaces are defined as a static\nsequence of decisions and a set of available actions for each decision. Each\npossible sequence of actions defines an architecture. We propose a more\nexpressive class of search space: directed graphs. In our formalism, each\ndecision is a vertex and each action is an edge. This allows us to model\niterative and branching architecture design decisions. We demonstrate in\nsimulation, and on image classification experiments, basic iterative and\nbranching search structures, and show that the graph representation improves\nsample efficiency.\nPublication Date: 2018-12-27\narXiv paper ID: 1812.10666v2\n\nTitle: Discretization-Aware Architecture Search\nSummary: The search cost of neural architecture search (NAS) has been largely reduced\nby weight-sharing methods. These methods optimize a super-network with all\npossible edges and operations, and determine the optimal sub-network by\ndiscretization, \\textit{i.e.}, pruning off weak candidates. The discretization\nprocess, performed on either operations or edges, incurs significant inaccuracy\nand thus the quality of the final architecture is not guaranteed. This paper\npresents discretization-aware architecture search (DA\\textsuperscript{2}S),\nwith the core idea being adding a loss term to push the super-network towards\nthe configuration of desired topology, so that the accuracy loss brought by\ndiscretization is largely alleviated. Experiments on standard image\nclassification benchmarks demonstrate the superiority of our approach, in\nparticular, under imbalanced target network configurations that were not\nstudied before.\nPublication Date: 2020-07-07\narXiv paper ID: 2007.03154v1\n\nNotes: \nYour previous command was: ```SUMMARY\nedge image classification hardware-aware neural architecture search\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 37:
\n~~~~~~~~~~\nHistory: Step #19, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification energy-efficient hardware acceleration, here was the response\nTitle: Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware\nSummary: As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.\nPublication Date: 2022-10-10\narXiv paper ID: 2210.05006v2\n\nTitle: Accelerator-aware Neural Network Design using AutoML\nSummary: While neural network hardware accelerators provide a substantial amount of\nraw compute throughput, the models deployed on them must be co-designed for the\nunderlying hardware architecture to obtain the optimal system performance. We\npresent a class of computer vision models designed using hardware-aware neural\narchitecture search and customized to run on the Edge TPU, Google's neural\nnetwork hardware accelerator for low-power, edge devices. For the Edge TPU in\nCoral devices, these models enable real-time image classification performance\nwhile achieving accuracy typically seen only with larger, compute-heavy models\nrunning in data centers. On Pixel 4's Edge TPU, these models improve the\naccuracy-latency tradeoff over existing SoTA mobile models.\nPublication Date: 2020-03-05\narXiv paper ID: 2003.02838v1\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: EH-DNAS: End-to-End Hardware-aware Differentiable Neural Architecture Search\nSummary: In hardware-aware Differentiable Neural Architecture Search (DNAS), it is\nchallenging to compute gradients of hardware metrics to perform architecture\nsearch. Existing works rely on linear approximations with limited support to\ncustomized hardware accelerators. In this work, we propose End-to-end\nHardware-aware DNAS (EH-DNAS), a seamless integration of end-to-end hardware\nbenchmarking, and fully automated DNAS to deliver hardware-efficient deep\nneural networks on various platforms, including Edge GPUs, Edge TPUs, Mobile\nCPUs, and customized accelerators. Given a desired hardware platform, we\npropose to learn a differentiable model predicting the end-to-end hardware\nperformance of neural network architectures for DNAS. We also introduce\nE2E-Perf, an end-to-end hardware benchmarking tool for customized accelerators.\nExperiments on CIFAR10 and ImageNet show that EH-DNAS improves the hardware\nperformance by an average of $1.4\\times$ on customized accelerators and\n$1.6\\times$ on existing hardware processors while maintaining the\nclassification accuracy.\nPublication Date: 2021-11-24\narXiv paper ID: 2111.12299v1\n\nTitle: Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification\nSummary: In this work, we evaluate the energy usage of fully embedded medical\ndiagnosis aids based on both segmentation and classification of medical images\nimplemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis\nbased on color fundus images as an example to show the possibility of\nperforming segmentation and classification in real time on embedded boards and\nto highlight the different energy requirements of the studied implementations.\n  Several other works develop the use of segmentation and feature extraction\ntechniques to detect glaucoma, among many other pathologies, with deep neural\nnetworks. Memory limitations and low processing capabilities of embedded\naccelerated systems (EAS) limit their use for deep network-based system\ntraining. However, including specific acceleration hardware, such as NVIDIA's\nMaxwell GPU or Google's Edge TPU, enables them to perform inferences using\ncomplex pre-trained networks in very reasonable times.\n  In this study, we evaluate the timing and energy performance of two EAS\nequipped with Machine Learning (ML) accelerators executing an example\ndiagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)\nsegmentation, the obtained prediction times per image are under 29 and 43 ms\nusing Edge TPUs and Maxwell GPUs, respectively. Prediction times for the\nclassification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell\nGPUs, respectively. Regarding energy usage, in approximate terms, for OD\nsegmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,\nrespectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and\n70 mJ, respectively.\nPublication Date: 2023-11-20\narXiv paper ID: 2311.12876v1\n, Your response: ```SUMMARY\nedge image classification FPGA acceleration techniques\n```\nStep #20, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification FPGA acceleration techniques, here was the response\nTitle: TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA\nSummary: In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\nPublication Date: 2019-11-15\narXiv paper ID: 1911.06777v1\n\nTitle: FFCNN: Fast FPGA based Acceleration for Convolution neural network inference\nSummary: We present a new efficient OpenCL-based Accelerator for large scale\nConvolutional Neural Networks called Fast Inference on FPGAs for Convolution\nNeural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels\narchitecture. As pointed out before, high-level synthesis tools such as the\nOpenCL framework can easily port codes originally designed for CPUs and GPUs to\nFPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs.\nThis work aims to propose an efficient FPGA implementation of OpenCL\nHigh-Performance Computing Applications. To do so, a Data reuse and task\nmapping techniques are also presented to improve design efficiency. In\naddition, the following motivations were taken into account when developing\nFFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK\nbased FPGA design flow. 2) In FFFCN, different techniques have been integrated\nto improve the memory band with and throughput. A performance analysis is\nconducted on two deep CNN for Large-Scale Images classification. The obtained\nresults, and the comparison with other works designed to accelerate the same\ntypes of architectures, show the efficiency and the competitiveness of the\nproposed accelerator design by significantly improved performance and resource\nutilization.\nPublication Date: 2022-08-28\narXiv paper ID: 2208.13250v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n\nTitle: FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review\nSummary: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\nPublication Date: 2019-01-01\narXiv paper ID: 1901.00121v1\n\nTitle: PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks\nSummary: Convolutional neural networks (CNNs) have been widely employed in many\napplications such as image classification, video analysis and speech\nrecognition. Being compute-intensive, CNN computations are mainly accelerated\nby GPUs with high power dissipations. Recently, studies were carried out\nexploiting FPGA as CNN accelerator because of its reconfigurability and energy\nefficiency advantage over GPU, especially when OpenCL-based high-level\nsynthesis tools are now available providing fast verification and\nimplementation flows. Previous OpenCL-based design only focused on creating a\ngeneric framework to identify performance-related hardware parameters, without\nutilizing FPGA's special capability of pipelining kernel functions to minimize\nmemory bandwidth requirement. In this work, we propose an FPGA accelerator with\na new architecture of deeply pipelined OpenCL kernels. Data reuse and task\nmapping techniques are also presented to improve design efficiency. The\nproposed schemes are verified by implementing two representative large-scale\nCNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar\npeak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks\ncompared to previous work. Our design is openly accessible and thus can be\nreused to explore new architectures for neural network accelerators.\nPublication Date: 2016-11-08\narXiv paper ID: 1611.02450v1\n, Your response: ```FULL_TEXT\n1911.06777v1\n```\nStep #22, Phase: literature review, Feedback: Successfully added paper 1911.06777v1, Your response: ```SUMMARY\nedge image classification model compression NAS\n```\nStep #23, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification model compression NAS, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: Are Visual Recognition Models Robust to Image Compression?\nSummary: Reducing the data footprint of visual content via image compression is\nessential to reduce storage requirements, but also to reduce the bandwidth and\nlatency requirements for transmission. In particular, the use of compressed\nimages allows for faster transfer of data, and faster response times for visual\nrecognition in edge devices that rely on cloud-based services. In this paper,\nwe first analyze the impact of image compression using traditional codecs, as\nwell as recent state-of-the-art neural compression approaches, on three visual\nrecognition tasks: image classification, object detection, and semantic\nsegmentation. We consider a wide range of compression levels, ranging from 0.1\nto 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition\nability is significantly impacted when using strong compression. For example,\nfor segmentation mIoU is reduced from 44.5 to 30.5 mIoU when compressing to 0.1\nbpp using the best compression model we evaluated. Second, we test to what\nextent this performance drop can be ascribed to a loss of relevant information\nin the compressed image, or to a lack of generalization of visual recognition\nmodels to images with compression artefacts. We find that to a large extent the\nperformance loss is due to the latter: by finetuning the recognition models on\ncompressed training images, most of the performance loss is recovered. For\nexample, bringing segmentation accuracy back up to 42 mIoU, i.e. recovering 82%\nof the original drop in accuracy.\nPublication Date: 2023-04-10\narXiv paper ID: 2304.04518v1\n\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms\nSummary: Deep learning on an edge device requires energy efficient operation due to\never diminishing power budget. Intentional low quality data during the data\nacquisition for longer battery life, and natural noise from the low cost sensor\ndegrade the quality of target output which hinders adoption of deep learning on\nan edge device. To overcome these problems, we propose simple yet efficient\nmixture of pre-processing experts (MoPE) model to handle various image\ndistortions including low resolution and noisy images. We also propose to use\nadversarially trained auto encoder as a pre-processing expert for the noisy\nimages. We evaluate our proposed method for various machine learning tasks\nincluding object detection on MS-COCO 2014 dataset, multiple object tracking\nproblem on MOT-Challenge dataset, and human activity classification on UCF 101\ndataset. Experimental results show that the proposed method achieves better\ndetection, tracking and activity classification accuracies under noise without\nsacrificing accuracies for the clean images. The overheads of our proposed MoPE\nare 0.67% and 0.17% in terms of memory and computation compared to the baseline\nobject detection network.\nPublication Date: 2019-04-29\narXiv paper ID: 1904.12426v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\nStep #25, Phase: literature review, Feedback: Successfully added paper 2307.04443v1, Your response: ```SUMMARY\nedge image classification dynamic computation optimization\n```\nStep #26, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification dynamic computation optimization, here was the response\nTitle: Immiscible Color Flows in Optimal Transport Networks for Image Classification\nSummary: In classification tasks, it is crucial to meaningfully exploit the\ninformation contained in data. While much of the work in addressing these tasks\nis devoted to building complex algorithmic infrastructures to process inputs in\na black-box fashion, less is known about how to exploit the various facets of\nthe data, before inputting this into an algorithm. Here, we focus on this\nlatter perspective, by proposing a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nThe resulting optimal flows can then be fed into standard classifiers to\ndistinguish images in different classes. We show how our method can outperform\ncompeting approaches on image classification tasks in datasets where color\ninformation matters.\nPublication Date: 2022-05-04\narXiv paper ID: 2205.02938v2\n\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)\nSummary: Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\nPublication Date: 2022-04-10\narXiv paper ID: 2204.04704v1\n\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Efficient Feature Compression for Edge-Cloud Systems\nSummary: Optimizing computation in an edge-cloud system is an important yet\nchallenging problem. In this paper, we consider a three-way trade-off between\nbit rate, classification accuracy, and encoding complexity in an edge-cloud\nimage classification system. Our method includes a new training strategy and an\nefficient encoder architecture to improve the rate-accuracy performance. Our\ndesign can also be easily scaled according to different computation resources\non the edge device, taking a step towards achieving a rate-accuracy-complexity\n(RAC) trade-off. Under various settings, our feature coding system consistently\noutperforms previous methods in terms of the RAC performance.\nPublication Date: 2022-11-17\narXiv paper ID: 2211.09897v1\n, Your response: ```SUMMARY\nedge image classification adaptive inference techniques\n```\nStep #27, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification adaptive inference techniques, here was the response\nTitle: Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading\nSummary: Rare events, despite their infrequency, often carry critical information and\nrequire immediate attentions in mission-critical applications such as\nautonomous driving, healthcare, and industrial automation. The data-intensive\nnature of these tasks and their need for prompt responses, combined with\ndesigning edge AI (or edge inference), pose significant challenges in systems\nand techniques. Existing edge inference approaches often suffer from\ncommunication bottlenecks due to high-dimensional data transmission and fail to\nprovide timely responses to rare events, limiting their effectiveness for\nmission-critical applications in the sixth-generation (6G) mobile networks. To\novercome these challenges, we propose a channel-adaptive, event-triggered\nedge-inference framework that prioritizes efficient rare-event processing.\nCentral to this framework is a dual-threshold, multi-exit architecture, which\nenables early local inference for rare events detected locally while offloading\nmore complex rare events to edge servers for detailed classification. To\nfurther enhance the system's performance, we developed a channel-adaptive\noffloading policy paired with an online algorithm to dynamically determine the\noptimal confidence thresholds for controlling offloading decisions. The\nassociated optimization problem is solved by reformulating the original\nnon-convex function into an equivalent strongly convex one. Using deep neural\nnetwork classifiers and real medical datasets, our experiments demonstrate that\nthe proposed framework not only achieves superior rare-event classification\naccuracy, but also effectively reduces communication overhead, as opposed to\nexisting edge-inference approaches.\nPublication Date: 2025-01-01\narXiv paper ID: 2501.02001v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies\nSummary: In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), application-specific integrated circuit\n(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire\nfurther research with a contemporary guide on optimizing ViTs for efficient\ndeployment on edge devices.\nPublication Date: 2025-02-26\narXiv paper ID: 2503.02891v3\n\nTitle: Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection to Suppress Adversarial Perturbation\nSummary: This paper presents a novel context-aware image denoising algorithm that\ncombines an adaptive image smoothing technique and color reduction techniques\nto remove perturbation from adversarial images. Adaptive image smoothing is\nachieved using auto-threshold canny edge detection to produce an accurate edge\nmap used to produce a blurred image that preserves more edge features. The\nproposed algorithm then uses color reduction techniques to reconstruct the\nimage using only a few representative colors. Through this technique, the\nalgorithm can reduce the effects of adversarial perturbations on images. We\nalso discuss experimental data on classification accuracy. Our results showed\nthat the proposed approach reduces adversarial perturbation in adversarial\nattacks and increases the robustness of the deep convolutional neural network\nmodels.\nPublication Date: 2021-01-14\narXiv paper ID: 2101.05833v1\n\nTitle: A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs\nSummary: High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\nPublication Date: 2021-07-27\narXiv paper ID: 2107.12824v4\n, Your response: ```SUMMARY\nedge image classification early exit networks\n```\nStep #28, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification early exit networks, here was the response\nTitle: Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead\nSummary: Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\nPublication Date: 2021-05-19\narXiv paper ID: 2105.09121v3\n\nTitle: Adaptive Early Exiting for Collaborative Inference over Noisy Wireless Channels\nSummary: Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.\nPublication Date: 2023-11-29\narXiv paper ID: 2311.18098v1\n\nTitle: Early-exit deep neural networks for distorted images: providing an efficient edge offloading\nSummary: Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\nPublication Date: 2021-08-20\narXiv paper ID: 2108.09343v2\n\nTitle: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit\nSummary: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is\ngaining popularity due to recent breakthroughs in ML and Internet of Things\n(IoT). Most tinyML research focuses on model compression techniques that trade\naccuracy (and model capacity) for compact models to fit into the KB-sized\ntiny-edge devices. In this paper, we show how such models can be enhanced by\nthe addition of an early exit intermediate classifier. If the intermediate\nclassifier exhibits sufficient confidence in its prediction, the network exits\nearly thereby, resulting in considerable savings in time. Although early exit\nclassifiers have been proposed in previous work, these previous proposals focus\non large networks, making their techniques suboptimal/impractical for tinyML\napplications. Our technique is optimized specifically for tiny-CNN sized\nmodels. In addition, we present a method to alleviate the effect of network\noverthinking by leveraging the representations learned by the early exit. We\nevaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image\nclassification, keyword spotting and visual wake word detection tasks. Our\nresults show that T-RecX 1) improves the accuracy of baseline network, 2)\nachieves 31.58% average reduction in FLOPS in exchange for one percent accuracy\nacross all evaluated models. Furthermore, we show that our methods consistently\noutperform popular prior works on the tiny-CNNs we evaluate.\nPublication Date: 2022-07-14\narXiv paper ID: 2207.06613v2\n\nTitle: Understanding the Robustness of Multi-Exit Models under Common Corruptions\nSummary: Multi-Exit models (MEMs) use an early-exit strategy to improve the accuracy\nand efficiency of deep neural networks (DNNs) by allowing samples to exit the\nnetwork before the last layer. However, the effectiveness of MEMs in the\npresence of distribution shifts remains largely unexplored. Our work examines\nhow distribution shifts generated by common image corruptions affect the\naccuracy/efficiency of MEMs. We find that under common corruptions,\nearly-exiting at the first correct exit reduces the inference cost and provides\na significant boost in accuracy ( 10%) over exiting at the last layer. However,\nwith realistic early-exit strategies, which do not assume knowledge about the\ncorrect exits, MEMs still reduce inference cost but provide a marginal\nimprovement in accuracy (1%) compared to exiting at the last layer. Moreover,\nthe presence of distribution shift widens the gap between an MEM's maximum\nclassification accuracy and realistic early-exit strategies by 5% on average\ncompared with the gap on in-distribution data. Our empirical analysis shows\nthat the lack of calibration due to a distribution shift increases the\nsusceptibility of such early-exit strategies to exit early and increases\nmisclassification rates. Furthermore, the lack of calibration increases the\ninconsistency in the predictions of the model across exits, leading to both\ninefficient inference and more misclassifications compared with evaluation on\nin-distribution data. Finally, we propose two metrics, underthinking and\noverthinking, that quantify the different behavior of practical early-exit\nstrategy under distribution shifts, and provide insights into improving the\npractical utility of MEMs.\nPublication Date: 2022-12-03\narXiv paper ID: 2212.01562v1\n, Your response: ```FULL_TEXT\n2105.09121v3\n```\nStep #30, Phase: literature review, Feedback: Successfully added paper 2105.09121v3, Your response: ```SUMMARY\nedge image classification sparsity techniques\n```\nStep #31, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification sparsity techniques, here was the response\nTitle: Exploiting Activation based Gradient Output Sparsity to Accelerate Backpropagation in CNNs\nSummary: Machine/deep-learning (ML/DL) based techniques are emerging as a driving\nforce behind many cutting-edge technologies, achieving high accuracy on\ncomputer vision workloads such as image classification and object detection.\nHowever, training these models involving large parameters is both\ntime-consuming and energy-hogging. In this regard, several prior works have\nadvocated for sparsity to speed up the of DL training and more so, the\ninference phase. This work begins with the observation that during training,\nsparsity in the forward and backward passes are correlated. In that context, we\ninvestigate two types of sparsity (input and output type) inherent in gradient\ndescent-based optimization algorithms and propose a hardware micro-architecture\nto leverage the same. Our experimental results use five state-of-the-art CNN\nmodels on the Imagenet dataset, and show back propagation speedups in the range\nof 1.69$\\times$ to 5.43$\\times$, compared to the dense baseline execution. By\nexploiting sparsity in both the forward and backward passes, speedup\nimprovements range from 1.68$\\times$ to 3.30$\\times$ over the sparsity-agnostic\nbaseline execution. Our work also achieves significant reduction in training\niteration time over several previously proposed dense as well as sparse\naccelerator based platforms, in addition to achieving order of magnitude energy\nefficiency improvements over GPU based execution.\nPublication Date: 2021-09-16\narXiv paper ID: 2109.07710v1\n\nTitle: Super-Resolution of Brain MRI Images using Overcomplete Dictionaries and Nonlocal Similarity\nSummary: Recently, the Magnetic Resonance Imaging (MRI) images have limited and\nunsatisfactory resolutions due to various constraints such as physical,\ntechnological and economic considerations. Super-resolution techniques can\nobtain high-resolution MRI images. The traditional methods obtained the\nresolution enhancement of brain MRI by interpolations, affecting the accuracy\nof the following diagnose process. The requirement for brain image quality is\nfast increasing. In this paper, we propose an image super-resolution (SR)\nmethod based on overcomplete dictionaries and inherent similarity of an image\nto recover the high-resolution (HR) image from a single low-resolution (LR)\nimage. We explore the nonlocal similarity of the image to tentatively search\nfor similar blocks in the whole image and present a joint reconstruction method\nbased on compressive sensing (CS) and similarity constraints. The sparsity and\nself-similarity of the image blocks are taken as the constraints. The proposed\nmethod is summarized in the following steps. First, a dictionary classification\nmethod based on the measurement domain is presented. The image blocks are\nclassified into smooth, texture and edge parts by analyzing their features in\nthe measurement domain. Then, the corresponding dictionaries are trained using\nthe classified image blocks. Equally important, in the reconstruction part, we\nuse the CS reconstruction method to recover the HR brain MRI image, considering\nboth nonlocal similarity and the sparsity of an image as the constraints. This\nmethod performs better both visually and quantitatively than some existing\nmethods.\nPublication Date: 2019-02-13\narXiv paper ID: 1902.04902v1\n\nTitle: Sparsity Turns Adversarial: Energy and Latency Attacks on Deep Neural Networks\nSummary: Adversarial attacks have exposed serious vulnerabilities in Deep Neural\nNetworks (DNNs) through their ability to force misclassifications through\nhuman-imperceptible perturbations to DNN inputs. We explore a new direction in\nthe field of adversarial attacks by suggesting attacks that aim to degrade the\ncomputational efficiency of DNNs rather than their classification accuracy.\nSpecifically, we propose and demonstrate sparsity attacks, which adversarial\nmodify a DNN's inputs so as to reduce sparsity (or the presence of zero values)\nin its internal activation values. In resource-constrained systems, a wide\nrange of hardware and software techniques have been proposed that exploit\nsparsity to improve DNN efficiency. The proposed attack increases the execution\ntime and energy consumption of sparsity-optimized DNN implementations, raising\nconcern over their deployment in latency and energy-critical applications.\n  We propose a systematic methodology to generate adversarial inputs for\nsparsity attacks by formulating an objective function that quantifies the\nnetwork's activation sparsity, and minimizing this function using iterative\ngradient-descent techniques. We launch both white-box and black-box versions of\nadversarial sparsity attacks on image recognition DNNs and demonstrate that\nthey decrease activation sparsity by up to 1.82x. We also evaluate the impact\nof the attack on a sparsity-optimized DNN accelerator and demonstrate\ndegradations up to 1.59x in latency, and also study the performance of the\nattack on a sparsity-optimized general-purpose processor. Finally, we evaluate\ndefense techniques such as activation thresholding and input quantization and\ndemonstrate that the proposed attack is able to withstand them, highlighting\nthe need for further efforts in this new direction within the field of\nadversarial machine learning.\nPublication Date: 2020-06-14\narXiv paper ID: 2006.08020v3\n\nTitle: Leaf segmentation through the classification of edges\nSummary: We present an approach to leaf level segmentation of images of Arabidopsis\nthaliana plants based upon detected edges. We introduce a novel approach to\nedge classification, which forms an important part of a method to both count\nthe leaves and establish the leaf area of a growing plant from images obtained\nin a high-throughput phenotyping system. Our technique uses a relatively\nshallow convolutional neural network to classify image edges as background,\nplant edge, leaf-on-leaf edge or internal leaf noise. The edges themselves were\nfound using the Canny edge detector and the classified edges can be used with\nsimple image processing techniques to generate a region-based segmentation in\nwhich the leaves are distinct. This approach is strong at distinguishing\noccluding pairs of leaves where one leaf is largely hidden, a situation which\nhas proved troublesome for plant image analysis systems in the past. In\naddition, we introduce the publicly available plant image dataset that was used\nfor this work.\nPublication Date: 2019-04-05\narXiv paper ID: 1904.03124v1\n\nTitle: Network classification with applications to brain connectomics\nSummary: While statistical analysis of a single network has received a lot of\nattention in recent years, with a focus on social networks, analysis of a\nsample of networks presents its own challenges which require a different set of\nanalytic tools. Here we study the problem of classification of networks with\nlabeled nodes, motivated by applications in neuroimaging. Brain networks are\nconstructed from imaging data to represent functional connectivity between\nregions of the brain, and previous work has shown the potential of such\nnetworks to distinguish between various brain disorders, giving rise to a\nnetwork classification problem. Existing approaches tend to either treat all\nedge weights as a long vector, ignoring the network structure, or focus on\ngraph topology as represented by summary measures while ignoring the edge\nweights. Our goal is to design a classification method that uses both the\nindividual edge information and the network structure of the data in a\ncomputationally efficient way, and that can produce a parsimonious and\ninterpretable representation of differences in brain connectivity patterns\nbetween classes. We propose a graph classification method that uses edge\nweights as predictors but incorporates the network nature of the data via\npenalties that promote sparsity in the number of nodes, in addition to the\nusual sparsity penalties that encourage selection of edges. We implement the\nmethod via efficient convex optimization and provide a detailed analysis of\ndata from two fMRI studies of schizophrenia.\nPublication Date: 2017-01-27\narXiv paper ID: 1701.08140v3\n, Your response: ```SUMMARY\nedge image classification pruning quantization techniques\n```\nStep #32, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification pruning quantization techniques, here was the response\nTitle: Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks\nSummary: This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.\nPublication Date: 2024-09-02\narXiv paper ID: 2409.02134v1\n\nTitle: Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\nSummary: Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\nPublication Date: 2021-08-13\narXiv paper ID: 2108.06128v3\n\nTitle: Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques\nSummary: Tomato maturity plays a pivotal role in optimizing harvest timing and\nensuring product quality, but current methods struggle to achieve high accuracy\nalong computational efficiency simultaneously. Existing deep learning\napproaches, while accurate, are often too computationally demanding for\npractical use in resource-constrained agricultural settings. In contrast,\nsimpler techniques fail to capture the nuanced features needed for precise\nclassification. This study aims to develop a computationally efficient tomato\nclassification model using the ResNet-18 architecture optimized through\ntransfer learning, pruning, and quantization techniques. Our objective is to\naddress the dual challenge of maintaining high accuracy while enabling\nreal-time performance on low-power edge devices. Then, these models were\ndeployed on an edge device to investigate their performance for tomato maturity\nclassification. The quantized model achieved an accuracy of 97.81%, with an\naverage classification time of 0.000975 seconds per image. The pruned and\nauto-tuned model also demonstrated significant improvements in deployment\nmetrics, further highlighting the benefits of optimization techniques. These\nresults underscore the potential for a balanced solution that meets the\naccuracy and efficiency demands of modern agricultural production, paving the\nway for practical, real-world deployment in resource-limited environments.\nPublication Date: 2025-03-13\narXiv paper ID: 2503.10940v1\n\nTitle: Digital-SC: Digital Semantic Communication with Adaptive Network Split and Learned Non-Linear Quantization\nSummary: Semantic communication, an intelligent communication paradigm that aims to\ntransmit useful information in the semantic domain, is facilitated by deep\nlearning techniques. Robust semantic features can be learned and transmitted in\nan analog fashion, but it poses new challenges to hardware, protocol, and\nencryption. In this paper, we propose a digital semantic communication system,\nwhich consists of an encoding network deployed on a resource-limited device and\na decoding network deployed at the edge. To acquire better semantic\nrepresentation for digital transmission, a novel non-linear quantization module\nis proposed to efficiently quantize semantic features with trainable\nquantization levels. Additionally, structured pruning is incorporated to reduce\nthe dimension of the transmitted features. We also introduce a semantic\nlearning loss (SLL) function to reduce semantic error. To adapt to various\nchannel conditions and inputs under constraints of communication and computing\nresources, a policy network is designed to adaptively choose the split point\nand the dimension of the transmitted semantic features. Experiments using the\nCIFAR-10 and ImageNet dataset for image classification are employed to evaluate\nthe proposed digital semantic communication network, and ablation studies are\nconducted to assess the proposed quantization module, structured pruning and\nSLL.\nPublication Date: 2023-05-22\narXiv paper ID: 2305.13553v2\n\nTitle: A 1Mb mixed-precision quantized encoder for image classification and patch-based compression\nSummary: Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.\nPublication Date: 2025-01-09\narXiv paper ID: 2501.05097v1\n, Your response: ```FULL_TEXT\n2409.02134v1\n```\nStep #33, Phase: literature review, Feedback: 3\n--- Page 1 ---1Edge AI: Evaluation of Model CompressionTechniques for Convolutional Neural NetworksSamer Francy, Member, IEEE,Raghubir Singh, Member, IEEEAbstract\u2014This work evaluates the compression techniqueson ConvNeXt models in image classification tasks using theCIFAR-10 dataset. Structured pruning, unstructured pruning,and dynamic quantization methods are evaluated to reduce modelsize and computational complexity while maintaining accuracy.The experiments, conducted on cloud-based platforms and edgedevice, assess the performance of these techniques. Results showsignificant reductions in model size, with up to 75% reduc-tion achieved using structured pruning techniques. Additionally,dynamic quantization achieves a reduction of up to 95% inthe number of parameters. Fine-tuned models exhibit improvedcompression performance, indicating the benefits of pre-trainingin conjunction with compression techniques. Unstructured prun-ing methods reveal trends in accuracy and compression, withlimited reductions in computational complexity. The combinationof OTOV3 pruning and dynamic quantization further enhancescompression performance, resulting 89.7% reduction in size, 95%reduction with number of parameters and MACs, and 3.8%increase with accuracy. The deployment of the final compressedmodel on edge device demonstrates high accuracy 92.5% and lowinference time 20 ms, validating the effectiveness of compressiontechniques for real-world edge computing applications.Index Terms\u2014edge AI, ConvNeXt, CNN, pruning, quantiza-tion, compression, OTO.I. I NTRODUCTIONEDGE devices such as Internet of Things (IoT) are becom-ing increasingly important and widely used in our dailylives and industrial facilities. IoT is a network of things thatempowered by sensors, identifiers, software intelligence, andinternet connectivity, it can be considered as the intersectionof the internet, things/objects (anything/everything), and data[1]. The number of these devices is expected to increase evenmore [2]. These devices have the potential to perform complexArtificial Intelligence (AI) tasks locally, without relying heav-ily on cloud infrastructure [3]. The rapid advancement of AIhas led to the development of complex deep learning modelsthat show high performance in different domains. DeployingAI models on edge devices has many advantages such as lowlatency, privacy and data security, bandwidth optimization, andreduced network dependence. Low latency is achieved due toreal-time processing by instant data analysis on edge withoutwaiting for remote server processing, this data analysis on theedge reduces transmitting data to the cloud which enhancessecurity against breaches, reduces the bandwidth consumption,and reduces network dependence.Samer Francy is with the Department of Computer Science, University ofBath, Bath, England (e-mail: samer.francy@bath.edu).Raghubir Singh is with the Department of Computer Science, Universityof Bath, Bath, England (e-mail: rs3022@bath.ac.uk).A. Overview of Edge AIEdge AI represents a paradigm shift in the way AI isimplemented in the context of the IoT. It capitalizes on thecapabilities of IoT devices, enhancing real-time processing,analytics, and decision-making directly at the edge of thenetwork. The IoT architecture, which is the foundation forEdge AI, typically involves three core layers [1]. The layersare perceptual layer, where data is collected from varioussensors and devices, network layer, where data is transmittedand routed through this layer, which is responsible for commu-nication between devices and cloud services, and applicationlayer, which processes and utilizes the data, providing insightsand enabling actions.B. Convolutional Neural Networks (CNNs)CNN models are subsets of Deep Neural Networks (DNN)models. CNN models are effective for image and video-relatedtasks due to their ability to learn relevant features from thedata by recognizing patterns, shapes, and structures in images,which is challenging for traditional machine learning models,that\u2019s why they are used for computer vision tasks such asimage classification, object detection, and image segmentation[4].1) CNN Architecture: In general CNN models consist ofbelow parts:\u2022 Input Image : Pixels form the binary basis of computerimages, while the human visual system operates throughneurons with receptive fields. Similarly, CNNs functionwithin their receptive areas, starting with simple patternsand advancing to more complex ones, making CNNs apromising tool for computer vision [4].\u2022 Convolutional Layer : A convolutional layer in a CNNuses a small filter (e.g., 3x3 or 5x5) that slides over theinput image. At each position, it multiplies its valueswith the overlapping image pixels and sums the results toproduce an output. This sliding operation helps identifylocal features like edges and colors, building a hierar-chical representation. The depth of the filter matches theinput image\u2019s channels (e.g., 3 for RGB images). Stackingmultiple filters allows the network to learn features atdifferent abstraction levels [4].\u2022 Pooling Layer : Pooling reduces the spatial size of fea-ture maps. This not only lowers computational demandsbut also helps in extracting position and orientation-independent features essential for training. Two com-mon pooling methods are maximum pooling and averagepooling. In maximum pooling, a small kernel (e.g., 2x2)arXiv:2409.02134v1  [cs.LG]  2 Sep 2024--- Page 2 ---2Fig. 1. Unbalanced Demand For Computation (Left) and Memory (Right) in AlexNet [5].selects the maximum value within its range and places itin the output. In average pooling, a similar-sized kernelcomputes the average value within its area for eachchannel, maintaining the same depth. Pooling simplifiescomputation and weight requirements, with maximumpooling being the most commonly used method [4].\u2022 Activation Function : The activation function, applies amathematical operation to the filter\u2019s output to concludethe output of the network. The common choice is the Rec-tified Linear Unit (ReLU). They fall into two categories,linear and non-linear activation functions [4].\u2022 Fully Connected Layer : It functions as a feed-forwardneural network (NN) typically situated at the network\u2019slower layers. It receives input from the output of the lastpooling or convolutional layer, which is flattened into aone-dimensional vector, enabling it to learn complex datarelationships [4].2) Computation and Memory Demands: In CNNs, unbal-ance exists in resource demands between the layers. Convolu-tional layers primarily serve as feature extractors and heavilydominate the computational workload. In the case of AlexNet,for instance, the convolutional layers account for just 2 millionweights but demand a substantial 1.33 Giga Operations PerSecond (GOPS) of computation. In contrast, fully connectedlayers function as classifiers, accumulating information forhigh-level decisions, and bear the weight of the network witharound 59 million parameters, yet they contribute significantlyless to computation, requiring only 0.12 GOPS. This obviouscontrast in resource allocation (Figure 1) highlights the unbal-anced demands between these two layers in CNNs [5].3) Key CNN Architectures:In 1989, the use of a NN archi-tecture with convolutional layers for recognizing handwrittendigits in the context of ZIP code recognition was introduced[6], That architecture consisted of input layer, 3 hidden layers,and output layer. Since then, CNN models have developed(Figure 2) and became much deeper.4) CNN on Edge: Deploying CNN models on edge hasa wide range of practical and industrial applications acrossvarious sectors. Here are some specific examples:\u2022 Surveillance and Security : It can perform real-time ob-ject detection and facial recognition for security monitor-ing, identifying intruders, and managing access control.Face identification was deployed using VGGFace [7].Video analysis was deployed using YOLOX [8]. Infraredand Visible Image Fusion for security systems was de-ployed using DenseNet [9]. Human action recognitionapplications were deployed using ResNet [10].\u2022 Manufacturing and Quality Control : It can inspectproducts on assembly lines for defects, ensuring qualitycontrol and minimizing errors. Real-time detection ofsteel strip surface defects was deployed using Faster R-CNN model [11].\u2022 Agriculture: Drones can monitor crops, detect pests,diseases, and nutrient deficiencies, enabling precisionagriculture. Identifying rice leaf diseases in natural en-vironments was deployed using GoogLeNet [12]. Pepperleaf disease identification was deployed using GoogLeNet[13]. Detection for insect pests was deployed on YOLOX[14].\u2022 Healthcare and Wearables : Wearable devices can con-tinuously monitor vital signs, detect anomalies, and evendiagnose certain health conditions. Medical diagnosis(Covid and Lung Disease Detection) was deployed usingVGG, MobileNet, and AlexNet [15]. Automatically diag-nose pneumonia and COVID-19 from chest X-ray imageswas deployed on DenseNet [16]. Medical applications(e.g., COVID-19 detection, cardiomegaly diagnosis, braintumor classification) were deployed using ResNet [10].\u2022 Energy Management: It can monitor energy usage, opti-mize consumption patterns, and identify areas for energyefficiency improvements. Wind Turbine Maintenance andfault diagnosis was deployed using AlexNet [17].\u2022 Environmental Monitoring : It can monitor air quality,pollution levels, and weather conditions, providing valu-able insights for urban planning. A smartphone app toperform fine-grained classification of animals in the wildwas deployed using AlexNet, GoogLeNet, and ResNet[18]. Identification of mosquito species was deployedusing AlexNet, DenseNet, Inception, ResNet, and VGG[19].--- Page 3 ---3Fig. 2. Evolution of Key CNN Architectures Over Time.\u2022 Logistics and Inventory Management : It can automatepackage sorting, inventory tracking, and warehouse man-agement. Mobile robot to map its surroundings while de-tecting objects and people was deployed using AlexNet,GoogLeNet, and ResNet [18].\u2022 Autonomous Vehicles : Vehicles can process real-timedata from cameras and sensors using CNNs, aiding inautonomous navigation and collision avoidance. Instanceobjects detection system for intelligent service robots wasdeployed using Alexnet [20]. Advanced driving assistancesystems (ADASs) and automated vehicles (A Vs) weredeployed using Faster R-CNN [21].Deploying CNNs on edge devices presents significant chal-lenges mainly due to the limited computational resources,constrained memory, and power consumption constraints in-herent to these devices. CNN models, known for their depthand complexity, often demand substantial computational powerand memory, which may exceed the capabilities of edgehardware. Hence, compressing the model before deploymentbecomes imperative. Model compression techniques aim toreduce the size of the CNN model while preserving its perfor-mance, thereby enabling efficient utilization of computationalresources and memory on edge devices. By compressingthe model, we can mitigate the challenges associated withdeploying CNNs on edge devices, ensuring that they caneffectively perform tasks such as real-time image processing,object detection, and classification within resource-constrainedenvironments.With the enormous number of compression techniques pro-posed for CNNs, the rapid evolution of CNN architectures hascreated a gap in the field. This dynamic shift in architecturedesign requires an evaluation of existing compression methods,particularly in light of the demand to make these advancedCNN models suitable for deployment on edge devices. AsCNN designs continue to advance, the challenge lies inadapting compression techniques to smoothly integrate withthese modern architectures. This evaluation (either for eachindividual techniques or combined with each other) becomesimportant, as it not only ensures the continued relevance ofcompression techniques but also addresses the urgent need tomake resource-intensive CNN models accessible and deploy-able on edge devices.This work aims to evaluate CNN compression techniquesthat assure appropriate performance on edge devices. In thesubsequent sections, this work reveals in a structured mannerto evaluate the compression techniques for CNN models.section 2 provides a detailed review of related work, offeringinsights into prior research and establishing a foundationalunderstanding of the topic. section 3 explains the methodologyemployed in conducting the experiments, describing the designand execution of the study. Following this, section 4 presentsthe experimental results and analyzes the findings to recognizetrends and implications. Section 5 critically evaluates the re-sults. Section 6 draws conclusions regarding the effectivenessand significance of the compression techniques. This organizedframework aims to comprehensively explore and contribute tothe field of model compression for efficient deployment inresource-constrained environments.II. R ELATED WORKWithin the context of edge AI, it is important to address thecritical need for model compression. The resource constrainednature of these devices requires more efficient AI modelsby minimizing memory and computational demands, ensur-ing faster inference speeds, and enhancing energy efficiency.Below will explore various model compression techniques andtheir implications for edge AI applications.A. PruningPruning is a key technique in DNN, aimed at enhancingefficiency and model generalization. It involves the removalof redundant components, such as parameters, neurons, filters,or entire layers, leading to several advantages. By reducingunnecessary parameters, it cuts down on storage require-ments, and important for models deployed on devices withlimited memory. Furthermore, it streamlines computational--- Page 4 ---4Fig. 3. Weight Pruning (a) and Neuron Pruning (b). x: input, w: weight. [22].complexity during inference, resulting in faster predictions andlower power consumption. Pruning also mitigates overfittingby simplifying the network. Various pruning techniques, likeweight pruning, neuron pruning, filter pruning, and layerpruning, offer different levels of granularity in componentremoval. Whether applied during or after training, pruningenables the creation of more compact and efficient CNNmodels tailored to specific needs, effectively balancing modelsize, computational efficiency, and accuracy. Weight pruningsets weight connections in the network to zero if they fallbelow a predefined threshold or are considered redundant.Neuron pruning focuses on removing entire neurons if they arefound to be redundant. Layer pruning allows for the removalof entire layers that are considered less important [22].1) Pruning For Fully Connected Layer: Fully connectedlayers are dense that makes the layer with high memorydemand. Pruning them effectively reduces the memory burdenand reduce size of the model.It involves selectively removing weight connections andneurons to reduce the model\u2019s complexity while preservingperformance. In a typical feed-forward NN, inputs are multi-plied by corresponding weights, and a linear sum is computedat each neuron, which is then transformed by an activationfunction. As shown in Figure 3, a network with 3 inputneurons, 2 hidden neurons, and 1 output neuron may havemultiple weight connections. Pruning can be applied to elim-inate specific weight connections or entire neurons. By doingso, the total number of weight connections can be significantlyreduced, leading to a more compact network. The concept ofpruning was first introduced by [6], who proposed removingweights based on their saliency, with small-magnitude weightshaving less impact on training error. The process involvesiterative retraining to regain accuracy, and the technique isknown as \u2019Optimal Brain Damage (OBD)\u2019 where the secondderivative of the objective function with respect to parametersis used to calculate the small saliency, facilitating informedpruning decisions. Since then, other pruning approaches havebeen introduced for fully connected layers [22].2) Pruning For Convolutional Layer: Each convolutionallayer typically consists of numerous filters that makes the layerwith high computational demand. Pruning these less significantfilters directly from the convolutional layer effectively reducesthe computational burden and speeds up the model. Inspired byearly pruning methods, new approaches have been introducedto be used to prune convolutional layers [22].Bayesian was used to decide what to prune and the level ofpruning, in this context involves employing scale mixtures ofnormals as priors for parameter weights in LeNet and VGG[23]. Differential evolution based layer-wise weight methodalongside three other pruning techniques (Naive Cut, Itera-tive Pruning, and Multi-Objective NN Pruning) was used toprune LeNet, AlexNet, and VGG16 [24]. Two fully connectedlayers are removed from the AlexNet architecture, and BatchNormalization (BN) is introduced to mitigate overfitting [20].Filters Similarity in Consecutive Layers (FSCL) for CNNs wasused to reduce the number of filters while preserving impor-tant filters, ultimately improving model efficiency for VGG,GoogLeNet, and ResNet [25]. Structured pruning throughsparsity-induced pruning was used to enhance the real-timeimplementation of the DEtection TRansformer (DETR) [26].Structured pruning was used to compress YOLOX, this processincluded sparse training to prune unimportant channels, withfine-tuning to recover accuracy [14]. Evolutionary approachto filter pruning involved sequential application of multiplepruners in a specific order to sparsify LeNet and VGG-19while maintaining model accuracy [27]. Multilayer networkswere used to represent and compress ResNets, it involvedcreating class networks, calculating arc weights, and forminga multilayer network. The overall degree of nodes in themultilayer network is used to select a subset of nodes forcompression, and convolutional layers are pruned [10]. Tooptimize the Fused-DenseNet-Tiny model for efficient detec-tion of COVID-19 and pneumonia in chest X-ray images,three steps were implemented including removing insignif-icant weights, discarding pruning casings, and applying acompression algorithm [16]. Deep Scalable Zerotree-based(DeepSZ) framework was used to address resource constraints--- Page 5 ---5by achieving significant compression for LeNet, AlexNet,and VGG while maintaining acceptable inference accuracy[28]. Compressing without retraining that was used withResNet, AlexNet, VGGNet and SqueezeNet. It focused onconvolutional and fully connected layers, while maintainingor improving classification accuracy [29].B. QuantizationQuantization plays an important role in addressing theresource-intensive nature of CNNs. By reducing the bit pre-cision of model parameters, quantization not only conservesmemory and energy but also enhances inference speed, makingit an essential technique for deploying CNNs in resource-constrained environments such as edge devices. Weight clus-tering takes quantization to a more advanced level by orga-nizing weights into clusters, where each cluster shares thesame weight value. This approach minimizes the need forfine-tuning individual weights and can lead to substantialreductions in memory and computational overhead [22].Single Level Quantization (SLQ) and Multiple Level Quan-tization (MLQ) technique were used to quantize AlexNet,VGG, GoogleNet, and ResNet to the deployment of thesemodels on resource-constrained mobile devices like mobilephones and drones [30].C. Low-Rank Decomposition/FactorizationIt is a compression technique used with feed-forward NNsand CNNs, to reduce the size of weight matrices whilepreserving model performance. Singular Value Decomposition(SVD) is a popular factorization scheme that decomposes aweight matrix A into three smaller matrices: U, S, and V T . Urepresents the left singular vectors, S is a diagonal matrix ofsingular values, and V T is the transpose of the right singularvectors. This factorization offers several advantages, such asreduced storage requirements, which is crucial for memory-constrained environments, and accelerated inference, espe-cially in CNNs, as smaller matrices can be convolved faster.Low-rank factorization can be applied to fully connected andconvolutional layers, making models more storage-efficientand faster without sacrificing performance. Careful selectionof the rank is essential for achieving a balance between sizereduction and model accuracy. Later, more approaches havebeen introduced [22].Tucker decomposition for weight tensors was used tooptimizes weight tensor dimensions of LeNet and ResNetmodels [31]. Low-rank decomposition was used as an efficientmethod for compressing AlexNet, VGG, and ResNet withoutthe need for fine-tuning to significantly reduce model sizeand computational complexity to make them more suitablefor resource-constrained mobile and embedded devices [32].Hardware-Aware Automatic Low-Rank Compression frame-work HALOC was used to compress ResNet, VGG andMobileNet, with the goal of efficiently exploring the structure-level redundancy in NNs by integrating principles from neuralarchitecture search (NAS) [33]. Automatically DifferentiableTensor Network (ADTN) method was used to significantlyreduce the number of parameters of fully connected NN,LeNet, and VGG while maintaining or enhancing the per-formance [34]. Joint Matrix Decomposition, specifically JointSVD (JSVD) was used to address the challenge of deployingResNet with numerous parameters on resource-constrainedplatforms. It included Right JSVD, Left JSVD, and BinaryJSVD algorithms [35]. Tensor Ring Networks (TR-Nets) wasused as a method to effectively factorize LeNet and ResNet,thereby reducing computational and memory requirements[36]. Tucker decomposition with rank selection and fine tuningwas used as a one-shot whole network compression schemefor deploying AlexNet, VGG, and GoogLeNet on mobiledevices while maintaining reasonable accuracy [37]. TensorDynamic Low-Rank Training (TDLRT) was used to createa training algorithm with VGG and AlexNet that maintainshigh model performance while significantly reducing memoryrequirements for convolutional layers [38].D. Knowledge Distillation (KD)It is a technique used to transfer the knowledge learnedby a larger, more complex model (the teacher model) to asmaller and lighter model (the student model). The primarygoal of KD is to enable the student model to benefit from thegeneralization capabilities of the teacher model while beingmore lightweight in terms of parameters and computations.This technique helps to recover the accuracy drop occurs dueto implementing other compression techniques.Knowledge transfer and distillation, initially introduced by[39], aimed to compress large ensemble models into smaller,faster counterparts with minimal performance loss. [40] ex-tended this concept by empirically demonstrating that theintricate knowledge within larger DNNs could be effectivelytransferred to smaller, shallower models, yielding comparableaccuracy. This involved training a large DNN and transfer-ring its knowledge to a shallower network while minimizingthe squared difference between the logits produced by thetwo models. These foundational ideas produced knowledgedistillation, a widely used technique for training efficientmodels by transferring knowledge from larger ones. Later,more approaches have been introduced [22].KD was used to improve the compression of LeNet andResNet models when fresh training data is scarce, primarilythrough the use of synthetic data generated by GenerativeAdversarial Networks (GANs) [41]. To fuse information frominfrared and visible images while reducing DenseNet complex-ity and improving inference speed. Insights from pre-trainedteacher models are transferred to the smaller student model[9]. KD was used to develop a lightweight mosquito speciesidentification model (EfficientNet) that balances efficiency andaccuracy through the compression [42].E. Mixed TechniquesDifferent compression techniques are often combined andused together to achieve more effective and comprehensivemodel compression. Each compression technique targets spe-cific aspects of the model, such as reducing model size,computation complexity, or memory footprint.--- Page 6 ---6In-Parallel Pruning-Quantization CLIP-Q method combinesnetwork pruning and weight quantization was used to com-press AlexNet, GoogLeNet, and ResNet [18]. Pruning andquantization were used to optimize the compression ofAlexNet and reduce the number of parameters significantlywhile maintaining accuracy to be implemented on Field-Programmable Gate Array (FPGA) [5]. Pruning, quantization,and Huffman encoding combined with adversarial trainingwere used to enhance the robustness and compression ofAlexNet while also addressing the model vulnerability toadversarial attacks [43]. Pruning and quantization were usedto compress VGG and ResNet for remote sensing imageclassification, balancing computational complexity constraintswhile preserving model accuracy [44]. Low-rank decompo-sition and quantization were used to compress ResNet andMobileNet, and reduce the computational complexity whilepreserving high performance [45]. Pruning, quantization, andchanging the model architecture were used to design a com-pact SqueezeNet with competitive accuracy while significantlyreducing the number of parameters [46]. Quantization andpruning were used to develop an effective model compressionframework for ResNet and MobileNet. The objective was tooptimize the allocation of compression ratios to minimizeperformance degradation while reducing model size [47]. Jointquantization and pruning were used to develop a post-trainingmodel size compression method that efficiently combines lossyand lossless compression techniques to reduce the size ofResNet, MobileNet, RegNet, MNasNet, and YOLOv5 withoutsacrificing accuracy [48].F . Other TechniquesDepthwise separable convolutions was used to improvesteel strip defect detection by creating a real-time and ef-ficient model while maintaining high accuracy using FasterR-CNN [11]. Deferential Evolution was used to develop anefficient and optimized AlexNet, VGG, and MobileNet forCovid and liver disease detection [15]. Genetic Algorithmwas used to reduce the storage space and inference timeof VGG, ResNet, AlexNet, and SqueezeNet models [49].Factorization (changing kernel size) was used to improvethe accuracy and computing efficiency of pepper leaf diseasedetection using GoogLeNet, specifically for the agriculturalindustry [13]. Flexible and Separable Convolution (FSConv)was used to reduce computational costs without compromisingthe accuracy of VGG, ResNet, Faster R-CNN and RetinaNet[50]. Efficient Layer Compression (ELC) was used to enhancethe computational efficiency of VGG, ResNet, and ConvNeXtwhile preserving their representation capabilities [51].III. D ESIGN OF THE EXPERIMENTSThe experiments aimed to evaluate various compressiontechniques, namely pruning and quantization, on differenttypes of ConvNext [52] model. The experiments includedtraining, fine-tuning, and evaluating of models using CIFAR-10 dataset. The setup involved conducting experiments bothon cloud-based platforms and on edge devices to evaluate theperformance of the compressed models.Fig. 4. Block modifications and resulted specifications. (a) is a ResNeXtblock; in (b) we create an inverted bottleneck block and in (c) the positionof the spatial depthwise conv layer is moved up [52].A. ConvNeXt [52]Is a modern CNN family produced as a journey of graduallymodernize a standard ResNet toward the design of a visionTransformer. The journey starts from a ResNet-50 model, intoa CNN architecture that mirrors some aspects of Transformers,particularly Swin Transformers. The roadmap:1) Training Techniques: Vision Transformer training pro-cedures were used to train ResNet-50 model, this includedextending the training to 300 epochs (90 epochs originally),using AdamW optimizer, and data augmentation techniques(Mixup, Cutmix, RandAugment, Random Erasing, and regu-larization schemes including Stochastic Depth).2) Macro Design: Number of blocks in each stage wasadjusted from (3, 4, 6, 3) to (3, 3, 9, 3) and the stem wasreplaced with a patchify layer implemented using a 4x4, stride4 convolutional layer (non-overlapping convolution).3) ResNeXt-ify: ResNeXt approach was adopted which isutilize grouped convolutions, where convolutional filters aredivided into groups, each handling a subset of input channels,a variation of grouped convolution known as depthwise convo-lution was adopted, and the network\u2019s width was expanded byincreasing the number of channels in the convolutional layers.4) Inverted Bottleneck: The hidden dimension of the MLPblock was changed to be four times wider than the inputdimension as shown in Figure 4 (a and b)5) Large Kernel Sizes: The position of the convolutionallayer is moved up and the kernel size was changed from (3x3)to (7x7) as shown in Figure 4 (a and c).B. Micro DesignReplacing ReLU with Gaussian Error Linear Unit (GELU),fewer normalization layers, Substituting Batch Normalization(BN) with Layer Normalization (LN), and introducing separatedownsampling layers as shown in Figure 5.C. Compression Techniques1) Pruning: Different pruning techniques have been usedincluding structured and unstructured techniques.\u2022 Only Train Once (OTO) [53]: OTO version 3 (OTOV3)is automated framework for structured pruning which in-volves removing entire structures or groups of parametersfrom a DNN. OTOv3 begins by analyzing the depen-dencies between the vertices of the target DNN. Thisanalysis involves identifying accessory, Shape-Dependent--- Page 7 ---7Fig. 5. Block designs for a ResNet, a Swin Transformer, and a ConvNeXt.Swin Transformer\u2019s block is more sophisticated due to the presence of multiplespecialized modules and two residual connections [52].(SD) joint, and unknown vertices that are adjacent andestablishing their interdependencies. The goal is to formnode groups based on these dependencies, laying thefoundation for identifying interdependent vertices duringstructured pruning.Using the information gathered from the dependencyanalysis, OTOv3 constructs a pruning dependency graph.This graph represents the interdependencies between ver-tices, with vertices in the same node group indicatingtheir interdependency during structured pruning. Thepruning dependency graph ensures the validity of theproduced subnetwork by preserving essential connectionsbetween vertices.OTOv3 partitions the trainable variables of the DNNinto Pruning Zero-Invariant Groups (PZIGs) based on thepruning dependency graph. PZIGs consist of pairwisetrainable variables grouped together, with each grouprepresenting a potential pruning structure. Node groupsadjacent to the DNN output and containing unknownvertices are excluded from forming PZIGs to preserveoutput shapes and ensure model robustness as shown inFigure 6.To jointly search for redundant pruning structures andtrain the remaining groups for optimal performance,OTOv3 employs the Dual Half-Space Projected Gradient(DHSPG) algorithm. DHSPG minimizes the objectivefunction while introducing a sparsity constraint to identifyredundant groups for removal. It employs saliency-drivenredundant identification and a hybrid training paradigmto control sparsity and achieve better generalization per-formance as shown in Figure 7.\u2022 L1 Unstructured [54] L1 unstructured pruning is atechnique used in machine learning, to reduce the sizeof neural networks by eliminating less important con-nections. Each weight in the network is assigned a scorebased on its magnitude. This score reflects the importanceof the weight in the network\u2019s performance. In l1 pruning,this score is often the absolute value of the weight.A threshold is set, typically by selecting the top x% ofweights based on their magnitude scores. The thresholddetermines which weights will be pruned and which willbe retained.Weights that fall below the threshold are pruned, meaningthey are set to zero and effectively removed from thenetwork. This results in a sparser network architecturewith fewer connections.\u2022 Random Unstructured [55] Similar to l1 unstructuredpruning, random unstructured pruning is also a techniqueused in machine learning, to reduce the size of neuralnetworks by eliminating less important connections. Thedifference is the pruned weight are selected randomlyinstead of using l1 to decide the importance of theweights.2) Dynamic Quantization [30]:Dynamic quantization is anapproach aimed at optimizing the deployment of neural net-works by reducing the precision of the weights. Unlike tradi-tional quantization methods that apply a fixed quantization bit-width across all layers of the network, dynamic quantizationadapts the quantization bit-width for each layer individuallybased on its representation abilities and capacities. This isachieved through the use of a bit-width controller module,which employs a policy gradient-based training approach tolearn the optimal bit-width for each layer. By dynamicallyadjusting the quantization bit-width, dynamic quantization canstrike a balance between maintaining accuracy and reducingmemory size and computational costs.D. CIFAR-10 [56]CIFAR-10 is a dataset used for computer vision and ma-chine learning research, offering a rich resource for trainingand evaluating image classification algorithms. Comprising60,000 32x32 RGB color images across 10 distinct classes(Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse,Ship, and Truck), CIFAR-10 facilitates comprehensive explo-ration of diverse visual concepts. With each class containing6,000 images and a balanced distribution across the dataset,CIFAR-10 presents a well-structured foundation for model de-velopment. Its division into 50,000 training images and 10,000test images, further segmented into multiple batches, enablesstrict evaluation and benchmarking of algorithms. In terms ofcomputational requirements, CIFAR-10 generally requires lesscomputation compared to CIFAR-100 and ImageNet due to itssmaller image size and fewer classes which makes it suitablefor experiments with limited computational resources.E. Experiment SetupTwo types of experiments have been conducted, cloud-basedexperiments that focused on compressing the models and eval-uating the techniques and edge-based experiment experimentto evaluate the performance of one of the compressed models.--- Page 8 ---8Fig. 6. Automated PZIG Partition [53].Fig. 7. Search Direction in DHSPG [53].1) Cloud-based Experiment Setup: Google Colab Pro+was used to utilize GPU resources (NVIDIA A100 and V100Tensor Core GPUs), facilitating accelerated model trainingand evaluation and background execution. The integrationwith Google Drive reduced overheads associated with upload-ing and downloading model data to and from cloud. Theevaluation framework was designed to profile the originalmodel, compress it, profile the compressed model, and conductcomparison between the measurements before and after thecompression as shown in Figure 8.This profiling process involved measuring several key met-rics:\u2022 Accuracy: The classification accuracy achieved by themodel on the validation dataset.\u2022 Model Size: The size of the model in megabytes (MB).\u2022 Number of Parameters : The total count of trainableparameters in the model, measured in millions (M).\u2022 Number of MACs : The number of multiply-accumulateoperations performed during inference, measured in mil-lions (M).\u2022 Number of Non-Zero Parameters : The count of non-zero parameters in the model, essential for pruning-basedtechniques.2) Edge-based Experiment Setup: A compressed modelwas deployed on edge with CPU (11th Gen Intel(R) Core(TM)i7-1165G7 @ 2.80GHz 2.80 GHz), RAM (16GB), and laptopintegrated camera.2 samples from each of CIFAR-10 classes have been se-lected randomly from the internet, printed on A4 papers, andplaced in front of the camera to measure the accuracy and theinference time.--- Page 9 ---9Fig. 8. Compression Evaluation Framework.TABLE IOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT TINY, SMALL , BASE AND LARGE .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)Tiny Full 63.81 106.26 26.53 86.88 18.41Compressed 63.81 41.31 10.29 47.80 10.30Small Full 63.48 188.89 47.16 169.28 29.55Compressed 63.48 48.04 11.94 68.24 11.96Base Full 61.22 334.28 83.50 299.20 52.22Compressed 61.22 84.12 20.96 119.40 20.98Large Full 63.40 748.82 187.12 669.30 116.93Compressed 63.40 187.32 46.75 264.69 46.78IV. R UNNING THE EXPERIMENTS AND EXPERIMENTALRESULTSA. Cloud-Base Experimentsdifferent experiments have been conducted on cloud to eval-uate different compressing techniques and different versions ofConvNeXt model.1) Evaluate OTOV3 on Untrained Torch ConvNext Tiny,Small, Base, and Large: Untrained ConvNeXt tiny, small,base, and large have been imported from Torch and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and 200epochs were used for training and pruning. OTOV3 achievedhigh performance (Table I) with reducing the model size(61% for tiny and 75% for small, base, and large), numberof parameters (61% for tiny and 75% for small, base, andlarge), and MACs (45% for tiny and 60% for small, base,and large) as shown in Figure 9. Meanwhile OTOV3 wasable to increase both the full and compressed model accuracythrough the training and pruning without any accuracy dropafter pruning comparing to the full model.To investigate the effect of OTOV3 on the model archi-tecture, a comparison has been conducted between ConvNeXtsmall before and after compression. The Torch implementationof the model consist of many CNBlocks, each CNBlock con-sist of Conv2d, Permute, LayerNorm, Linear, GELU, Linear,and Permute layers. As shown in Figure 10, OTOV3 reducednumber of output features of the Linear layer (sequence 3)and the input features of the next Linear layer (sequence 5)and considering the big number of CNBlock in the modelarchitecture, the reduction in model size and number ofparameters after compression is justified as shown in TableII.2) Evaluate OTOV3 on Untrained ConvNext Small (Torchvs. TIMM): Two untrained ConvNeXt small have been im-ported, one from Torch and one from TIMM [57] and beenused to evaluate OTOV3 which train and prune at the sametime, CIFAR-10 was used for training and evaluation, and200 epochs were used for training and pruning. Although thecompression performance was same with size reduction (75%)but the accuracy after 200 epochs was less for Torch model(63%) comparing to TIMM model (73%) as shown in Figure11.To investigate the accuracy performance of OTOV3 withTorch and Timm ConvNeXt Small, a comparison has beenconducted between the two model architectures. The Torchmodel uses the CNBlock structure, which includes additionaloperations such as Permute and varying StochasticDepth prob-abilities. The TIMM model follows a simpler structure withConv2d and LayerNorm, lacking the additional complexitiesintroduced by CNBlock and associated operations in theTorch model as shown in Figure 12 which effects OTOV3performance regarding the accuracy.3) Evaluate OTOV3 on Fine-Tuned Torch ConvNext Small:A pre-trained ConvNeXt small have been imported from Torchand fine-tuned on CIFAR-10 with 100 epochs, the accuracyreached 89.5%. This fine-tuned ConvNeXt small will be usedfor the rest of cloud-base experiments. This model was usedto evaluate OTOV3, CIFAR-10 was used for training andevaluation, and 200 epochs were used for training and pruning.OTOV3 achieved high performance (Table III) 74% reductionin model size and number of parameters, 60% reduction inMACs, and 3.8% increase with accuracy as shown Figure13. The accuracy of the full model in (Table III) (92.86%)is different that the accuracy of the original model used inthe experiment (89.5%), that because OTOV3 trained the full--- Page 10 ---10Fig. 9. OTOV3 Compression Performance with ConvNeXt Tiny, Small, Base and Large.Fig. 10. Comparison Between ConvNeXt Small Full (Left) and Compressed (Right).Fig. 11. OTOV3 Comparison Between Torch and Timm ConvNeXt Small.model during the process which increased the model accuracy.4) Evaluate Unstructured Pruning: The Fine-tuned Con-vNext Small was used to evaluate Pytorch L1 UnstructuredPruning and Random Unstructured Pruning by using differentcombinations of weights pruning percentages for linear (.1 to.9) and convolutional (.1 to .9) layers. In both experiments,the accuracy and the number of non-zero parameters weredropping as the values of weights pruning percentages for bothlinear and convolutional amounts were increasing as shown inFigure 14 a and b. Although the accuracy dropped but themodel size, number of parameters, and MACs didn\u2019t changeas these techniques zero the weights instead of removing them.5) Evaluate Dynamic Quantization: The Fine-tuned Con-vNext Small was used to evaluate Pytorch dynamic quantiza-tion, 8-bit integer was used during the experiment and CIFAR-10 was used for evaluation. Dynamic quantization achievedhigh performance (Table IV) 71% reduction in model size,95% reduction with number of parameters and MACs, and0.1% drop with accuracy as shown in Figure 156) Evaluate Combination of OTOV3 and Dynamic Quan-tization: Two compression stages were evaluated together asone compression pipeline, OTOV3 and Dynamic Quantization.The compressed ConvNeXt small model that was produced inexperiment 4.1.3 (OTOV3) was furtherly compressed using--- Page 11 ---11Fig. 12. Comparison Between ConvNeXt Small TIMM (Left) and Torch (Right).Fig. 13. OTOV3 Compression Performance with ConvNeXt Small Tuned.Pytorch dynamic quantization using 8-bits integer (similar toexperiment 4.1.5). The pruned models using OTOV2 [58]had dependencies on Open Nural Network Exchange (ONNX)[59] which made it not applicable to be combined with othercompression technique like quantization. In OTOv3, there wasengineering changes produced the pruned model directly inPytorch format, which enhanced the flexibility to be combinedwith quantization as this experiment did [53].Pruning using OTOV3 and Quantization using Pytorchdynamic quantization achieved high performance (Table V)89.7% reduction in model size, 95% reduction with numberof parameters and MACs, and 3.8% increase with accuracy asshown in 16.B. Edge-Base ExperimentThe final compressed ConvNeXt Small model in experimentIV .A.6 (OTV3 and Dynamic Quantization) was deployed onedge and the printed samples used to measure the accuracyand inference time by placing them in front of the camera.The compressed model achieved 92.5% accuracy and 20msinference time. Figure 17 shows samples of the output.V. A NALYSIS OF EXPERIMENTAL RESULTSHere, the experimental results obtained from running aseries of experiments will be analyzed, the experiments aimedat evaluating various compression techniques applied to Con-vNeXt models. The experiments were designed to evaluate theperformance of techniques such as pruning and quantizationon different sizes of ConvNeXt models, with a focus on bothcloud-based compression experiments and edge-based deploy-ment experiment. The analysis will be conducted with respectto the work\u2019s aim of evaluating CNN compression techniquesthat assure appropriate performance (size and inference time)on edge devices and resource-constrained environments.A. OTOV3 Evaluation on Untrained ConvNeXt ModelsThe evaluation of OTOV3 on untrained ConvNeXt modelsdemonstrated its effectiveness in achieving substantial com-pression while increasing model accuracy for both full andcompressed models. Across varying sizes of ConvNeXt mod-els, OTOV3 consistently produced impressive reductions inmodel size, number of parameters, and MACs, highlighting itsability to prune redundant structures effectively. This suggests--- Page 12 ---12Fig. 14. Unstructured Pruning Performance with ConvMeXt Small.TABLE IICOMPARISON FOR NUMBER OF OUTPUT FEATURES AND INPUTFEATURES OF THE LINEAR LAYERS IN THE CNB LOCKS BEFORE ANDAFTER OTOV3 C OMPRESSION .Layer Index Input & Output FeaturesSequential CNBlock Full Model CompressedModel1 0 384 3841 1 384 3841 2 384 3843 0 768 7673 1 768 7043 2 768 7265 0 1536 2515 1 1536 3495 2 1536 2425 3 1536 3785 4 1536 2935 5 1536 3775 6 1536 3405 7 1536 4005 8 1536 3945 9 1536 4785 10 1536 4145 11 1536 4245 12 1536 4105 13 1536 3185 14 1536 4885 15 1536 4885 16 1536 4025 17 1536 2465 18 1536 4025 19 1536 4585 20 1536 3235 21 1536 4195 22 1536 4465 23 1536 4445 24 1536 4415 25 1536 4685 26 1536 10707 0 3072 2087 1 3072 2547 2 3072 262Fig. 15. Dynamic Quantization Compression Performance with ConvNeXtSmall Tuned.Fig. 16. OTOV3 and Dynamic Quantization Compression Performance withConvNeXt Small Tuned.--- Page 13 ---13TABLE IIIOTOV3 C OMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 92.86 188.89 47.16 169.28 29.80Compressed 92.86 50.03 12.44 67.41 12.46TABLE IVDYNAMIC QUANTIZATION COMPRESSION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy (%) Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number ofNon-ZeroParameters (M)SmallTunedFull 89.53 188.89 47.16 169.28 47.17Compressed 89.40 54.21 2.15 7.27 2.17TABLE VOTOV3 AND DYNAMIC QUANTIZATION NUMBERS WITH CONV NEXT SMALL TUNED .Model Accuracy % Model Size(MB)Number ofParameters (M)Number ofMACs (M)Number of NonZeroParameters (M)Full 89.53 188.89 47.16 169.28 47.17Phase 1 (OTOV3) 92.86 50.03 12.44 67.41 12.46Phase 2 (Dynamic Quantization) 92.93 19.39 2.15 7.27 2.17Fig. 17. Samples From the Compressed Model Output on Edge.that OTOV3 efficiently identifies and removes unnecessaryparameters from the models, leading to more streamlinedarchitectures without compromising predictive performance.However, an unexpected observation arose when comparingthe accuracy of the Torch implementation with that of theTIMM implementation of ConvNeXt Small. The Torch im-plementation exhibited lower accuracy compared to its TIMMcounterpart, indicating that OTOV3\u2019s performance regardingaccuracy may be influenced by the details of the modelarchitecture. This unexpected result suggests that differenttraining strategies or adjustments may be necessary to optimizeOTOV3\u2019s performance across various model implementations,emphasizing the importance of considering architectural dif-ferences when applying compression techniques like OTOV3.B. OTOV3 Evaluation on Fine-Tuned ConvNeXt ModelsWhen evaluating OTOV3\u2019s performance on a fine-tunedConvNeXt model, notable improvements in compression per-formance were observed, confirming its effectiveness in re-ducing model size, parameters, and MACs while marginallyenhancing accuracy. This outcome highlights the potential ofintegrating fine-tuning with structured pruning techniques toachieve even greater optimization of model performance. Thefine-tuned model displayed enhanced compression capabilitiescompared to untrained models, suggesting that pre-trainingcan significantly increase the effectiveness of compressiontechniques. This finding highlights the importance of lever-aging pre-existing knowledge within models to maximize thebenefits of compression, ultimately resulting in CNN modelswith higher performance.C. Unstructured Pruning TechniquesDuring the evaluation of l1 unstructured and random un-structured pruning techniques, expected trends were observedregarding accuracy and compression. As the pruning per-centages increased for both linear and convolutional layers,a corresponding decrease in accuracy was noted, while themodel size, parameters, and MACs remained unaltered. Thisoutcome aligns with the inherent nature of unstructured prun-ing, wherein weights are zeroed out but not entirely eliminated,resulting in sparse models without substantial reductions incomputational complexity. However, the lack of significantreductions in computational complexity may constrain their ef-fectiveness, particularly in resource-constrained environmentswhere efficient utilization of computational resources is essen-tial. This highlights the importance of considering the trade-offs between model compression and computational efficiencywhen selecting pruning techniques for deployment in real---- Page 14 ---14TABLE VIOTOV3 AND DYNAMIC QUANTIZATION REDUCTION NUMBERS WITH CONV NEXT SMALL TUNED .Technique AccuracyChange Size Reduction ParametersReductionMACsReductionOTOV3 3.75% 73.51% 73.62% 60.18%Dynamic Quantization -0.15% 71.30% 95.43% 95.71%OTOV3 and DynamicQuantization 3.80% 89.74% 95.43% 95.71%world applications, especially in edge computing scenarioswhere computational resources are limited.D. Dynamic QuantizationDynamic quantization emerged as a highly effective tech-nique for model compression, demonstrating remarkable re-ductions in model size, parameters, and, Your response: ```ADD_PAPER\n2409.02134v1\nThis paper comprehensively evaluates model compression techniques (structured/unstructured pruning and dynamic quantization) for ConvNeXt models on image classification tasks. Key findings include: 1) OTOV3 pruning achieves 75% reduction in model size/parameters while maintaining accuracy, 2) Dynamic quantization achieves 95% parameter reduction with minimal accuracy drop, 3) Combined OTOV3+quantization achieves 89.7% size reduction and 95% parameter/MAC reduction while improving accuracy by 3.8%. The compressed model demonstrates 92.5% accuracy with 20ms inference time on edge devices, validating effectiveness for real-world deployment.\n```\nStep #34, Phase: literature review, Feedback: Successfully added paper 2409.02134v1, Your response: ```SUMMARY\nedge image classification hardware-aware neural architecture search\n```\nStep #35, Phase: literature review, Feedback: You requested arXiv papers related to the query edge image classification hardware-aware neural architecture search, here was the response\nTitle: Pushing Joint Image Denoising and Classification to the Edge\nSummary: In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\nPublication Date: 2024-09-13\narXiv paper ID: 2409.08943v1\n\nTitle: Search-time Efficient Device Constraints-Aware Neural Architecture Search\nSummary: Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.\nPublication Date: 2023-07-10\narXiv paper ID: 2307.04443v1\n\nTitle: AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture\nSummary: Resource is an important constraint when deploying Deep Neural Networks\n(DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based\nsearch approach, which limits the flexibility of network patterns in learned\ncell structures. Moreover, due to the topology-agnostic nature of existing\nworks, including both cell-based and node-based approaches, the search process\nis time consuming and the performance of found architecture may be sub-optimal.\nTo address these problems, we propose AutoShrink, a topology-aware Neural\nArchitecture Search(NAS) for searching efficient building blocks of neural\narchitectures. Our method is node-based and thus can learn flexible network\npatterns in cell structures within a topological search space. Directed Acyclic\nGraphs (DAGs) are used to abstract DNN architectures and progressively optimize\nthe cell structure through edge shrinking. As the search space intrinsically\nreduces as the edges are progressively shrunk, AutoShrink explores more\nflexible search space with even less search time. We evaluate AutoShrink on\nimage classification and language tasks by crafting ShrinkCNN and ShrinkRNN\nmodels. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34%\nMultiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of\nstate-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are\ncrafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting\ntime of SOTA CNN and RNN models, respectively.\nPublication Date: 2019-11-21\narXiv paper ID: 1911.09251v1\n\nTitle: Neural Architecture Search Over a Graph Search Space\nSummary: Neural Architecture Search (NAS) enabled the discovery of state-of-the-art\narchitectures in many domains. However, the success of NAS depends on the\ndefinition of the search space. Current search spaces are defined as a static\nsequence of decisions and a set of available actions for each decision. Each\npossible sequence of actions defines an architecture. We propose a more\nexpressive class of search space: directed graphs. In our formalism, each\ndecision is a vertex and each action is an edge. This allows us to model\niterative and branching architecture design decisions. We demonstrate in\nsimulation, and on image classification experiments, basic iterative and\nbranching search structures, and show that the graph representation improves\nsample efficiency.\nPublication Date: 2018-12-27\narXiv paper ID: 1812.10666v2\n\nTitle: Discretization-Aware Architecture Search\nSummary: The search cost of neural architecture search (NAS) has been largely reduced\nby weight-sharing methods. These methods optimize a super-network with all\npossible edges and operations, and determine the optimal sub-network by\ndiscretization, \\textit{i.e.}, pruning off weak candidates. The discretization\nprocess, performed on either operations or edges, incurs significant inaccuracy\nand thus the quality of the final architecture is not guaranteed. This paper\npresents discretization-aware architecture search (DA\\textsuperscript{2}S),\nwith the core idea being adding a loss term to push the super-network towards\nthe configuration of desired topology, so that the accuracy loss brought by\ndiscretization is largely alleviated. Experiments on standard image\nclassification benchmarks demonstrate the superiority of our approach, in\nparticular, under imbalanced target network configurations that were not\nstudied before.\nPublication Date: 2020-07-07\narXiv paper ID: 2007.03154v1\n, Your response: ```FULL_TEXT\n2307.04443v1\n```\n~~~~~~~~~~\nCurrent Step #36, Phase: literature review\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: ```EXPIRATION 3\n--- Page 1 ---Search-time Efficient Device Constraints-AwareNeural Architecture SearchOshin Dutta, Tanu Kanvar, and Sumeet AgarwalIndian Institute of Technology{oshin.dutta,sumeet}@ee.iitd.ac.in, kanvar.tanu@gmail.comAbstract. Edge computing aims to enable edge devices, such as IoTdevices, to process data locally instead of relying on the cloud. However,deep learning techniques like computer vision and natural language pro-cessing can be computationally expensive and memory-intensive. Creat-ing manual architectures specialized for each device is infeasible due totheir varying memory and computational constraints. To address theseconcerns, we automate the construction of task-specific deep learning ar-chitectures optimized for device constraints through Neural ArchitectureSearch (NAS). We present DCA-NAS, a principled method of fast neu-ral network architecture search that incorporates edge-device constraintssuch as model size and floating-point operations. It incorporates weightsharing and channel bottleneck techniques to speed up the search time.Based on our experiments, we see that DCA-NAS outperforms manualarchitectures for similar sized models and is comparable to popular mo-bile architectures on various image classification datasets like CIFAR-10,CIFAR-100, and Imagenet-1k. Experiments with search spaces\u2014DARTSand NAS-Bench-201 show the generalization capabilities of DCA-NAS.On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art per-formance were discovered.Keywords: Neural Architecture Search \xb7 DARTS \xb7 Meta-Learning \xb7Edge Inference \xb7 Constrained Optimization1 IntroductionIn recent years, there has been significant progress in developing Deep NeuralNetwork (DNN) architectures [33,47,34] for edge and mobile devices.However,designing DNN architectures for specific hardware constraints and tasks is atime-consuming and computationally expensive process [3]. To address this, Neu-ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-timal architectures given a task and network operations. Despite its success,traditional NAS techniques cannot guarantee optimal architecture for specificdevices with hardware constraints such as storage memory and maximum sup-ported FLOPs. To address this concern, researchers have developed hardware-aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023--- Page 2 ---2 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalFig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cellslowers the search time from other DNAS. Target device constraint is used to querysearch constraint from look-up graph for constrained optimization.training overhead and search time. These methods often use inference latency [4],FLOPs [36] or a combination of hardware metrics [36] as constraints scaled bya tunable factor. However, the time to tune the scaling factor is often not con-sidered within the NAS search time and can be ten times the reported searchtime. To address these issues, we propose the Device Constraints-Aware NAS(DCA-NAS), a principled differentiable NAS method that introduces total al-lowable model size or floating-point operations (FLOPs) as constraints withinthe optimization problem, with minimal hyper-parameter tuning. Unlike infer-ence latency which is task dependent, FLOPs and memory are specified with agiven hardware and thus are appropriate for our generic method. The approachis adaptable to other hardware metrics such as energy consumption or infer-ence latency using additional metric-measuring functions. The paper make thefollowing significant contributions:\u2013 It introduces a fast method that uses weight sharing among operations inthe search space and channel bottleneck, along with a differentiable resourceconstraint, for continuous exploration of the search space.\u2013 A training pipeline that allows a user to input device memory or FLOPs andsearch for optimal architecture with minimal hyper-parameter tuning.\u2013 Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,TinyImagenet, Imagenet-1k and inference-latency comparisons of trainedmodels on Hardware-NAS-bench demonstrate the efficiency of our method.The generalization of our method to different search spaces is shown withexperiments on DARTS and NAS-Bench.2 Related WorkNeural Architecture Search Popular approaches [12,22,1] designed architec-tures for high performance on specific tasks or datasets with the traditionaldeep learning perspective that bigger is better, resulting in computationallyand memory-intensive inference on edge devices. Network pruning [13], chan-nels removal [26,34] and weights/activations quantization [8,50] can compress--- Page 3 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 3architectures, but require pre-training, hyperparameter tuning, and often lacktransferability.Neural Architecture Search (NAS) methods such as Reinforce-ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable NeuralArchitecture Search (DNAS) [25,43] can automatically search for architectureswithout user intervention, and can transfer across similar tasks. DNAS withsurrogate metrics [42,48] have also been used to explore the architecture searchspace. However, architectures found by DNAS methods are not optimized fordeployment on edge devices and smaller models obtained by reducing layers orchannels are often sub-optimal.Hardware-aware Neural Architecture search Certain NAS methods opti-mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],memory usage [24]. Some use a separate DNN to predict constraint metrics andevolutionary search to obtain hardware-aware optimal models [36,3], while oth-ers consider real-time latencies of edge devices or provide specific architecturesfor specific devices [27,7]. However, these methods require significant search timeand tuning of scaling factors controlling the trade-off between the performanceand the constraint, and do not always account for optimal architectures. In con-trast, we use a differentiable hardware-aware objective function with generichardware metrics, and do not require a tunable scaling factor. Certain meth-ods [3,29,9] train a supernet first and then search for a smaller architecture,but this is only efficient when there are more than fifteen different edge deviceswith different limitations or deployment scenarios [3] as training the supernettakes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-lowed by evaluation, as done in our approach is more efficient when the differentnumber of possible edge devices is less than fifteen.3 DCA-NAS: Device Constraints Aware Fast NeuralArchitecture SearchWe present the preliminary gradient-based NAS objective function in section 3.1and then formulate the problem of incorporating the hardware-awareness in NASas a constrained optimization problem in section 3.2 followed by techniquesto reduce the search time in section 3.3. The framework of our approach isillustrated in Figure 1.3.1 Gradient-based NAS Objective FunctionPopular DNAS techniques [25,46] have two stages, the search phase and the eval-uation phase. During the search phase, given a task or a dataset the techniquessearch for a network of cells, which are directed acyclic graphs withN nodes. Theedges of the graph are network layers, whose operations are to be selected froma pre-defined set O containing operations such as 3x3 separable convolution andidentity operations with trainable weights wo. The search is made differentiableby making the choice of a particular operation to be a softmax of architectureweights \u03b1 of all operations. Thus, the intermediate output zj at node j is givenby,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owi,jo , zi(1)--- Page 4 ---4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal3.2 DCA-NAS formulationPrevious DNAS approaches [25,45,46] did not focus on searching architecturesspecifically for inference on resource-constrained devices. In contrast, we for-mulate the DNAS objective function as a constrained optimization problem byincorporating device resource constraints (memory or FLOPs) in the search ob-jective function. The constrained bi-level optimization problem is written as,min\u03b1 Lval (w\u2217(\u03b1), \u03b1)s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1)s.t. ks(\u03b1) \u2264 Kd(2)where training dataset is split into train and val to optimize w and \u03b1 simultane-ously in each iteration subject to the constraint that the architecture\u2019s numberof parameters or FLOPs ks must be less than or equal to the device resourceconstraint Kd. The following equation calculates the architecture\u2019s number ofparameters or FLOPs during search given the number of cells cn . Our methodcan also be adapted to use other metrics such as latency and energy consumptionwith additional metric measuring functions.ks(\u03b1) = cnX(i,j)\u2208NXo\u2208Oexp{\u03b1i,jo } \u2217b (o)Po\u2032\u2208O exp{\u03b1i,jo\u2032 }(3)Tackling the difference in search and evaluation networks The size ofthe architecture in the search phase ks is different from the architecture size inevaluation phase due to the softmax weighting factor in equation 3 (demonstra-tion can be found in the appendix). To address this, we introduce a tighter boundon the search constraint Kd\u2032 , which is less than the device resource constraintKd. A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032within appropriate bounds and running the algorithm until convergence eachtime to obtain the corresponding device resource constraint Kd. The computa-tion time of the LUG can be reduced by running the searches in parallel. Thus,on incorporating the tighter constraint by looking-up the graph for the givendevice resource constraint Kd along with the trainable Lagrange multiplier \u03bb inEquation 2, the objective function is re-written as,eL = Lval (w\u2217(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 LUG(Kd))s.t. w\u2217(\u03b1) = argminw Ltrain (w, \u03b1) (4)3.3 Techniques to reduce search timeChannel Bottleneck We use convolutional layers of 1x1 kernel to reduce thedepth of output channels of operations in the search space to save computationtime and memory overhead.Derived Cell and Weight sharing. During architecture search, only one cellwith trainable \u03b1 is used to optimize architecture parameters. The target networkfor inference is built by stacking cells with architectures derived from highlyweighted operations. This can be done during search by deriving the other cell--- Page 5 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 5Table 1: Performance comparison of architectures evaluated on visual datasets-CIFAR-10 and TinyImagenet. \u2019(CIFAR-10)\u2019 indicates search with CIFAR-10. \u2019X M\u2019 in\u2019DCA-NAS-X M\u2019 denotes the input memory constraint. RCAS- Resource ConstrainedArchitecture SearchDataset Search Method Accuracy Parameters GPUStrategy (%) (Million) HoursCIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -manual VGG-16 pruned (2017) [16] 93.4 5.4 -evolution Evolution + Cutout (2019) [39] 96.43 5.8 12random NAO Random-WS (2019) [31] 96.08 3.9 7.2gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12gradient DARTS + Cutout (2nd) (2018) [25] 97.24\xb10.09 3.3 24gradient SNAS + Cutout (2018) [43] 97.15 2.8 36gradient PC-DARTS (2019) [45] 97.43\xb10.07 3.6 2.4gradient SGAS (2020) [23] 97.34 3.7 6gradient DrNAS (2020) [6] 97.46 \xb10.03 4.0 9.6gradient DARTS+PT (2021) [38] 97.39\xb10.08 3.0 19.2gradient Shapley-NAS (2022) [42] 97.53\xb10.04 3.4 7.2RCAS DCA-NAS- 3.5 M (CIFAR-10)97.2\xb10.09 3.4 1.37Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -manual PreActResNet18 (2020) [22] 63.48 - -manual ResNet18 (2016) [15] 58.4 6.4 -manual DenseNet (2020) [1] 62.73 11.8 -gradient DARTS+ Cutout (2018) [25] 62.15\xb10.15 7.3 219RCAS DCA-NAS- 3.5 M 61.34\xb10.09 3.5 12.5RCAS DCA-NAS- 3.5 M (CIFAR-10)61.4\xb10.15 3.4 1.37Fig. 2: Plots show that DCA-NAS method discovers models with fewer parametersthan other NAS methods and manual architectures without sacrificing prediction per-formance to a large extent.architectures from the first at each iteration [46]. The arrangement of the cellsfor search is given in the appendix. This derived cell saves computation andmemory overhead. A weight sharing strategy [46] among same operations withthe same originating node i to all nodes i < j < Nhas been applied within a cell.This is motivated by the observation that non-parametric operations operatingon the representation of a node produce the same feature map irrespective of theoutput node and thereby extended to parametric operations. Thus, Equation 1may be re-written to the following,zj =Xo\u2208Oexp\u03b1i,joPo\u2032\u2208O exp\u03b1i,jo\u2032 \xb7 owio, zi(5)4 Experimental ResultsOur approach is evaluated on two search spaces- DARTS and NAS-Bench withvision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. Thedetails of the search space and implementation is given in the appendix--- Page 6 ---6 Oshin Dutta, Tanu Kanvar, and Sumeet AgarwalTable 2: Performance and comparison of architectures evaluated on Imagenet-1k. Thelabel \u201d(Imagenet)\u201d indicates that the architecture has been searched and evaluated onImagenet-1k.; else it is searched on CIFAR-10. \u2019X M\u2019 in \u2019DCA-NAS-X M\u2019 denotes theinput memory constraintMethod Test Error (%) Parameters FLOPS Search Cost Searchtop-1 top-5 (Mil) (Mil) (GPU days) StrategyInception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manualMobileNetV1 (2017) [17] 29.4 10.5 4.2 569 - manualMobileNetV2 (2018) [33] 72.0 91.0 3.4 300 - manualShuffleNet 2\xd7(v2) (2018) [28] 25.1 - 5 591 - manualMnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RLAmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolutionDARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradientSNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradientGDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradientBayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradientP-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradientSGAS (Cri 1. best) (2020) [23]24.2 7.2 5.3 585 0.25 gradientSDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradientShapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradientRC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCASDCA-NAS 25.1 8.1 5.1 578 0.06 RCASProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradientPC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradientDrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradientDARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradientShapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradientRCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCASDCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS4.1 Results on DARTS search spaceTransferability- learning of coarse features during search. We transferthe architecture searched on CIFAR-10 to train and evaluate the model weightson TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred modelyields higher performance than manually designed architectures [33,28] for thetarget dataset. It is observed that performance of the transferred model is com-parable to the architecture searched on the target dataset itself which can beattributed to the architecture learning coarse features than objects during search.Performance versus Device-Constraints trade-off DCA-NAS discovers2 to 4% better-performing architectures than manual designs with a memoryconstraint of 3.5 million parameters on CIFAR-10 and similar performance onTinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 millionparameters (taken to yield similar sized models as other NAS methods) as in Ta-ble 2. We vary the input device resource constraint and plot the performance ofthe searched models against the number of parameters in Figure 2. As observed,DCA-NAS searched models can yield 15x lower sized models than manual ar-chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy onCIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performancebut 6x smaller in size than the manual Resnet variant. In comparison to Prox-ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms ofmodel parameters for similar accuracy. In comparison to DNAS methods [25,45]for each of the three datasets, we observe that the performance of the DCA-NASsearched models is retained to a certain extent as resources are further limited--- Page 7 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 7Fig. 3: Plots show DCA-NAS searched models with similar performance but lowerinference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NASmethod- PC-DARTS when evaluated on NAS-Bench dataset.after which the model performance degrades. DCA-NAS model of similar size hasthe advantage of better performance (by 1%) and being automatically searchedover MobileNet-v2 [33], a manually designed network on Imagenet-1k.Search time comparison For evaluation on TinyImagenet in Table 1, thearchitecture searched on CIFAR-10 with DCA-NAS yields model in the low-est search time which indicates the search-time efficiency of the transferabilityproperty. Our method requires about 4x lower search cost than SGAS [23] whichperforms the best among the other transferred architectures and 16x lower searchtime than the other resource-constrained approach [20] for similar performanceas seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search timethan DCA-NAS whereas PC-DARTS takes about 2x more search time with nocapability to constraint model size.4.2 Results on NAS-Bench-201 search spacePerformance and Latency comparisons on different devicesOur methodreports the mean by averaging over five runs with different random seed. Figure3 compares the performance of models searched with DCA-NAS and PC-DARTSby varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAScan search for more efficient models which have lower inference latency for similartest accuracy. Moreover, we observe that models with similar performance havelower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a fasterRAM in Pixel 3. DCA-NAS takes the lowest search time among all the NASmethods due to the addition of search-time-efficient techniques while being at-par in terms of performance across all datasets.5 Ablation StudyEffectiveness of various algorithmic augmentations for faster search:We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3to reduce search cost in our study. We sequentially add weight sharing, channelbottleneck, and derived cells to the baseline DARTS [25] method and measuresearch time and accuracy. Weight sharing, channel bottleneck, and derived cellswas observed to significantly reduce search memory overhead, enabling us to uselarger batch sizes and reducing overall search cost as seen in Figure 4a. Addingthe resource-constraint in the final DCA-NAS method negligibly increases search--- Page 8 ---8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal(a) (b)Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTSleads to the reduction in the search cost of DCA-NAS while performance is retained.WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-els for runs with varying seeds on CIFAR-10 dataset.cost while maintaining performance.Stability of the approach: We test stability by running the search algorithmindependently five times with different initial seeds and the same constraints andhyperparameters. The architectures found during each run have similar perfor-mance when re-trained and evaluated as shown in Fig. 4b. Smaller models havelower performance due to restrictions in model complexity compared to largermodels.6 ConclusionWe present DCA-NAS, a device constraints-aware neural architecture searchframework which discovers architectures optimized to the memory and compu-tational constraints of an edge device in a time-efficient manner. It does so byincorporating a constraint in terms of the number of parameters or floating pointoperations (FLOPs) in the objective function with the help of a Lagrange multi-plier. DCA-NAS in essence searches for a Pareto optimal solution given the edgedevice memory or FLOPs constraint. Moreover, it enables architecture searchwith search cost 4 to 17 times lower than the previous state-of-the-art Hardware-aware NAS approaches. DCA-NAS can discover models with size about 10 to15 times lower than manually designed architectures for similar performance.In comparison to DARTS and its other NAS variants, DCA-NAS can discovermodels upto 3x smaller in size with similar performance. This hardware-awareapproach can be generalized to any future updates to differential neural architec-ture search and possibly to training-free methods of NAS with some adaptation.AcknowledgementWe thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall ofIIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.This research is supported by funding from Cadence India; the first author isalso supported by a fellowship from the Ministry of Education, India.--- Page 9 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 9References1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture searchusing performance prediction (2017)3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Networkand Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.09791, arXiv:1908.09791 [cs, stat]4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on targettask and hardware (2019)5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search viaperturbation-based regularization. In: International conference on machine learn-ing. pp. 1554\u20131565. PMLR (2020)6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neuralarchitecture search. arXiv preprint arXiv:2006.10355 (2020)7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models viaarchitecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 3022\u20133031 (2021)8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neuralnetworks with binary weights during propagations (2016)9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:Nap: Neural architecture search with pruning. Neurocomputing 477, 85\u201395 (2022)10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 1761\u20131770 (2019)11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecturesearch via lamarckian evolution. arXiv: Machine Learning (2019)12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings ofthe IEEE conference on computer vision and pattern recognition. pp. 5927\u20135935(2017)13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding (2016)14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture searchvia mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. pp. 11993\u201312002 (2020)15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:Proceedings of the IEEE conference on computer vision and pattern recognition.pp. 770\u2013778 (2016)16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-works. In: Proceedings of the IEEE International Conference on Computer Vision.pp. 1389\u20131397 (2017)17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks formobile vision applications (2017)18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \xa10.5mb modelsize (2016)19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-EndHardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs](Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299--- Page 10 ---10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-darts: Resource constrained differentiable architecture search. arXiv preprintarXiv:1912.12814 (2019)21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrentnetwork architectures. In: Proceedings of the 32nd International Conference on In-ternational Conference on Machine Learning - Volume 37. p. 2342\u20132350. ICML\u201915,JMLR.org (2015)22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-tics for optimal mixup (2020)23. Li, G., Qian, G., Delgadillo, I.C., M\xa8 uller, M., Thabet, A., Ghanem, B.: Sgas: Se-quential greedy architecture search (2020)24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learningon iot devices. Advances in Neural Information Processing Systems 33, 11711\u201311722 (2020)25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXivpreprint arXiv:1806.09055 (2018)26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-tional networks through network slimming (2017)27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural ArchitectureSearch on Edge Devices. IEEE Transactions on Network Science and Engineer-ing 9(1), 134\u2013142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-ference Name: IEEE Transactions on Network Science and Engineering28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines forefficient cnn architecture design. In: Proceedings of the European conference oncomputer vision (ECCV). pp. 116\u2013131 (2018)29. Mu\u02dc noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:Enabling NAS with Automated Super-Network Generation (Dec 2021), http://arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecturesearch via parameter sharing. In: ICML (2018)31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-sifier architecture search (2019)32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-ternational Conference on Machine Learning - Volume 70. p. 2902\u20132911. ICML\u201917,JMLR.org (2017)33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. pp. 4510\u20134520 (2018)34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-tion bottleneck based method to compress sequential networks for human actionrecognition. In: Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision. pp. 2745\u20132754 (2021)35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedingsof the IEEE conference on computer vision and pattern recognition. pp. 1\u20139 (2015)36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815\u20132823 (2019)--- Page 11 ---Search-time Efficient Device Constraints-Aware Neural Architecture Search 1137. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neuralnetworks. In: International conference on machine learning. pp. 6105\u20136114. PMLR(2019)38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architectureselection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution withfunction-preserving mutations. In: Berlingerio, M., Bonchi, F., G\xa8 artner, T., Hurley,N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.243\u2013258. Springer International Publishing, Cham (2019)40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-ral architecture search. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. pp. 10734\u201310742 (2019)41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-timeSuper-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577[cs, eess]42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering OperationContribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/2206.09811, arXiv:2206.09811 [cs]43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:International Conference on Learning Representations (2018)44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecturesearch: Will a submodularity assumption help? In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision. pp. 1901\u20131910 (2019)45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-tial channel connections for memory-efficient architecture search. arXiv preprintarXiv:1907.05737 (2019)46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving theconsistency, efficiency, and flexibility of differentiable neural architecture search.In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. pp. 6667\u20136676 (2021)47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-tional neural network for mobile devices. In: Proceedings of the IEEE conferenceon computer vision and pattern recognition. pp. 6848\u20136856 (2018)48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-mation. In: 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). pp. 11902\u201311911. IEEE, New Orleans, LA, USA (Jun2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.org/document/9878903/49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neuralarchitecture search (2019)50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)--- Page 12 ---AppendixA Deriving cell architecturesThe searched cells are stacked to form the network whose weights are trained andevaluated. The layers of this network during the evaluation phase is varied from4 to 20. It can be seen that the models searched with DARTS with only 2-cellsperform equally well as those of 8-cell search for target model with layers morethan 10. Hence, in our experiments, instead of training architecture parametersfor all 8 cells, we train only 2 cells- one normal and the other reduction cell. Thearchitecture of the other 6 cells stacked to form the network during search arederived from either the normal or the reduction cell as shown in Figure 1.B Calculation of search-stage architecture sizeThe size of the architecture in the search phase ks is different from the architec-ture size in evaluation phase due to the softmax weighting factor in equation 3(demonstrated in Figure 2). To address this, we introduce a tighter bound onthe search constraint Kd\u2032 , which is less than the device resource constraint Kd.A lookup graph (LUG) needs to be made for each dataset by varying Kd\u2032 withinappropriate bounds and running the algorithm until convergence each time toobtain the corresponding device resource constraint Kd. The computation timeof the LUG can be reduced by running the searches in parallel.C AlgorithmThe practical implementation of our resource-constrained gradient descent-basedapproach is illustrated in Algrorithm 1.D Implementation DetailsThe experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.Training and evaluation on Imagenet-1k was performed on a cluster containingeight V100 GPUs.The super-net used for search with smaller vision datasets except Imagenet-1kconsists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell kserving as input nodes. The super-net is trained for 50 epochs with a batchsizeof 512, and optimized using SGD with a momentum of 0.9 and weight decay of3e \u2212 4. The learning rate is initially set to 0.2 and gradually reduced to zero--- Page 13 ---Appendix 13Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-tions and the derived cell made of top-weighted operations. Bottom: Shows the networkcomprising the normal cell (bold border) and reduction cells (dotted border) with train-able architecture parameters (red border) and the derived cells (green border) withoutany architecture parameters.Fig. 2: Demonstrates the calculation of memory size of a single cell in the architectureduring - Left: search phase. Right: evaluation phaseusing a cosine scheduler. Architecture parameters \u03b1 are optimized using Adamoptimizer, with a learning rate of 6 e \u2212 4, a momentum of (0 .5, 0.999), and aweight decay of 1e \u2212 3. The search is run 5 times, and the architecture with thehighest validation accuracy is chosen. For evaluation, the target-net has 20 cells,with 18 normal cells and 2 reduction cells, and an initial number of channels setto 36. The target-net is trained for 600 epochs with a batchsize of 96, optimizedusing SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clippingof 5. The initial learning rate is set to 0.025 and gradually reduced to zero usinga cosine scheduler. Additional settings include a cutout length of 16, dropoutrate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the inputsize from 224 \xd7 224 to 28 \xd7 28 using three convolution layers with a stride of 2.The super-net for search has 8 cells starting with 16 channels, and the target-netfor evaluation has 14 cells starting with 48 channels. Both search and evaluationuse a batch size of 1,024. In search, we train for 50 epochs with a learning rateof 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of6e \u2212 3 for architecture parameters. In evaluation, we train for 250 epochs using--- Page 14 ---14Algorithm 1 DCA-NAS - gradient descent based search methodAssign random weights to \u03b1i,j on edges i, jdenoting weights of operations in themixed setInput look-up graph G and device memory constraint KdLook-up corresponding search memory constraint Kd\u2032 from GCalculate total search time memory size ks(\u03b1)while not convergeddoCalculate eL(w, \u03b1, \u03bb) = Lval (w(\u03b1), \u03b1) + \u03bb(ks(\u03b1) \u2212 Kd\u2032 )Update weights w by descending \u25bdw eLtrain(w, \u03b1, \u03bb)Update \u03b1 by descending \u25bd\u03b1 eLval(w\u2217, \u03b1, \u03bb)Calculate total search time memory size ks(\u03b1)Calculate loss as in equation 4Update \u03bbend whileDerive the final architecture based on the learned \u03b1 by connecting the top weightedoperations among the mixed setthe SGD optimizer with a momentum of 0.9 and a weight decay of 3 e \u2212 5, andadopt an auxiliary head and the label smoothing technique.E Model performance by varying FLOPs constraint onCIFAR10, TinyImagenet and Imagenet-1kInstead of model parameters, we also experiment with FLOPs as the constraintin our objective function. As shown in Figure 3, our method DCA-NAS retainsperformance till a certain FLOPs constraint, after which it degrades. In com-parison to manual architectures, our NAS approach yields models which requiremuch smaller FLOPs and hence would have lower latency.Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPsthan other NAS methods and manual architectures without sacrificing predictionperformance.\n```\nNotes: \nYour previous command was: ```FULL_TEXT\n2307.04443v1\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================


=== PHD RESPONSE LOG ===

Interaction 1:
```SUMMARY
real-time edge image classification optimization
```
==================================================
Interaction 2:
```FULL_TEXT
2409.08943v1
```
==================================================
Interaction 3:
```ADD_PAPER
2409.08943v1
This paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for latency, accuracy, and denoising performance. Experimental results show their NAS-designed models outperform manually designed alternatives, achieving significant improvements in both denoising (PSNR/SSIM) and classification accuracy while meeting real-time latency constraints. The method is particularly effective for applications like medical imaging and surveillance systems where human verification of automated decisions is required.
```
==================================================
Interaction 4:
```SUMMARY
edge device image classification NAS optimization
```
==================================================
Interaction 5:
```FULL_TEXT
2307.04443v1
```
==================================================
Interaction 6:
```ADD_PAPER
2307.04443v1
This paper presents DCA-NAS, a device constraints-aware neural architecture search framework that optimizes models for edge devices with strict memory and computational constraints. The method incorporates weight sharing and channel bottleneck techniques to significantly reduce search time while maintaining performance. Key innovations include a differentiable hardware-aware objective function that directly optimizes for device constraints like FLOPs and memory usage without requiring tunable scaling factors. Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 10-15x smaller than manual architectures with comparable accuracy, while reducing search time by 4-17x compared to prior hardware-aware NAS methods. The approach demonstrates strong transferability across datasets and maintains stable performance across different resource constraints.
```
==================================================
Interaction 7:
```SUMMARY
edge device image classification latency optimization
```
==================================================
Interaction 8:
```FULL_TEXT
2210.06223v1
```
==================================================
Interaction 9:
```ADD_PAPER
2210.06223v1
This paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach that balances computation allocation with practical latency considerations through a novel latency prediction model. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method outperforms pixel-level dynamic networks by optimizing spatial granularity and scheduling strategies, demonstrating significant improvements in real-world deployment scenarios. The latency predictor considers algorithm design, scheduling, and hardware properties to guide network optimization, making it particularly effective for resource-constrained edge devices.
```
==================================================
Interaction 10:
```SUMMARY
edge image classification energy efficiency optimization
```
==================================================
Interaction 11:
```FULL_TEXT
2211.09897v1
```
==================================================
Interaction 12:
```ADD_PAPER
2211.09897v1
This paper presents an efficient feature compression method for edge-cloud image classification systems, addressing the critical rate-accuracy-complexity (RAC) trade-off. The authors propose a flexible encoder architecture with adjustable residual blocks that can scale to different edge device capabilities while maintaining classification accuracy. Key innovations include a single-stage training strategy that outperforms previous two-stage approaches, and an 8 downsampling scheme that reduces encoding latency by 51.4% while improving accuracy by 0.93%. Experimental results on ImageNet demonstrate superior RAC performance compared to both traditional image codecs and state-of-the-art feature compression methods, with particular advantages at lower bit rates. The system achieves this through optimized network design choices including patch embedding layers and GeLU activations, making it particularly suitable for resource-constrained edge deployments where computational efficiency is crucial.
```
==================================================
Interaction 13:
```SUMMARY
edge image classification spiking neural networks
```
==================================================
Interaction 14:
```FULL_TEXT
2002.00860v4
```
==================================================
Interaction 15:
```ADD_PAPER
2002.00860v4
This paper introduces an optimized spiking neuron model for efficient image classification on edge devices through temporal coding with just two spikes per neuron. The authors propose a novel ANN-to-SNN conversion method called FS-conversion that significantly improves upon traditional rate-based approaches by encoding information in spike timing rather than just spike counts. Key innovations include: 1) A modified spiking neuron model (FS-neuron) with optimized internal dynamics for emulating ANN activation functions with minimal spikes, 2) Demonstration on ImageNet and CIFAR10 showing near-identical accuracy to original ANNs (83.57% on ImageNet with EfficientNet-B7) while using only ~2 spikes per neuron, 3) Substantial improvements in latency and throughput compared to rate-based conversions, enabling new image classification every 32 time steps. The method is particularly effective for edge deployment as it achieves biological-level energy efficiency while maintaining high accuracy, making it suitable for mobile and embedded vision applications.
```
==================================================
Interaction 16:
```SUMMARY
edge image classification model compression techniques
```
==================================================
Interaction 17:
```SUMMARY
edge image classification quantization techniques
```
==================================================
Interaction 18:
```SUMMARY
edge image classification hardware-aware optimization
```
==================================================
Interaction 19:
```SUMMARY
edge image classification energy-efficient hardware acceleration
```
==================================================
Interaction 20:
```SUMMARY
edge image classification FPGA acceleration techniques
```
==================================================
Interaction 21:
```FULL_TEXT
1911.06777v1
```
==================================================
Interaction 22:
```ADD_PAPER
1911.06777v1
This paper presents TinyCNN, a framework for designing CNN accelerators on embedded FPGAs for image classification. The framework provides automated hardware generation from a software-designed CNN, with three key components: (1) a Python-based software backend for CNN design/training, (2) a CHISEL-based hardware generator, and (3) a precision adjustment backend. The system uses 16-bit fixed-point data, achieving only 3% accuracy loss compared to 32-bit floating-point while providing up to 15.75x speedup on a Xilinx Zynq-7000 FPGA. The framework addresses resource constraints through configurable convolution units (shared or exclusive modes), inter-layer precision adjustment, and automatic hardware resource checking. Experimental results on CIFAR-10 show the FPGA implementation achieves 62.28% accuracy (vs 65.54% software) with 2.7ms inference time (vs 42.54ms on ARM CPU). The work demonstrates effective FPGA acceleration for CNNs while maintaining accuracy through precision optimization techniques.
```
==================================================
Interaction 23:
```SUMMARY
edge image classification model compression NAS
```
==================================================
Interaction 24:
```FULL_TEXT
2307.04443v1
```
==================================================
Interaction 25:
```ADD_PAPER
2307.04443v1
This paper presents DCA-NAS, a device constraints-aware neural architecture search framework for optimizing deep learning models for edge devices. The method incorporates hardware constraints (memory and FLOPs) directly into the NAS objective function through a constrained optimization formulation. Key innovations include: 1) A differentiable resource constraint using a Lagrange multiplier, 2) Weight sharing and channel bottleneck techniques to reduce search time (4-17x faster than prior hardware-aware NAS), and 3) A lookup graph mechanism to handle differences between search and evaluation architectures. Experiments on CIFAR-10, TinyImagenet and ImageNet show DCA-NAS can discover models 10-15x smaller than manual architectures with comparable accuracy, and 3x smaller than DARTS variants. The approach achieves 97.2% accuracy on CIFAR-10 with only 3.4M parameters, and maintains performance when transferred across datasets, demonstrating its effectiveness for edge deployment scenarios.
```
==================================================
Interaction 26:
```SUMMARY
edge image classification dynamic computation optimization
```
==================================================
Interaction 27:
```SUMMARY
edge image classification adaptive inference techniques
```
==================================================
Interaction 28:
```SUMMARY
edge image classification early exit networks
```
==================================================
Interaction 29:
```FULL_TEXT
2105.09121v3
```
==================================================
Interaction 30:
```ADD_PAPER
2105.09121v3
This paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with just one transformer encoder layer. Experiments across image classification, audio classification, and audiovisual crowd counting demonstrate SL-ViT's effectiveness, achieving higher accuracy with fewer parameters and FLOPS compared to CNN-based approaches. The paper also presents a novel fine-tuning strategy (CC-SL-ViT) that further enhances performance. Key results show SL-ViT branches outperform CNN counterparts in 26 out of 27 tested scenarios while reducing computational costs, making it particularly suitable for resource-constrained edge devices requiring dynamic inference capabilities.
```
==================================================
Interaction 31:
```SUMMARY
edge image classification sparsity techniques
```
==================================================
Interaction 32:
```SUMMARY
edge image classification pruning quantization techniques
```
==================================================
Interaction 33:
```FULL_TEXT
2409.02134v1
```
==================================================
Interaction 34:
```ADD_PAPER
2409.02134v1
This paper comprehensively evaluates model compression techniques (structured/unstructured pruning and dynamic quantization) for ConvNeXt models on image classification tasks. Key findings include: 1) OTOV3 pruning achieves 75% reduction in model size/parameters while maintaining accuracy, 2) Dynamic quantization achieves 95% parameter reduction with minimal accuracy drop, 3) Combined OTOV3+quantization achieves 89.7% size reduction and 95% parameter/MAC reduction while improving accuracy by 3.8%. The compressed model demonstrates 92.5% accuracy with 20ms inference time on edge devices, validating effectiveness for real-world deployment.
```
==================================================
Interaction 35:
```SUMMARY
edge image classification hardware-aware neural architecture search
```
==================================================
Interaction 36:
```FULL_TEXT
2307.04443v1
```
==================================================
Interaction 37:
```ADD_PAPER
2307.04443v1
This paper presents DCA-NAS, a hardware-aware neural architecture search method that optimizes for edge device constraints like model size and FLOPs. Key innovations include: 1) A differentiable constrained optimization formulation that directly incorporates device resource constraints, 2) Weight sharing and channel bottleneck techniques to reduce search time by 4-17x compared to prior NAS methods, 3) Demonstrated effectiveness on image classification tasks (CIFAR-10/100, ImageNet) where it achieves comparable accuracy to manual architectures with 10-15x smaller models. The method discovers Pareto-optimal architectures for given hardware constraints without requiring extensive hyperparameter tuning.
```
==================================================
