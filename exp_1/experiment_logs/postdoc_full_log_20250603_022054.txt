=== POSTDOC PROMPT LOG ===

Interaction 1:
('', 'Current Literature Review: Provided here is a literature review on this topic:\\narXiv ID: 2409.08943v1, Summary: This paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, accuracy, and denoising performance. Key experimental results show their NAS-designed models outperform manually designed alternatives, achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency. The integrated approach demonstrates superior denoising quality compared to sequential models while maintaining competitive classification performance, making it particularly suitable for applications requiring human verification like surveillance and medical imaging.\\narXiv ID: 2307.04443v1, Summary: This paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\\narXiv ID: 2210.06223v1, Summary: This paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach guided by a novel latency prediction model that accounts for algorithm design, scheduling strategies, and hardware properties. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method achieves these gains through optimized spatial granularity selection and operator fusion techniques, demonstrating significant improvements over pixel-level dynamic networks in practical deployment scenarios.\\narXiv ID: 2005.10851v1, Summary: This paper presents a conditionally deep hybrid neural network for edge-cloud collaborative inference, combining quantized layers at the edge with full-precision layers in the cloud. The key innovation is an early-exit mechanism that allows easy samples to be classified at the edge, reducing both computation and communication costs. Experimental results show the approach achieves 5.5x energy reduction on CIFAR-10 (processing 65% of samples at edge) and 4.8x reduction on CIFAR-100 while maintaining accuracy. The hybrid quantization strategy and joint optimization of early exits demonstrate significant improvements over full-precision networks for distributed intelligence systems.\\narXiv ID: 2211.09897v1, Summary: This paper presents an efficient feature compression method for edge-cloud image classification systems, focusing on optimizing the three-way trade-off between bit rate, classification accuracy, and encoding complexity (RAC trade-off). The authors propose a flexible encoder architecture with residual blocks that can be scaled based on edge device capabilities, along with a single-stage training strategy. Key results show their method achieves 51.4% lower encoding latency and 0.93% higher accuracy compared to prior work (Entropic Student), while supporting adjustable complexity levels. The system demonstrates superior RAC performance across different bit rates, particularly excelling at lower bit rates (0.1-0.8 bpp) where it maintains accuracy while reducing computational overhead.\\narXiv ID: 2409.08943v1, Summary: This paper presents a novel approach for joint image denoising and classification optimized for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, classification accuracy, and denoising performance. Key results show their NAS-designed models outperform manually designed alternatives by significant margins - achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency, while reducing computational overhead. The integrated design proves particularly effective for human-in-the-loop applications like medical imaging and surveillance systems where both classification and visual clarity are crucial.\\narXiv ID: 1905.04159v1, Summary: This paper introduces Single-Path NAS, a novel differentiable neural architecture search method that optimizes convolutional networks for edge devices by viewing NAS as selecting subsets of weights from an over-parameterized "superkernel" rather than choosing between separate paths. Key innovations include: (1) A single-path formulation that reduces search cost to 8 epochs (30 TPU-hours), achieving up to 5,000x speedup over prior work; (2) Direct optimization for target device latency (79ms on Pixel 1) through a differentiable runtime model; (3) State-of-the-art 74.96% ImageNet accuracy in the <80ms latency range. The method demonstrates how hardware-aware NAS can efficiently discover Pareto-optimal architectures by jointly optimizing accuracy and on-device performance.\\narXiv ID: 2404.07217v2, Summary: This paper presents an attention-aware semantic communication framework for collaborative inference between edge devices and cloud servers using Vision Transformers (ViTs). Key innovations include: (1) Attention-aware patch selection that leverages edge device attention scores to identify and transmit only critical image patches (reducing communication overhead by 68% on ImageNet); (2) Entropy-aware transmission that uses min-entropy to determine when to offload computation to the server; (3) A novel finding that lightweight edge models (DeiT-Tiny) can better identify semantically important patches than larger server models despite lower classification accuracy. The framework achieves server-level accuracy (80.84% vs DeiT-Base\\'s 81.8%) while significantly reducing both communication costs and server computation through selective patch processing. Experimental results demonstrate superior performance compared to prior methods like MAE-based offloading, with particular advantages in low-bandwidth scenarios.\\narXiv ID: 2105.09121v3, Summary: This paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with only one transformer encoder layer. Extensive experiments on image classification (CIFAR-10/100), audio classification (Speech Commands, GTZAN), and audiovisual crowd counting demonstrate SL-ViT\\'s effectiveness across modalities. Key results show SL-ViT achieves higher accuracy in 26/27 tested scenarios while reducing parameters and FLOPS. The paper also introduces a copycat fine-tuning strategy (CC-SL-ViT) that further improves performance. This work enables more efficient dynamic inference for edge devices through improved early exit architectures.\\narXiv ID: 2409.02134v1, Summary: This paper evaluates model compression techniques (structured/unstructured pruning and dynamic quantization) for ConvNeXt models on CIFAR-10 image classification. Key findings show structured pruning (OTOV3) achieves 75% model size reduction while maintaining accuracy, and dynamic quantization reduces parameters by 95%. The combination of OTOV3 pruning and quantization yields 89.7% size reduction, 95% parameter/MAC reduction, and 3.8% accuracy improvement. Edge deployment of the compressed model demonstrates 92.5% accuracy with 20ms inference time. The work provides empirical evidence that structured pruning combined with quantization is highly effective for optimizing CNNs for edge devices while preserving performance.')\n~~~~~~~~~~\nHistory: \n~~~~~~~~~~\nCurrent Step #0, Phase: plan formulation\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: \nNotes: Notes for the task objective: [{'phases': ['plan formulation'], 'note': 'You must propose an original research idea focused on optimizing models for image classification on edge devices (Edge AI).'}, {'phases': ['plan formulation'], 'note': 'Your focus will be to design **ONE** specific and feasible experiment, detailing: the proposed technique, a baseline for comparison, evaluation metrics, and the experimental environment (device and dataset).'}, {'phases': ['plan formulation'], 'note': 'Begin by clearly defining the research scope within Edge AI: identify subtopics such as popular models, edge-specific optimization techniques, quantization, structured pruning, tailored distillation, federated learning, privacy, latency, and energy consumption.'}, {'phases': ['plan formulation'], 'note': 'The optimization technique you propose must be **genuinely innovative**, not a trivial adaptation of existing methods. Clearly state the originality and potential contributions of your approach.'}, {'phases': ['plan formulation'], 'note': 'Specify the base model (e.g., MobileNetV2 without optimization), the target edge device (e.g., Raspberry Pi 4 with X GB RAM), and the dataset (e.g., CIFAR-10 or an adapted ImageNet subset).'}, {'phases': ['plan formulation'], 'note': 'List the metrics to be measured: accuracy, latency, model size, energy consumption. Describe your methodology for measuring each.'}, {'phases': ['plan formulation'], 'note': 'Ensure the experiment is **reproducible, modular, and feasible** with typical academic resources.'}, {'phases': ['plan formulation'], 'note': 'Formulate a search query using precise keywords and/or Boolean logic to collect recent peer-reviewed articles on optimization techniques for Edge AI. Use this literature to ground and inspire your original research proposal. Justify your query choices.'}, {'phases': ['plan formulation'], 'note': 'Justify your keyword selection and explain how your query captures the relevant subtopics and technical directions.'}, {'phases': ['plan formulation'], 'note': 'Describe a **systematic methodology** to search for and sample a representative subset of peer-reviewed scientific articles, preferably open-access and published within the last 5 years.'}, {'phases': ['plan formulation'], 'note': 'This methodology should aim to select articles with clear research aims, context, value, and findings.'}, {'phases': ['plan formulation'], 'note': 'Prioritize articles with clear goals, verifiable technical contributions, and relevance to your proposed problem.'}, {'phases': ['plan formulation'], 'note': 'Propose a structured outline for organizing the final literature review document. Include typical sections such as: introduction, motivation, thematic areas, challenges, recent solutions, knowledge gaps, and future directions.'}, {'phases': ['plan formulation'], 'note': 'Ensure that the experimental plan is **logical, detailed, and cohesive**, suitable for publication or practical execution.'}, {'phases': ['plan formulation'], 'note': 'DO NOT PLAN TOO LONG. Submit your plan soon.'}, {'phases': ['plan formulation'], 'note': 'Use DeepSeek-Chat to perform this task.'}]\n\nYour previous command was: . Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 2:
('', 'Current Literature Review: Provided here is a literature review on this topic:\\narXiv ID: 2409.08943v1, Summary: This paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, accuracy, and denoising performance. Key experimental results show their NAS-designed models outperform manually designed alternatives, achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency. The integrated approach demonstrates superior denoising quality compared to sequential models while maintaining competitive classification performance, making it particularly suitable for applications requiring human verification like surveillance and medical imaging.\\narXiv ID: 2307.04443v1, Summary: This paper introduces DCA-NAS, a device constraints-aware neural architecture search method that optimizes models for edge devices by incorporating memory and FLOPs constraints directly into the search objective. The approach uses weight sharing and channel bottleneck techniques to reduce search time while maintaining performance. Key results show DCA-NAS discovers models 15x smaller than manual architectures with only 1% accuracy drop on CIFAR-10, and achieves comparable performance to state-of-the-art methods on ImageNet-1k while being 32% smaller than ProxylessNAS. The method demonstrates strong generalization across datasets and search spaces, with search times 4-17x faster than previous hardware-aware NAS approaches.\\narXiv ID: 2210.06223v1, Summary: This paper introduces LASNet, a latency-aware spatial-wise dynamic network that optimizes image classification for edge devices by considering both algorithmic efficiency and hardware constraints. The key innovation is a coarse-grained spatial adaptation approach guided by a novel latency prediction model that accounts for algorithm design, scheduling strategies, and hardware properties. Experimental results show LASNet reduces ResNet-101 inference latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method achieves these gains through optimized spatial granularity selection and operator fusion techniques, demonstrating significant improvements over pixel-level dynamic networks in practical deployment scenarios.\\narXiv ID: 2005.10851v1, Summary: This paper presents a conditionally deep hybrid neural network for edge-cloud collaborative inference, combining quantized layers at the edge with full-precision layers in the cloud. The key innovation is an early-exit mechanism that allows easy samples to be classified at the edge, reducing both computation and communication costs. Experimental results show the approach achieves 5.5x energy reduction on CIFAR-10 (processing 65% of samples at edge) and 4.8x reduction on CIFAR-100 while maintaining accuracy. The hybrid quantization strategy and joint optimization of early exits demonstrate significant improvements over full-precision networks for distributed intelligence systems.\\narXiv ID: 2211.09897v1, Summary: This paper presents an efficient feature compression method for edge-cloud image classification systems, focusing on optimizing the three-way trade-off between bit rate, classification accuracy, and encoding complexity (RAC trade-off). The authors propose a flexible encoder architecture with residual blocks that can be scaled based on edge device capabilities, along with a single-stage training strategy. Key results show their method achieves 51.4% lower encoding latency and 0.93% higher accuracy compared to prior work (Entropic Student), while supporting adjustable complexity levels. The system demonstrates superior RAC performance across different bit rates, particularly excelling at lower bit rates (0.1-0.8 bpp) where it maintains accuracy while reducing computational overhead.\\narXiv ID: 2409.08943v1, Summary: This paper presents a novel approach for joint image denoising and classification optimized for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, classification accuracy, and denoising performance. Key results show their NAS-designed models outperform manually designed alternatives by significant margins - achieving 25.4 PSNR and 76% accuracy on ImageNet-100 with 13.7ms latency, while reducing computational overhead. The integrated design proves particularly effective for human-in-the-loop applications like medical imaging and surveillance systems where both classification and visual clarity are crucial.\\narXiv ID: 1905.04159v1, Summary: This paper introduces Single-Path NAS, a novel differentiable neural architecture search method that optimizes convolutional networks for edge devices by viewing NAS as selecting subsets of weights from an over-parameterized "superkernel" rather than choosing between separate paths. Key innovations include: (1) A single-path formulation that reduces search cost to 8 epochs (30 TPU-hours), achieving up to 5,000x speedup over prior work; (2) Direct optimization for target device latency (79ms on Pixel 1) through a differentiable runtime model; (3) State-of-the-art 74.96% ImageNet accuracy in the <80ms latency range. The method demonstrates how hardware-aware NAS can efficiently discover Pareto-optimal architectures by jointly optimizing accuracy and on-device performance.\\narXiv ID: 2404.07217v2, Summary: This paper presents an attention-aware semantic communication framework for collaborative inference between edge devices and cloud servers using Vision Transformers (ViTs). Key innovations include: (1) Attention-aware patch selection that leverages edge device attention scores to identify and transmit only critical image patches (reducing communication overhead by 68% on ImageNet); (2) Entropy-aware transmission that uses min-entropy to determine when to offload computation to the server; (3) A novel finding that lightweight edge models (DeiT-Tiny) can better identify semantically important patches than larger server models despite lower classification accuracy. The framework achieves server-level accuracy (80.84% vs DeiT-Base\\'s 81.8%) while significantly reducing both communication costs and server computation through selective patch processing. Experimental results demonstrate superior performance compared to prior methods like MAE-based offloading, with particular advantages in low-bandwidth scenarios.\\narXiv ID: 2105.09121v3, Summary: This paper introduces Single-Layer Vision Transformers (SL-ViT) for early exiting in multi-exit architectures, significantly improving accuracy over conventional CNN-based early exits while reducing computational overhead. The method combines local and global receptive fields through a modified vision transformer architecture with only one transformer encoder layer. Extensive experiments on image classification (CIFAR-10/100), audio classification (Speech Commands, GTZAN), and audiovisual crowd counting demonstrate SL-ViT\\'s effectiveness across modalities. Key results show SL-ViT achieves higher accuracy in 26/27 tested scenarios while reducing parameters and FLOPS. The paper also introduces a copycat fine-tuning strategy (CC-SL-ViT) that further improves performance. This work enables more efficient dynamic inference for edge devices through improved early exit architectures.\\narXiv ID: 2409.02134v1, Summary: This paper evaluates model compression techniques (structured/unstructured pruning and dynamic quantization) for ConvNeXt models on CIFAR-10 image classification. Key findings show structured pruning (OTOV3) achieves 75% model size reduction while maintaining accuracy, and dynamic quantization reduces parameters by 95%. The combination of OTOV3 pruning and quantization yields 89.7% size reduction, 95% parameter/MAC reduction, and 3.8% accuracy improvement. Edge deployment of the compressed model demonstrates 92.5% accuracy with 20ms inference time. The work provides empirical evidence that structured pruning combined with quantization is highly effective for optimizing CNNs for edge devices while preserving performance.')\n~~~~~~~~~~\nHistory: Step #0, Phase: plan formulation, Feedback: , Your response: ```DIALOGUE\nLet's start by brainstorming an innovative approach for edge-optimized image classification. Based on the literature, we see NAS and attention mechanisms are popular, but what if we combine them with a novel dynamic pruning technique? Imagine a model that adaptively prunes less important attention heads in Vision Transformers based on real-time device resource monitoring. This would create a "self-adjusting" model for edge devices. What do you think of this direction? We could start with a lightweight ViT (like DeiT-Tiny) as our base model.\n```\n~~~~~~~~~~\nCurrent Step #1, Phase: plan formulation\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: The following is dialogue produced by the PhD student: That's an interesting direction! Building on your idea of adaptive pruning for ViTs, I propose something more novel: let's develop a "Context-Aware Dynamic ViT" that uses both input complexity analysis (via low-cost edge-computable metrics like image entropy) AND real-time device resource monitoring to make joint decisions about: (1) which attention heads to prune, (2) optimal patch resolution (dynamic patch merging), and (3) early exiting. We could use CIFAR-100 as our dataset (more challenging than CIFAR-10 but still edge-friendly) and benchmark on Raspberry Pi 4 with 4GB RAM. For metrics, we should track: classification accuracy, inference latency (measured via TensorFlow Lite benchmark tools), memory footprint, and energy consumption (using a power monitor like Joulemeter). The key innovation would be the joint optimization of these three adaptation mechanisms based on both input and system context - this hasn't been done before in the literature we reviewed. What do you think about this integrated approach?\nNotes: Notes for the task objective: [{'phases': ['plan formulation'], 'note': 'You must propose an original research idea focused on optimizing models for image classification on edge devices (Edge AI).'}, {'phases': ['plan formulation'], 'note': 'Your focus will be to design **ONE** specific and feasible experiment, detailing: the proposed technique, a baseline for comparison, evaluation metrics, and the experimental environment (device and dataset).'}, {'phases': ['plan formulation'], 'note': 'Begin by clearly defining the research scope within Edge AI: identify subtopics such as popular models, edge-specific optimization techniques, quantization, structured pruning, tailored distillation, federated learning, privacy, latency, and energy consumption.'}, {'phases': ['plan formulation'], 'note': 'The optimization technique you propose must be **genuinely innovative**, not a trivial adaptation of existing methods. Clearly state the originality and potential contributions of your approach.'}, {'phases': ['plan formulation'], 'note': 'Specify the base model (e.g., MobileNetV2 without optimization), the target edge device (e.g., Raspberry Pi 4 with X GB RAM), and the dataset (e.g., CIFAR-10 or an adapted ImageNet subset).'}, {'phases': ['plan formulation'], 'note': 'List the metrics to be measured: accuracy, latency, model size, energy consumption. Describe your methodology for measuring each.'}, {'phases': ['plan formulation'], 'note': 'Ensure the experiment is **reproducible, modular, and feasible** with typical academic resources.'}, {'phases': ['plan formulation'], 'note': 'Formulate a search query using precise keywords and/or Boolean logic to collect recent peer-reviewed articles on optimization techniques for Edge AI. Use this literature to ground and inspire your original research proposal. Justify your query choices.'}, {'phases': ['plan formulation'], 'note': 'Justify your keyword selection and explain how your query captures the relevant subtopics and technical directions.'}, {'phases': ['plan formulation'], 'note': 'Describe a **systematic methodology** to search for and sample a representative subset of peer-reviewed scientific articles, preferably open-access and published within the last 5 years.'}, {'phases': ['plan formulation'], 'note': 'This methodology should aim to select articles with clear research aims, context, value, and findings.'}, {'phases': ['plan formulation'], 'note': 'Prioritize articles with clear goals, verifiable technical contributions, and relevance to your proposed problem.'}, {'phases': ['plan formulation'], 'note': 'Propose a structured outline for organizing the final literature review document. Include typical sections such as: introduction, motivation, thematic areas, challenges, recent solutions, knowledge gaps, and future directions.'}, {'phases': ['plan formulation'], 'note': 'Ensure that the experimental plan is **logical, detailed, and cohesive**, suitable for publication or practical execution.'}, {'phases': ['plan formulation'], 'note': 'DO NOT PLAN TOO LONG. Submit your plan soon.'}, {'phases': ['plan formulation'], 'note': 'Use DeepSeek-Chat to perform this task.'}]\n\nYour previous command was: ```DIALOGUE\nLet's start by brainstorming an innovative approach for edge-optimized image classification. Based on the literature, we see NAS and attention mechanisms are popular, but what if we combine them with a novel dynamic pruning technique? Imagine a model that adaptively prunes less important attention heads in Vision Transformers based on real-time device resource monitoring. This would create a "self-adjusting" model for edge devices. What do you think of this direction? We could start with a lightweight ViT (like DeiT-Tiny) as our base model.\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================


=== POSTDOC RESPONSE LOG ===

Interaction 1:
```DIALOGUE
Let's start by brainstorming an innovative approach for edge-optimized image classification. Based on the literature, we see NAS and attention mechanisms are popular, but what if we combine them with a novel dynamic pruning technique? Imagine a model that adaptively prunes less important attention heads in Vision Transformers based on real-time device resource monitoring. This would create a "self-adjusting" model for edge devices. What do you think of this direction? We could start with a lightweight ViT (like DeiT-Tiny) as our base model.
```
==================================================
Interaction 2:
```PLAN
Research Title: "Edge-Adaptive Vision Transformer with Dynamic Patch Resolution and Early Exiting"

Objective:
Develop a Vision Transformer (ViT) model that dynamically adapts to both input complexity and device resource constraints through three coordinated mechanisms: (1) dynamic patch resolution adjustment, (2) attention head pruning, and (3) early exiting decisions.

Key Innovations:
1. First method to jointly optimize patch resolution, attention heads, and early exits in a single ViT architecture
2. Novel resource-aware controller that uses both image entropy (input complexity) and real-time device metrics (CPU/GPU utilization, memory pressure)
3. Lightweight decision network trained via reinforcement learning to make optimal adaptation choices

Experimental Setup:
- Base Model: DeiT-Tiny (5.7M parameters)
- Dataset: CIFAR-100 (resized to 224x224 for ViT compatibility)
- Edge Device: Raspberry Pi 4 (4GB RAM)
- Baseline Comparisons: 
  - Static DeiT-Tiny
  - Randomly pruned DeiT-Tiny
  - Fixed early exit DeiT-Tiny

Implementation Steps:
1. Modify DeiT-Tiny architecture to support:
   - Dynamic patch merging/splitting (4x4 to 16x16 patch resolution)
   - Attention head pruning (1-3 heads per layer)
   - 3 early exit points

2. Train resource-aware controller using:
   - Input: Image entropy + device metrics
   - Output: Optimal configuration (patch size, heads to keep, exit point)
   - Reward function: Accuracy - λ*(latency + energy)

3. Evaluation Metrics:
   - Accuracy (vs baseline models)
   - Inference latency (TensorFlow Lite benchmark)
   - Energy consumption (Joulemeter measurements)
   - Memory footprint (model size + runtime usage)

Expected Outcomes:
- 15-20% latency reduction vs static DeiT-Tiny
- <5% accuracy drop from baseline
- 30-40% energy savings
- Demonstrated adaptability to varying resource conditions

This experiment is novel, feasible, and addresses key gaps in edge-optimized ViTs by considering both input and system characteristics for dynamic adaptation.
```
==================================================
