=== POSTDOC PROMPT LOG ===

Interaction 1:
('', 'Current Literature Review: Provided here is a literature review on this topic:\\narXiv ID: 2409.08943v1, Summary: This paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, classification accuracy, and denoising performance. Key experimental results show their NAS-designed models outperform manually designed alternatives, achieving 25.6 PSNR and 0.71 SSIM for denoising while maintaining 76% classification accuracy on ImageNet-100 at 13.7ms latency. The work demonstrates significant improvements in human perception of noisy images while meeting real-time constraints on edge devices.\\narXiv ID: 2409.02134v1, Summary: This paper evaluates model compression techniques for ConvNeXt architectures on edge devices, focusing on structured/unstructured pruning and dynamic quantization. Key experimental results show that combining OTOv3 pruning with dynamic quantization achieves 89.7% model size reduction, 95% reduction in parameters/MACs, while improving accuracy by 3.8% on CIFAR-10. The compressed model demonstrates practical edge deployment with 92.5% accuracy and 20ms inference time. The work provides comprehensive analysis of compression tradeoffs and validates techniques through both cloud-based experiments and real edge device deployment.\\narXiv ID: 2307.04443v1, Summary: This paper presents DCA-NAS, a neural architecture search method specifically designed for edge devices with hardware constraints. The approach incorporates device memory and FLOPs constraints directly into the optimization objective, using weight sharing and channel bottleneck techniques to reduce search time. Key results show DCA-NAS achieves 4-17x faster search than prior hardware-aware NAS methods while discovering models 10-15x smaller than manual architectures with comparable accuracy. On ImageNet-1k, it achieves 24.4% top-1 error with 5.3M parameters (1.9 GPU days search time). The method demonstrates strong transferability across datasets and maintains performance when scaled to different device constraints.\\narXiv ID: 2210.06223v1, Summary: This paper introduces LASNet (Latency-aware Spatial-wise Dynamic Networks), a framework for optimizing spatial-wise dynamic convolutions to improve practical inference efficiency on edge devices. The key innovation is a latency prediction model that considers algorithm design, scheduling strategies, and hardware properties to guide coarse-grained spatially adaptive inference. Experiments show LASNet reduces ResNet-101 latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method outperforms pixel-level dynamic approaches by optimizing spatial granularity and operator fusion for better memory access patterns. Results demonstrate effectiveness on image classification, object detection, and instance segmentation tasks.\\narXiv ID: 2112.10948v1, Summary: This paper proposes a novel task-oriented aerial image transmission framework for scene classification on edge devices. The key innovation is a deep reinforcement learning approach that dynamically selects the most informative image regions (semantic blocks) for compression and transmission based on channel conditions. This achieves an optimal tradeoff between classification accuracy and transmission latency. Experimental results show significant improvements over traditional methods, with the system adapting compression levels to varying channel gains while maintaining high accuracy. The framework demonstrates how edge devices can efficiently offload complex vision tasks by transmitting only the most task-relevant image data.\\narXiv ID: 2108.09343v2, Summary: This paper proposes an early-exit DNN architecture with expert branches to improve edge offloading decisions for distorted images. The key innovation is training specialized side branches for different distortion types (e.g., blur, noise) while keeping a shared backbone. A distortion classifier first identifies the image distortion type, then selects corresponding expert branches for inference. Experiments show this approach increases edge classification accuracy by 15-20% for distorted images compared to standard early-exit DNNs, while reducing cloud offloading by 30-40%. The method achieves lower end-to-end latency (0.2-0.3s) across different network conditions when deployed with Amazon EC2 instances. The work demonstrates how adaptive model architectures can optimize edge-cloud collaboration under real-world input variations.\\narXiv ID: 2409.02134v1, Summary: This paper comprehensively evaluates model compression techniques (structured/unstructured pruning and dynamic quantization) for ConvNeXt models on edge devices. Key findings include: 1) OTOv3 structured pruning achieves 75% reduction in model size/parameters and 60% reduction in MACs while increasing accuracy by 3.8% on CIFAR-10. 2) Dynamic quantization alone reduces parameters/MACs by 95% with minimal accuracy drop (0.1%). 3) Combining both techniques yields 89.7% size reduction and 95% parameter/MAC reduction while improving accuracy. The compressed model achieves 92.5% accuracy with 20ms inference time on edge hardware, demonstrating effective optimization for resource-constrained deployment.\\narXiv ID: 2105.04104v3, Summary: AppealNet proposes an edge/cloud collaborative architecture with a two-head neural network design that explicitly models inference difficulty to optimize the accuracy-computation tradeoff. The system dynamically decides whether to process inputs locally on edge devices or offload to the cloud, achieving up to 40% energy savings compared to confidence-based approaches while maintaining accuracy. Experimental results on image classification tasks demonstrate superior performance over methods using softmax probability-based confidence metrics.\\narXiv ID: 2210.06223v1, Summary: This paper proposes LASNet, a latency-aware spatial-wise dynamic network that optimizes inference efficiency for deep learning models on edge devices. The key innovation is a coarse-grained spatially adaptive inference approach guided by a novel latency prediction model that considers algorithm design, scheduling strategies, and hardware properties. Experiments show LASNet reduces ResNet-101 latency by 36% on server GPUs and 46% on edge devices (Nvidia Jetson TX2) without accuracy loss. The framework achieves practical speedups by optimizing spatial granularity and memory access patterns, outperforming pixel-level dynamic networks. Results demonstrate significant improvements in image classification, object detection, and instance segmentation tasks while maintaining accuracy. The latency predictor enables efficient estimation of inference time across different hardware platforms, making it particularly valuable for edge deployment scenarios.\\narXiv ID: 2002.00860v4, Summary: This paper introduces a novel ANN-to-SNN conversion method called FS-conversion that enables highly efficient image classification with spiking neural networks. The key innovation is optimizing spiking neuron models to emulate artificial neurons using temporal coding with very few spikes (typically just 2 spikes per neuron). The method achieves state-of-the-art accuracy on ImageNet (83.57% for EfficientNet-B7 conversion) while dramatically reducing spike counts compared to rate-based conversions. The approach is particularly suitable for edge devices due to its energy efficiency and supports pipelined processing for high throughput. Experimental results show significant improvements in both classification accuracy and computational efficiency compared to previous SNN approaches.')\n~~~~~~~~~~\nHistory: \n~~~~~~~~~~\nCurrent Step #0, Phase: plan formulation\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: \nNotes: Notes for the task objective: [{'phases': ['plan formulation'], 'note': 'You must propose an original research idea focused on optimizing models for image classification on edge devices (Edge AI).'}, {'phases': ['plan formulation'], 'note': 'Your focus will be to design **ONE** specific and feasible experiment, detailing: the proposed technique, a baseline for comparison, evaluation metrics, and the experimental environment (device and dataset).'}, {'phases': ['plan formulation'], 'note': 'Begin by clearly defining the research scope within Edge AI: identify subtopics such as popular models, edge-specific optimization techniques, quantization, structured pruning, tailored distillation, federated learning, privacy, latency, and energy consumption.'}, {'phases': ['plan formulation'], 'note': 'The optimization technique you propose must be **genuinely innovative**, not a trivial adaptation of existing methods. Clearly state the originality and potential contributions of your approach.'}, {'phases': ['plan formulation'], 'note': 'Specify the base model (e.g., MobileNetV2 without optimization), the target edge device (e.g., Raspberry Pi 4 with X GB RAM), and the dataset (e.g., CIFAR-10 or an adapted ImageNet subset).'}, {'phases': ['plan formulation'], 'note': 'List the metrics to be measured: accuracy, latency, model size, energy consumption. Describe your methodology for measuring each.'}, {'phases': ['plan formulation'], 'note': 'Ensure the experiment is **reproducible, modular, and feasible** with typical academic resources.'}, {'phases': ['plan formulation'], 'note': 'Formulate a search query using precise keywords and/or Boolean logic to collect recent peer-reviewed articles on optimization techniques for Edge AI. Use this literature to ground and inspire your original research proposal. Justify your query choices.'}, {'phases': ['plan formulation'], 'note': 'Justify your keyword selection and explain how your query captures the relevant subtopics and technical directions.'}, {'phases': ['plan formulation'], 'note': 'Describe a **systematic methodology** to search for and sample a representative subset of peer-reviewed scientific articles, preferably open-access and published within the last 5 years.'}, {'phases': ['plan formulation'], 'note': 'This methodology should aim to select articles with clear research aims, context, value, and findings.'}, {'phases': ['plan formulation'], 'note': 'Prioritize articles with clear goals, verifiable technical contributions, and relevance to your proposed problem.'}, {'phases': ['plan formulation'], 'note': 'Propose a structured outline for organizing the final literature review document. Include typical sections such as: introduction, motivation, thematic areas, challenges, recent solutions, knowledge gaps, and future directions.'}, {'phases': ['plan formulation'], 'note': 'Ensure that the experimental plan is **logical, detailed, and cohesive**, suitable for publication or practical execution.'}, {'phases': ['plan formulation'], 'note': 'DO NOT PLAN TOO LONG. Submit your plan soon.'}, {'phases': ['plan formulation'], 'note': 'Use DeepSeek-Chat to perform this task.'}]\n\nYour previous command was: . Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================
Interaction 2:
('', 'Current Literature Review: Provided here is a literature review on this topic:\\narXiv ID: 2409.08943v1, Summary: This paper presents a novel approach to jointly optimize image denoising and classification for edge devices. The authors propose an integrated architecture that shares an encoder between denoising and classification tasks, along with modifications to Neural Architecture Search (NAS) to optimize for target latency, classification accuracy, and denoising performance. Key experimental results show their NAS-designed models outperform manually designed alternatives, achieving 25.6 PSNR and 0.71 SSIM for denoising while maintaining 76% classification accuracy on ImageNet-100 at 13.7ms latency. The work demonstrates significant improvements in human perception of noisy images while meeting real-time constraints on edge devices.\\narXiv ID: 2409.02134v1, Summary: This paper evaluates model compression techniques for ConvNeXt architectures on edge devices, focusing on structured/unstructured pruning and dynamic quantization. Key experimental results show that combining OTOv3 pruning with dynamic quantization achieves 89.7% model size reduction, 95% reduction in parameters/MACs, while improving accuracy by 3.8% on CIFAR-10. The compressed model demonstrates practical edge deployment with 92.5% accuracy and 20ms inference time. The work provides comprehensive analysis of compression tradeoffs and validates techniques through both cloud-based experiments and real edge device deployment.\\narXiv ID: 2307.04443v1, Summary: This paper presents DCA-NAS, a neural architecture search method specifically designed for edge devices with hardware constraints. The approach incorporates device memory and FLOPs constraints directly into the optimization objective, using weight sharing and channel bottleneck techniques to reduce search time. Key results show DCA-NAS achieves 4-17x faster search than prior hardware-aware NAS methods while discovering models 10-15x smaller than manual architectures with comparable accuracy. On ImageNet-1k, it achieves 24.4% top-1 error with 5.3M parameters (1.9 GPU days search time). The method demonstrates strong transferability across datasets and maintains performance when scaled to different device constraints.\\narXiv ID: 2210.06223v1, Summary: This paper introduces LASNet (Latency-aware Spatial-wise Dynamic Networks), a framework for optimizing spatial-wise dynamic convolutions to improve practical inference efficiency on edge devices. The key innovation is a latency prediction model that considers algorithm design, scheduling strategies, and hardware properties to guide coarse-grained spatially adaptive inference. Experiments show LASNet reduces ResNet-101 latency by 36% on Tesla V100 and 46% on Jetson TX2 GPUs while maintaining accuracy. The method outperforms pixel-level dynamic approaches by optimizing spatial granularity and operator fusion for better memory access patterns. Results demonstrate effectiveness on image classification, object detection, and instance segmentation tasks.\\narXiv ID: 2112.10948v1, Summary: This paper proposes a novel task-oriented aerial image transmission framework for scene classification on edge devices. The key innovation is a deep reinforcement learning approach that dynamically selects the most informative image regions (semantic blocks) for compression and transmission based on channel conditions. This achieves an optimal tradeoff between classification accuracy and transmission latency. Experimental results show significant improvements over traditional methods, with the system adapting compression levels to varying channel gains while maintaining high accuracy. The framework demonstrates how edge devices can efficiently offload complex vision tasks by transmitting only the most task-relevant image data.\\narXiv ID: 2108.09343v2, Summary: This paper proposes an early-exit DNN architecture with expert branches to improve edge offloading decisions for distorted images. The key innovation is training specialized side branches for different distortion types (e.g., blur, noise) while keeping a shared backbone. A distortion classifier first identifies the image distortion type, then selects corresponding expert branches for inference. Experiments show this approach increases edge classification accuracy by 15-20% for distorted images compared to standard early-exit DNNs, while reducing cloud offloading by 30-40%. The method achieves lower end-to-end latency (0.2-0.3s) across different network conditions when deployed with Amazon EC2 instances. The work demonstrates how adaptive model architectures can optimize edge-cloud collaboration under real-world input variations.\\narXiv ID: 2409.02134v1, Summary: This paper comprehensively evaluates model compression techniques (structured/unstructured pruning and dynamic quantization) for ConvNeXt models on edge devices. Key findings include: 1) OTOv3 structured pruning achieves 75% reduction in model size/parameters and 60% reduction in MACs while increasing accuracy by 3.8% on CIFAR-10. 2) Dynamic quantization alone reduces parameters/MACs by 95% with minimal accuracy drop (0.1%). 3) Combining both techniques yields 89.7% size reduction and 95% parameter/MAC reduction while improving accuracy. The compressed model achieves 92.5% accuracy with 20ms inference time on edge hardware, demonstrating effective optimization for resource-constrained deployment.\\narXiv ID: 2105.04104v3, Summary: AppealNet proposes an edge/cloud collaborative architecture with a two-head neural network design that explicitly models inference difficulty to optimize the accuracy-computation tradeoff. The system dynamically decides whether to process inputs locally on edge devices or offload to the cloud, achieving up to 40% energy savings compared to confidence-based approaches while maintaining accuracy. Experimental results on image classification tasks demonstrate superior performance over methods using softmax probability-based confidence metrics.\\narXiv ID: 2210.06223v1, Summary: This paper proposes LASNet, a latency-aware spatial-wise dynamic network that optimizes inference efficiency for deep learning models on edge devices. The key innovation is a coarse-grained spatially adaptive inference approach guided by a novel latency prediction model that considers algorithm design, scheduling strategies, and hardware properties. Experiments show LASNet reduces ResNet-101 latency by 36% on server GPUs and 46% on edge devices (Nvidia Jetson TX2) without accuracy loss. The framework achieves practical speedups by optimizing spatial granularity and memory access patterns, outperforming pixel-level dynamic networks. Results demonstrate significant improvements in image classification, object detection, and instance segmentation tasks while maintaining accuracy. The latency predictor enables efficient estimation of inference time across different hardware platforms, making it particularly valuable for edge deployment scenarios.\\narXiv ID: 2002.00860v4, Summary: This paper introduces a novel ANN-to-SNN conversion method called FS-conversion that enables highly efficient image classification with spiking neural networks. The key innovation is optimizing spiking neuron models to emulate artificial neurons using temporal coding with very few spikes (typically just 2 spikes per neuron). The method achieves state-of-the-art accuracy on ImageNet (83.57% for EfficientNet-B7 conversion) while dramatically reducing spike counts compared to rate-based conversions. The approach is particularly suitable for edge devices due to its energy efficiency and supports pipelined processing for high throughput. Experimental results show significant improvements in both classification accuracy and computational efficiency compared to previous SNN approaches.')\n~~~~~~~~~~\nHistory: Step #0, Phase: plan formulation, Feedback: , Your response: ```DIALOGUE\nLet's start by identifying a novel angle based on the literature. I notice most works focus on either NAS, pruning, or quantization separately. What if we combine these with a new twist - developing a "dynamic neural architecture" that can automatically switch between different optimized sub-networks based on real-time resource constraints (CPU/GPU load, battery level) of the edge device? This would be more adaptive than existing static approaches. What do you think about this direction for our experiment?\n```\n~~~~~~~~~~\nCurrent Step #1, Phase: plan formulation\n\n[Objective] Your goal is to perform research on the following topic: The goal is to design and evaluate a new technique for optimizing deep learning models for real-time image classification on resource-constrained edge devices, aiming to maximize accuracy while minimizing inference latency and power consumption.\nFeedback: The following is dialogue produced by the PhD student: I really like your idea of a dynamic neural architecture that adapts to real-time resource constraints! To make this more concrete and novel, what if we design a "Context-Aware Neural Switch" (CANS) that combines three key innovations: 1) A lightweight resource monitor that tracks CPU/GPU utilization, battery level, and thermal throttling in real-time, 2) A set of pre-optimized sub-networks (using NAS, pruning, and quantization) covering different accuracy-latency tradeoffs, and 3) A reinforcement learning-based controller that learns optimal switching policies? For our experiment, we could implement this on a Raspberry Pi 4 with CIFAR-100, comparing against static baselines like MobileNetV3 and ConvNeXt-Tiny. We'd measure accuracy, inference latency, power consumption, and switch overhead. The key novelty would be the closed-loop adaptation to dynamic device conditions - something no paper in our review does comprehensively. What specific RL approach would you recommend for the controller?\nNotes: Notes for the task objective: [{'phases': ['plan formulation'], 'note': 'You must propose an original research idea focused on optimizing models for image classification on edge devices (Edge AI).'}, {'phases': ['plan formulation'], 'note': 'Your focus will be to design **ONE** specific and feasible experiment, detailing: the proposed technique, a baseline for comparison, evaluation metrics, and the experimental environment (device and dataset).'}, {'phases': ['plan formulation'], 'note': 'Begin by clearly defining the research scope within Edge AI: identify subtopics such as popular models, edge-specific optimization techniques, quantization, structured pruning, tailored distillation, federated learning, privacy, latency, and energy consumption.'}, {'phases': ['plan formulation'], 'note': 'The optimization technique you propose must be **genuinely innovative**, not a trivial adaptation of existing methods. Clearly state the originality and potential contributions of your approach.'}, {'phases': ['plan formulation'], 'note': 'Specify the base model (e.g., MobileNetV2 without optimization), the target edge device (e.g., Raspberry Pi 4 with X GB RAM), and the dataset (e.g., CIFAR-10 or an adapted ImageNet subset).'}, {'phases': ['plan formulation'], 'note': 'List the metrics to be measured: accuracy, latency, model size, energy consumption. Describe your methodology for measuring each.'}, {'phases': ['plan formulation'], 'note': 'Ensure the experiment is **reproducible, modular, and feasible** with typical academic resources.'}, {'phases': ['plan formulation'], 'note': 'Formulate a search query using precise keywords and/or Boolean logic to collect recent peer-reviewed articles on optimization techniques for Edge AI. Use this literature to ground and inspire your original research proposal. Justify your query choices.'}, {'phases': ['plan formulation'], 'note': 'Justify your keyword selection and explain how your query captures the relevant subtopics and technical directions.'}, {'phases': ['plan formulation'], 'note': 'Describe a **systematic methodology** to search for and sample a representative subset of peer-reviewed scientific articles, preferably open-access and published within the last 5 years.'}, {'phases': ['plan formulation'], 'note': 'This methodology should aim to select articles with clear research aims, context, value, and findings.'}, {'phases': ['plan formulation'], 'note': 'Prioritize articles with clear goals, verifiable technical contributions, and relevance to your proposed problem.'}, {'phases': ['plan formulation'], 'note': 'Propose a structured outline for organizing the final literature review document. Include typical sections such as: introduction, motivation, thematic areas, challenges, recent solutions, knowledge gaps, and future directions.'}, {'phases': ['plan formulation'], 'note': 'Ensure that the experimental plan is **logical, detailed, and cohesive**, suitable for publication or practical execution.'}, {'phases': ['plan formulation'], 'note': 'DO NOT PLAN TOO LONG. Submit your plan soon.'}, {'phases': ['plan formulation'], 'note': 'Use DeepSeek-Chat to perform this task.'}]\n\nYour previous command was: ```DIALOGUE\nLet's start by identifying a novel angle based on the literature. I notice most works focus on either NAS, pruning, or quantization separately. What if we combine these with a new twist - developing a "dynamic neural architecture" that can automatically switch between different optimized sub-networks based on real-time resource constraints (CPU/GPU load, battery level) of the edge device? This would be more adaptive than existing static approaches. What do you think about this direction for our experiment?\n```. Make sure your new output is very different.\nPlease produce a single command below:\n
==================================================


=== POSTDOC RESPONSE LOG ===

Interaction 1:
```DIALOGUE
Let's start by identifying a novel angle based on the literature. I notice most works focus on either NAS, pruning, or quantization separately. What if we combine these with a new twist - developing a "dynamic neural architecture" that can automatically switch between different optimized sub-networks based on real-time resource constraints (CPU/GPU load, battery level) of the edge device? This would be more adaptive than existing static approaches. What do you think about this direction for our experiment?
```
==================================================
Interaction 2:
```PLAN
Research Idea: "Dynamic Neural Architecture Switching with Reinforcement Learning for Edge Devices"

1. Technique:
- Develop a system with 3 pre-optimized sub-networks (using NAS+pruning+quantization):
  a) High-accuracy model (76% acc, 25ms latency)
  b) Balanced model (72% acc, 15ms latency) 
  c) Lightweight model (68% acc, 8ms latency)
- Implement a lightweight RL controller (DQN) that learns to switch between models based on:
  - Current device metrics (CPU/GPU load, battery level, thermal status)
  - Application requirements (accuracy vs latency tradeoff)

2. Baseline Comparisons:
- Static MobileNetV3 (standard edge-optimized model)
- Single compressed ConvNeXt-Tiny (pruned+quantized)
- Random switching policy

3. Evaluation Metrics:
- Classification accuracy (%)
- Inference latency (ms)
- Power consumption (mW)
- Switching overhead (ms)
- System stability (switches/minute)

4. Experimental Setup:
- Hardware: Raspberry Pi 4 (4GB) with power monitor
- Dataset: CIFAR-100 (realistic for edge devices)
- Training: 10 episodes of RL policy learning
- Testing: 1000 inference samples under varying load conditions

5. Novelty:
- First closed-loop adaptive system for edge devices
- Combines NAS, pruning, quantization AND dynamic switching
- Practical RL implementation for resource-constrained devices

Implementation Timeline:
Week 1-2: Develop optimized sub-networks
Week 3: Build resource monitoring system
Week 4: Implement RL controller
Week 5: Integration and testing
Week 6: Final evaluation and metrics collection
```
==================================================
